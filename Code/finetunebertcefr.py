# -*- coding: utf-8 -*-
"""FineTuneBertCEFR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cH3JG7n-dohvk2aL9UfMFOc97dUOseGt
    
    https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=gFsCTp_mporB
    
    https://colab.research.google.com/drive/1PHv-IRLPCtv7oTcIGbsgZHqrB5LPvB7S#scrollTo=mUKLyKc7I6Qp
"""

import tensorflow as tf
import numpy as np
import random, time
from sklearn.model_selection import StratifiedKFold 
import glob, sys
from collections import defaultdict
from sklearn.metrics import confusion_matrix, classification_report, f1_score

# Get the GPU device name.
device_name = tf.test.gpu_device_name()

import torch

# If there's a GPU available...
if torch.cuda.is_available():    

    # Tell PyTorch to use the GPU.    
    device = torch.device("cuda")

    print('There are %d GPU(s) available.' % torch.cuda.device_count())

    print('We will use the GPU:', torch.cuda.get_device_name(0))

# If not...
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")


from transformers import *
#model_name = "bert-base-multilingual-cased"
model_name = "xlm-roberta-base"

#tokenizer = BertTokenizer.from_pretrained(model_name) 
tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)

seed_val = 1234

random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)                                            

dir_names = ["DE", "CZ", "IT"]

dimensions = ["OverallCEFRrating","Grammaticalaccuracy","Orthography","Vocabularyrange","Vocabularycontrol","CoherenceCohesion","Sociolinguisticappropriateness"]

def get_metadata(fname):
    labels = defaultdict(lambda: defaultdict())
    for line in open(fname, "r"):
        arr = line.strip().split(",")
        file_name = arr[0]
        
        for k in arr[1:]:
          x, y = k.split(":")
          labels[file_name][x] = y
          if y == "0":
              labels[file_name][x] = "A1"
              

    #print(list(labels.keys()))
    return labels
    
def monolingual(lang_name, dimension = "OverallCEFRrating"):
#    print("LANGUAGE: {}".format(lang_name))
    labels_info = defaultdict()
    data = defaultdict()
    sent_lens = []
    data_dir = "../Datasets/"+lang_name
    files_names = sorted(glob.glob(data_dir+"/*txt"))
    fnames = []

    for fname in files_names:
        file_name = fname.split("/")[-1].replace(".txt","").split("_")
        file_name = "_".join(file_name[:-2])
        fnames.append(file_name)
        if file_name == '1031_0002007': print(fname, file_name)
        #print(file_name)
        text = open(fname, "r").read()
        data[file_name] = text
        sent_lens.append(len(text.split(" ")))
    print(np.mean(sent_lens), np.std(sent_lens))
    labels_info.update(get_metadata(data_dir+"Metadata.txt"))

    #print("LABELS INFO")
    #print(*list(labels_info.keys()), sep="\n")

    #print("\nFILE NAMES")
    #print(*fnames, sep="\n")

    #print(labels_info["1023_0001416"])

    #fnames = sorted(list(labels_info.keys()))
    #fnames =  [fname for fname in fnames if labels_info[fname][dimension] not in ["0", "-1"]]
    fnames =  [fname for fname in fnames]
    labels = [labels_info[fname][dimension] for fname in fnames]
    #labels.append(labels_info[fname][dimension])
    set_labels = sorted(list(set(labels)))
    
    print("LABEL SET", set_labels)
    
    y = [set_labels.index(x) for x in labels]
    
    skf = StratifiedKFold(n_splits=5, random_state=seed_val, shuffle=True)
    
    nr_fold = 0 
    wt_f1_score = 0
    
    for train_index, test_index in skf.split(fnames, y):
        #print(train_index, test_index, sep="\t")
        print("LANGUAGE: {}, {}th Fold, Dimension = {}".format(lang_name, nr_fold, dimension))
        train_fnames = [fnames[i] for i in train_index]
        #print(train_fnames)
        train_data = {fname:data[fname] for fname in train_fnames}
        train_labels = [y[i] for i in train_index]

        test_fnames = [fnames[i] for i in test_index]
        test_data = {fname:data[fname] for fname in test_fnames}
        test_labels = [y[i] for i in test_index]
        
        wt_f1_score += train_test(train_data, train_labels, test_data, test_labels, nr_labels = len(set_labels))
        
        
        nr_fold += 1
    print("Averaged weighted F1-scores {}".format(wt_f1_score/skf.get_n_splits()))    
        
def multilingual(dimension):
    labels_info = defaultdict()
    data = defaultdict()
    fnames = []
    
    for lang_name in dir_names:
        print("LANGUAGE: {}".format(lang_name))        
        sent_lens = []
        data_dir = "../Datasets/"+lang_name
        files_names = sorted(glob.glob(data_dir+"/*txt"))

        for fname in files_names:
            file_name = fname.split("/")[-1].replace(".txt","").split("_")
            file_name = "_".join(file_name[:-2])
            fnames.append(file_name)
            if file_name == '1031_0002007': print(fname, file_name)
            #print(file_name)
            text = open(fname, "r").read()
            data[file_name] = text
            sent_lens.append(len(text.split(" ")))
        print(np.mean(sent_lens), np.std(sent_lens))
        labels_info.update(get_metadata(data_dir+"Metadata.txt"))

    #fnames = sorted(list(labels_info.keys()))
    fnames =  [fname for fname in fnames if labels_info[fname][dimension] not in ["0", "-1"]]
    labels = [labels_info[fname][dimension] for fname in fnames]
    #labels.append(labels_info[fname][dimension])
    set_labels = sorted(list(set(labels)))
    
    print("LABEL SET", set_labels)
    
    y = [set_labels.index(x) for x in labels]
    
    skf = StratifiedKFold(n_splits=5, random_state=seed_val, shuffle=True)
    
    nr_fold = 1 
    wt_f1_score = 0
    
    for train_index, test_index in skf.split(fnames, y):
        #print(train_index, test_index, sep="\t")
        print("{} Fold, Dimension = {}".format(nr_fold,dimension))
        train_fnames = [fnames[i] for i in train_index]
        #print(train_fnames)
        train_data = {fname:data[fname] for fname in train_fnames}
        train_labels = [y[i] for i in train_index]

        test_fnames = [fnames[i] for i in test_index]
        test_data = {fname:data[fname] for fname in test_fnames}
        test_labels = [y[i] for i in test_index]
        
        wt_f1_score += train_test(train_data, train_labels, test_data, test_labels, nr_labels = len(set_labels))
        nr_fold += 1

    print("Averaged weighted F1-scores {}".format(wt_f1_score/skf.get_n_splits()))        

def crosslingual(dimension, language):
    labels_info = defaultdict()
    data = defaultdict()
    train_fnames, test_fnames = [], []
    
    for lang_name in dir_names:
        print("LANGUAGE: {}".format(lang_name))        
        sent_lens = []
        data_dir = "../Datasets/"+lang_name
        files_names = sorted(glob.glob(data_dir+"/*txt"))

        for fname in files_names:
            file_name = fname.split("/")[-1].replace(".txt","").split("_")
            file_name = "_".join(file_name[:-2])
            
            if lang_name == "DE": 
                train_fnames.append(file_name)
            elif lang_name == language:
                test_fnames.append(file_name)
            else:
                continue              
            
#            fnames.append(file_name)
            if file_name == '1031_0002007': print(fname, file_name)
            #print(file_name)
            text = open(fname, "r").read()
            data[file_name] = text
            sent_lens.append(len(text.split(" ")))
        print(np.mean(sent_lens), np.std(sent_lens))
        labels_info.update(get_metadata(data_dir+"Metadata.txt"))

    #fnames = sorted(list(labels_info.keys()))
    train_fnames = [fname for fname in train_fnames if labels_info[fname][dimension] not in ["0", "-1"]]
    test_fnames = [fname for fname in test_fnames if labels_info[fname][dimension] not in ["0", "-1"]]
    
    train_labels = [labels_info[fname][dimension] for fname in train_fnames]
    test_labels = [labels_info[fname][dimension] for fname in test_fnames]
    #labels.append(labels_info[fname][dimension])
    set_labels = sorted(list(set(train_labels)))
    
    print("LABEL SET", set_labels)

    train_labels = [set_labels.index(x) for x in train_labels]
    test_labels = [set_labels.index(x) for x in test_labels]
    
    wt_f1_score = 0
    
    train_data = {fname:data[fname] for fname in train_fnames}
    test_data = {fname:data[fname] for fname in test_fnames}

    print("")        
    wt_f1_score = train_test(train_data, train_labels, test_data, test_labels, nr_labels = len(set_labels))
    print("Language = {}, Weighted F1-score = {}, Dimension = {}".format(language, wt_f1_score, dimension))      
    print()

def train_test(train_data, train_labels, test_data, test_labels, epochs=4, nr_labels=5):
    tokenizer = tokenizer 

    # Tokenize all of the sentences and map the tokens to thier word IDs.
    input_ids = []
    attention_masks = []

    for fname in train_data:
        text = train_data[fname]
        encoded_dict = tokenizer.encode_plus(
                            text,                      # Sentence to encode.
                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'
                            max_length = 400,           # Pad & truncate all sentences.
                            pad_to_max_length = True,
                            return_attention_mask = True,   # Construct attn. masks.
                            return_tensors = 'pt',     # Return pytorch tensors.
                       )
        # Add the encoded sentence to the list.    
        input_ids.append(encoded_dict['input_ids'])
        attention_masks.append(encoded_dict['attention_mask'])

# Convert the lists into tensors.
    train_input_ids = torch.cat(input_ids, dim=0)
    train_attention_masks = torch.cat(attention_masks, dim=0)
    train_labels = torch.tensor(train_labels)

#    set_labels = sorted(list(set(labels)))
#    labels = [set_labels.index(x) for x in labels]

# Print sentence 0, now as a list of IDs.

    #print('Token IDs:', input_ids[0])

    from torch.utils.data import TensorDataset

# Set the batch size.  
    batch_size = 8#16 

    train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)

    input_ids = []
    attention_masks = []

    for fname in test_data:
        text = test_data[fname]
        encoded_dict = tokenizer.encode_plus(
                            text,                      # Sentence to encode.
                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'
                            max_length = 400,           # Pad & truncate all sentences.
                            pad_to_max_length = True,
                            return_attention_mask = True,   # Construct attn. masks.
                            return_tensors = 'pt',     # Return pytorch tensors.
                       )
        # Add the encoded sentence to the list.    
        input_ids.append(encoded_dict['input_ids'])
        attention_masks.append(encoded_dict['attention_mask'])

# Convert the lists into tensors.
    test_input_ids = torch.cat(input_ids, dim=0)
    test_attention_masks = torch.cat(attention_masks, dim=0)
    test_labels = torch.tensor(test_labels)

    test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)


    from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
    train_dataloader = DataLoader(
                train_dataset,  # The training samples.
                sampler = RandomSampler(train_dataset), # Select batches randomly
                batch_size = batch_size # Trains with this batch size.
            )
        
    validation_dataloader = DataLoader(
                test_dataset, # The validation samples.
                sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.
                batch_size = batch_size # Evaluate with this batch size.
            )        

# 4. Train Our Classification Model

    from transformers import BertForSequenceClassification, AdamW, BertConfig
    from transformers import XLMRobertaForSequenceClassification 
    model = XLMRobertaForSequenceClassification.from_pretrained(
        model_name, # Use the 12-layer BERT model, with an uncased vocab.
        num_labels = nr_labels, # The number of output labels--2 for binary classification.
                        # You can increase this for multi-class tasks.   
        output_attentions = False, # Whether the model returns attentions weights.
        output_hidden_states = False, # Whether the model returns all hidden-states.
    )

    model.cuda()


    optimizer = AdamW(model.parameters(),
                      lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5
                      eps = 1e-8 # args.adam_epsilon  - default is 1e-8.
                    )

    from transformers import get_linear_schedule_with_warmup
                            
    total_steps = len(train_dataloader) * epochs

    scheduler = get_linear_schedule_with_warmup(optimizer, 
                                                num_warmup_steps = 0, # Default value in run_glue.py
                                                num_training_steps = total_steps)


    training_stats = []

    weighted_f1_score = None

    for epoch_i in range(0, epochs):    
        print("")
        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))
        print('Training...')
        t0 = time.time()
        
        total_train_loss = 0
        
        model.train() #changes the mode of training
        
        for step, batch in enumerate(train_dataloader):
            if step % 40 == 0 and not step == 0:
            
                elapsed = round(time.time() - t0)

                print("Elapsed time {}".format(elapsed))
            
            
            b_input_ids = batch[0].to(device)
            b_input_mask = batch[1].to(device)
            b_labels = batch[2].to(device)
            
            model.zero_grad()
            
            loss, logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
                                 
            total_train_loss += loss.item() #Accumulate the training loss
             
            loss.backward() #gradients computation
             
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
             
            optimizer.step() #gradient descent step. Depends on the optimizer.
             
            scheduler.step() #Learning rate update
             
        avg_train_loss = total_train_loss / len(train_dataloader)
         
        training_time = int(round((time.time() - t0)))
            

        print("")
        print("  Average training loss: {0:.2f}".format(avg_train_loss))
        print("  Training epoch took: {:}".format(training_time))        


        print("Running Validation...")        
        t0 = time.time()
        model.eval()
         
        total_eval_accuracy = 0
        total_eval_loss = 0
        nb_eval_steps = 0
         
        predictions , true_labels = [], []
         
        for batch in validation_dataloader:
           b_input_ids = batch[0].to(device)
           b_input_mask = batch[1].to(device)
           b_labels = batch[2].to(device)     
         
           with torch.no_grad():
               (loss, logits) = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
                               
               total_eval_loss += loss.item()
            
               logits = logits.detach().cpu().numpy()
               label_ids = b_labels.to('cpu').numpy()
               pred_flat = np.argmax(logits, axis=1).flatten()
               #print(pred_flat, logits, label_ids, sep="\n")
               predictions = np.hstack((predictions, pred_flat))
               true_labels = np.hstack((true_labels, label_ids))
               #true_labels += label_ids
        avg_val_loss = total_eval_loss / len(validation_dataloader)

        print("  Average evaluation loss: {0:.2f}".format(avg_val_loss))
        
        print(classification_report(true_labels, predictions))
        print(confusion_matrix(true_labels, predictions))

        weighted_f1_score = f1_score(true_labels,predictions,average='weighted')
        print(weighted_f1_score)

        if epoch_i == epochs-1:            
            return weighted_f1_score
                        
        print()
        print()

print("SINGLE LANGUAGE/ MONOLINGUAL EXPERIMENTS")

for dimension in dimensions:
    for language in ["CZ", "IT"]:
        crosslingual(dimension, language)

print("SINGLE LANGUAGE/ MONOLINGUAL EXPERIMENTS")

for lang_name in dir_names:
    for dimension in dimensions:
        monolingual(lang_name, dimension=dimension)

print("MULTILINGUAL EXPERIMENTS")

for dimension in dimensions:
    multilingual(dimension)
    
    
    
   
