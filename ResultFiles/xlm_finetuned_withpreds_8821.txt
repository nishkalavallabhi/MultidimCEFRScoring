There are 1 GPU(s) available.
We will use the GPU: Tesla V100-SXM2-16GB
CROSSLINGUAL EXPERIMENTS
Train on DE, test on  CZ
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
144.90552995391704 65.35717405024758
LANGUAGE: IT
nan nan
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']


======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 1.16
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 0.92
              precision    recall  f1-score   support

         1.0       0.96      0.29      0.45       188
         2.0       0.40      0.56      0.47       165
         3.0       0.51      0.94      0.66        81

    accuracy                           0.51       434
   macro avg       0.63      0.60      0.53       434
weighted avg       0.67      0.51      0.50       434

[[ 55 131   2]
 [  2  92  71]
 [  0   5  76]]
0.49583066001120135
434 434 434



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.79
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.82
              precision    recall  f1-score   support

         1.0       0.96      0.14      0.24       188
         2.0       0.30      0.40      0.34       165
         3.0       0.43      0.99      0.59        81

    accuracy                           0.40       434
   macro avg       0.56      0.51      0.39       434
weighted avg       0.61      0.40      0.35       434

[[ 26 152  10]
 [  1  66  98]
 [  0   1  80]]
0.3464676407916371
434 434 434



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36

  Average training loss: 0.63
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.71
              precision    recall  f1-score   support

         1.0       1.00      0.05      0.09       188
         2.0       0.32      0.51      0.39       165
         3.0       0.50      0.99      0.66        81

    accuracy                           0.40       434
   macro avg       0.61      0.51      0.38       434
weighted avg       0.65      0.40      0.31       434

[[  9 179   0]
 [  0  84  81]
 [  0   1  80]]
0.31185891382089487
434 434 434



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 0.52
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.85
              precision    recall  f1-score   support

         1.0       1.00      0.13      0.23       188
         2.0       0.31      0.44      0.36       165
         3.0       0.44      0.99      0.61        81

    accuracy                           0.41       434
   macro avg       0.59      0.52      0.40       434
weighted avg       0.64      0.41      0.35       434

[[ 24 157   7]
 [  0  72  93]
 [  0   1  80]]
0.35109011263552853
434 434 434
Filename	True Label	Prediction
0601	1.0	2.0
0602	1.0	2.0
0603	2.0	2.0
0604	2.0	2.0
0605	2.0	2.0
0606	2.0	2.0
0607	2.0	2.0
0608	1.0	2.0
0609	1.0	2.0
0610	1.0	3.0
0611	2.0	2.0
0612	1.0	2.0
0613	1.0	2.0
0614	2.0	2.0
0615	1.0	2.0
0616	1.0	2.0
0617	1.0	2.0
0618	1.0	2.0
0619	2.0	2.0
0620	1.0	2.0
0621	2.0	2.0
0622	1.0	2.0
0623	1.0	2.0
0624	2.0	2.0
0625	1.0	2.0
0626	2.0	2.0
0627	2.0	2.0
0628	2.0	2.0
0629	2.0	2.0
0630	1.0	1.0
0631	2.0	2.0
0632	1.0	2.0
0633	2.0	2.0
0634	2.0	2.0
0635	1.0	2.0
0636	2.0	2.0
0637	2.0	2.0
0638	1.0	2.0
0639	1.0	2.0
0640	2.0	2.0
0641	1.0	2.0
0642	1.0	2.0
0643	2.0	2.0
0644	1.0	2.0
0645	2.0	2.0
0714	2.0	2.0
0715	2.0	2.0
0716	2.0	2.0
0717	1.0	2.0
0718	1.0	2.0
0719	2.0	2.0
0720	1.0	2.0
0721	2.0	2.0
0722	2.0	2.0
0723	1.0	2.0
0724	2.0	2.0
0725	1.0	2.0
0801	1.0	2.0
0802	1.0	2.0
0803	1.0	2.0
0804	1.0	2.0
0805	2.0	2.0
0806	1.0	2.0
0807	2.0	2.0
0808	1.0	2.0
0809	2.0	2.0
0810	1.0	2.0
0811	2.0	2.0
0812	1.0	2.0
0813	1.0	2.0
0814	1.0	2.0
0815	2.0	2.0
0816	2.0	2.0
0817	1.0	2.0
0818	1.0	2.0
0819	3.0	2.0
0820	1.0	1.0
0821	2.0	2.0
0822	1.0	2.0
0823	2.0	2.0
0824	1.0	2.0
0825	1.0	2.0
0826	1.0	2.0
0827	1.0	2.0
0828	2.0	2.0
0829	1.0	2.0
0901	2.0	2.0
0902	2.0	2.0
0903	1.0	2.0
0904	1.0	2.0
0905	2.0	2.0
0906	2.0	2.0
0907	2.0	2.0
0910	1.0	2.0
0911	1.0	2.0
0912	2.0	2.0
0913	2.0	2.0
0914	1.0	2.0
0915	2.0	2.0
0916	1.0	2.0
0917	1.0	2.0
0918	1.0	2.0
0919	1.0	2.0
0920	2.0	3.0
0921	1.0	2.0
0922	1.0	2.0
0923	2.0	2.0
0924	1.0	2.0
0925	2.0	2.0
0926	2.0	2.0
0927	2.0	2.0
0928	2.0	2.0
0929	1.0	2.0
0930	1.0	2.0
1001	1.0	2.0
1002	2.0	2.0
1003	1.0	2.0
1004	1.0	2.0
1005	1.0	2.0
1006	2.0	2.0
1007	2.0	2.0
1008	1.0	2.0
1009	2.0	2.0
1010	1.0	2.0
1014	2.0	2.0
1015	1.0	2.0
1016	1.0	2.0
1017	1.0	2.0
1018	1.0	2.0
1019	1.0	2.0
1020	2.0	3.0
1021	1.0	3.0
1022	2.0	2.0
1023	1.0	2.0
1111	1.0	2.0
1112	1.0	2.0
1113	1.0	3.0
1114	2.0	3.0
1115	1.0	3.0
1116	1.0	3.0
1117	1.0	2.0
9999	1.0	2.0
BER0609003	2.0	3.0
BER0611003	2.0	3.0
BER0611005	2.0	3.0
BER0611006	2.0	3.0
BER0611007	2.0	3.0
KYJ0611003A	1.0	2.0
KYJ0611004A	1.0	2.0
KYJ0611005A	1.0	1.0
KYJ0611005B	1.0	2.0
KYJ0611006A	1.0	2.0
KYJ0611006B	1.0	2.0
KYJ0611009A	1.0	2.0
KYJ0611009B	1.0	2.0
LIB0611001A	1.0	1.0
LIB0611001B	1.0	2.0
LIB0611002A	1.0	2.0
LIB0611002B	1.0	2.0
LIB0611003A	1.0	2.0
LIB0611004A	1.0	2.0
LIB0611004B	1.0	2.0
LIB0611011	1.0	3.0
LON0610002A	1.0	2.0
LON0610002B	1.0	2.0
LON0611002A	1.0	1.0
LON0611002B	1.0	2.0
LON0611003	3.0	3.0
LON0611004A	1.0	1.0
LON0611004B	1.0	2.0
MOS0509001	2.0	3.0
MOS0509004	2.0	2.0
MOS0611012	2.0	3.0
MOS0611013	2.0	3.0
MOS0611014	1.0	3.0
MOS0611015	2.0	3.0
PAR1011008A	1.0	2.0
PAR1011009A	1.0	2.0
PAR1011009B	1.0	2.0
PAR1011013	2.0	3.0
PAR1011014	2.0	3.0
PAR1011015	2.0	3.0
PAR1011016	3.0	3.0
PAR1011017	3.0	3.0
PAR1011018	3.0	3.0
PHA0111001A	1.0	1.0
PHA0111001B	1.0	2.0
PHA0111002A	2.0	2.0
PHA0111002B	2.0	2.0
PHA0111003A	1.0	1.0
PHA0111003B	1.0	2.0
PHA0111004A	1.0	1.0
PHA0111004B	1.0	2.0
PHA0111005A	1.0	1.0
PHA0111005B	1.0	2.0
PHA0111010	3.0	3.0
PHA0111011	2.0	3.0
PHA0111012	2.0	3.0
PHA0111014	1.0	2.0
PHA0111015	3.0	3.0
PHA0111016	3.0	3.0
PHA0111018	2.0	3.0
PHA0112002A	1.0	2.0
PHA0112002B	1.0	2.0
PHA0112003A	1.0	1.0
PHA0112003B	1.0	2.0
PHA0112006A	2.0	2.0
PHA0112006B	2.0	2.0
PHA0112007A	1.0	2.0
PHA0112007B	1.0	2.0
PHA0112009A	2.0	2.0
PHA0112009B	2.0	2.0
PHA0112012A	1.0	2.0
PHA0112012B	1.0	2.0
PHA0209001	1.0	2.0
PHA0209008	1.0	2.0
PHA0209013	1.0	2.0
PHA0209024	2.0	3.0
PHA0209026	3.0	3.0
PHA0209028	2.0	3.0
PHA0209031	3.0	3.0
PHA0209034	2.0	3.0
PHA0209038	3.0	3.0
PHA0209039	2.0	3.0
PHA0210001	1.0	2.0
PHA0210004	1.0	2.0
PHA0210007	1.0	2.0
PHA0210008	1.0	2.0
PHA0411008A	1.0	2.0
PHA0411008B	1.0	2.0
PHA0411009A	1.0	2.0
PHA0411009B	1.0	2.0
PHA0411010A	1.0	1.0
PHA0411010B	1.0	2.0
PHA0411011A	1.0	1.0
PHA0411011B	1.0	2.0
PHA0411012A	1.0	1.0
PHA0411012B	1.0	2.0
PHA0411027	2.0	3.0
PHA0411028	2.0	3.0
PHA0411029	2.0	2.0
PHA0411030	3.0	3.0
PHA0411031	3.0	3.0
PHA0411032	2.0	3.0
PHA0411033	2.0	3.0
PHA0411034	1.0	2.0
PHA0411035	3.0	3.0
PHA0411036	3.0	3.0
PHA0411037	2.0	3.0
PHA0411038	3.0	3.0
PHA0411039	2.0	3.0
PHA0411041	3.0	3.0
PHA0411042	2.0	3.0
PHA0411043	2.0	2.0
PHA0411044	3.0	3.0
PHA0411045	2.0	3.0
PHA0411047	2.0	3.0
PHA0411051	3.0	3.0
PHA0411053	3.0	3.0
PHA0411054	3.0	3.0
PHA0411055	3.0	3.0
PHA0411056	3.0	3.0
PHA0411058	3.0	3.0
PHA0411059	3.0	3.0
PHA0411060	2.0	3.0
PHA0411061	3.0	3.0
PHA0411062	2.0	3.0
PHA0509002	1.0	2.0
PHA0509007	1.0	2.0
PHA0509013	1.0	2.0
PHA0509015	3.0	3.0
PHA0509017	2.0	3.0
PHA0509018	3.0	3.0
PHA0509019	2.0	3.0
PHA0509020	3.0	3.0
PHA0509021	2.0	3.0
PHA0509022	3.0	3.0
PHA0509024	2.0	3.0
PHA0509025	3.0	3.0
PHA0509026	3.0	3.0
PHA0509027	2.0	3.0
PHA0509028	3.0	3.0
PHA0509030	3.0	3.0
PHA0509031	2.0	3.0
PHA0509032	2.0	3.0
PHA0509033	1.0	2.0
PHA0509034	2.0	3.0
PHA0509035	2.0	3.0
PHA0509036	3.0	3.0
PHA0509037	3.0	3.0
PHA0509038	2.0	3.0
PHA0509039	3.0	3.0
PHA0509040	2.0	3.0
PHA0509041	2.0	3.0
PHA0509042	3.0	3.0
PHA0509043	3.0	3.0
PHA0509044	2.0	3.0
PHA0509045	2.0	2.0
PHA0510002A	1.0	2.0
PHA0510002B	1.0	2.0
PHA0510003A	1.0	2.0
PHA0510003B	1.0	2.0
PHA0510004A	1.0	1.0
PHA0510004B	1.0	2.0
PHA0510010A	1.0	1.0
PHA0510010B	1.0	1.0
PHA0510013A	1.0	2.0
PHA0510013B	1.0	2.0
PHA0510023	3.0	3.0
PHA0510027	2.0	3.0
PHA0510029	3.0	3.0
PHA0510030	2.0	3.0
PHA0510031	2.0	3.0
PHA0510032	3.0	3.0
PHA0510034	3.0	3.0
PHA0510035	3.0	3.0
PHA0510036	3.0	3.0
PHA0510037	2.0	3.0
PHA0510038	3.0	3.0
PHA0510039	2.0	3.0
PHA0510040	2.0	3.0
PHA0510046	2.0	3.0
PHA0510047	2.0	2.0
PHA0510048	2.0	3.0
PHA0510049	2.0	3.0
PHA0510050	2.0	3.0
PHA0610005A	1.0	1.0
PHA0610005B	1.0	2.0
PHA0610006A	1.0	1.0
PHA0610006B	1.0	2.0
PHA0610007A	1.0	1.0
PHA0610007B	1.0	2.0
PHA0610015	2.0	3.0
PHA0610016	3.0	3.0
PHA0610017	3.0	3.0
PHA0610018	3.0	3.0
PHA0610019A	1.0	2.0
PHA0610019B	1.0	2.0
PHA0610025	3.0	3.0
PHA0610026	3.0	3.0
PHA0709008	3.0	3.0
PHA0710009	2.0	3.0
PHA0710010	2.0	3.0
PHA0710011	3.0	3.0
PHA0710012	3.0	3.0
PHA0710013	3.0	3.0
PHA0710014	3.0	3.0
PHA0710015	3.0	3.0
PHA0710016	3.0	3.0
PHA0710017	3.0	3.0
PHA0710018	3.0	3.0
PHA0710019	3.0	3.0
PHA0710021	3.0	3.0
PHA0809009	2.0	3.0
PHA0809010	2.0	3.0
PHA0810001	3.0	3.0
PHA0810002	2.0	3.0
PHA0810003	2.0	3.0
PHA0810004	2.0	3.0
PHA0810006	2.0	3.0
PHA0810008	2.0	3.0
PHA0810009	3.0	3.0
PHA0810010	2.0	3.0
PHA0810011	2.0	3.0
PHA0810012	2.0	3.0
PHA0810015	3.0	3.0
PHA0811010	2.0	3.0
PHA0811012	3.0	3.0
PHA0811013	3.0	3.0
PHA0811014	2.0	3.0
PHA0811016	2.0	3.0
PHA0811017	3.0	3.0
PHA0811019	3.0	3.0
PHA0811020	2.0	3.0
PHA1109001	1.0	2.0
PHA1109002	3.0	3.0
PHA1109003	2.0	2.0
PHA1109004	3.0	3.0
PHA1109005	2.0	3.0
PHA1109006	2.0	3.0
PHA1109007	2.0	3.0
PHA1109008	1.0	2.0
PHA1109023	1.0	2.0
PHA1109024	3.0	3.0
PHA1109025	1.0	2.0
PHA1109026	3.0	3.0
PHA1109027	3.0	3.0
PHA1109028	2.0	3.0
PHA1110001A	1.0	2.0
PHA1110001B	1.0	2.0
PHA1110002A	2.0	2.0
PHA1110002B	2.0	2.0
PHA1110003A	1.0	2.0
PHA1110003B	1.0	2.0
PHA1110004A	1.0	2.0
PHA1110013	2.0	3.0
PHA1110014	2.0	3.0
PHA1110015	3.0	3.0
PHA1110016	2.0	3.0
PHA1110017	2.0	3.0
PHA1110019	2.0	3.0
PHA1110021	2.0	3.0
PHA1110022	3.0	3.0
PHA1111001A	1.0	2.0
PHA1111001B	1.0	2.0
PHA1111002A	1.0	1.0
PHA1111002B	1.0	1.0
PHA1111003A	1.0	2.0
PHA1111003B	1.0	2.0
PHA1111004A	1.0	2.0
PHA1111004B	1.0	2.0
PHA1111006A	1.0	2.0
PHA1111006B	1.0	2.0
PHA1111008A	1.0	1.0
PHA1111008B	1.0	2.0
PHA1111009A	1.0	1.0
ST071122B	1.0	2.0
TI071122B	1.0	2.0
VAR0209036	2.0	3.0
VAR0909003	2.0	3.0
VAR0909004	2.0	3.0
VAR0909005	2.0	3.0
VAR0909006	3.0	3.0
VAR0909007	2.0	3.0
VAR0909008	2.0	2.0
VAR0909009	3.0	3.0
VAR0909010	2.0	3.0
VAR0910004	3.0	3.0
VAR0910005	2.0	3.0
VAR0910006	2.0	3.0
VAR0910007	2.0	3.0
VAR0910009	3.0	3.0
VAR0910010	3.0	3.0
VAR0910011	2.0	3.0
Language = CZ, Weighted F1-score = 0.35109011263552853, Dimension = OverallCEFRrating

Train on DE, test on  IT
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
nan nan
LANGUAGE: IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']


======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36

  Average training loss: 1.11
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 0.74
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        28
         1.0       0.80      0.78      0.79       380
         2.0       0.77      0.72      0.75       392
         3.0       0.00      0.00      0.00         0

    accuracy                           0.72       800
   macro avg       0.39      0.37      0.38       800
weighted avg       0.76      0.72      0.74       800

[[  0  28   0   0]
 [  0 296  83   1]
 [  0  47 282  63]
 [  0   0   0   0]]
0.7395067431007886
800 800 800



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 0.75
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.64
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        28
         1.0       0.78      0.27      0.40       380
         2.0       0.37      0.40      0.38       392
         3.0       0.00      0.00      0.00         0

    accuracy                           0.33       800
   macro avg       0.29      0.17      0.20       800
weighted avg       0.55      0.33      0.38       800

[[  0  26   2   0]
 [  0 103 268   9]
 [  0   3 157 232]
 [  0   0   0   0]]
0.37897652911324786
800 800 800



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 0.59
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.27
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        28
         1.0       0.82      0.43      0.56       380
         2.0       0.51      0.57      0.54       392
         3.0       0.00      0.00      0.00         0

    accuracy                           0.48       800
   macro avg       0.33      0.25      0.28       800
weighted avg       0.64      0.48      0.53       800

[[  0  28   0   0]
 [  0 162 212   6]
 [  0   8 225 159]
 [  0   0   0   0]]
0.5322460879619001
800 800 800



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36

  Average training loss: 0.47
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.79
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        28
         1.0       0.76      0.25      0.38       380
         2.0       0.43      0.54      0.48       392
         3.0       0.00      0.00      0.00         0

    accuracy                           0.38       800
   macro avg       0.30      0.20      0.21       800
weighted avg       0.57      0.38      0.41       800

[[  0  25   3   0]
 [  0  96 278   6]
 [  0   5 211 176]
 [  0   0   0   0]]
0.41415118129951894
800 800 800
Filename	True Label	Prediction
1325_1001008	2.0	2.0
1325_1001009	2.0	2.0
1325_1001010	2.0	3.0
1325_1001011	2.0	2.0
1325_1001012	2.0	3.0
1325_1001013	2.0	3.0
1325_1001014	2.0	3.0
1325_1001015	2.0	3.0
1325_1001016	2.0	2.0
1325_1001017	2.0	2.0
1325_1001018	2.0	3.0
1325_1001019	2.0	2.0
1325_1001020	2.0	2.0
1325_1001021	2.0	3.0
1325_1001022	2.0	2.0
1325_1001023	2.0	2.0
1325_1001024	2.0	3.0
1325_1001025	2.0	2.0
1325_1001027	2.0	2.0
1325_1001028	2.0	3.0
1325_1001029	2.0	3.0
1325_1001032	2.0	2.0
1325_1001033	2.0	3.0
1325_1001035	2.0	3.0
1325_1001036	2.0	3.0
1325_1001037	2.0	2.0
1325_1001039	2.0	3.0
1325_1001040	2.0	3.0
1325_1001041	2.0	3.0
1325_1001042	2.0	3.0
1325_1001043	2.0	3.0
1325_1001044	2.0	3.0
1325_1001045	2.0	3.0
1325_1001046	2.0	2.0
1325_1001047	2.0	2.0
1325_1001048	2.0	3.0
1325_1001050	2.0	3.0
1325_1001051	2.0	3.0
1325_1001052	2.0	2.0
1325_1001053	1.0	2.0
1325_1001054	2.0	3.0
1325_1001055	2.0	3.0
1325_1001056	2.0	3.0
1325_1001057	2.0	2.0
1325_1001058	2.0	2.0
1325_1001059	2.0	3.0
1325_1001062	2.0	3.0
1325_1001063	2.0	3.0
1325_1001075	1.0	2.0
1325_1001076	2.0	3.0
1325_1001077	2.0	2.0
1325_1001078	2.0	3.0
1325_1001079	2.0	2.0
1325_1001080	2.0	3.0
1325_1001081	2.0	2.0
1325_1001082	2.0	2.0
1325_1001083	2.0	2.0
1325_1001084	2.0	2.0
1325_1001085	2.0	2.0
1325_1001086	2.0	2.0
1325_1001087	2.0	3.0
1325_1001088	2.0	2.0
1325_1001089	2.0	2.0
1325_1001090	2.0	2.0
1325_1001091	2.0	3.0
1325_1001092	2.0	2.0
1325_1001093	2.0	2.0
1325_1001094	2.0	2.0
1325_1001095	2.0	2.0
1325_1001096	2.0	2.0
1325_1001097	1.0	2.0
1325_1001098	2.0	3.0
1325_1001099	2.0	3.0
1325_1001100	2.0	3.0
1325_1001101	2.0	3.0
1325_1001107	2.0	3.0
1325_1001108	2.0	3.0
1325_1001109	2.0	2.0
1325_1001110	2.0	3.0
1325_1001111	2.0	3.0
1325_1001113	2.0	3.0
1325_1001119	2.0	3.0
1325_1001120	2.0	3.0
1325_1001121	2.0	3.0
1325_1001122	2.0	2.0
1325_1001123	2.0	3.0
1325_1001124	2.0	2.0
1325_1001125	2.0	3.0
1325_1001126	2.0	2.0
1325_1001127	2.0	3.0
1325_1001128	2.0	3.0
1325_1001129	1.0	3.0
1325_1001130	2.0	2.0
1325_1001131	2.0	3.0
1325_1001132	2.0	3.0
1325_1001133	2.0	2.0
1325_1001134	2.0	3.0
1325_1001135	2.0	3.0
1325_1001136	2.0	3.0
1325_1001138	2.0	3.0
1325_1001139	2.0	2.0
1325_1001141	1.0	3.0
1325_1001142	2.0	3.0
1325_1001143	2.0	3.0
1325_1001144	2.0	2.0
1325_1001152	2.0	3.0
1325_1001153	2.0	2.0
1325_1001154	2.0	3.0
1325_1001155	2.0	3.0
1325_1001156	2.0	3.0
1325_1001157	2.0	2.0
1325_1001158	2.0	3.0
1325_1001159	2.0	3.0
1325_1001160	2.0	3.0
1325_1001161	2.0	3.0
1325_1001162	2.0	2.0
1325_1001163	2.0	3.0
1325_1001164	2.0	2.0
1325_1001165	2.0	2.0
1325_1001166	2.0	2.0
1325_1001167	2.0	3.0
1325_1001168	2.0	3.0
1325_1001169	2.0	3.0
1325_1001170	2.0	3.0
1325_9000059	2.0	3.0
1325_9000087	2.0	2.0
1325_9000088	2.0	3.0
1325_9000089	2.0	3.0
1325_9000090	2.0	3.0
1325_9000095	2.0	3.0
1325_9000099	2.0	2.0
1325_9000102	2.0	2.0
1325_9000104	2.0	2.0
1325_9000105	2.0	2.0
1325_9000106	2.0	3.0
1325_9000107	2.0	3.0
1325_9000136	2.0	3.0
1325_9000137	2.0	3.0
1325_9000138	2.0	3.0
1325_9000139	2.0	3.0
1325_9000140	2.0	3.0
1325_9000143	2.0	3.0
1325_9000144	2.0	3.0
1325_9000152	2.0	3.0
1325_9000185	2.0	3.0
1325_9000186	2.0	3.0
1325_9000187	2.0	3.0
1325_9000188	2.0	3.0
1325_9000209	2.0	3.0
1325_9000210	1.0	3.0
1325_9000211	2.0	3.0
1325_9000213	2.0	2.0
1325_9000214	2.0	3.0
1325_9000215	2.0	3.0
1325_9000237	2.0	3.0
1325_9000239	2.0	3.0
1325_9000240	2.0	2.0
1325_9000241	2.0	3.0
1325_9000278	2.0	2.0
1325_9000279	2.0	3.0
1325_9000296	2.0	3.0
1325_9000302	2.0	3.0
1325_9000303	2.0	2.0
1325_9000304	2.0	2.0
1325_9000314	2.0	2.0
1325_9000315	1.0	2.0
1325_9000316	2.0	2.0
1325_9000317	2.0	3.0
1325_9000318	2.0	3.0
1325_9000319	2.0	3.0
1325_9000320	2.0	2.0
1325_9000321	2.0	3.0
1325_9000322	2.0	3.0
1325_9000323	2.0	2.0
1325_9000503	2.0	3.0
1325_9000504	2.0	3.0
1325_9000505	2.0	3.0
1325_9000533	2.0	3.0
1325_9000534	2.0	3.0
1325_9000536	2.0	3.0
1325_9000554	2.0	2.0
1325_9000601	2.0	3.0
1325_9000602	2.0	3.0
1325_9000611	2.0	3.0
1325_9000612	1.0	2.0
1325_9000674	2.0	3.0
1325_9000675	2.0	3.0
1325_9000676	2.0	2.0
1325_9000677	2.0	3.0
1325_9000678	2.0	3.0
1325_9000684	2.0	3.0
1325_9000685	2.0	3.0
1325_9000686	2.0	3.0
1325_9000700	2.0	3.0
1325_9000750	2.0	3.0
1365_0100002	2.0	2.0
1365_0100003	1.0	1.0
1365_0100004	1.0	2.0
1365_0100005	1.0	2.0
1365_0100006	2.0	2.0
1365_0100007	1.0	1.0
1365_0100008	1.0	2.0
1365_0100009	1.0	1.0
1365_0100010	1.0	1.0
1365_0100011	2.0	2.0
1365_0100012	2.0	2.0
1365_0100013	2.0	3.0
1365_0100014	2.0	2.0
1365_0100015	1.0	1.0
1365_0100016	2.0	3.0
1365_0100017	2.0	2.0
1365_0100018	1.0	2.0
1365_0100019	1.0	1.0
1365_0100020	2.0	2.0
1365_0100021	2.0	2.0
1365_0100022	2.0	2.0
1365_0100023	1.0	2.0
1365_0100024	1.0	2.0
1365_0100026	1.0	1.0
1365_0100027	2.0	2.0
1365_0100028	2.0	2.0
1365_0100029	1.0	1.0
1365_0100030	1.0	2.0
1365_0100031	2.0	1.0
1365_0100051	1.0	2.0
1365_0100056	2.0	3.0
1365_0100057	2.0	3.0
1365_0100058	2.0	3.0
1365_0100061	2.0	2.0
1365_0100063	2.0	3.0
1365_0100064	2.0	2.0
1365_0100065	1.0	2.0
1365_0100066	1.0	2.0
1365_0100067	1.0	2.0
1365_0100069	2.0	2.0
1365_0100070	2.0	2.0
1365_0100071	2.0	2.0
1365_0100072	2.0	2.0
1365_0100073	2.0	2.0
1365_0100074	1.0	2.0
1365_0100079	2.0	2.0
1365_0100080	2.0	2.0
1365_0100092	2.0	2.0
1365_0100093	2.0	2.0
1365_0100094	2.0	2.0
1365_0100095	2.0	2.0
1365_0100096	2.0	3.0
1365_0100097	2.0	2.0
1365_0100098	1.0	2.0
1365_0100099	1.0	2.0
1365_0100100	2.0	3.0
1365_0100101	2.0	2.0
1365_0100102	2.0	3.0
1365_0100103	2.0	3.0
1365_0100104	1.0	2.0
1365_0100105	2.0	2.0
1365_0100106	1.0	2.0
1365_0100107	2.0	3.0
1365_0100116	2.0	2.0
1365_0100117	2.0	2.0
1365_0100118	2.0	3.0
1365_0100119	2.0	3.0
1365_0100120	2.0	3.0
1365_0100121	2.0	2.0
1365_0100123	2.0	2.0
1365_0100125	2.0	2.0
1365_0100133	2.0	2.0
1365_0100134	2.0	2.0
1365_0100135	2.0	2.0
1365_0100136	2.0	2.0
1365_0100137	2.0	2.0
1365_0100138	2.0	2.0
1365_0100139	2.0	2.0
1365_0100145	2.0	3.0
1365_0100146	2.0	2.0
1365_0100147	2.0	2.0
1365_0100148	2.0	3.0
1365_0100151	1.0	2.0
1365_0100162	2.0	2.0
1365_0100163	2.0	3.0
1365_0100164	2.0	3.0
1365_0100165	2.0	3.0
1365_0100166	1.0	2.0
1365_0100167	1.0	2.0
1365_0100168	2.0	2.0
1365_0100169	2.0	2.0
1365_0100170	1.0	3.0
1365_0100171	1.0	2.0
1365_0100172	2.0	2.0
1365_0100173	2.0	2.0
1365_0100174	1.0	2.0
1365_0100175	2.0	2.0
1365_0100176	2.0	2.0
1365_0100177	2.0	2.0
1365_0100178	2.0	2.0
1365_0100179	2.0	2.0
1365_0100180	1.0	1.0
1365_0100181	1.0	2.0
1365_0100182	2.0	3.0
1365_0100183	2.0	2.0
1365_0100184	2.0	2.0
1365_0100185	1.0	2.0
1365_0100186	2.0	2.0
1365_0100187	2.0	2.0
1365_0100188	2.0	2.0
1365_0100190	2.0	3.0
1365_0100191	1.0	2.0
1365_0100192	2.0	3.0
1365_0100194	2.0	2.0
1365_0100195	1.0	2.0
1365_0100196	1.0	2.0
1365_0100198	1.0	2.0
1365_0100199	2.0	2.0
1365_0100200	2.0	3.0
1365_0100201	2.0	2.0
1365_0100202	1.0	2.0
1365_0100203	2.0	2.0
1365_0100204	2.0	2.0
1365_0100205	2.0	2.0
1365_0100211	2.0	3.0
1365_0100212	2.0	3.0
1365_0100213	2.0	2.0
1365_0100215	2.0	2.0
1365_0100217	2.0	3.0
1365_0100218	2.0	3.0
1365_0100219	2.0	3.0
1365_0100220	2.0	3.0
1365_0100221	2.0	2.0
1365_0100222	2.0	2.0
1365_0100223	2.0	3.0
1365_0100224	2.0	3.0
1365_0100225	2.0	2.0
1365_0100226	2.0	2.0
1365_0100227	2.0	3.0
1365_0100228	1.0	2.0
1365_0100229	2.0	3.0
1365_0100230	2.0	3.0
1365_0100231	2.0	2.0
1365_0100232	2.0	2.0
1365_0100233	2.0	3.0
1365_0100251	2.0	2.0
1365_0100252	2.0	3.0
1365_0100253	1.0	2.0
1365_0100255	1.0	2.0
1365_0100256	2.0	2.0
1365_0100257	2.0	2.0
1365_0100258	2.0	2.0
1365_0100259	2.0	2.0
1365_0100260	2.0	2.0
1365_0100261	2.0	2.0
1365_0100262	2.0	2.0
1365_0100263	2.0	3.0
1365_0100265	2.0	3.0
1365_0100266	2.0	2.0
1365_0100267	2.0	3.0
1365_0100268	1.0	2.0
1365_0100269	2.0	2.0
1365_0100270	2.0	2.0
1365_0100274	2.0	3.0
1365_0100275	2.0	2.0
1365_0100276	2.0	3.0
1365_0100277	2.0	3.0
1365_0100278	2.0	2.0
1365_0100279	2.0	2.0
1365_0100280	1.0	2.0
1365_0100281	2.0	2.0
1365_0100282	2.0	2.0
1365_0100285	2.0	2.0
1365_0100286	1.0	2.0
1365_0100287	2.0	2.0
1365_0100288	2.0	2.0
1365_0100289	2.0	2.0
1365_0100290	2.0	2.0
1365_0100299	2.0	2.0
1365_0100447	2.0	2.0
1365_0100448	1.0	2.0
1365_0100451	2.0	2.0
1365_0100455	2.0	3.0
1365_0100456	2.0	2.0
1365_0100457	2.0	2.0
1365_0100458	2.0	3.0
1365_0100459	2.0	3.0
1365_0100461	2.0	2.0
1365_0100469	2.0	2.0
1365_0100470	2.0	3.0
1365_0100471	1.0	3.0
1365_0100472	2.0	2.0
1365_0100473	2.0	3.0
1365_0100474	2.0	2.0
1365_0100475	2.0	3.0
1365_0100476	2.0	3.0
1365_0100477	1.0	3.0
1365_0100478	2.0	2.0
1365_0100479	2.0	3.0
1365_0100480	2.0	2.0
1365_0100481	2.0	3.0
1365_0100482	2.0	2.0
1385_0000011	0.0	1.0
1385_0000012	1.0	2.0
1385_0000013	0.0	2.0
1385_0000016	1.0	1.0
1385_0000017	1.0	1.0
1385_0000020	1.0	1.0
1385_0000021	1.0	2.0
1385_0000022	1.0	2.0
1385_0000023	1.0	1.0
1385_0000033	1.0	1.0
1385_0000034	1.0	2.0
1385_0000035	1.0	2.0
1385_0000036	1.0	2.0
1385_0000037	1.0	2.0
1385_0000038	1.0	2.0
1385_0000039	1.0	1.0
1385_0000040	1.0	1.0
1385_0000041	1.0	2.0
1385_0000042	1.0	2.0
1385_0000043	1.0	2.0
1385_0000044	1.0	2.0
1385_0000045	1.0	2.0
1385_0000047	1.0	2.0
1385_0000048	1.0	2.0
1385_0000049	1.0	2.0
1385_0000050	1.0	2.0
1385_0000051	1.0	2.0
1385_0000052	1.0	1.0
1385_0000053	1.0	1.0
1385_0000054	1.0	2.0
1385_0000057	1.0	1.0
1385_0000058	1.0	2.0
1385_0000059	1.0	2.0
1385_0000095	1.0	1.0
1385_0000097	1.0	2.0
1385_0000098	1.0	1.0
1385_0000099	1.0	1.0
1385_0000100	1.0	1.0
1385_0000101	1.0	1.0
1385_0000102	1.0	2.0
1385_0000103	1.0	1.0
1385_0000104	1.0	2.0
1385_0000114	1.0	2.0
1385_0000119	1.0	2.0
1385_0000120	0.0	1.0
1385_0000122	1.0	2.0
1385_0000123	1.0	2.0
1385_0000124	1.0	2.0
1385_0000125	1.0	2.0
1385_0000126	1.0	2.0
1385_0000127	1.0	2.0
1385_0000128	1.0	2.0
1385_0000129	1.0	2.0
1385_0000130	1.0	1.0
1385_0001103	1.0	1.0
1385_0001104	1.0	1.0
1385_0001105	1.0	2.0
1385_0001107	1.0	2.0
1385_0001108	1.0	2.0
1385_0001109	1.0	2.0
1385_0001110	1.0	2.0
1385_0001111	1.0	2.0
1385_0001112	1.0	2.0
1385_0001113	1.0	1.0
1385_0001118	1.0	2.0
1385_0001119	1.0	2.0
1385_0001120	1.0	1.0
1385_0001121	1.0	2.0
1385_0001122	1.0	1.0
1385_0001123	1.0	1.0
1385_0001124	1.0	1.0
1385_0001125	1.0	2.0
1385_0001126	0.0	1.0
1385_0001127	1.0	2.0
1385_0001128	0.0	1.0
1385_0001129	1.0	2.0
1385_0001130	1.0	1.0
1385_0001131	1.0	1.0
1385_0001132	1.0	1.0
1385_0001133	1.0	2.0
1385_0001134	1.0	2.0
1385_0001135	1.0	1.0
1385_0001136	1.0	1.0
1385_0001137	1.0	2.0
1385_0001138	1.0	2.0
1385_0001147	1.0	2.0
1385_0001148	1.0	2.0
1385_0001149	1.0	2.0
1385_0001150	1.0	1.0
1385_0001151	1.0	2.0
1385_0001152	1.0	2.0
1385_0001153	2.0	2.0
1385_0001154	1.0	2.0
1385_0001155	1.0	2.0
1385_0001156	1.0	1.0
1385_0001157	1.0	2.0
1385_0001158	1.0	2.0
1385_0001159	1.0	1.0
1385_0001160	1.0	2.0
1385_0001161	1.0	2.0
1385_0001162	1.0	2.0
1385_0001163	1.0	2.0
1385_0001164	1.0	2.0
1385_0001165	1.0	2.0
1385_0001166	1.0	2.0
1385_0001167	1.0	2.0
1385_0001169	1.0	1.0
1385_0001170	1.0	1.0
1385_0001171	1.0	1.0
1385_0001172	1.0	1.0
1385_0001173	0.0	1.0
1385_0001174	1.0	1.0
1385_0001175	1.0	1.0
1385_0001178	0.0	1.0
1385_0001188	1.0	2.0
1385_0001189	1.0	1.0
1385_0001190	1.0	1.0
1385_0001191	1.0	2.0
1385_0001192	1.0	2.0
1385_0001193	2.0	2.0
1385_0001194	1.0	2.0
1385_0001195	1.0	2.0
1385_0001196	1.0	2.0
1385_0001197	1.0	2.0
1385_0001198	2.0	2.0
1385_0001199	1.0	2.0
1385_0001501	1.0	1.0
1385_0001503	1.0	2.0
1385_0001522	1.0	1.0
1385_0001523	1.0	2.0
1385_0001524	1.0	2.0
1385_0001525	1.0	2.0
1385_0001526	0.0	1.0
1385_0001527	1.0	1.0
1385_0001528	1.0	2.0
1385_0001712	1.0	2.0
1385_0001714	1.0	2.0
1385_0001715	1.0	2.0
1385_0001716	1.0	2.0
1385_0001717	2.0	2.0
1385_0001718	1.0	1.0
1385_0001719	1.0	1.0
1385_0001720	0.0	1.0
1385_0001723	0.0	1.0
1385_0001724	2.0	2.0
1385_0001725	1.0	1.0
1385_0001726	1.0	2.0
1385_0001727	0.0	2.0
1385_0001728	1.0	2.0
1385_0001729	1.0	2.0
1385_0001730	2.0	2.0
1385_0001732	1.0	2.0
1385_0001733	1.0	2.0
1385_0001734	1.0	2.0
1385_0001736	2.0	2.0
1385_0001737	1.0	2.0
1385_0001738	0.0	1.0
1385_0001739	1.0	2.0
1385_0001740	1.0	2.0
1385_0001741	0.0	1.0
1385_0001742	0.0	1.0
1385_0001744	0.0	1.0
1385_0001746	1.0	2.0
1385_0001747	1.0	1.0
1385_0001748	1.0	2.0
1385_0001749	1.0	2.0
1385_0001750	0.0	1.0
1385_0001751	1.0	2.0
1385_0001752	1.0	2.0
1385_0001753	1.0	2.0
1385_0001754	1.0	2.0
1385_0001756	1.0	2.0
1385_0001757	2.0	1.0
1385_0001758	1.0	1.0
1385_0001759	1.0	2.0
1385_0001760	1.0	2.0
1385_0001761	1.0	2.0
1385_0001762	1.0	2.0
1385_0001764	1.0	1.0
1385_0001765	0.0	1.0
1385_0001766	1.0	2.0
1385_0001767	1.0	2.0
1385_0001768	1.0	2.0
1385_0001771	1.0	2.0
1385_0001772	1.0	2.0
1385_0001773	1.0	1.0
1385_0001774	0.0	1.0
1385_0001775	1.0	2.0
1385_0001785	1.0	1.0
1385_0001786	1.0	2.0
1385_0001787	1.0	1.0
1385_0001788	1.0	2.0
1385_0001789	1.0	2.0
1385_0001790	1.0	2.0
1385_0001791	1.0	1.0
1385_0001792	1.0	2.0
1385_0001793	1.0	2.0
1385_0001794	1.0	1.0
1385_0001795	1.0	1.0
1385_0001796	1.0	2.0
1385_0001798	1.0	2.0
1385_0001799	1.0	2.0
1385_0001800	1.0	1.0
1395_0000333	1.0	2.0
1395_0000337	0.0	1.0
1395_0000338	1.0	1.0
1395_0000340	1.0	2.0
1395_0000341	1.0	1.0
1395_0000353	1.0	2.0
1395_0000354	1.0	1.0
1395_0000355	1.0	2.0
1395_0000356	1.0	1.0
1395_0000357	2.0	2.0
1395_0000359	1.0	2.0
1395_0000360	2.0	2.0
1395_0000361	1.0	2.0
1395_0000364	1.0	2.0
1395_0000365	2.0	2.0
1395_0000366	2.0	2.0
1395_0000368	0.0	1.0
1395_0000369	2.0	2.0
1395_0000376	2.0	2.0
1395_0000378	1.0	2.0
1395_0000379	1.0	1.0
1395_0000380	1.0	2.0
1395_0000383	1.0	2.0
1395_0000387	2.0	2.0
1395_0000388	2.0	2.0
1395_0000389	0.0	1.0
1395_0000390	1.0	1.0
1395_0000391	2.0	2.0
1395_0000392	1.0	2.0
1395_0000396	1.0	2.0
1395_0000398	2.0	2.0
1395_0000399	1.0	2.0
1395_0000402	1.0	2.0
1395_0000403	1.0	2.0
1395_0000404	1.0	2.0
1395_0000409	2.0	2.0
1395_0000413	1.0	2.0
1395_0000414	1.0	2.0
1395_0000415	1.0	2.0
1395_0000432	1.0	2.0
1395_0000438	2.0	3.0
1395_0000443	2.0	2.0
1395_0000446	2.0	2.0
1395_0000447	1.0	2.0
1395_0000448	1.0	1.0
1395_0000449	2.0	2.0
1395_0000450	1.0	2.0
1395_0000451	1.0	2.0
1395_0000452	1.0	1.0
1395_0000454	1.0	2.0
1395_0000455	1.0	2.0
1395_0000458	1.0	1.0
1395_0000460	1.0	1.0
1395_0000462	2.0	2.0
1395_0000465	1.0	1.0
1395_0000469	1.0	2.0
1395_0000470	1.0	2.0
1395_0000471	1.0	2.0
1395_0000499	1.0	2.0
1395_0000500	1.0	2.0
1395_0000504	1.0	2.0
1395_0000512	1.0	2.0
1395_0000513	2.0	2.0
1395_0000514	2.0	2.0
1395_0000515	2.0	2.0
1395_0000516	1.0	1.0
1395_0000518	2.0	2.0
1395_0000525	2.0	1.0
1395_0000526	1.0	1.0
1395_0000527	1.0	1.0
1395_0000528	2.0	2.0
1395_0000529	1.0	2.0
1395_0000531	1.0	1.0
1395_0000533	2.0	2.0
1395_0000534	1.0	2.0
1395_0000535	1.0	1.0
1395_0000537	1.0	2.0
1395_0000547	1.0	2.0
1395_0000548	1.0	2.0
1395_0000549	1.0	2.0
1395_0000550	1.0	2.0
1395_0000551	2.0	2.0
1395_0000552	1.0	2.0
1395_0000553	1.0	1.0
1395_0000554	1.0	2.0
1395_0000555	1.0	2.0
1395_0000556	1.0	2.0
1395_0000557	2.0	3.0
1395_0000559	1.0	2.0
1395_0000560	1.0	2.0
1395_0000563	1.0	2.0
1395_0000564	1.0	1.0
1395_0000565	1.0	2.0
1395_0000572	1.0	2.0
1395_0000575	1.0	2.0
1395_0000579	1.0	1.0
1395_0000581	1.0	2.0
1395_0000582	0.0	1.0
1395_0000583	1.0	2.0
1395_0000584	0.0	1.0
1395_0000585	1.0	2.0
1395_0000587	0.0	2.0
1395_0000591	0.0	1.0
1395_0000593	1.0	2.0
1395_0000595	0.0	1.0
1395_0000596	2.0	1.0
1395_0000597	1.0	2.0
1395_0000598	1.0	2.0
1395_0000599	1.0	2.0
1395_0000602	1.0	2.0
1395_0000604	1.0	2.0
1395_0000606	1.0	2.0
1395_0000607	1.0	1.0
1395_0000608	1.0	2.0
1395_0000609	1.0	2.0
1395_0000610	1.0	2.0
1395_0000611	1.0	2.0
1395_0000612	1.0	2.0
1395_0000626	2.0	2.0
1395_0000627	1.0	2.0
1395_0000628	1.0	2.0
1395_0000630	1.0	2.0
1395_0000631	1.0	2.0
1395_0000635	1.0	1.0
1395_0000636	1.0	2.0
1395_0000639	1.0	2.0
1395_0000642	1.0	2.0
1395_0000644	1.0	2.0
1395_0000646	1.0	2.0
1395_0000649	2.0	2.0
1395_0001010	2.0	2.0
1395_0001013	1.0	2.0
1395_0001015	1.0	2.0
1395_0001016	2.0	1.0
1395_0001017	1.0	1.0
1395_0001019	1.0	1.0
1395_0001020	1.0	2.0
1395_0001021	1.0	2.0
1395_0001022	1.0	2.0
1395_0001023	1.0	2.0
1395_0001024	1.0	2.0
1395_0001028	1.0	2.0
1395_0001033	1.0	2.0
1395_0001034	1.0	2.0
1395_0001040	1.0	1.0
1395_0001045	1.0	2.0
1395_0001058	1.0	2.0
1395_0001060	2.0	2.0
1395_0001061	2.0	2.0
1395_0001064	1.0	2.0
1395_0001065	1.0	2.0
1395_0001066	1.0	2.0
1395_0001067	1.0	2.0
1395_0001068	1.0	2.0
1395_0001069	1.0	2.0
1395_0001070	2.0	2.0
1395_0001071	1.0	1.0
1395_0001073	2.0	2.0
1395_0001074	1.0	2.0
1395_0001075	1.0	2.0
1395_0001076	1.0	2.0
1395_0001078	1.0	2.0
1395_0001080	1.0	2.0
1395_0001084	1.0	2.0
1395_0001090	2.0	2.0
1395_0001093	1.0	2.0
1395_0001101	1.0	2.0
1395_0001103	1.0	2.0
1395_0001104	1.0	2.0
1395_0001108	1.0	1.0
1395_0001109	1.0	1.0
1395_0001114	1.0	2.0
1395_0001115	2.0	2.0
1395_0001116	1.0	2.0
1395_0001117	1.0	2.0
1395_0001118	1.0	2.0
1395_0001119	2.0	2.0
1395_0001120	1.0	1.0
1395_0001121	0.0	1.0
1395_0001122	1.0	1.0
1395_0001123	1.0	2.0
1395_0001124	1.0	1.0
1395_0001126	1.0	2.0
1395_0001131	1.0	1.0
1395_0001132	2.0	2.0
1395_0001133	1.0	2.0
1395_0001141	1.0	2.0
1395_0001145	2.0	2.0
1395_0001146	0.0	1.0
1395_0001147	1.0	2.0
1395_0001149	1.0	2.0
1395_0001150	1.0	2.0
1395_0001158	2.0	2.0
1395_0001160	2.0	2.0
1395_0001161	1.0	2.0
1395_0001164	2.0	2.0
1395_0001167	1.0	2.0
1395_0001169	1.0	2.0
1395_0001170	1.0	2.0
1395_0001171	1.0	2.0
Language = IT, Weighted F1-score = 0.41415118129951894, Dimension = OverallCEFRrating

Train on DE, test on  CZ
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
144.90552995391704 65.35717405024758
LANGUAGE: IT
nan nan
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']


======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 1.24
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.28
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         6
         1.0       0.83      0.05      0.10       185
         2.0       0.37      0.74      0.50       156
         3.0       0.50      0.71      0.59        82
         4.0       0.00      0.00      0.00         5

    accuracy                           0.42       434
   macro avg       0.34      0.30      0.24       434
weighted avg       0.59      0.42      0.33       434

[[  0   0   6   0   0]
 [  0  10 162  13   0]
 [  0   2 115  39   0]
 [  0   0  24  58   0]
 [  0   0   0   5   0]]
0.33308891769511795
434 434 434



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 1.01
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.31
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         6
         1.0       0.76      0.28      0.40       185
         2.0       0.42      0.54      0.48       156
         3.0       0.46      0.93      0.61        82
         4.0       0.00      0.00      0.00         5

    accuracy                           0.49       434
   macro avg       0.33      0.35      0.30       434
weighted avg       0.56      0.49      0.46       434

[[  0   3   3   0   0]
 [  0  51 107  27   0]
 [  0  13  85  58   0]
 [  0   0   6  76   0]
 [  0   0   0   5   0]]
0.45950398890045085
434 434 434



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36

  Average training loss: 0.89
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.17
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         6
         1.0       0.76      0.16      0.26       185
         2.0       0.39      0.77      0.52       156
         3.0       0.60      0.65      0.62        82
         4.0       0.00      0.00      0.00         5

    accuracy                           0.47       434
   macro avg       0.35      0.31      0.28       434
weighted avg       0.58      0.47      0.41       434

[[  0   3   3   0   0]
 [  0  29 156   0   0]
 [  0   6 120  30   0]
 [  0   0  29  53   0]
 [  0   0   0   5   0]]
0.41459829231652257
434 434 434



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 0.82
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.49
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         6
         1.0       0.75      0.15      0.24       185
         2.0       0.38      0.62      0.47       156
         3.0       0.50      0.87      0.63        82
         4.0       0.00      0.00      0.00         5

    accuracy                           0.45       434
   macro avg       0.33      0.33      0.27       434
weighted avg       0.55      0.45      0.39       434

[[  0   3   3   0   0]
 [  0  27 146  12   0]
 [  0   6  96  54   0]
 [  0   0  11  71   0]
 [  0   0   0   5   0]]
0.39143961265635646
434 434 434
Filename	True Label	Prediction
0601	1.0	2.0
0602	2.0	2.0
0603	2.0	2.0
0604	2.0	2.0
0605	2.0	2.0
0606	1.0	2.0
0607	2.0	2.0
0608	1.0	2.0
0609	1.0	2.0
0610	2.0	2.0
0611	2.0	2.0
0612	1.0	2.0
0613	1.0	2.0
0614	1.0	2.0
0615	1.0	2.0
0616	1.0	2.0
0617	1.0	2.0
0618	1.0	2.0
0619	2.0	2.0
0620	1.0	2.0
0621	2.0	2.0
0622	1.0	2.0
0623	2.0	2.0
0624	2.0	2.0
0625	2.0	2.0
0626	2.0	2.0
0627	2.0	2.0
0628	2.0	2.0
0629	2.0	2.0
0630	1.0	2.0
0631	2.0	2.0
0632	1.0	2.0
0633	2.0	2.0
0634	2.0	2.0
0635	1.0	2.0
0636	2.0	2.0
0637	2.0	2.0
0638	2.0	2.0
0639	1.0	2.0
0640	2.0	2.0
0641	1.0	2.0
0642	1.0	2.0
0643	2.0	2.0
0644	1.0	2.0
0645	2.0	2.0
0714	2.0	2.0
0715	2.0	2.0
0716	2.0	2.0
0717	1.0	2.0
0718	1.0	2.0
0719	2.0	2.0
0720	1.0	2.0
0721	2.0	2.0
0722	2.0	2.0
0723	2.0	2.0
0724	2.0	2.0
0725	1.0	2.0
0801	1.0	2.0
0802	2.0	2.0
0803	2.0	2.0
0804	1.0	2.0
0805	2.0	2.0
0806	1.0	2.0
0807	2.0	2.0
0808	1.0	2.0
0809	2.0	2.0
0810	2.0	2.0
0811	2.0	2.0
0812	2.0	2.0
0813	1.0	2.0
0814	1.0	2.0
0815	2.0	2.0
0816	2.0	2.0
0817	1.0	2.0
0818	1.0	2.0
0819	3.0	2.0
0820	1.0	2.0
0821	2.0	2.0
0822	1.0	2.0
0823	2.0	2.0
0824	2.0	2.0
0825	1.0	2.0
0826	1.0	2.0
0827	1.0	2.0
0828	2.0	2.0
0829	1.0	2.0
0901	2.0	2.0
0902	2.0	2.0
0903	1.0	2.0
0904	1.0	2.0
0905	2.0	2.0
0906	2.0	2.0
0907	2.0	2.0
0910	1.0	2.0
0911	1.0	2.0
0912	2.0	2.0
0913	2.0	2.0
0914	1.0	2.0
0915	2.0	2.0
0916	1.0	2.0
0917	1.0	2.0
0918	1.0	2.0
0919	1.0	2.0
0920	2.0	2.0
0921	1.0	2.0
0922	1.0	2.0
0923	2.0	2.0
0924	1.0	2.0
0925	2.0	2.0
0926	2.0	2.0
0927	1.0	2.0
0928	1.0	2.0
0929	0.0	2.0
0930	2.0	2.0
1001	1.0	2.0
1002	2.0	2.0
1003	1.0	2.0
1004	1.0	2.0
1005	1.0	2.0
1006	2.0	2.0
1007	2.0	2.0
1008	2.0	2.0
1009	2.0	2.0
1010	1.0	2.0
1014	2.0	2.0
1015	1.0	2.0
1016	1.0	2.0
1017	1.0	2.0
1018	1.0	2.0
1019	1.0	2.0
1020	2.0	2.0
1021	1.0	2.0
1022	2.0	2.0
1023	1.0	2.0
1111	1.0	2.0
1112	1.0	2.0
1113	1.0	2.0
1114	2.0	2.0
1115	1.0	2.0
1116	1.0	2.0
1117	2.0	2.0
9999	1.0	2.0
BER0609003	2.0	3.0
BER0611003	2.0	3.0
BER0611005	2.0	2.0
BER0611006	2.0	3.0
BER0611007	2.0	3.0
KYJ0611003A	1.0	1.0
KYJ0611004A	1.0	1.0
KYJ0611005A	1.0	1.0
KYJ0611005B	1.0	2.0
KYJ0611006A	1.0	1.0
KYJ0611006B	1.0	2.0
KYJ0611009A	1.0	1.0
KYJ0611009B	1.0	2.0
LIB0611001A	1.0	2.0
LIB0611001B	1.0	2.0
LIB0611002A	1.0	1.0
LIB0611002B	2.0	2.0
LIB0611003A	1.0	1.0
LIB0611004A	1.0	1.0
LIB0611004B	1.0	2.0
LIB0611011	1.0	2.0
LON0610002A	2.0	2.0
LON0610002B	1.0	2.0
LON0611002A	1.0	1.0
LON0611002B	1.0	2.0
LON0611003	2.0	3.0
LON0611004A	1.0	1.0
LON0611004B	1.0	2.0
MOS0509001	1.0	2.0
MOS0509004	1.0	2.0
MOS0611012	2.0	3.0
MOS0611013	2.0	3.0
MOS0611014	1.0	3.0
MOS0611015	2.0	3.0
PAR1011008A	1.0	2.0
PAR1011009A	2.0	1.0
PAR1011009B	1.0	2.0
PAR1011013	2.0	3.0
PAR1011014	2.0	3.0
PAR1011015	2.0	2.0
PAR1011016	3.0	3.0
PAR1011017	3.0	3.0
PAR1011018	3.0	2.0
PHA0111001A	1.0	1.0
PHA0111001B	1.0	2.0
PHA0111002A	2.0	1.0
PHA0111002B	3.0	2.0
PHA0111003A	1.0	1.0
PHA0111003B	2.0	2.0
PHA0111004A	1.0	1.0
PHA0111004B	1.0	2.0
PHA0111005A	1.0	1.0
PHA0111005B	2.0	2.0
PHA0111010	3.0	2.0
PHA0111011	2.0	2.0
PHA0111012	1.0	3.0
PHA0111014	1.0	2.0
PHA0111015	3.0	3.0
PHA0111016	3.0	3.0
PHA0111018	1.0	3.0
PHA0112002A	2.0	2.0
PHA0112002B	1.0	2.0
PHA0112003A	1.0	1.0
PHA0112003B	1.0	2.0
PHA0112006A	3.0	2.0
PHA0112006B	2.0	2.0
PHA0112007A	1.0	2.0
PHA0112007B	1.0	2.0
PHA0112009A	2.0	2.0
PHA0112009B	2.0	2.0
PHA0112012A	1.0	2.0
PHA0112012B	1.0	2.0
PHA0209001	1.0	2.0
PHA0209008	1.0	2.0
PHA0209013	1.0	2.0
PHA0209024	1.0	3.0
PHA0209026	2.0	3.0
PHA0209028	2.0	3.0
PHA0209031	4.0	3.0
PHA0209034	2.0	3.0
PHA0209038	4.0	3.0
PHA0209039	2.0	3.0
PHA0210001	1.0	2.0
PHA0210004	1.0	2.0
PHA0210007	1.0	2.0
PHA0210008	1.0	2.0
PHA0411008A	1.0	1.0
PHA0411008B	1.0	2.0
PHA0411009A	2.0	1.0
PHA0411009B	1.0	2.0
PHA0411010A	0.0	1.0
PHA0411010B	0.0	2.0
PHA0411011A	1.0	1.0
PHA0411011B	1.0	2.0
PHA0411012A	1.0	1.0
PHA0411012B	1.0	2.0
PHA0411027	2.0	3.0
PHA0411028	2.0	2.0
PHA0411029	2.0	2.0
PHA0411030	3.0	3.0
PHA0411031	3.0	3.0
PHA0411032	1.0	3.0
PHA0411033	2.0	3.0
PHA0411034	1.0	2.0
PHA0411035	3.0	3.0
PHA0411036	3.0	3.0
PHA0411037	2.0	3.0
PHA0411038	3.0	3.0
PHA0411039	2.0	3.0
PHA0411041	3.0	3.0
PHA0411042	2.0	3.0
PHA0411043	1.0	2.0
PHA0411044	3.0	3.0
PHA0411045	3.0	2.0
PHA0411047	2.0	3.0
PHA0411051	3.0	3.0
PHA0411053	3.0	3.0
PHA0411054	3.0	3.0
PHA0411055	3.0	3.0
PHA0411056	3.0	3.0
PHA0411058	3.0	3.0
PHA0411059	3.0	3.0
PHA0411060	2.0	3.0
PHA0411061	3.0	3.0
PHA0411062	3.0	3.0
PHA0509002	1.0	1.0
PHA0509007	1.0	2.0
PHA0509013	1.0	2.0
PHA0509015	3.0	2.0
PHA0509017	2.0	3.0
PHA0509018	3.0	3.0
PHA0509019	3.0	3.0
PHA0509020	3.0	3.0
PHA0509021	1.0	2.0
PHA0509022	4.0	3.0
PHA0509024	2.0	3.0
PHA0509025	3.0	3.0
PHA0509026	3.0	3.0
PHA0509027	1.0	2.0
PHA0509028	2.0	3.0
PHA0509030	2.0	3.0
PHA0509031	1.0	3.0
PHA0509032	2.0	3.0
PHA0509033	1.0	2.0
PHA0509034	1.0	3.0
PHA0509035	2.0	3.0
PHA0509036	3.0	3.0
PHA0509037	3.0	3.0
PHA0509038	1.0	2.0
PHA0509039	3.0	3.0
PHA0509040	2.0	3.0
PHA0509041	2.0	3.0
PHA0509042	3.0	3.0
PHA0509043	2.0	3.0
PHA0509044	2.0	3.0
PHA0509045	1.0	2.0
PHA0510002A	2.0	2.0
PHA0510002B	2.0	2.0
PHA0510003A	1.0	2.0
PHA0510003B	1.0	2.0
PHA0510004A	1.0	2.0
PHA0510004B	0.0	2.0
PHA0510010A	1.0	1.0
PHA0510010B	0.0	1.0
PHA0510013A	2.0	2.0
PHA0510013B	1.0	2.0
PHA0510023	3.0	3.0
PHA0510027	1.0	2.0
PHA0510029	2.0	3.0
PHA0510030	2.0	3.0
PHA0510031	2.0	2.0
PHA0510032	3.0	3.0
PHA0510034	3.0	3.0
PHA0510035	3.0	3.0
PHA0510036	3.0	3.0
PHA0510037	1.0	2.0
PHA0510038	3.0	3.0
PHA0510039	3.0	3.0
PHA0510040	2.0	3.0
PHA0510046	2.0	3.0
PHA0510047	1.0	2.0
PHA0510048	1.0	3.0
PHA0510049	3.0	2.0
PHA0510050	2.0	3.0
PHA0610005A	1.0	1.0
PHA0610005B	0.0	1.0
PHA0610006A	1.0	2.0
PHA0610006B	1.0	2.0
PHA0610007A	1.0	1.0
PHA0610007B	1.0	2.0
PHA0610015	2.0	3.0
PHA0610016	2.0	3.0
PHA0610017	3.0	3.0
PHA0610018	2.0	3.0
PHA0610019A	2.0	2.0
PHA0610019B	2.0	2.0
PHA0610025	2.0	3.0
PHA0610026	3.0	3.0
PHA0709008	3.0	3.0
PHA0710009	3.0	2.0
PHA0710010	2.0	3.0
PHA0710011	3.0	3.0
PHA0710012	3.0	3.0
PHA0710013	4.0	3.0
PHA0710014	3.0	3.0
PHA0710015	2.0	3.0
PHA0710016	3.0	3.0
PHA0710017	3.0	3.0
PHA0710018	3.0	3.0
PHA0710019	3.0	3.0
PHA0710021	3.0	3.0
PHA0809009	2.0	3.0
PHA0809010	2.0	2.0
PHA0810001	3.0	3.0
PHA0810002	1.0	3.0
PHA0810003	3.0	3.0
PHA0810004	1.0	2.0
PHA0810006	2.0	3.0
PHA0810008	3.0	3.0
PHA0810009	3.0	3.0
PHA0810010	2.0	3.0
PHA0810011	2.0	3.0
PHA0810012	3.0	3.0
PHA0810015	3.0	3.0
PHA0811010	2.0	3.0
PHA0811012	3.0	3.0
PHA0811013	3.0	3.0
PHA0811014	1.0	2.0
PHA0811016	1.0	2.0
PHA0811017	3.0	3.0
PHA0811019	3.0	3.0
PHA0811020	1.0	3.0
PHA1109001	1.0	2.0
PHA1109002	3.0	3.0
PHA1109003	1.0	2.0
PHA1109004	3.0	3.0
PHA1109005	1.0	2.0
PHA1109006	2.0	2.0
PHA1109007	1.0	3.0
PHA1109008	1.0	1.0
PHA1109023	1.0	2.0
PHA1109024	4.0	3.0
PHA1109025	1.0	2.0
PHA1109026	3.0	3.0
PHA1109027	3.0	3.0
PHA1109028	3.0	3.0
PHA1110001A	2.0	2.0
PHA1110001B	1.0	2.0
PHA1110002A	2.0	2.0
PHA1110002B	2.0	2.0
PHA1110003A	1.0	2.0
PHA1110003B	1.0	2.0
PHA1110004A	1.0	2.0
PHA1110013	2.0	3.0
PHA1110014	2.0	3.0
PHA1110015	3.0	3.0
PHA1110016	1.0	2.0
PHA1110017	1.0	2.0
PHA1110019	2.0	3.0
PHA1110021	2.0	2.0
PHA1110022	3.0	3.0
PHA1111001A	1.0	2.0
PHA1111001B	1.0	2.0
PHA1111002A	2.0	1.0
PHA1111002B	1.0	1.0
PHA1111003A	2.0	1.0
PHA1111003B	1.0	2.0
PHA1111004A	1.0	1.0
PHA1111004B	1.0	2.0
PHA1111006A	1.0	1.0
PHA1111006B	1.0	2.0
PHA1111008A	2.0	1.0
PHA1111008B	1.0	2.0
PHA1111009A	1.0	1.0
ST071122B	1.0	2.0
TI071122B	1.0	2.0
VAR0209036	3.0	3.0
VAR0909003	2.0	3.0
VAR0909004	2.0	3.0
VAR0909005	1.0	2.0
VAR0909006	3.0	3.0
VAR0909007	2.0	3.0
VAR0909008	2.0	2.0
VAR0909009	3.0	3.0
VAR0909010	1.0	3.0
VAR0910004	3.0	3.0
VAR0910005	3.0	2.0
VAR0910006	3.0	3.0
VAR0910007	2.0	3.0
VAR0910009	3.0	3.0
VAR0910010	3.0	2.0
VAR0910011	3.0	3.0
Language = CZ, Weighted F1-score = 0.39143961265635646, Dimension = Grammaticalaccuracy

Train on DE, test on  IT
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
nan nan
LANGUAGE: IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']


======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 1.28
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.10
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        71
         1.0       0.43      0.90      0.58       242
         2.0       0.61      0.37      0.46       376
         3.0       0.43      0.24      0.31       111

    accuracy                           0.48       800
   macro avg       0.37      0.38      0.34       800
weighted avg       0.47      0.48      0.43       800

[[  0  71   0   0]
 [  0 217  24   1]
 [  0 203 138  35]
 [  0  19  65  27]]
0.4327658399583155
800 800 800



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36

  Average training loss: 1.00
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       1.00      0.01      0.03        71
         1.0       0.48      0.63      0.54       242
         2.0       0.58      0.66      0.62       376
         3.0       0.50      0.23      0.32       111

    accuracy                           0.54       800
   macro avg       0.64      0.38      0.38       800
weighted avg       0.58      0.54      0.50       800

[[  1  61   9   0]
 [  0 152  90   0]
 [  0 101 249  26]
 [  0   3  82  26]]
0.501634153880711
800 800 800



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 0.88
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.07
              precision    recall  f1-score   support

         0.0       1.00      0.03      0.05        71
         1.0       0.47      0.56      0.51       242
         2.0       0.57      0.61      0.59       376
         3.0       0.47      0.46      0.46       111

    accuracy                           0.52       800
   macro avg       0.63      0.41      0.41       800
weighted avg       0.56      0.52      0.50       800

[[  2  59  10   0]
 [  0 135 105   2]
 [  0  90 230  56]
 [  0   3  57  51]]
0.5014796750676672
800 800 800



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 0.79
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.14
              precision    recall  f1-score   support

         0.0       0.67      0.03      0.05        71
         1.0       0.49      0.27      0.35       242
         2.0       0.53      0.78      0.64       376
         3.0       0.47      0.47      0.47       111

    accuracy                           0.52       800
   macro avg       0.54      0.39      0.38       800
weighted avg       0.52      0.52      0.47       800

[[  2  43  26   0]
 [  1  65 174   2]
 [  0  24 295  57]
 [  0   1  58  52]]
0.47315696719324274
800 800 800
Filename	True Label	Prediction
1325_1001008	2.0	2.0
1325_1001009	2.0	2.0
1325_1001010	2.0	2.0
1325_1001011	2.0	3.0
1325_1001012	2.0	3.0
1325_1001013	2.0	2.0
1325_1001014	3.0	3.0
1325_1001015	2.0	2.0
1325_1001016	2.0	2.0
1325_1001017	2.0	2.0
1325_1001018	2.0	3.0
1325_1001019	2.0	2.0
1325_1001020	2.0	2.0
1325_1001021	2.0	3.0
1325_1001022	2.0	2.0
1325_1001023	2.0	2.0
1325_1001024	2.0	2.0
1325_1001025	2.0	2.0
1325_1001027	3.0	2.0
1325_1001028	2.0	2.0
1325_1001029	2.0	3.0
1325_1001032	2.0	2.0
1325_1001033	3.0	3.0
1325_1001035	3.0	3.0
1325_1001036	2.0	3.0
1325_1001037	2.0	2.0
1325_1001039	3.0	3.0
1325_1001040	3.0	3.0
1325_1001041	3.0	2.0
1325_1001042	2.0	3.0
1325_1001043	2.0	2.0
1325_1001044	3.0	2.0
1325_1001045	2.0	2.0
1325_1001046	2.0	2.0
1325_1001047	1.0	2.0
1325_1001048	1.0	2.0
1325_1001050	2.0	3.0
1325_1001051	2.0	2.0
1325_1001052	2.0	2.0
1325_1001053	1.0	2.0
1325_1001054	2.0	2.0
1325_1001055	2.0	3.0
1325_1001056	2.0	3.0
1325_1001057	2.0	2.0
1325_1001058	2.0	2.0
1325_1001059	2.0	2.0
1325_1001062	2.0	3.0
1325_1001063	2.0	2.0
1325_1001075	1.0	2.0
1325_1001076	3.0	3.0
1325_1001077	2.0	2.0
1325_1001078	2.0	3.0
1325_1001079	3.0	3.0
1325_1001080	2.0	2.0
1325_1001081	3.0	2.0
1325_1001082	2.0	2.0
1325_1001083	2.0	2.0
1325_1001084	2.0	2.0
1325_1001085	2.0	2.0
1325_1001086	2.0	2.0
1325_1001087	2.0	2.0
1325_1001088	2.0	2.0
1325_1001089	2.0	2.0
1325_1001090	2.0	2.0
1325_1001091	3.0	3.0
1325_1001092	2.0	2.0
1325_1001093	2.0	3.0
1325_1001094	2.0	2.0
1325_1001095	2.0	2.0
1325_1001096	2.0	2.0
1325_1001097	1.0	2.0
1325_1001098	2.0	3.0
1325_1001099	3.0	3.0
1325_1001100	2.0	3.0
1325_1001101	3.0	3.0
1325_1001107	3.0	3.0
1325_1001108	3.0	3.0
1325_1001109	2.0	2.0
1325_1001110	3.0	3.0
1325_1001111	3.0	3.0
1325_1001113	3.0	3.0
1325_1001119	3.0	2.0
1325_1001120	3.0	3.0
1325_1001121	2.0	2.0
1325_1001122	2.0	2.0
1325_1001123	3.0	3.0
1325_1001124	2.0	2.0
1325_1001125	2.0	3.0
1325_1001126	2.0	2.0
1325_1001127	3.0	2.0
1325_1001128	2.0	2.0
1325_1001129	1.0	2.0
1325_1001130	2.0	2.0
1325_1001131	3.0	2.0
1325_1001132	2.0	3.0
1325_1001133	2.0	2.0
1325_1001134	2.0	3.0
1325_1001135	2.0	2.0
1325_1001136	2.0	2.0
1325_1001138	2.0	3.0
1325_1001139	2.0	3.0
1325_1001141	2.0	3.0
1325_1001142	2.0	3.0
1325_1001143	2.0	3.0
1325_1001144	3.0	2.0
1325_1001152	2.0	3.0
1325_1001153	2.0	2.0
1325_1001154	3.0	3.0
1325_1001155	2.0	3.0
1325_1001156	2.0	2.0
1325_1001157	2.0	3.0
1325_1001158	2.0	3.0
1325_1001159	3.0	2.0
1325_1001160	2.0	3.0
1325_1001161	2.0	3.0
1325_1001162	2.0	2.0
1325_1001163	2.0	2.0
1325_1001164	2.0	2.0
1325_1001165	2.0	2.0
1325_1001166	2.0	2.0
1325_1001167	3.0	3.0
1325_1001168	2.0	3.0
1325_1001169	2.0	2.0
1325_1001170	3.0	2.0
1325_9000059	2.0	3.0
1325_9000087	2.0	2.0
1325_9000088	3.0	2.0
1325_9000089	2.0	2.0
1325_9000090	2.0	2.0
1325_9000095	2.0	3.0
1325_9000099	2.0	2.0
1325_9000102	2.0	2.0
1325_9000104	2.0	2.0
1325_9000105	1.0	2.0
1325_9000106	2.0	3.0
1325_9000107	2.0	2.0
1325_9000136	2.0	3.0
1325_9000137	3.0	3.0
1325_9000138	3.0	3.0
1325_9000139	2.0	2.0
1325_9000140	3.0	3.0
1325_9000143	3.0	3.0
1325_9000144	3.0	3.0
1325_9000152	3.0	3.0
1325_9000185	3.0	2.0
1325_9000186	3.0	3.0
1325_9000187	3.0	3.0
1325_9000188	2.0	3.0
1325_9000209	2.0	2.0
1325_9000210	2.0	2.0
1325_9000211	2.0	3.0
1325_9000213	3.0	2.0
1325_9000214	2.0	2.0
1325_9000215	3.0	3.0
1325_9000237	3.0	3.0
1325_9000239	3.0	2.0
1325_9000240	2.0	2.0
1325_9000241	3.0	3.0
1325_9000278	3.0	2.0
1325_9000279	3.0	2.0
1325_9000296	2.0	3.0
1325_9000302	2.0	2.0
1325_9000303	3.0	2.0
1325_9000304	2.0	2.0
1325_9000314	2.0	2.0
1325_9000315	2.0	2.0
1325_9000316	1.0	2.0
1325_9000317	3.0	3.0
1325_9000318	3.0	2.0
1325_9000319	2.0	3.0
1325_9000320	3.0	2.0
1325_9000321	3.0	3.0
1325_9000322	3.0	3.0
1325_9000323	2.0	2.0
1325_9000503	3.0	3.0
1325_9000504	3.0	3.0
1325_9000505	2.0	3.0
1325_9000533	3.0	3.0
1325_9000534	2.0	3.0
1325_9000536	2.0	3.0
1325_9000554	2.0	2.0
1325_9000601	3.0	3.0
1325_9000602	3.0	3.0
1325_9000611	2.0	3.0
1325_9000612	2.0	2.0
1325_9000674	2.0	3.0
1325_9000675	3.0	2.0
1325_9000676	3.0	2.0
1325_9000677	3.0	2.0
1325_9000678	3.0	3.0
1325_9000684	3.0	3.0
1325_9000685	3.0	3.0
1325_9000686	3.0	3.0
1325_9000700	2.0	3.0
1325_9000750	3.0	2.0
1365_0100002	2.0	2.0
1365_0100003	1.0	1.0
1365_0100004	2.0	2.0
1365_0100005	2.0	2.0
1365_0100006	2.0	2.0
1365_0100007	1.0	1.0
1365_0100008	1.0	2.0
1365_0100009	2.0	1.0
1365_0100010	1.0	1.0
1365_0100011	1.0	2.0
1365_0100012	2.0	1.0
1365_0100013	3.0	2.0
1365_0100014	2.0	2.0
1365_0100015	1.0	1.0
1365_0100016	2.0	2.0
1365_0100017	2.0	2.0
1365_0100018	1.0	1.0
1365_0100019	1.0	1.0
1365_0100020	2.0	2.0
1365_0100021	2.0	2.0
1365_0100022	2.0	2.0
1365_0100023	2.0	2.0
1365_0100024	1.0	2.0
1365_0100026	1.0	1.0
1365_0100027	2.0	2.0
1365_0100028	2.0	2.0
1365_0100029	1.0	1.0
1365_0100030	1.0	1.0
1365_0100031	2.0	1.0
1365_0100051	1.0	2.0
1365_0100056	2.0	2.0
1365_0100057	2.0	3.0
1365_0100058	2.0	2.0
1365_0100061	3.0	2.0
1365_0100063	3.0	3.0
1365_0100064	2.0	2.0
1365_0100065	1.0	2.0
1365_0100066	1.0	2.0
1365_0100067	2.0	2.0
1365_0100069	1.0	2.0
1365_0100070	2.0	2.0
1365_0100071	2.0	2.0
1365_0100072	2.0	2.0
1365_0100073	2.0	2.0
1365_0100074	2.0	2.0
1365_0100079	2.0	2.0
1365_0100080	2.0	2.0
1365_0100092	2.0	2.0
1365_0100093	2.0	2.0
1365_0100094	2.0	2.0
1365_0100095	2.0	2.0
1365_0100096	2.0	2.0
1365_0100097	2.0	2.0
1365_0100098	1.0	2.0
1365_0100099	2.0	2.0
1365_0100100	2.0	3.0
1365_0100101	3.0	2.0
1365_0100102	3.0	2.0
1365_0100103	3.0	2.0
1365_0100104	2.0	2.0
1365_0100105	3.0	2.0
1365_0100106	2.0	2.0
1365_0100107	3.0	3.0
1365_0100116	2.0	2.0
1365_0100117	3.0	2.0
1365_0100118	3.0	2.0
1365_0100119	3.0	3.0
1365_0100120	3.0	3.0
1365_0100121	3.0	2.0
1365_0100123	2.0	2.0
1365_0100125	3.0	2.0
1365_0100133	2.0	2.0
1365_0100134	2.0	2.0
1365_0100135	2.0	2.0
1365_0100136	2.0	2.0
1365_0100137	2.0	2.0
1365_0100138	2.0	1.0
1365_0100139	2.0	2.0
1365_0100145	2.0	2.0
1365_0100146	2.0	2.0
1365_0100147	2.0	2.0
1365_0100148	2.0	2.0
1365_0100151	2.0	1.0
1365_0100162	2.0	2.0
1365_0100163	3.0	3.0
1365_0100164	2.0	2.0
1365_0100165	3.0	2.0
1365_0100166	2.0	2.0
1365_0100167	1.0	2.0
1365_0100168	3.0	2.0
1365_0100169	2.0	2.0
1365_0100170	1.0	2.0
1365_0100171	2.0	2.0
1365_0100172	2.0	2.0
1365_0100173	2.0	2.0
1365_0100174	2.0	1.0
1365_0100175	2.0	2.0
1365_0100176	2.0	2.0
1365_0100177	2.0	2.0
1365_0100178	2.0	2.0
1365_0100179	2.0	2.0
1365_0100180	1.0	1.0
1365_0100181	1.0	2.0
1365_0100182	2.0	2.0
1365_0100183	1.0	2.0
1365_0100184	2.0	2.0
1365_0100185	1.0	2.0
1365_0100186	2.0	2.0
1365_0100187	2.0	2.0
1365_0100188	2.0	2.0
1365_0100190	2.0	2.0
1365_0100191	2.0	2.0
1365_0100192	3.0	3.0
1365_0100194	3.0	2.0
1365_0100195	1.0	2.0
1365_0100196	1.0	2.0
1365_0100198	2.0	2.0
1365_0100199	2.0	2.0
1365_0100200	3.0	2.0
1365_0100201	2.0	2.0
1365_0100202	1.0	2.0
1365_0100203	2.0	2.0
1365_0100204	2.0	2.0
1365_0100205	3.0	1.0
1365_0100211	2.0	2.0
1365_0100212	3.0	3.0
1365_0100213	2.0	1.0
1365_0100215	2.0	2.0
1365_0100217	3.0	2.0
1365_0100218	3.0	2.0
1365_0100219	2.0	3.0
1365_0100220	2.0	2.0
1365_0100221	2.0	2.0
1365_0100222	1.0	3.0
1365_0100223	2.0	2.0
1365_0100224	2.0	3.0
1365_0100225	2.0	2.0
1365_0100226	2.0	2.0
1365_0100227	3.0	2.0
1365_0100228	1.0	2.0
1365_0100229	2.0	3.0
1365_0100230	2.0	3.0
1365_0100231	2.0	2.0
1365_0100232	3.0	2.0
1365_0100233	3.0	3.0
1365_0100251	2.0	3.0
1365_0100252	2.0	2.0
1365_0100253	2.0	2.0
1365_0100255	2.0	2.0
1365_0100256	2.0	2.0
1365_0100257	2.0	2.0
1365_0100258	2.0	2.0
1365_0100259	2.0	2.0
1365_0100260	2.0	2.0
1365_0100261	2.0	2.0
1365_0100262	2.0	2.0
1365_0100263	3.0	3.0
1365_0100265	3.0	2.0
1365_0100266	3.0	2.0
1365_0100267	2.0	3.0
1365_0100268	2.0	2.0
1365_0100269	2.0	2.0
1365_0100270	2.0	2.0
1365_0100274	2.0	3.0
1365_0100275	2.0	2.0
1365_0100276	3.0	2.0
1365_0100277	3.0	2.0
1365_0100278	2.0	2.0
1365_0100279	1.0	2.0
1365_0100280	1.0	2.0
1365_0100281	1.0	2.0
1365_0100282	2.0	2.0
1365_0100285	2.0	2.0
1365_0100286	1.0	2.0
1365_0100287	2.0	2.0
1365_0100288	2.0	2.0
1365_0100289	2.0	2.0
1365_0100290	2.0	2.0
1365_0100299	3.0	2.0
1365_0100447	3.0	2.0
1365_0100448	1.0	2.0
1365_0100451	2.0	2.0
1365_0100455	3.0	3.0
1365_0100456	2.0	2.0
1365_0100457	2.0	2.0
1365_0100458	2.0	2.0
1365_0100459	2.0	2.0
1365_0100461	2.0	2.0
1365_0100469	2.0	2.0
1365_0100470	2.0	3.0
1365_0100471	2.0	3.0
1365_0100472	2.0	2.0
1365_0100473	2.0	2.0
1365_0100474	2.0	2.0
1365_0100475	2.0	3.0
1365_0100476	2.0	2.0
1365_0100477	1.0	3.0
1365_0100478	2.0	2.0
1365_0100479	3.0	2.0
1365_0100480	2.0	2.0
1365_0100481	2.0	3.0
1365_0100482	2.0	2.0
1385_0000011	0.0	1.0
1385_0000012	2.0	1.0
1385_0000013	1.0	2.0
1385_0000016	2.0	1.0
1385_0000017	1.0	1.0
1385_0000020	2.0	1.0
1385_0000021	2.0	2.0
1385_0000022	1.0	2.0
1385_0000023	2.0	2.0
1385_0000033	1.0	2.0
1385_0000034	2.0	2.0
1385_0000035	2.0	2.0
1385_0000036	1.0	2.0
1385_0000037	1.0	2.0
1385_0000038	1.0	2.0
1385_0000039	1.0	2.0
1385_0000040	1.0	1.0
1385_0000041	2.0	2.0
1385_0000042	2.0	2.0
1385_0000043	2.0	1.0
1385_0000044	2.0	2.0
1385_0000045	2.0	2.0
1385_0000047	1.0	2.0
1385_0000048	2.0	2.0
1385_0000049	2.0	2.0
1385_0000050	2.0	1.0
1385_0000051	2.0	2.0
1385_0000052	1.0	1.0
1385_0000053	2.0	2.0
1385_0000054	2.0	2.0
1385_0000057	1.0	1.0
1385_0000058	2.0	2.0
1385_0000059	2.0	2.0
1385_0000095	1.0	1.0
1385_0000097	2.0	1.0
1385_0000098	1.0	1.0
1385_0000099	0.0	1.0
1385_0000100	1.0	1.0
1385_0000101	1.0	0.0
1385_0000102	1.0	2.0
1385_0000103	1.0	1.0
1385_0000104	2.0	2.0
1385_0000114	2.0	2.0
1385_0000119	2.0	2.0
1385_0000120	0.0	1.0
1385_0000122	2.0	2.0
1385_0000123	1.0	2.0
1385_0000124	2.0	2.0
1385_0000125	1.0	2.0
1385_0000126	2.0	2.0
1385_0000127	2.0	2.0
1385_0000128	1.0	2.0
1385_0000129	2.0	2.0
1385_0000130	2.0	1.0
1385_0001103	2.0	1.0
1385_0001104	1.0	1.0
1385_0001105	2.0	2.0
1385_0001107	2.0	2.0
1385_0001108	2.0	2.0
1385_0001109	2.0	2.0
1385_0001110	2.0	2.0
1385_0001111	2.0	2.0
1385_0001112	2.0	2.0
1385_0001113	1.0	1.0
1385_0001118	1.0	1.0
1385_0001119	1.0	2.0
1385_0001120	1.0	1.0
1385_0001121	1.0	2.0
1385_0001122	1.0	1.0
1385_0001123	2.0	1.0
1385_0001124	1.0	1.0
1385_0001125	1.0	2.0
1385_0001126	0.0	1.0
1385_0001127	2.0	2.0
1385_0001128	0.0	2.0
1385_0001129	1.0	1.0
1385_0001130	1.0	1.0
1385_0001131	1.0	1.0
1385_0001132	1.0	1.0
1385_0001133	2.0	2.0
1385_0001134	1.0	1.0
1385_0001135	1.0	2.0
1385_0001136	1.0	1.0
1385_0001137	1.0	2.0
1385_0001138	1.0	2.0
1385_0001147	1.0	2.0
1385_0001148	2.0	2.0
1385_0001149	2.0	2.0
1385_0001150	2.0	1.0
1385_0001151	2.0	2.0
1385_0001152	2.0	2.0
1385_0001153	3.0	2.0
1385_0001154	2.0	2.0
1385_0001155	2.0	1.0
1385_0001156	2.0	1.0
1385_0001157	1.0	1.0
1385_0001158	1.0	2.0
1385_0001159	1.0	2.0
1385_0001160	3.0	2.0
1385_0001161	2.0	2.0
1385_0001162	1.0	1.0
1385_0001163	2.0	2.0
1385_0001164	1.0	2.0
1385_0001165	1.0	2.0
1385_0001166	0.0	2.0
1385_0001167	1.0	2.0
1385_0001169	1.0	1.0
1385_0001170	1.0	1.0
1385_0001171	1.0	1.0
1385_0001172	0.0	1.0
1385_0001173	0.0	1.0
1385_0001174	0.0	1.0
1385_0001175	0.0	1.0
1385_0001178	0.0	1.0
1385_0001188	1.0	2.0
1385_0001189	0.0	1.0
1385_0001190	0.0	1.0
1385_0001191	1.0	2.0
1385_0001192	1.0	2.0
1385_0001193	2.0	2.0
1385_0001194	1.0	2.0
1385_0001195	1.0	2.0
1385_0001196	1.0	2.0
1385_0001197	1.0	2.0
1385_0001198	2.0	2.0
1385_0001199	1.0	2.0
1385_0001501	1.0	1.0
1385_0001503	1.0	2.0
1385_0001522	0.0	1.0
1385_0001523	1.0	2.0
1385_0001524	0.0	2.0
1385_0001525	1.0	2.0
1385_0001526	0.0	1.0
1385_0001527	1.0	1.0
1385_0001528	1.0	2.0
1385_0001712	1.0	2.0
1385_0001714	1.0	1.0
1385_0001715	1.0	2.0
1385_0001716	1.0	2.0
1385_0001717	2.0	2.0
1385_0001718	0.0	1.0
1385_0001719	0.0	1.0
1385_0001720	0.0	1.0
1385_0001723	0.0	1.0
1385_0001724	1.0	2.0
1385_0001725	1.0	1.0
1385_0001726	1.0	2.0
1385_0001727	0.0	1.0
1385_0001728	1.0	1.0
1385_0001729	1.0	2.0
1385_0001730	1.0	2.0
1385_0001732	1.0	2.0
1385_0001733	2.0	2.0
1385_0001734	1.0	2.0
1385_0001736	2.0	2.0
1385_0001737	2.0	2.0
1385_0001738	0.0	0.0
1385_0001739	0.0	2.0
1385_0001740	1.0	2.0
1385_0001741	0.0	1.0
1385_0001742	0.0	0.0
1385_0001744	0.0	1.0
1385_0001746	0.0	2.0
1385_0001747	1.0	2.0
1385_0001748	1.0	2.0
1385_0001749	1.0	2.0
1385_0001750	0.0	1.0
1385_0001751	1.0	2.0
1385_0001752	0.0	2.0
1385_0001753	1.0	2.0
1385_0001754	1.0	2.0
1385_0001756	1.0	2.0
1385_0001757	2.0	1.0
1385_0001758	1.0	1.0
1385_0001759	0.0	2.0
1385_0001760	1.0	2.0
1385_0001761	0.0	2.0
1385_0001762	1.0	2.0
1385_0001764	0.0	2.0
1385_0001765	0.0	1.0
1385_0001766	2.0	2.0
1385_0001767	0.0	2.0
1385_0001768	1.0	2.0
1385_0001771	0.0	2.0
1385_0001772	0.0	1.0
1385_0001773	0.0	1.0
1385_0001774	0.0	1.0
1385_0001775	0.0	2.0
1385_0001785	0.0	2.0
1385_0001786	1.0	2.0
1385_0001787	0.0	1.0
1385_0001788	1.0	2.0
1385_0001789	1.0	2.0
1385_0001790	2.0	2.0
1385_0001791	0.0	1.0
1385_0001792	0.0	2.0
1385_0001793	1.0	2.0
1385_0001794	0.0	2.0
1385_0001795	1.0	1.0
1385_0001796	2.0	1.0
1385_0001798	1.0	2.0
1385_0001799	1.0	2.0
1385_0001800	1.0	2.0
1395_0000333	2.0	2.0
1395_0000337	0.0	1.0
1395_0000338	1.0	2.0
1395_0000340	2.0	2.0
1395_0000341	1.0	1.0
1395_0000353	1.0	2.0
1395_0000354	1.0	1.0
1395_0000355	2.0	2.0
1395_0000356	1.0	1.0
1395_0000357	3.0	2.0
1395_0000359	2.0	2.0
1395_0000360	3.0	2.0
1395_0000361	2.0	2.0
1395_0000364	1.0	2.0
1395_0000365	2.0	2.0
1395_0000366	2.0	2.0
1395_0000368	0.0	1.0
1395_0000369	2.0	2.0
1395_0000376	2.0	2.0
1395_0000378	1.0	2.0
1395_0000379	1.0	1.0
1395_0000380	2.0	2.0
1395_0000383	2.0	2.0
1395_0000387	3.0	2.0
1395_0000388	2.0	2.0
1395_0000389	0.0	1.0
1395_0000390	1.0	1.0
1395_0000391	3.0	2.0
1395_0000392	2.0	2.0
1395_0000396	2.0	2.0
1395_0000398	2.0	2.0
1395_0000399	1.0	2.0
1395_0000402	1.0	2.0
1395_0000403	2.0	2.0
1395_0000404	2.0	2.0
1395_0000409	2.0	2.0
1395_0000413	2.0	2.0
1395_0000414	2.0	2.0
1395_0000415	1.0	1.0
1395_0000432	2.0	2.0
1395_0000438	3.0	2.0
1395_0000443	2.0	2.0
1395_0000446	2.0	2.0
1395_0000447	1.0	2.0
1395_0000448	1.0	1.0
1395_0000449	2.0	2.0
1395_0000450	1.0	2.0
1395_0000451	2.0	2.0
1395_0000452	1.0	1.0
1395_0000454	2.0	2.0
1395_0000455	1.0	1.0
1395_0000458	2.0	2.0
1395_0000460	1.0	2.0
1395_0000462	2.0	2.0
1395_0000465	1.0	1.0
1395_0000469	1.0	2.0
1395_0000470	1.0	2.0
1395_0000471	1.0	2.0
1395_0000499	1.0	2.0
1395_0000500	1.0	1.0
1395_0000504	1.0	2.0
1395_0000512	2.0	2.0
1395_0000513	2.0	2.0
1395_0000514	3.0	2.0
1395_0000515	2.0	2.0
1395_0000516	1.0	1.0
1395_0000518	2.0	2.0
1395_0000525	2.0	1.0
1395_0000526	1.0	1.0
1395_0000527	1.0	1.0
1395_0000528	2.0	2.0
1395_0000529	2.0	1.0
1395_0000531	2.0	2.0
1395_0000533	3.0	2.0
1395_0000534	2.0	2.0
1395_0000535	1.0	1.0
1395_0000537	1.0	2.0
1395_0000547	2.0	2.0
1395_0000548	2.0	2.0
1395_0000549	2.0	2.0
1395_0000550	1.0	2.0
1395_0000551	2.0	2.0
1395_0000552	2.0	2.0
1395_0000553	2.0	2.0
1395_0000554	2.0	2.0
1395_0000555	1.0	2.0
1395_0000556	1.0	2.0
1395_0000557	3.0	2.0
1395_0000559	2.0	2.0
1395_0000560	2.0	2.0
1395_0000563	1.0	2.0
1395_0000564	1.0	2.0
1395_0000565	1.0	2.0
1395_0000572	1.0	2.0
1395_0000575	1.0	2.0
1395_0000579	1.0	2.0
1395_0000581	1.0	2.0
1395_0000582	0.0	1.0
1395_0000583	1.0	2.0
1395_0000584	0.0	1.0
1395_0000585	1.0	2.0
1395_0000587	0.0	1.0
1395_0000591	0.0	1.0
1395_0000593	1.0	2.0
1395_0000595	0.0	2.0
1395_0000596	2.0	2.0
1395_0000597	1.0	2.0
1395_0000598	1.0	2.0
1395_0000599	1.0	1.0
1395_0000602	1.0	1.0
1395_0000604	1.0	1.0
1395_0000606	1.0	1.0
1395_0000607	1.0	1.0
1395_0000608	1.0	2.0
1395_0000609	0.0	1.0
1395_0000610	2.0	2.0
1395_0000611	1.0	1.0
1395_0000612	1.0	2.0
1395_0000626	1.0	2.0
1395_0000627	1.0	2.0
1395_0000628	0.0	2.0
1395_0000630	1.0	2.0
1395_0000631	1.0	2.0
1395_0000635	0.0	1.0
1395_0000636	0.0	1.0
1395_0000639	1.0	2.0
1395_0000642	0.0	2.0
1395_0000644	1.0	2.0
1395_0000646	1.0	2.0
1395_0000649	2.0	2.0
1395_0001010	1.0	2.0
1395_0001013	1.0	2.0
1395_0001015	1.0	2.0
1395_0001016	2.0	2.0
1395_0001017	1.0	2.0
1395_0001019	0.0	2.0
1395_0001020	1.0	2.0
1395_0001021	1.0	2.0
1395_0001022	1.0	2.0
1395_0001023	1.0	2.0
1395_0001024	1.0	2.0
1395_0001028	1.0	2.0
1395_0001033	1.0	2.0
1395_0001034	1.0	2.0
1395_0001040	0.0	1.0
1395_0001045	2.0	2.0
1395_0001058	1.0	2.0
1395_0001060	1.0	2.0
1395_0001061	1.0	2.0
1395_0001064	2.0	1.0
1395_0001065	1.0	2.0
1395_0001066	1.0	2.0
1395_0001067	1.0	2.0
1395_0001068	1.0	2.0
1395_0001069	2.0	2.0
1395_0001070	1.0	2.0
1395_0001071	1.0	1.0
1395_0001073	2.0	2.0
1395_0001074	1.0	2.0
1395_0001075	1.0	2.0
1395_0001076	0.0	2.0
1395_0001078	0.0	2.0
1395_0001080	1.0	2.0
1395_0001084	1.0	2.0
1395_0001090	2.0	2.0
1395_0001093	1.0	2.0
1395_0001101	1.0	2.0
1395_0001103	1.0	2.0
1395_0001104	0.0	2.0
1395_0001108	0.0	1.0
1395_0001109	0.0	2.0
1395_0001114	1.0	2.0
1395_0001115	1.0	2.0
1395_0001116	2.0	2.0
1395_0001117	1.0	2.0
1395_0001118	0.0	2.0
1395_0001119	1.0	2.0
1395_0001120	1.0	2.0
1395_0001121	0.0	1.0
1395_0001122	1.0	1.0
1395_0001123	1.0	2.0
1395_0001124	0.0	1.0
1395_0001126	1.0	2.0
1395_0001131	0.0	1.0
1395_0001132	1.0	2.0
1395_0001133	0.0	2.0
1395_0001141	1.0	2.0
1395_0001145	2.0	2.0
1395_0001146	0.0	1.0
1395_0001147	1.0	2.0
1395_0001149	0.0	2.0
1395_0001150	1.0	2.0
1395_0001158	2.0	2.0
1395_0001160	2.0	2.0
1395_0001161	1.0	2.0
1395_0001164	2.0	2.0
1395_0001167	1.0	2.0
1395_0001169	2.0	2.0
1395_0001170	1.0	2.0
1395_0001171	1.0	2.0
Language = IT, Weighted F1-score = 0.47315696719324274, Dimension = Grammaticalaccuracy

Train on DE, test on  CZ
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
144.90552995391704 65.35717405024758
LANGUAGE: IT
nan nan
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']


======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 1.41
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.62
              precision    recall  f1-score   support

         1.0       0.00      0.00      0.00        46
         2.0       0.59      0.16      0.26       269
         3.0       0.21      0.52      0.29       114
         4.0       0.07      1.00      0.13         5

    accuracy                           0.25       434
   macro avg       0.22      0.42      0.17       434
weighted avg       0.42      0.25      0.24       434

[[  0  24  22   0]
 [  0  44 206  19]
 [  0   6  59  49]
 [  0   0   0   5]]
0.23779202203650346
434 434 434



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 1.11
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.70
              precision    recall  f1-score   support

         1.0       0.50      0.04      0.08        46
         2.0       0.58      0.18      0.28       269
         3.0       0.26      0.70      0.38       114
         4.0       0.13      1.00      0.23         5

    accuracy                           0.31       434
   macro avg       0.37      0.48      0.24       434
weighted avg       0.48      0.31      0.28       434

[[  2  28  16   0]
 [  2  49 211   7]
 [  0   7  80  27]
 [  0   0   0   5]]
0.2829992304121559
434 434 434



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 0.97
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.68
              precision    recall  f1-score   support

         1.0       0.50      0.07      0.12        46
         2.0       0.61      0.22      0.33       269
         3.0       0.23      0.52      0.31       114
         4.0       0.07      1.00      0.14         5

    accuracy                           0.29       434
   macro avg       0.35      0.45      0.22       434
weighted avg       0.49      0.29      0.30       434

[[  3  32  11   0]
 [  2  60 191  16]
 [  1   6  59  48]
 [  0   0   0   5]]
0.2991054637168727
434 434 434



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.87
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.45
              precision    recall  f1-score   support

         1.0       0.60      0.07      0.12        46
         2.0       0.64      0.32      0.42       269
         3.0       0.26      0.54      0.35       114
         4.0       0.08      1.00      0.16         5

    accuracy                           0.36       434
   macro avg       0.40      0.48      0.26       434
weighted avg       0.53      0.36      0.37       434

[[  3  36   7   0]
 [  2  85 168  14]
 [  0  12  62  40]
 [  0   0   0   5]]
0.36917681540290154
434 434 434
Filename	True Label	Prediction
0601	2.0	3.0
0602	2.0	3.0
0603	2.0	3.0
0604	2.0	3.0
0605	2.0	3.0
0606	2.0	3.0
0607	3.0	3.0
0608	1.0	2.0
0609	2.0	3.0
0610	2.0	3.0
0611	2.0	3.0
0612	2.0	2.0
0613	2.0	2.0
0614	2.0	3.0
0615	2.0	3.0
0616	2.0	3.0
0617	2.0	2.0
0618	2.0	2.0
0619	2.0	2.0
0620	2.0	3.0
0621	2.0	3.0
0622	2.0	2.0
0623	2.0	3.0
0624	3.0	2.0
0625	2.0	2.0
0626	3.0	2.0
0627	2.0	3.0
0628	2.0	2.0
0629	2.0	3.0
0630	2.0	2.0
0631	2.0	3.0
0632	2.0	2.0
0633	3.0	2.0
0634	3.0	3.0
0635	2.0	3.0
0636	3.0	3.0
0637	2.0	3.0
0638	2.0	3.0
0639	2.0	3.0
0640	2.0	3.0
0641	1.0	2.0
0642	2.0	3.0
0643	2.0	3.0
0644	2.0	2.0
0645	3.0	3.0
0714	2.0	3.0
0715	2.0	2.0
0716	2.0	3.0
0717	2.0	2.0
0718	2.0	3.0
0719	2.0	2.0
0720	2.0	3.0
0721	3.0	3.0
0722	2.0	3.0
0723	2.0	3.0
0724	3.0	3.0
0725	2.0	3.0
0801	2.0	3.0
0802	2.0	3.0
0803	2.0	3.0
0804	2.0	3.0
0805	2.0	3.0
0806	2.0	3.0
0807	3.0	3.0
0808	2.0	3.0
0809	2.0	3.0
0810	2.0	3.0
0811	3.0	3.0
0812	2.0	3.0
0813	2.0	3.0
0814	2.0	3.0
0815	3.0	3.0
0816	3.0	3.0
0817	2.0	3.0
0818	2.0	3.0
0819	3.0	3.0
0820	1.0	2.0
0821	2.0	3.0
0822	2.0	3.0
0823	3.0	3.0
0824	2.0	3.0
0825	2.0	3.0
0826	2.0	3.0
0827	2.0	2.0
0828	2.0	3.0
0829	2.0	3.0
0901	3.0	3.0
0902	3.0	3.0
0903	2.0	3.0
0904	2.0	2.0
0905	2.0	3.0
0906	3.0	3.0
0907	2.0	3.0
0910	2.0	3.0
0911	2.0	3.0
0912	3.0	3.0
0913	2.0	3.0
0914	2.0	3.0
0915	2.0	3.0
0916	2.0	2.0
0917	2.0	3.0
0918	2.0	3.0
0919	2.0	3.0
0920	3.0	3.0
0921	2.0	2.0
0922	2.0	3.0
0923	2.0	3.0
0924	2.0	2.0
0925	2.0	3.0
0926	3.0	3.0
0927	3.0	2.0
0928	3.0	2.0
0929	1.0	2.0
0930	2.0	2.0
1001	2.0	3.0
1002	2.0	3.0
1003	1.0	3.0
1004	2.0	2.0
1005	2.0	3.0
1006	2.0	3.0
1007	2.0	3.0
1008	1.0	3.0
1009	2.0	3.0
1010	2.0	3.0
1014	3.0	3.0
1015	2.0	3.0
1016	2.0	2.0
1017	2.0	3.0
1018	2.0	3.0
1019	2.0	2.0
1020	2.0	3.0
1021	2.0	3.0
1022	2.0	2.0
1023	2.0	2.0
1111	2.0	3.0
1112	2.0	3.0
1113	2.0	3.0
1114	2.0	3.0
1115	2.0	3.0
1116	2.0	3.0
1117	2.0	3.0
9999	1.0	2.0
BER0609003	2.0	3.0
BER0611003	3.0	4.0
BER0611005	3.0	4.0
BER0611006	3.0	4.0
BER0611007	1.0	3.0
KYJ0611003A	2.0	2.0
KYJ0611004A	2.0	2.0
KYJ0611005A	1.0	2.0
KYJ0611005B	1.0	2.0
KYJ0611006A	1.0	2.0
KYJ0611006B	1.0	2.0
KYJ0611009A	2.0	2.0
KYJ0611009B	1.0	2.0
LIB0611001A	2.0	2.0
LIB0611001B	2.0	2.0
LIB0611002A	2.0	2.0
LIB0611002B	2.0	2.0
LIB0611003A	1.0	2.0
LIB0611004A	3.0	2.0
LIB0611004B	2.0	3.0
LIB0611011	1.0	2.0
LON0610002A	2.0	3.0
LON0610002B	2.0	2.0
LON0611002A	2.0	2.0
LON0611002B	1.0	2.0
LON0611003	3.0	4.0
LON0611004A	2.0	2.0
LON0611004B	1.0	2.0
MOS0509001	2.0	3.0
MOS0509004	2.0	2.0
MOS0611012	3.0	3.0
MOS0611013	3.0	3.0
MOS0611014	1.0	2.0
MOS0611015	3.0	3.0
PAR1011008A	2.0	2.0
PAR1011009A	2.0	2.0
PAR1011009B	2.0	3.0
PAR1011013	3.0	3.0
PAR1011014	2.0	3.0
PAR1011015	3.0	3.0
PAR1011016	3.0	4.0
PAR1011017	3.0	3.0
PAR1011018	2.0	3.0
PHA0111001A	2.0	2.0
PHA0111001B	2.0	3.0
PHA0111002A	3.0	2.0
PHA0111002B	2.0	3.0
PHA0111003A	1.0	2.0
PHA0111003B	2.0	2.0
PHA0111004A	3.0	2.0
PHA0111004B	2.0	2.0
PHA0111005A	3.0	2.0
PHA0111005B	2.0	3.0
PHA0111010	3.0	3.0
PHA0111011	2.0	3.0
PHA0111012	2.0	3.0
PHA0111014	1.0	2.0
PHA0111015	3.0	4.0
PHA0111016	3.0	4.0
PHA0111018	2.0	2.0
PHA0112002A	2.0	2.0
PHA0112002B	2.0	3.0
PHA0112003A	2.0	2.0
PHA0112003B	1.0	2.0
PHA0112006A	3.0	3.0
PHA0112006B	3.0	3.0
PHA0112007A	2.0	2.0
PHA0112007B	1.0	2.0
PHA0112009A	2.0	3.0
PHA0112009B	2.0	2.0
PHA0112012A	2.0	3.0
PHA0112012B	2.0	3.0
PHA0209001	2.0	3.0
PHA0209008	2.0	3.0
PHA0209013	2.0	2.0
PHA0209024	1.0	2.0
PHA0209026	3.0	4.0
PHA0209028	3.0	3.0
PHA0209031	3.0	4.0
PHA0209034	2.0	3.0
PHA0209038	3.0	4.0
PHA0209039	2.0	4.0
PHA0210001	2.0	2.0
PHA0210004	2.0	2.0
PHA0210007	2.0	3.0
PHA0210008	1.0	2.0
PHA0411008A	2.0	2.0
PHA0411008B	2.0	2.0
PHA0411009A	2.0	2.0
PHA0411009B	1.0	3.0
PHA0411010A	2.0	2.0
PHA0411010B	2.0	2.0
PHA0411011A	3.0	2.0
PHA0411011B	2.0	3.0
PHA0411012A	2.0	2.0
PHA0411012B	2.0	2.0
PHA0411027	2.0	3.0
PHA0411028	2.0	3.0
PHA0411029	2.0	2.0
PHA0411030	4.0	4.0
PHA0411031	3.0	3.0
PHA0411032	2.0	4.0
PHA0411033	2.0	3.0
PHA0411034	1.0	2.0
PHA0411035	3.0	3.0
PHA0411036	2.0	3.0
PHA0411037	2.0	3.0
PHA0411038	3.0	3.0
PHA0411039	2.0	3.0
PHA0411041	2.0	3.0
PHA0411042	3.0	3.0
PHA0411043	2.0	2.0
PHA0411044	4.0	4.0
PHA0411045	2.0	2.0
PHA0411047	2.0	4.0
PHA0411051	3.0	4.0
PHA0411053	3.0	4.0
PHA0411054	3.0	3.0
PHA0411055	3.0	4.0
PHA0411056	3.0	4.0
PHA0411058	3.0	4.0
PHA0411059	3.0	4.0
PHA0411060	3.0	4.0
PHA0411061	3.0	4.0
PHA0411062	2.0	4.0
PHA0509002	1.0	2.0
PHA0509007	2.0	3.0
PHA0509013	1.0	2.0
PHA0509015	2.0	3.0
PHA0509017	2.0	4.0
PHA0509018	3.0	4.0
PHA0509019	2.0	3.0
PHA0509020	3.0	3.0
PHA0509021	3.0	3.0
PHA0509022	3.0	4.0
PHA0509024	2.0	3.0
PHA0509025	4.0	4.0
PHA0509026	3.0	3.0
PHA0509027	2.0	3.0
PHA0509028	3.0	4.0
PHA0509030	3.0	3.0
PHA0509031	2.0	3.0
PHA0509032	3.0	3.0
PHA0509033	2.0	3.0
PHA0509034	2.0	4.0
PHA0509035	2.0	3.0
PHA0509036	3.0	3.0
PHA0509037	3.0	4.0
PHA0509038	2.0	3.0
PHA0509039	3.0	4.0
PHA0509040	2.0	3.0
PHA0509041	3.0	3.0
PHA0509042	3.0	4.0
PHA0509043	2.0	4.0
PHA0509044	2.0	3.0
PHA0509045	2.0	2.0
PHA0510002A	2.0	2.0
PHA0510002B	2.0	2.0
PHA0510003A	2.0	2.0
PHA0510003B	2.0	2.0
PHA0510004A	1.0	2.0
PHA0510004B	1.0	3.0
PHA0510010A	1.0	1.0
PHA0510010B	1.0	1.0
PHA0510013A	2.0	3.0
PHA0510013B	2.0	2.0
PHA0510023	3.0	4.0
PHA0510027	2.0	2.0
PHA0510029	3.0	4.0
PHA0510030	3.0	3.0
PHA0510031	2.0	2.0
PHA0510032	3.0	4.0
PHA0510034	3.0	4.0
PHA0510035	3.0	3.0
PHA0510036	3.0	3.0
PHA0510037	2.0	3.0
PHA0510038	3.0	3.0
PHA0510039	2.0	3.0
PHA0510040	3.0	3.0
PHA0510046	2.0	3.0
PHA0510047	2.0	3.0
PHA0510048	2.0	3.0
PHA0510049	3.0	3.0
PHA0510050	3.0	3.0
PHA0610005A	2.0	2.0
PHA0610005B	1.0	1.0
PHA0610006A	1.0	2.0
PHA0610006B	1.0	3.0
PHA0610007A	2.0	1.0
PHA0610007B	2.0	2.0
PHA0610015	3.0	3.0
PHA0610016	3.0	3.0
PHA0610017	3.0	3.0
PHA0610018	3.0	3.0
PHA0610019A	1.0	3.0
PHA0610019B	2.0	3.0
PHA0610025	3.0	4.0
PHA0610026	2.0	4.0
PHA0709008	2.0	3.0
PHA0710009	2.0	3.0
PHA0710010	3.0	3.0
PHA0710011	3.0	4.0
PHA0710012	3.0	4.0
PHA0710013	3.0	4.0
PHA0710014	3.0	3.0
PHA0710015	3.0	3.0
PHA0710016	3.0	2.0
PHA0710017	2.0	4.0
PHA0710018	3.0	3.0
PHA0710019	2.0	4.0
PHA0710021	3.0	3.0
PHA0809009	2.0	3.0
PHA0809010	2.0	3.0
PHA0810001	3.0	4.0
PHA0810002	2.0	3.0
PHA0810003	2.0	3.0
PHA0810004	2.0	2.0
PHA0810006	2.0	2.0
PHA0810008	2.0	3.0
PHA0810009	2.0	3.0
PHA0810010	2.0	3.0
PHA0810011	2.0	3.0
PHA0810012	2.0	3.0
PHA0810015	4.0	4.0
PHA0811010	2.0	2.0
PHA0811012	4.0	4.0
PHA0811013	3.0	3.0
PHA0811014	2.0	3.0
PHA0811016	2.0	2.0
PHA0811017	3.0	3.0
PHA0811019	3.0	4.0
PHA0811020	2.0	4.0
PHA1109001	2.0	3.0
PHA1109002	3.0	4.0
PHA1109003	2.0	2.0
PHA1109004	2.0	4.0
PHA1109005	2.0	2.0
PHA1109006	3.0	3.0
PHA1109007	2.0	3.0
PHA1109008	1.0	2.0
PHA1109023	2.0	2.0
PHA1109024	3.0	4.0
PHA1109025	1.0	2.0
PHA1109026	2.0	3.0
PHA1109027	2.0	3.0
PHA1109028	2.0	3.0
PHA1110001A	2.0	2.0
PHA1110001B	2.0	2.0
PHA1110002A	2.0	2.0
PHA1110002B	2.0	2.0
PHA1110003A	2.0	3.0
PHA1110003B	2.0	3.0
PHA1110004A	2.0	3.0
PHA1110013	3.0	4.0
PHA1110014	2.0	4.0
PHA1110015	3.0	3.0
PHA1110016	1.0	2.0
PHA1110017	2.0	3.0
PHA1110019	3.0	3.0
PHA1110021	2.0	3.0
PHA1110022	3.0	4.0
PHA1111001A	2.0	2.0
PHA1111001B	1.0	2.0
PHA1111002A	2.0	1.0
PHA1111002B	1.0	2.0
PHA1111003A	3.0	2.0
PHA1111003B	2.0	2.0
PHA1111004A	1.0	2.0
PHA1111004B	1.0	2.0
PHA1111006A	2.0	2.0
PHA1111006B	1.0	2.0
PHA1111008A	2.0	2.0
PHA1111008B	2.0	3.0
PHA1111009A	1.0	2.0
ST071122B	2.0	3.0
TI071122B	2.0	3.0
VAR0209036	3.0	4.0
VAR0909003	2.0	3.0
VAR0909004	2.0	3.0
VAR0909005	2.0	3.0
VAR0909006	3.0	4.0
VAR0909007	2.0	3.0
VAR0909008	2.0	2.0
VAR0909009	2.0	4.0
VAR0909010	2.0	3.0
VAR0910004	2.0	3.0
VAR0910005	1.0	2.0
VAR0910006	2.0	3.0
VAR0910007	2.0	3.0
VAR0910009	2.0	3.0
VAR0910010	2.0	3.0
VAR0910011	2.0	3.0
Language = CZ, Weighted F1-score = 0.36917681540290154, Dimension = Orthography

Train on DE, test on  IT
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
nan nan
LANGUAGE: IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']


======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 1.36
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.68
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       1.00      0.01      0.01       143
         2.0       0.45      0.83      0.58       220
         3.0       0.14      0.73      0.24        77
         4.0       0.00      0.00      0.00       262
         5.0       0.00      0.00      0.00        76

    accuracy                           0.30       800
   macro avg       0.27      0.26      0.14       800
weighted avg       0.32      0.30      0.19       800

[[  0   0  21   1   0   0]
 [  0   1 125  17   0   0]
 [  0   0 183  37   0   0]
 [  0   0  21  56   0   0]
 [  0   0  41 221   0   0]
 [  0   0  18  58   0   0]]
0.18558204891738317
800 800 800



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 1.06
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.83
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.25      0.01      0.01       143
         2.0       0.46      0.77      0.58       220
         3.0       0.14      0.77      0.24        77
         4.0       0.00      0.00      0.00       262
         5.0       0.00      0.00      0.00        76

    accuracy                           0.29       800
   macro avg       0.14      0.26      0.14       800
weighted avg       0.18      0.29      0.18       800

[[  0   3  18   1   0   0]
 [  0   1 121  21   0   0]
 [  0   0 170  50   0   0]
 [  0   0  18  59   0   0]
 [  0   0  31 231   0   0]
 [  0   0  13  63   0   0]]
0.18326290456118138
800 800 800



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 0.92
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 2.03
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.44      0.08      0.14       143
         2.0       0.41      0.37      0.39       220
         3.0       0.12      0.86      0.21        77
         4.0       0.73      0.09      0.16       262
         5.0       0.00      0.00      0.00        76

    accuracy                           0.23       800
   macro avg       0.28      0.23      0.15       800
weighted avg       0.44      0.23      0.21       800

[[  0   8  13   1   0   0]
 [  0  12  82  49   0   0]
 [  0   6  81 133   0   0]
 [  0   0  10  66   1   0]
 [  0   0  11 227  24   0]
 [  0   1   2  65   8   0]]
0.20540626447695398
800 800 800



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 0.80
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.98
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.50      0.02      0.04       143
         2.0       0.41      0.43      0.42       220
         3.0       0.14      0.84      0.24        77
         4.0       0.77      0.30      0.43       262
         5.0       0.00      0.00      0.00        76

    accuracy                           0.30       800
   macro avg       0.30      0.27      0.19       800
weighted avg       0.47      0.30      0.29       800

[[  0   3  18   1   0   0]
 [  0   3  96  44   0   0]
 [  0   0  94 124   2   0]
 [  0   0   7  65   5   0]
 [  0   0  13 170  79   0]
 [  0   0   3  56  17   0]]
0.286900001085273
800 800 800
Filename	True Label	Prediction
1325_1001008	5.0	3.0
1325_1001009	5.0	3.0
1325_1001010	5.0	3.0
1325_1001011	4.0	3.0
1325_1001012	4.0	3.0
1325_1001013	4.0	3.0
1325_1001014	4.0	3.0
1325_1001015	4.0	3.0
1325_1001016	5.0	3.0
1325_1001017	4.0	3.0
1325_1001018	4.0	3.0
1325_1001019	4.0	3.0
1325_1001020	4.0	3.0
1325_1001021	5.0	4.0
1325_1001022	4.0	3.0
1325_1001023	4.0	3.0
1325_1001024	4.0	3.0
1325_1001025	4.0	4.0
1325_1001027	4.0	3.0
1325_1001028	4.0	4.0
1325_1001029	4.0	3.0
1325_1001032	4.0	3.0
1325_1001033	5.0	4.0
1325_1001035	5.0	4.0
1325_1001036	5.0	4.0
1325_1001037	5.0	4.0
1325_1001039	5.0	3.0
1325_1001040	4.0	4.0
1325_1001041	4.0	4.0
1325_1001042	5.0	4.0
1325_1001043	4.0	3.0
1325_1001044	3.0	4.0
1325_1001045	5.0	3.0
1325_1001046	4.0	3.0
1325_1001047	4.0	2.0
1325_1001048	4.0	3.0
1325_1001050	4.0	3.0
1325_1001051	4.0	4.0
1325_1001052	5.0	3.0
1325_1001053	5.0	3.0
1325_1001054	4.0	3.0
1325_1001055	5.0	4.0
1325_1001056	4.0	3.0
1325_1001057	4.0	3.0
1325_1001058	4.0	3.0
1325_1001059	4.0	3.0
1325_1001062	4.0	3.0
1325_1001063	4.0	3.0
1325_1001075	3.0	3.0
1325_1001076	5.0	3.0
1325_1001077	4.0	3.0
1325_1001078	4.0	3.0
1325_1001079	4.0	4.0
1325_1001080	4.0	4.0
1325_1001081	4.0	3.0
1325_1001082	5.0	3.0
1325_1001083	5.0	3.0
1325_1001084	4.0	3.0
1325_1001085	4.0	4.0
1325_1001086	4.0	4.0
1325_1001087	4.0	4.0
1325_1001088	4.0	3.0
1325_1001089	3.0	3.0
1325_1001090	4.0	3.0
1325_1001091	4.0	3.0
1325_1001092	4.0	3.0
1325_1001093	4.0	4.0
1325_1001094	4.0	3.0
1325_1001095	5.0	3.0
1325_1001096	4.0	3.0
1325_1001097	1.0	3.0
1325_1001098	4.0	3.0
1325_1001099	4.0	4.0
1325_1001100	4.0	3.0
1325_1001101	4.0	3.0
1325_1001107	4.0	4.0
1325_1001108	4.0	4.0
1325_1001109	4.0	3.0
1325_1001110	4.0	4.0
1325_1001111	4.0	4.0
1325_1001113	4.0	4.0
1325_1001119	4.0	3.0
1325_1001120	5.0	3.0
1325_1001121	4.0	3.0
1325_1001122	4.0	3.0
1325_1001123	4.0	4.0
1325_1001124	3.0	3.0
1325_1001125	4.0	4.0
1325_1001126	3.0	3.0
1325_1001127	4.0	3.0
1325_1001128	4.0	4.0
1325_1001129	3.0	3.0
1325_1001130	4.0	3.0
1325_1001131	3.0	3.0
1325_1001132	4.0	4.0
1325_1001133	5.0	3.0
1325_1001134	4.0	4.0
1325_1001135	3.0	4.0
1325_1001136	3.0	4.0
1325_1001138	4.0	4.0
1325_1001139	4.0	4.0
1325_1001141	2.0	3.0
1325_1001142	4.0	4.0
1325_1001143	4.0	4.0
1325_1001144	4.0	4.0
1325_1001152	4.0	3.0
1325_1001153	4.0	3.0
1325_1001154	4.0	4.0
1325_1001155	4.0	4.0
1325_1001156	4.0	4.0
1325_1001157	4.0	4.0
1325_1001158	4.0	4.0
1325_1001159	5.0	4.0
1325_1001160	4.0	4.0
1325_1001161	4.0	3.0
1325_1001162	4.0	3.0
1325_1001163	4.0	4.0
1325_1001164	3.0	3.0
1325_1001165	4.0	3.0
1325_1001166	4.0	3.0
1325_1001167	4.0	4.0
1325_1001168	4.0	3.0
1325_1001169	4.0	3.0
1325_1001170	4.0	3.0
1325_9000059	5.0	4.0
1325_9000087	5.0	4.0
1325_9000088	4.0	4.0
1325_9000089	4.0	3.0
1325_9000090	4.0	4.0
1325_9000095	4.0	4.0
1325_9000099	5.0	3.0
1325_9000102	3.0	2.0
1325_9000104	3.0	3.0
1325_9000105	4.0	3.0
1325_9000106	4.0	4.0
1325_9000107	4.0	4.0
1325_9000136	4.0	4.0
1325_9000137	4.0	4.0
1325_9000138	4.0	4.0
1325_9000139	4.0	4.0
1325_9000140	4.0	3.0
1325_9000143	4.0	4.0
1325_9000144	4.0	4.0
1325_9000152	4.0	4.0
1325_9000185	4.0	4.0
1325_9000186	4.0	3.0
1325_9000187	4.0	3.0
1325_9000188	4.0	4.0
1325_9000209	4.0	3.0
1325_9000210	3.0	3.0
1325_9000211	4.0	3.0
1325_9000213	4.0	3.0
1325_9000214	4.0	3.0
1325_9000215	4.0	4.0
1325_9000237	3.0	3.0
1325_9000239	3.0	3.0
1325_9000240	2.0	3.0
1325_9000241	4.0	4.0
1325_9000278	3.0	3.0
1325_9000279	3.0	3.0
1325_9000296	3.0	3.0
1325_9000302	3.0	3.0
1325_9000303	4.0	3.0
1325_9000304	3.0	3.0
1325_9000314	4.0	3.0
1325_9000315	3.0	3.0
1325_9000316	3.0	3.0
1325_9000317	4.0	3.0
1325_9000318	4.0	3.0
1325_9000319	4.0	3.0
1325_9000320	3.0	3.0
1325_9000321	4.0	4.0
1325_9000322	4.0	3.0
1325_9000323	4.0	3.0
1325_9000503	4.0	4.0
1325_9000504	4.0	4.0
1325_9000505	4.0	4.0
1325_9000533	4.0	4.0
1325_9000534	4.0	4.0
1325_9000536	4.0	4.0
1325_9000554	3.0	3.0
1325_9000601	4.0	4.0
1325_9000602	4.0	4.0
1325_9000611	3.0	3.0
1325_9000612	2.0	3.0
1325_9000674	4.0	4.0
1325_9000675	4.0	3.0
1325_9000676	3.0	3.0
1325_9000677	4.0	3.0
1325_9000678	4.0	4.0
1325_9000684	4.0	3.0
1325_9000685	4.0	4.0
1325_9000686	3.0	4.0
1325_9000700	4.0	4.0
1325_9000750	4.0	2.0
1365_0100002	4.0	2.0
1365_0100003	2.0	2.0
1365_0100004	3.0	3.0
1365_0100005	3.0	3.0
1365_0100006	4.0	3.0
1365_0100007	2.0	2.0
1365_0100008	4.0	3.0
1365_0100009	3.0	2.0
1365_0100010	1.0	2.0
1365_0100011	4.0	3.0
1365_0100012	4.0	3.0
1365_0100013	4.0	3.0
1365_0100014	4.0	3.0
1365_0100015	4.0	2.0
1365_0100016	4.0	3.0
1365_0100017	4.0	3.0
1365_0100018	3.0	2.0
1365_0100019	4.0	2.0
1365_0100020	3.0	3.0
1365_0100021	3.0	2.0
1365_0100022	4.0	3.0
1365_0100023	2.0	2.0
1365_0100024	4.0	3.0
1365_0100026	2.0	2.0
1365_0100027	4.0	3.0
1365_0100028	4.0	3.0
1365_0100029	1.0	2.0
1365_0100030	4.0	2.0
1365_0100031	5.0	2.0
1365_0100051	2.0	3.0
1365_0100056	4.0	3.0
1365_0100057	4.0	3.0
1365_0100058	4.0	3.0
1365_0100061	5.0	3.0
1365_0100063	4.0	3.0
1365_0100064	4.0	3.0
1365_0100065	2.0	2.0
1365_0100066	3.0	3.0
1365_0100067	3.0	3.0
1365_0100069	4.0	3.0
1365_0100070	4.0	3.0
1365_0100071	4.0	3.0
1365_0100072	4.0	3.0
1365_0100073	3.0	3.0
1365_0100074	3.0	3.0
1365_0100079	5.0	3.0
1365_0100080	4.0	3.0
1365_0100092	4.0	2.0
1365_0100093	4.0	3.0
1365_0100094	5.0	3.0
1365_0100095	4.0	3.0
1365_0100096	4.0	3.0
1365_0100097	4.0	3.0
1365_0100098	2.0	3.0
1365_0100099	3.0	3.0
1365_0100100	4.0	3.0
1365_0100101	4.0	3.0
1365_0100102	4.0	3.0
1365_0100103	5.0	3.0
1365_0100104	3.0	3.0
1365_0100105	5.0	3.0
1365_0100106	3.0	3.0
1365_0100107	4.0	3.0
1365_0100116	4.0	3.0
1365_0100117	5.0	3.0
1365_0100118	4.0	3.0
1365_0100119	5.0	4.0
1365_0100120	5.0	4.0
1365_0100121	4.0	4.0
1365_0100123	4.0	3.0
1365_0100125	4.0	3.0
1365_0100133	5.0	3.0
1365_0100134	5.0	3.0
1365_0100135	4.0	3.0
1365_0100136	4.0	3.0
1365_0100137	5.0	3.0
1365_0100138	5.0	3.0
1365_0100139	4.0	3.0
1365_0100145	5.0	4.0
1365_0100146	4.0	3.0
1365_0100147	4.0	3.0
1365_0100148	4.0	3.0
1365_0100151	4.0	2.0
1365_0100162	4.0	3.0
1365_0100163	5.0	4.0
1365_0100164	3.0	3.0
1365_0100165	4.0	4.0
1365_0100166	3.0	3.0
1365_0100167	2.0	3.0
1365_0100168	4.0	3.0
1365_0100169	5.0	3.0
1365_0100170	4.0	3.0
1365_0100171	4.0	3.0
1365_0100172	5.0	3.0
1365_0100173	5.0	3.0
1365_0100174	3.0	2.0
1365_0100175	4.0	3.0
1365_0100176	4.0	3.0
1365_0100177	5.0	3.0
1365_0100178	4.0	3.0
1365_0100179	4.0	3.0
1365_0100180	5.0	2.0
1365_0100181	3.0	3.0
1365_0100182	4.0	3.0
1365_0100183	4.0	3.0
1365_0100184	4.0	3.0
1365_0100185	3.0	3.0
1365_0100186	5.0	3.0
1365_0100187	5.0	3.0
1365_0100188	5.0	3.0
1365_0100190	4.0	2.0
1365_0100191	3.0	3.0
1365_0100192	4.0	4.0
1365_0100194	4.0	3.0
1365_0100195	4.0	2.0
1365_0100196	4.0	3.0
1365_0100198	4.0	3.0
1365_0100199	4.0	3.0
1365_0100200	5.0	3.0
1365_0100201	5.0	3.0
1365_0100202	3.0	3.0
1365_0100203	4.0	3.0
1365_0100204	5.0	3.0
1365_0100205	5.0	3.0
1365_0100211	4.0	4.0
1365_0100212	5.0	4.0
1365_0100213	4.0	3.0
1365_0100215	4.0	3.0
1365_0100217	5.0	3.0
1365_0100218	4.0	3.0
1365_0100219	4.0	3.0
1365_0100220	4.0	4.0
1365_0100221	3.0	3.0
1365_0100222	3.0	3.0
1365_0100223	4.0	3.0
1365_0100224	3.0	3.0
1365_0100225	4.0	3.0
1365_0100226	5.0	3.0
1365_0100227	4.0	3.0
1365_0100228	3.0	3.0
1365_0100229	5.0	4.0
1365_0100230	4.0	4.0
1365_0100231	4.0	3.0
1365_0100232	4.0	3.0
1365_0100233	4.0	3.0
1365_0100251	3.0	3.0
1365_0100252	4.0	3.0
1365_0100253	3.0	3.0
1365_0100255	3.0	3.0
1365_0100256	4.0	3.0
1365_0100257	3.0	3.0
1365_0100258	5.0	3.0
1365_0100259	4.0	3.0
1365_0100260	4.0	3.0
1365_0100261	5.0	3.0
1365_0100262	4.0	3.0
1365_0100263	4.0	4.0
1365_0100265	4.0	3.0
1365_0100266	3.0	3.0
1365_0100267	4.0	4.0
1365_0100268	3.0	3.0
1365_0100269	4.0	3.0
1365_0100270	3.0	3.0
1365_0100274	4.0	3.0
1365_0100275	4.0	3.0
1365_0100276	4.0	3.0
1365_0100277	4.0	4.0
1365_0100278	4.0	3.0
1365_0100279	4.0	3.0
1365_0100280	2.0	3.0
1365_0100281	5.0	3.0
1365_0100282	5.0	3.0
1365_0100285	3.0	3.0
1365_0100286	3.0	3.0
1365_0100287	4.0	3.0
1365_0100288	4.0	2.0
1365_0100289	5.0	3.0
1365_0100290	4.0	3.0
1365_0100299	5.0	3.0
1365_0100447	5.0	3.0
1365_0100448	2.0	3.0
1365_0100451	2.0	3.0
1365_0100455	2.0	4.0
1365_0100456	2.0	4.0
1365_0100457	2.0	3.0
1365_0100458	4.0	3.0
1365_0100459	4.0	4.0
1365_0100461	2.0	3.0
1365_0100469	5.0	3.0
1365_0100470	4.0	4.0
1365_0100471	3.0	4.0
1365_0100472	3.0	3.0
1365_0100473	4.0	3.0
1365_0100474	4.0	3.0
1365_0100475	4.0	4.0
1365_0100476	4.0	4.0
1365_0100477	4.0	3.0
1365_0100478	4.0	3.0
1365_0100479	4.0	4.0
1365_0100480	5.0	3.0
1365_0100481	4.0	4.0
1365_0100482	4.0	3.0
1385_0000011	1.0	2.0
1385_0000012	2.0	3.0
1385_0000013	1.0	2.0
1385_0000016	1.0	2.0
1385_0000017	1.0	2.0
1385_0000020	2.0	2.0
1385_0000021	1.0	2.0
1385_0000022	0.0	2.0
1385_0000023	2.0	2.0
1385_0000033	1.0	3.0
1385_0000034	2.0	3.0
1385_0000035	2.0	2.0
1385_0000036	2.0	3.0
1385_0000037	2.0	3.0
1385_0000038	2.0	2.0
1385_0000039	2.0	2.0
1385_0000040	1.0	2.0
1385_0000041	2.0	3.0
1385_0000042	2.0	2.0
1385_0000043	2.0	2.0
1385_0000044	2.0	2.0
1385_0000045	2.0	3.0
1385_0000047	2.0	3.0
1385_0000048	2.0	3.0
1385_0000049	2.0	2.0
1385_0000050	2.0	2.0
1385_0000051	2.0	3.0
1385_0000052	1.0	2.0
1385_0000053	1.0	2.0
1385_0000054	2.0	3.0
1385_0000057	1.0	2.0
1385_0000058	2.0	3.0
1385_0000059	2.0	3.0
1385_0000095	1.0	2.0
1385_0000097	2.0	2.0
1385_0000098	2.0	2.0
1385_0000099	1.0	2.0
1385_0000100	2.0	2.0
1385_0000101	1.0	2.0
1385_0000102	2.0	3.0
1385_0000103	2.0	2.0
1385_0000104	2.0	2.0
1385_0000114	2.0	3.0
1385_0000119	2.0	2.0
1385_0000120	1.0	1.0
1385_0000122	2.0	2.0
1385_0000123	2.0	3.0
1385_0000124	2.0	3.0
1385_0000125	2.0	3.0
1385_0000126	2.0	3.0
1385_0000127	2.0	3.0
1385_0000128	1.0	2.0
1385_0000129	2.0	3.0
1385_0000130	2.0	2.0
1385_0001103	2.0	2.0
1385_0001104	1.0	1.0
1385_0001105	2.0	2.0
1385_0001107	2.0	2.0
1385_0001108	2.0	3.0
1385_0001109	2.0	2.0
1385_0001110	2.0	2.0
1385_0001111	2.0	2.0
1385_0001112	2.0	3.0
1385_0001113	1.0	2.0
1385_0001118	2.0	2.0
1385_0001119	2.0	2.0
1385_0001120	2.0	2.0
1385_0001121	2.0	2.0
1385_0001122	2.0	2.0
1385_0001123	2.0	2.0
1385_0001124	1.0	2.0
1385_0001125	2.0	2.0
1385_0001126	1.0	2.0
1385_0001127	2.0	3.0
1385_0001128	1.0	2.0
1385_0001129	1.0	2.0
1385_0001130	1.0	2.0
1385_0001131	2.0	2.0
1385_0001132	2.0	2.0
1385_0001133	2.0	2.0
1385_0001134	2.0	2.0
1385_0001135	2.0	2.0
1385_0001136	2.0	2.0
1385_0001137	2.0	2.0
1385_0001138	2.0	2.0
1385_0001147	2.0	2.0
1385_0001148	2.0	3.0
1385_0001149	2.0	3.0
1385_0001150	2.0	2.0
1385_0001151	2.0	3.0
1385_0001152	2.0	3.0
1385_0001153	2.0	2.0
1385_0001154	2.0	3.0
1385_0001155	2.0	2.0
1385_0001156	2.0	2.0
1385_0001157	2.0	2.0
1385_0001158	2.0	2.0
1385_0001159	1.0	2.0
1385_0001160	1.0	3.0
1385_0001161	2.0	3.0
1385_0001162	1.0	2.0
1385_0001163	1.0	2.0
1385_0001164	1.0	3.0
1385_0001165	2.0	3.0
1385_0001166	2.0	3.0
1385_0001167	2.0	3.0
1385_0001169	2.0	2.0
1385_0001170	1.0	2.0
1385_0001171	1.0	2.0
1385_0001172	1.0	2.0
1385_0001173	0.0	2.0
1385_0001174	1.0	2.0
1385_0001175	1.0	2.0
1385_0001178	1.0	2.0
1385_0001188	2.0	3.0
1385_0001189	1.0	2.0
1385_0001190	1.0	2.0
1385_0001191	2.0	3.0
1385_0001192	1.0	2.0
1385_0001193	2.0	3.0
1385_0001194	2.0	2.0
1385_0001195	2.0	3.0
1385_0001196	2.0	3.0
1385_0001197	2.0	2.0
1385_0001198	2.0	3.0
1385_0001199	2.0	3.0
1385_0001501	1.0	2.0
1385_0001503	2.0	3.0
1385_0001522	1.0	2.0
1385_0001523	2.0	3.0
1385_0001524	1.0	3.0
1385_0001525	2.0	3.0
1385_0001526	0.0	2.0
1385_0001527	2.0	2.0
1385_0001528	2.0	3.0
1385_0001712	1.0	3.0
1385_0001714	1.0	2.0
1385_0001715	1.0	2.0
1385_0001716	2.0	3.0
1385_0001717	2.0	3.0
1385_0001718	1.0	2.0
1385_0001719	1.0	2.0
1385_0001720	1.0	2.0
1385_0001723	0.0	2.0
1385_0001724	2.0	3.0
1385_0001725	1.0	2.0
1385_0001726	1.0	3.0
1385_0001727	1.0	2.0
1385_0001728	2.0	2.0
1385_0001729	2.0	3.0
1385_0001730	2.0	2.0
1385_0001732	1.0	3.0
1385_0001733	1.0	3.0
1385_0001734	2.0	2.0
1385_0001736	2.0	2.0
1385_0001737	2.0	2.0
1385_0001738	0.0	1.0
1385_0001739	1.0	2.0
1385_0001740	2.0	2.0
1385_0001741	1.0	2.0
1385_0001742	0.0	1.0
1385_0001744	0.0	2.0
1385_0001746	1.0	2.0
1385_0001747	0.0	2.0
1385_0001748	1.0	2.0
1385_0001749	1.0	2.0
1385_0001750	0.0	1.0
1385_0001751	1.0	3.0
1385_0001752	1.0	2.0
1385_0001753	1.0	2.0
1385_0001754	1.0	3.0
1385_0001756	2.0	2.0
1385_0001757	2.0	2.0
1385_0001758	1.0	2.0
1385_0001759	1.0	2.0
1385_0001760	0.0	2.0
1385_0001761	1.0	2.0
1385_0001762	2.0	2.0
1385_0001764	1.0	2.0
1385_0001765	0.0	2.0
1385_0001766	2.0	3.0
1385_0001767	1.0	3.0
1385_0001768	2.0	2.0
1385_0001771	1.0	3.0
1385_0001772	1.0	2.0
1385_0001773	1.0	2.0
1385_0001774	1.0	2.0
1385_0001775	2.0	3.0
1385_0001785	1.0	2.0
1385_0001786	2.0	2.0
1385_0001787	1.0	2.0
1385_0001788	1.0	3.0
1385_0001789	2.0	3.0
1385_0001790	2.0	2.0
1385_0001791	1.0	2.0
1385_0001792	1.0	2.0
1385_0001793	1.0	3.0
1385_0001794	1.0	2.0
1385_0001795	1.0	2.0
1385_0001796	1.0	2.0
1385_0001798	1.0	3.0
1385_0001799	2.0	3.0
1385_0001800	1.0	2.0
1395_0000333	2.0	3.0
1395_0000337	1.0	2.0
1395_0000338	2.0	2.0
1395_0000340	2.0	3.0
1395_0000341	2.0	2.0
1395_0000353	2.0	3.0
1395_0000354	1.0	2.0
1395_0000355	2.0	2.0
1395_0000356	2.0	2.0
1395_0000357	3.0	3.0
1395_0000359	3.0	3.0
1395_0000360	3.0	3.0
1395_0000361	4.0	3.0
1395_0000364	2.0	3.0
1395_0000365	3.0	3.0
1395_0000366	3.0	3.0
1395_0000368	1.0	1.0
1395_0000369	4.0	3.0
1395_0000376	5.0	3.0
1395_0000378	2.0	3.0
1395_0000379	1.0	2.0
1395_0000380	5.0	3.0
1395_0000383	5.0	3.0
1395_0000387	5.0	3.0
1395_0000388	4.0	3.0
1395_0000389	1.0	2.0
1395_0000390	1.0	2.0
1395_0000391	3.0	3.0
1395_0000392	5.0	3.0
1395_0000396	2.0	2.0
1395_0000398	5.0	3.0
1395_0000399	2.0	2.0
1395_0000402	2.0	2.0
1395_0000403	5.0	3.0
1395_0000404	4.0	3.0
1395_0000409	3.0	3.0
1395_0000413	2.0	3.0
1395_0000414	2.0	2.0
1395_0000415	2.0	2.0
1395_0000432	5.0	3.0
1395_0000438	5.0	4.0
1395_0000443	5.0	3.0
1395_0000446	3.0	3.0
1395_0000447	2.0	3.0
1395_0000448	2.0	2.0
1395_0000449	4.0	3.0
1395_0000450	2.0	3.0
1395_0000451	2.0	2.0
1395_0000452	2.0	2.0
1395_0000454	2.0	3.0
1395_0000455	2.0	2.0
1395_0000458	2.0	2.0
1395_0000460	2.0	2.0
1395_0000462	4.0	2.0
1395_0000465	2.0	2.0
1395_0000469	2.0	3.0
1395_0000470	2.0	3.0
1395_0000471	2.0	3.0
1395_0000499	2.0	3.0
1395_0000500	2.0	3.0
1395_0000504	2.0	3.0
1395_0000512	3.0	2.0
1395_0000513	4.0	3.0
1395_0000514	4.0	3.0
1395_0000515	4.0	2.0
1395_0000516	1.0	2.0
1395_0000518	4.0	3.0
1395_0000525	5.0	2.0
1395_0000526	2.0	2.0
1395_0000527	1.0	2.0
1395_0000528	4.0	3.0
1395_0000529	2.0	2.0
1395_0000531	2.0	2.0
1395_0000533	4.0	3.0
1395_0000534	2.0	3.0
1395_0000535	2.0	2.0
1395_0000537	2.0	3.0
1395_0000547	3.0	3.0
1395_0000548	2.0	3.0
1395_0000549	2.0	3.0
1395_0000550	2.0	3.0
1395_0000551	3.0	3.0
1395_0000552	3.0	3.0
1395_0000553	2.0	2.0
1395_0000554	2.0	3.0
1395_0000555	2.0	2.0
1395_0000556	2.0	3.0
1395_0000557	4.0	3.0
1395_0000559	2.0	3.0
1395_0000560	3.0	3.0
1395_0000563	2.0	3.0
1395_0000564	2.0	2.0
1395_0000565	2.0	3.0
1395_0000572	2.0	3.0
1395_0000575	2.0	3.0
1395_0000579	1.0	2.0
1395_0000581	1.0	3.0
1395_0000582	0.0	2.0
1395_0000583	1.0	3.0
1395_0000584	0.0	2.0
1395_0000585	1.0	2.0
1395_0000587	0.0	2.0
1395_0000591	0.0	2.0
1395_0000593	0.0	3.0
1395_0000595	0.0	2.0
1395_0000596	3.0	2.0
1395_0000597	1.0	3.0
1395_0000598	1.0	3.0
1395_0000599	1.0	3.0
1395_0000602	1.0	2.0
1395_0000604	1.0	2.0
1395_0000606	0.0	2.0
1395_0000607	1.0	2.0
1395_0000608	1.0	3.0
1395_0000609	1.0	2.0
1395_0000610	2.0	3.0
1395_0000611	1.0	2.0
1395_0000612	0.0	2.0
1395_0000626	2.0	3.0
1395_0000627	1.0	2.0
1395_0000628	1.0	2.0
1395_0000630	1.0	3.0
1395_0000631	1.0	3.0
1395_0000635	1.0	2.0
1395_0000636	1.0	2.0
1395_0000639	1.0	3.0
1395_0000642	1.0	3.0
1395_0000644	2.0	3.0
1395_0000646	2.0	3.0
1395_0000649	2.0	3.0
1395_0001010	2.0	3.0
1395_0001013	2.0	3.0
1395_0001015	2.0	3.0
1395_0001016	2.0	2.0
1395_0001017	2.0	2.0
1395_0001019	2.0	2.0
1395_0001020	1.0	3.0
1395_0001021	2.0	3.0
1395_0001022	2.0	3.0
1395_0001023	2.0	3.0
1395_0001024	1.0	3.0
1395_0001028	2.0	3.0
1395_0001033	2.0	3.0
1395_0001034	2.0	3.0
1395_0001040	1.0	2.0
1395_0001045	2.0	3.0
1395_0001058	2.0	3.0
1395_0001060	2.0	3.0
1395_0001061	2.0	3.0
1395_0001064	2.0	3.0
1395_0001065	1.0	3.0
1395_0001066	1.0	2.0
1395_0001067	1.0	2.0
1395_0001068	1.0	2.0
1395_0001069	2.0	3.0
1395_0001070	2.0	3.0
1395_0001071	2.0	3.0
1395_0001073	1.0	3.0
1395_0001074	1.0	2.0
1395_0001075	1.0	3.0
1395_0001076	1.0	3.0
1395_0001078	2.0	3.0
1395_0001080	2.0	3.0
1395_0001084	2.0	3.0
1395_0001090	2.0	3.0
1395_0001093	2.0	3.0
1395_0001101	2.0	3.0
1395_0001103	1.0	3.0
1395_0001104	1.0	3.0
1395_0001108	1.0	2.0
1395_0001109	0.0	2.0
1395_0001114	1.0	3.0
1395_0001115	2.0	3.0
1395_0001116	2.0	2.0
1395_0001117	1.0	2.0
1395_0001118	1.0	2.0
1395_0001119	2.0	3.0
1395_0001120	1.0	2.0
1395_0001121	1.0	2.0
1395_0001122	1.0	2.0
1395_0001123	1.0	3.0
1395_0001124	0.0	2.0
1395_0001126	1.0	2.0
1395_0001131	1.0	2.0
1395_0001132	1.0	3.0
1395_0001133	1.0	3.0
1395_0001141	2.0	3.0
1395_0001145	3.0	3.0
1395_0001146	0.0	2.0
1395_0001147	1.0	3.0
1395_0001149	1.0	2.0
1395_0001150	1.0	3.0
1395_0001158	2.0	3.0
1395_0001160	2.0	3.0
1395_0001161	1.0	3.0
1395_0001164	2.0	3.0
1395_0001167	1.0	3.0
1395_0001169	2.0	3.0
1395_0001170	1.0	3.0
1395_0001171	1.0	3.0
Language = IT, Weighted F1-score = 0.286900001085273, Dimension = Orthography

Train on DE, test on  CZ
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
144.90552995391704 65.35717405024758
LANGUAGE: IT
nan nan
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']


======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 1.18
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.14
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.00      0.00      0.00       132
         2.0       0.45      0.80      0.57       141
         3.0       0.78      0.73      0.76       143
         4.0       0.17      0.80      0.28        10

    accuracy                           0.52       434
   macro avg       0.28      0.47      0.32       434
weighted avg       0.41      0.52      0.44       434

[[  0   0   8   0   0]
 [  0   0 132   0   0]
 [  0   0 113  27   1]
 [  0   0   0 105  38]
 [  0   0   0   2   8]]
0.4426192223024482
434 434 434



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 0.85
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.52
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       1.00      0.05      0.10       132
         2.0       0.45      0.73      0.56       141
         3.0       0.64      0.55      0.59       143
         4.0       0.12      0.90      0.21        10

    accuracy                           0.46       434
   macro avg       0.44      0.45      0.29       434
weighted avg       0.66      0.46      0.41       434

[[  0   0   8   0   0]
 [  0   7 118   7   0]
 [  0   0 103  37   1]
 [  0   0   0  79  64]
 [  0   0   0   1   9]]
0.4114337482045593
434 434 434



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36

  Average training loss: 0.76
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.51
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.82      0.14      0.23       132
         2.0       0.42      0.55      0.48       141
         3.0       0.54      0.64      0.59       143
         4.0       0.15      0.90      0.25        10

    accuracy                           0.45       434
   macro avg       0.39      0.44      0.31       434
weighted avg       0.57      0.45      0.42       434

[[  0   3   5   0   0]
 [  0  18 101  13   0]
 [  0   1  77  62   1]
 [  0   0   0  91  52]
 [  0   0   0   1   9]]
0.4247246545745134
434 434 434



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 0.66
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.54
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.81      0.13      0.22       132
         2.0       0.43      0.61      0.51       141
         3.0       0.55      0.52      0.54       143
         4.0       0.12      0.90      0.20        10

    accuracy                           0.43       434
   macro avg       0.38      0.43      0.29       434
weighted avg       0.57      0.43      0.41       434

[[  0   3   5   0   0]
 [  0  17 108   7   0]
 [  0   1  86  53   1]
 [  0   0   0  75  68]
 [  0   0   0   1   9]]
0.41380165915576334
434 434 434
Filename	True Label	Prediction
0601	2.0	2.0
0602	2.0	3.0
0603	2.0	3.0
0604	2.0	2.0
0605	2.0	3.0
0606	2.0	2.0
0607	2.0	2.0
0608	1.0	3.0
0609	1.0	3.0
0610	2.0	3.0
0611	2.0	2.0
0612	1.0	2.0
0613	1.0	2.0
0614	2.0	3.0
0615	2.0	2.0
0616	1.0	2.0
0617	1.0	2.0
0618	2.0	2.0
0619	2.0	2.0
0620	2.0	2.0
0621	2.0	2.0
0622	1.0	2.0
0623	2.0	2.0
0624	2.0	2.0
0625	2.0	2.0
0626	2.0	2.0
0627	2.0	2.0
0628	1.0	2.0
0629	1.0	2.0
0630	1.0	2.0
0631	2.0	2.0
0632	1.0	2.0
0633	2.0	2.0
0634	2.0	2.0
0635	1.0	2.0
0636	2.0	2.0
0637	2.0	2.0
0638	1.0	2.0
0639	1.0	2.0
0640	2.0	2.0
0641	1.0	2.0
0642	2.0	2.0
0643	2.0	2.0
0644	2.0	2.0
0645	2.0	2.0
0714	2.0	2.0
0715	2.0	2.0
0716	2.0	2.0
0717	2.0	2.0
0718	1.0	2.0
0719	2.0	2.0
0720	2.0	2.0
0721	2.0	2.0
0722	2.0	2.0
0723	1.0	2.0
0724	2.0	2.0
0725	2.0	3.0
0801	2.0	2.0
0802	1.0	2.0
0803	1.0	2.0
0804	1.0	2.0
0805	1.0	2.0
0806	1.0	2.0
0807	2.0	2.0
0808	1.0	2.0
0809	2.0	2.0
0810	2.0	2.0
0811	2.0	2.0
0812	2.0	2.0
0813	2.0	2.0
0814	1.0	2.0
0815	2.0	2.0
0816	2.0	2.0
0817	2.0	2.0
0818	2.0	2.0
0819	2.0	2.0
0820	1.0	2.0
0821	2.0	2.0
0822	1.0	2.0
0823	2.0	2.0
0824	2.0	2.0
0825	1.0	2.0
0826	1.0	2.0
0827	1.0	2.0
0828	2.0	2.0
0829	2.0	3.0
0901	2.0	2.0
0902	1.0	2.0
0903	2.0	2.0
0904	2.0	2.0
0905	2.0	2.0
0906	2.0	3.0
0907	2.0	2.0
0910	1.0	2.0
0911	2.0	2.0
0912	2.0	2.0
0913	2.0	2.0
0914	2.0	2.0
0915	2.0	2.0
0916	1.0	2.0
0917	1.0	2.0
0918	2.0	2.0
0919	1.0	2.0
0920	2.0	3.0
0921	2.0	2.0
0922	2.0	2.0
0923	1.0	2.0
0924	2.0	2.0
0925	1.0	2.0
0926	2.0	2.0
0927	1.0	2.0
0928	2.0	3.0
0929	1.0	2.0
0930	1.0	2.0
1001	2.0	3.0
1002	2.0	3.0
1003	2.0	2.0
1004	2.0	2.0
1005	2.0	2.0
1006	2.0	2.0
1007	2.0	2.0
1008	2.0	3.0
1009	2.0	2.0
1010	2.0	2.0
1014	2.0	2.0
1015	2.0	3.0
1016	1.0	2.0
1017	1.0	2.0
1018	1.0	2.0
1019	2.0	3.0
1020	2.0	3.0
1021	2.0	3.0
1022	2.0	2.0
1023	2.0	3.0
1111	2.0	2.0
1112	2.0	3.0
1113	2.0	3.0
1114	2.0	3.0
1115	2.0	3.0
1116	2.0	3.0
1117	1.0	2.0
9999	0.0	2.0
BER0609003	3.0	3.0
BER0611003	2.0	3.0
BER0611005	2.0	3.0
BER0611006	2.0	4.0
BER0611007	3.0	3.0
KYJ0611003A	1.0	2.0
KYJ0611004A	1.0	2.0
KYJ0611005A	1.0	2.0
KYJ0611005B	1.0	2.0
KYJ0611006A	0.0	1.0
KYJ0611006B	0.0	2.0
KYJ0611009A	1.0	2.0
KYJ0611009B	1.0	2.0
LIB0611001A	1.0	2.0
LIB0611001B	1.0	2.0
LIB0611002A	1.0	2.0
LIB0611002B	1.0	2.0
LIB0611003A	1.0	2.0
LIB0611004A	1.0	1.0
LIB0611004B	1.0	2.0
LIB0611011	2.0	3.0
LON0610002A	1.0	2.0
LON0610002B	1.0	2.0
LON0611002A	1.0	1.0
LON0611002B	1.0	2.0
LON0611003	3.0	4.0
LON0611004A	1.0	2.0
LON0611004B	1.0	2.0
MOS0509001	2.0	3.0
MOS0509004	3.0	3.0
MOS0611012	3.0	3.0
MOS0611013	3.0	4.0
MOS0611014	2.0	3.0
MOS0611015	3.0	4.0
PAR1011008A	2.0	2.0
PAR1011009A	1.0	2.0
PAR1011009B	1.0	2.0
PAR1011013	3.0	3.0
PAR1011014	2.0	3.0
PAR1011015	3.0	3.0
PAR1011016	3.0	4.0
PAR1011017	3.0	3.0
PAR1011018	4.0	3.0
PHA0111001A	1.0	1.0
PHA0111001B	1.0	2.0
PHA0111002A	1.0	1.0
PHA0111002B	2.0	3.0
PHA0111003A	1.0	1.0
PHA0111003B	1.0	2.0
PHA0111004A	1.0	1.0
PHA0111004B	1.0	2.0
PHA0111005A	1.0	1.0
PHA0111005B	1.0	2.0
PHA0111010	2.0	3.0
PHA0111011	3.0	3.0
PHA0111012	2.0	3.0
PHA0111014	2.0	3.0
PHA0111015	3.0	3.0
PHA0111016	3.0	4.0
PHA0111018	3.0	3.0
PHA0112002A	1.0	2.0
PHA0112002B	1.0	3.0
PHA0112003A	1.0	2.0
PHA0112003B	1.0	2.0
PHA0112006A	2.0	2.0
PHA0112006B	2.0	3.0
PHA0112007A	1.0	2.0
PHA0112007B	1.0	2.0
PHA0112009A	2.0	2.0
PHA0112009B	1.0	2.0
PHA0112012A	2.0	2.0
PHA0112012B	1.0	3.0
PHA0209001	1.0	3.0
PHA0209008	1.0	2.0
PHA0209013	1.0	2.0
PHA0209024	3.0	3.0
PHA0209026	3.0	3.0
PHA0209028	3.0	3.0
PHA0209031	4.0	4.0
PHA0209034	3.0	4.0
PHA0209038	4.0	4.0
PHA0209039	3.0	4.0
PHA0210001	1.0	2.0
PHA0210004	1.0	2.0
PHA0210007	1.0	3.0
PHA0210008	1.0	2.0
PHA0411008A	1.0	1.0
PHA0411008B	2.0	2.0
PHA0411009A	1.0	2.0
PHA0411009B	1.0	2.0
PHA0411010A	0.0	1.0
PHA0411010B	0.0	2.0
PHA0411011A	1.0	1.0
PHA0411011B	1.0	2.0
PHA0411012A	1.0	1.0
PHA0411012B	1.0	2.0
PHA0411027	2.0	3.0
PHA0411028	2.0	3.0
PHA0411029	3.0	3.0
PHA0411030	3.0	4.0
PHA0411031	3.0	4.0
PHA0411032	3.0	3.0
PHA0411033	3.0	3.0
PHA0411034	2.0	3.0
PHA0411035	2.0	3.0
PHA0411036	3.0	3.0
PHA0411037	3.0	3.0
PHA0411038	3.0	4.0
PHA0411039	3.0	3.0
PHA0411041	3.0	4.0
PHA0411042	3.0	3.0
PHA0411043	3.0	3.0
PHA0411044	4.0	4.0
PHA0411045	3.0	3.0
PHA0411047	3.0	4.0
PHA0411051	4.0	4.0
PHA0411053	4.0	4.0
PHA0411054	3.0	3.0
PHA0411055	3.0	4.0
PHA0411056	3.0	4.0
PHA0411058	3.0	3.0
PHA0411059	3.0	4.0
PHA0411060	2.0	3.0
PHA0411061	3.0	4.0
PHA0411062	3.0	3.0
PHA0509002	1.0	2.0
PHA0509007	1.0	2.0
PHA0509013	1.0	2.0
PHA0509015	3.0	3.0
PHA0509017	3.0	3.0
PHA0509018	3.0	4.0
PHA0509019	3.0	3.0
PHA0509020	3.0	4.0
PHA0509021	2.0	3.0
PHA0509022	4.0	4.0
PHA0509024	3.0	3.0
PHA0509025	3.0	4.0
PHA0509026	3.0	4.0
PHA0509027	3.0	3.0
PHA0509028	3.0	3.0
PHA0509030	3.0	4.0
PHA0509031	2.0	3.0
PHA0509032	3.0	4.0
PHA0509033	2.0	3.0
PHA0509034	3.0	3.0
PHA0509035	3.0	4.0
PHA0509036	3.0	4.0
PHA0509037	2.0	3.0
PHA0509038	2.0	3.0
PHA0509039	3.0	4.0
PHA0509040	3.0	3.0
PHA0509041	3.0	3.0
PHA0509042	3.0	4.0
PHA0509043	3.0	3.0
PHA0509044	3.0	4.0
PHA0509045	3.0	3.0
PHA0510002A	1.0	2.0
PHA0510002B	1.0	2.0
PHA0510003A	1.0	2.0
PHA0510003B	1.0	2.0
PHA0510004A	1.0	2.0
PHA0510004B	1.0	2.0
PHA0510010A	2.0	1.0
PHA0510010B	0.0	1.0
PHA0510013A	1.0	1.0
PHA0510013B	1.0	2.0
PHA0510023	3.0	4.0
PHA0510027	3.0	3.0
PHA0510029	3.0	4.0
PHA0510030	3.0	3.0
PHA0510031	3.0	3.0
PHA0510032	3.0	4.0
PHA0510034	3.0	4.0
PHA0510035	3.0	4.0
PHA0510036	3.0	4.0
PHA0510037	2.0	3.0
PHA0510038	3.0	4.0
PHA0510039	3.0	3.0
PHA0510040	3.0	4.0
PHA0510046	3.0	3.0
PHA0510047	2.0	3.0
PHA0510048	3.0	3.0
PHA0510049	2.0	3.0
PHA0510050	3.0	4.0
PHA0610005A	1.0	1.0
PHA0610005B	0.0	2.0
PHA0610006A	2.0	2.0
PHA0610006B	1.0	2.0
PHA0610007A	1.0	1.0
PHA0610007B	1.0	2.0
PHA0610015	3.0	4.0
PHA0610016	3.0	4.0
PHA0610017	3.0	4.0
PHA0610018	3.0	4.0
PHA0610019A	2.0	2.0
PHA0610019B	1.0	2.0
PHA0610025	3.0	4.0
PHA0610026	3.0	4.0
PHA0709008	3.0	3.0
PHA0710009	3.0	3.0
PHA0710010	3.0	4.0
PHA0710011	3.0	3.0
PHA0710012	3.0	3.0
PHA0710013	4.0	4.0
PHA0710014	3.0	4.0
PHA0710015	3.0	4.0
PHA0710016	3.0	3.0
PHA0710017	3.0	4.0
PHA0710018	3.0	3.0
PHA0710019	3.0	4.0
PHA0710021	3.0	4.0
PHA0809009	3.0	3.0
PHA0809010	2.0	3.0
PHA0810001	3.0	4.0
PHA0810002	3.0	3.0
PHA0810003	3.0	4.0
PHA0810004	3.0	3.0
PHA0810006	3.0	3.0
PHA0810008	3.0	4.0
PHA0810009	3.0	4.0
PHA0810010	3.0	4.0
PHA0810011	3.0	4.0
PHA0810012	3.0	3.0
PHA0810015	3.0	4.0
PHA0811010	3.0	3.0
PHA0811012	3.0	4.0
PHA0811013	4.0	4.0
PHA0811014	3.0	3.0
PHA0811016	3.0	3.0
PHA0811017	3.0	3.0
PHA0811019	4.0	4.0
PHA0811020	3.0	3.0
PHA1109001	1.0	3.0
PHA1109002	3.0	3.0
PHA1109003	2.0	3.0
PHA1109004	3.0	4.0
PHA1109005	2.0	3.0
PHA1109006	2.0	3.0
PHA1109007	2.0	3.0
PHA1109008	1.0	2.0
PHA1109023	2.0	2.0
PHA1109024	3.0	4.0
PHA1109025	2.0	2.0
PHA1109026	3.0	4.0
PHA1109027	3.0	3.0
PHA1109028	3.0	4.0
PHA1110001A	1.0	2.0
PHA1110001B	1.0	2.0
PHA1110002A	2.0	2.0
PHA1110002B	1.0	2.0
PHA1110003A	1.0	2.0
PHA1110003B	0.0	2.0
PHA1110004A	1.0	2.0
PHA1110013	3.0	4.0
PHA1110014	3.0	4.0
PHA1110015	3.0	4.0
PHA1110016	3.0	3.0
PHA1110017	3.0	3.0
PHA1110019	3.0	3.0
PHA1110021	3.0	3.0
PHA1110022	3.0	4.0
PHA1111001A	2.0	2.0
PHA1111001B	1.0	2.0
PHA1111002A	1.0	1.0
PHA1111002B	1.0	2.0
PHA1111003A	1.0	2.0
PHA1111003B	1.0	2.0
PHA1111004A	1.0	2.0
PHA1111004B	1.0	2.0
PHA1111006A	1.0	1.0
PHA1111006B	1.0	2.0
PHA1111008A	1.0	1.0
PHA1111008B	1.0	2.0
PHA1111009A	1.0	1.0
ST071122B	1.0	2.0
TI071122B	1.0	2.0
VAR0209036	2.0	3.0
VAR0909003	3.0	3.0
VAR0909004	3.0	3.0
VAR0909005	3.0	3.0
VAR0909006	3.0	4.0
VAR0909007	3.0	3.0
VAR0909008	3.0	3.0
VAR0909009	3.0	3.0
VAR0909010	3.0	3.0
VAR0910004	3.0	4.0
VAR0910005	3.0	3.0
VAR0910006	3.0	4.0
VAR0910007	3.0	3.0
VAR0910009	3.0	4.0
VAR0910010	3.0	3.0
VAR0910011	3.0	3.0
Language = CZ, Weighted F1-score = 0.41380165915576334, Dimension = Vocabularyrange

Train on DE, test on  IT
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
nan nan
LANGUAGE: IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']


======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 1.23
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.16
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        39
         1.0       0.00      0.00      0.00       243
         2.0       0.43      0.66      0.52       328
         3.0       0.59      0.90      0.71       188
         4.0       0.00      0.00      0.00         2

    accuracy                           0.48       800
   macro avg       0.20      0.31      0.25       800
weighted avg       0.31      0.48      0.38       800

[[  0   0  39   0   0]
 [  0   0 235   8   0]
 [  0   0 215 110   3]
 [  0   0  10 170   8]
 [  0   0   0   2   0]]
0.3803349810020592
800 800 800



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 0.86
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.21
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        39
         1.0       0.00      0.00      0.00       243
         2.0       0.44      0.73      0.55       328
         3.0       0.65      0.89      0.75       188
         4.0       0.00      0.00      0.00         2

    accuracy                           0.51       800
   macro avg       0.22      0.32      0.26       800
weighted avg       0.33      0.51      0.40       800

[[  0   2  37   0   0]
 [  0   0 243   0   0]
 [  0   0 239  89   0]
 [  0   0  20 168   0]
 [  0   0   0   2   0]]
0.4026881245984379
800 800 800



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36

  Average training loss: 0.76
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 0.91
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        39
         1.0       0.62      0.35      0.44       243
         2.0       0.54      0.67      0.60       328
         3.0       0.64      0.88      0.74       188
         4.0       0.00      0.00      0.00         2

    accuracy                           0.59       800
   macro avg       0.36      0.38      0.36       800
weighted avg       0.56      0.59      0.56       800

[[  0  34   5   0   0]
 [  0  84 159   0   0]
 [  0  18 221  89   0]
 [  0   0  23 165   0]
 [  0   0   0   2   0]]
0.5555287877214142
800 800 800



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 0.66
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.07
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        39
         1.0       0.60      0.20      0.30       243
         2.0       0.50      0.67      0.57       328
         3.0       0.62      0.92      0.74       188
         4.0       0.00      0.00      0.00         2

    accuracy                           0.55       800
   macro avg       0.34      0.36      0.32       800
weighted avg       0.53      0.55      0.50       800

[[  0  27  12   0   0]
 [  0  49 194   0   0]
 [  0   6 220 102   0]
 [  0   0  15 173   0]
 [  0   0   0   2   0]]
0.5010428998592051
800 800 800
Filename	True Label	Prediction
1325_1001008	3.0	3.0
1325_1001009	3.0	3.0
1325_1001010	3.0	3.0
1325_1001011	3.0	3.0
1325_1001012	3.0	3.0
1325_1001013	3.0	3.0
1325_1001014	3.0	3.0
1325_1001015	3.0	3.0
1325_1001016	2.0	3.0
1325_1001017	3.0	3.0
1325_1001018	3.0	3.0
1325_1001019	3.0	3.0
1325_1001020	3.0	3.0
1325_1001021	3.0	3.0
1325_1001022	3.0	3.0
1325_1001023	3.0	3.0
1325_1001024	3.0	3.0
1325_1001025	2.0	3.0
1325_1001027	3.0	3.0
1325_1001028	3.0	3.0
1325_1001029	3.0	3.0
1325_1001032	3.0	3.0
1325_1001033	3.0	3.0
1325_1001035	3.0	3.0
1325_1001036	3.0	3.0
1325_1001037	2.0	3.0
1325_1001039	3.0	3.0
1325_1001040	3.0	3.0
1325_1001041	3.0	3.0
1325_1001042	2.0	3.0
1325_1001043	3.0	3.0
1325_1001044	3.0	3.0
1325_1001045	3.0	3.0
1325_1001046	2.0	3.0
1325_1001047	3.0	2.0
1325_1001048	2.0	3.0
1325_1001050	3.0	3.0
1325_1001051	3.0	3.0
1325_1001052	2.0	3.0
1325_1001053	2.0	2.0
1325_1001054	3.0	3.0
1325_1001055	3.0	3.0
1325_1001056	3.0	3.0
1325_1001057	2.0	3.0
1325_1001058	2.0	3.0
1325_1001059	3.0	3.0
1325_1001062	3.0	3.0
1325_1001063	2.0	3.0
1325_1001075	2.0	3.0
1325_1001076	3.0	3.0
1325_1001077	3.0	3.0
1325_1001078	3.0	3.0
1325_1001079	3.0	3.0
1325_1001080	2.0	3.0
1325_1001081	3.0	3.0
1325_1001082	3.0	3.0
1325_1001083	3.0	3.0
1325_1001084	3.0	3.0
1325_1001085	3.0	3.0
1325_1001086	3.0	3.0
1325_1001087	2.0	3.0
1325_1001088	2.0	3.0
1325_1001089	3.0	3.0
1325_1001090	2.0	3.0
1325_1001091	3.0	3.0
1325_1001092	2.0	3.0
1325_1001093	2.0	3.0
1325_1001094	2.0	3.0
1325_1001095	2.0	3.0
1325_1001096	3.0	3.0
1325_1001097	1.0	2.0
1325_1001098	3.0	3.0
1325_1001099	3.0	3.0
1325_1001100	2.0	3.0
1325_1001101	3.0	3.0
1325_1001107	3.0	3.0
1325_1001108	3.0	3.0
1325_1001109	2.0	3.0
1325_1001110	3.0	3.0
1325_1001111	3.0	3.0
1325_1001113	3.0	3.0
1325_1001119	3.0	3.0
1325_1001120	3.0	3.0
1325_1001121	2.0	3.0
1325_1001122	3.0	3.0
1325_1001123	3.0	3.0
1325_1001124	3.0	3.0
1325_1001125	3.0	3.0
1325_1001126	2.0	3.0
1325_1001127	3.0	3.0
1325_1001128	3.0	3.0
1325_1001129	2.0	3.0
1325_1001130	3.0	3.0
1325_1001131	3.0	3.0
1325_1001132	3.0	3.0
1325_1001133	3.0	3.0
1325_1001134	2.0	3.0
1325_1001135	3.0	3.0
1325_1001136	3.0	3.0
1325_1001138	2.0	3.0
1325_1001139	3.0	3.0
1325_1001141	2.0	3.0
1325_1001142	3.0	3.0
1325_1001143	3.0	3.0
1325_1001144	3.0	3.0
1325_1001152	3.0	3.0
1325_1001153	2.0	3.0
1325_1001154	3.0	3.0
1325_1001155	3.0	3.0
1325_1001156	3.0	3.0
1325_1001157	3.0	3.0
1325_1001158	3.0	3.0
1325_1001159	3.0	3.0
1325_1001160	3.0	3.0
1325_1001161	3.0	3.0
1325_1001162	3.0	3.0
1325_1001163	2.0	3.0
1325_1001164	3.0	3.0
1325_1001165	2.0	3.0
1325_1001166	3.0	3.0
1325_1001167	3.0	3.0
1325_1001168	3.0	3.0
1325_1001169	3.0	3.0
1325_1001170	3.0	3.0
1325_9000059	3.0	3.0
1325_9000087	2.0	3.0
1325_9000088	2.0	3.0
1325_9000089	2.0	3.0
1325_9000090	2.0	3.0
1325_9000095	3.0	3.0
1325_9000099	2.0	3.0
1325_9000102	3.0	2.0
1325_9000104	3.0	3.0
1325_9000105	2.0	3.0
1325_9000106	3.0	3.0
1325_9000107	3.0	3.0
1325_9000136	3.0	3.0
1325_9000137	3.0	3.0
1325_9000138	4.0	3.0
1325_9000139	3.0	3.0
1325_9000140	3.0	3.0
1325_9000143	3.0	3.0
1325_9000144	3.0	3.0
1325_9000152	3.0	3.0
1325_9000185	3.0	3.0
1325_9000186	3.0	3.0
1325_9000187	3.0	3.0
1325_9000188	3.0	3.0
1325_9000209	3.0	3.0
1325_9000210	2.0	3.0
1325_9000211	3.0	3.0
1325_9000213	3.0	3.0
1325_9000214	3.0	3.0
1325_9000215	3.0	3.0
1325_9000237	3.0	3.0
1325_9000239	3.0	3.0
1325_9000240	3.0	3.0
1325_9000241	3.0	3.0
1325_9000278	3.0	3.0
1325_9000279	3.0	3.0
1325_9000296	3.0	3.0
1325_9000302	3.0	3.0
1325_9000303	3.0	3.0
1325_9000304	3.0	3.0
1325_9000314	3.0	3.0
1325_9000315	2.0	2.0
1325_9000316	3.0	3.0
1325_9000317	3.0	3.0
1325_9000318	3.0	3.0
1325_9000319	3.0	3.0
1325_9000320	3.0	2.0
1325_9000321	3.0	3.0
1325_9000322	3.0	3.0
1325_9000323	3.0	3.0
1325_9000503	3.0	3.0
1325_9000504	3.0	3.0
1325_9000505	3.0	3.0
1325_9000533	3.0	3.0
1325_9000534	3.0	3.0
1325_9000536	3.0	3.0
1325_9000554	3.0	3.0
1325_9000601	3.0	3.0
1325_9000602	4.0	3.0
1325_9000611	3.0	3.0
1325_9000612	2.0	3.0
1325_9000674	3.0	3.0
1325_9000675	3.0	3.0
1325_9000676	3.0	3.0
1325_9000677	3.0	3.0
1325_9000678	3.0	3.0
1325_9000684	3.0	3.0
1325_9000685	3.0	3.0
1325_9000686	3.0	3.0
1325_9000700	3.0	3.0
1325_9000750	3.0	2.0
1365_0100002	2.0	2.0
1365_0100003	2.0	2.0
1365_0100004	2.0	3.0
1365_0100005	2.0	2.0
1365_0100006	2.0	2.0
1365_0100007	1.0	2.0
1365_0100008	2.0	2.0
1365_0100009	2.0	2.0
1365_0100010	2.0	2.0
1365_0100011	2.0	2.0
1365_0100012	2.0	2.0
1365_0100013	3.0	3.0
1365_0100014	2.0	2.0
1365_0100015	2.0	2.0
1365_0100016	2.0	3.0
1365_0100017	2.0	3.0
1365_0100018	2.0	2.0
1365_0100019	2.0	2.0
1365_0100020	2.0	2.0
1365_0100021	2.0	2.0
1365_0100022	2.0	2.0
1365_0100023	2.0	2.0
1365_0100024	2.0	2.0
1365_0100026	2.0	1.0
1365_0100027	2.0	2.0
1365_0100028	2.0	2.0
1365_0100029	1.0	2.0
1365_0100030	2.0	2.0
1365_0100031	2.0	2.0
1365_0100051	2.0	2.0
1365_0100056	2.0	2.0
1365_0100057	2.0	3.0
1365_0100058	2.0	3.0
1365_0100061	3.0	2.0
1365_0100063	3.0	3.0
1365_0100064	2.0	3.0
1365_0100065	1.0	2.0
1365_0100066	2.0	2.0
1365_0100067	2.0	2.0
1365_0100069	2.0	2.0
1365_0100070	2.0	2.0
1365_0100071	3.0	3.0
1365_0100072	2.0	2.0
1365_0100073	2.0	2.0
1365_0100074	2.0	3.0
1365_0100079	2.0	2.0
1365_0100080	2.0	2.0
1365_0100092	2.0	2.0
1365_0100093	2.0	2.0
1365_0100094	2.0	2.0
1365_0100095	2.0	2.0
1365_0100096	2.0	3.0
1365_0100097	2.0	2.0
1365_0100098	2.0	2.0
1365_0100099	2.0	2.0
1365_0100100	2.0	3.0
1365_0100101	3.0	2.0
1365_0100102	3.0	3.0
1365_0100103	2.0	3.0
1365_0100104	2.0	3.0
1365_0100105	3.0	2.0
1365_0100106	2.0	3.0
1365_0100107	2.0	3.0
1365_0100116	3.0	2.0
1365_0100117	2.0	2.0
1365_0100118	2.0	3.0
1365_0100119	3.0	3.0
1365_0100120	3.0	3.0
1365_0100121	2.0	3.0
1365_0100123	2.0	2.0
1365_0100125	3.0	3.0
1365_0100133	2.0	2.0
1365_0100134	2.0	2.0
1365_0100135	2.0	2.0
1365_0100136	2.0	2.0
1365_0100137	2.0	2.0
1365_0100138	2.0	2.0
1365_0100139	2.0	2.0
1365_0100145	2.0	3.0
1365_0100146	2.0	2.0
1365_0100147	2.0	2.0
1365_0100148	2.0	3.0
1365_0100151	2.0	1.0
1365_0100162	2.0	3.0
1365_0100163	3.0	3.0
1365_0100164	2.0	3.0
1365_0100165	3.0	3.0
1365_0100166	2.0	2.0
1365_0100167	2.0	2.0
1365_0100168	2.0	2.0
1365_0100169	2.0	2.0
1365_0100170	2.0	3.0
1365_0100171	2.0	2.0
1365_0100172	2.0	2.0
1365_0100173	2.0	2.0
1365_0100174	2.0	2.0
1365_0100175	2.0	2.0
1365_0100176	2.0	2.0
1365_0100177	2.0	2.0
1365_0100178	2.0	2.0
1365_0100179	2.0	2.0
1365_0100180	2.0	2.0
1365_0100181	2.0	2.0
1365_0100182	2.0	3.0
1365_0100183	2.0	3.0
1365_0100184	2.0	2.0
1365_0100185	2.0	2.0
1365_0100186	2.0	2.0
1365_0100187	2.0	2.0
1365_0100188	2.0	2.0
1365_0100190	2.0	2.0
1365_0100191	2.0	3.0
1365_0100192	3.0	3.0
1365_0100194	2.0	3.0
1365_0100195	2.0	2.0
1365_0100196	2.0	3.0
1365_0100198	2.0	2.0
1365_0100199	2.0	3.0
1365_0100200	3.0	3.0
1365_0100201	2.0	2.0
1365_0100202	2.0	2.0
1365_0100203	2.0	2.0
1365_0100204	2.0	2.0
1365_0100205	2.0	2.0
1365_0100211	3.0	3.0
1365_0100212	3.0	3.0
1365_0100213	2.0	2.0
1365_0100215	2.0	2.0
1365_0100217	3.0	3.0
1365_0100218	2.0	2.0
1365_0100219	2.0	3.0
1365_0100220	3.0	3.0
1365_0100221	2.0	2.0
1365_0100222	3.0	3.0
1365_0100223	2.0	3.0
1365_0100224	3.0	3.0
1365_0100225	2.0	2.0
1365_0100226	3.0	2.0
1365_0100227	3.0	3.0
1365_0100228	2.0	3.0
1365_0100229	2.0	3.0
1365_0100230	2.0	3.0
1365_0100231	2.0	2.0
1365_0100232	2.0	3.0
1365_0100233	2.0	3.0
1365_0100251	2.0	3.0
1365_0100252	2.0	3.0
1365_0100253	2.0	2.0
1365_0100255	2.0	2.0
1365_0100256	2.0	2.0
1365_0100257	2.0	2.0
1365_0100258	2.0	2.0
1365_0100259	2.0	2.0
1365_0100260	2.0	2.0
1365_0100261	2.0	2.0
1365_0100262	3.0	3.0
1365_0100263	3.0	3.0
1365_0100265	2.0	3.0
1365_0100266	2.0	3.0
1365_0100267	2.0	3.0
1365_0100268	2.0	2.0
1365_0100269	2.0	2.0
1365_0100270	2.0	3.0
1365_0100274	2.0	3.0
1365_0100275	3.0	3.0
1365_0100276	3.0	3.0
1365_0100277	3.0	3.0
1365_0100278	3.0	2.0
1365_0100279	2.0	3.0
1365_0100280	1.0	2.0
1365_0100281	2.0	2.0
1365_0100282	2.0	3.0
1365_0100285	2.0	2.0
1365_0100286	2.0	2.0
1365_0100287	2.0	2.0
1365_0100288	2.0	2.0
1365_0100289	2.0	2.0
1365_0100290	2.0	2.0
1365_0100299	2.0	2.0
1365_0100447	2.0	2.0
1365_0100448	2.0	3.0
1365_0100451	2.0	3.0
1365_0100455	2.0	3.0
1365_0100456	2.0	3.0
1365_0100457	2.0	3.0
1365_0100458	2.0	3.0
1365_0100459	3.0	3.0
1365_0100461	2.0	3.0
1365_0100469	2.0	2.0
1365_0100470	2.0	3.0
1365_0100471	2.0	3.0
1365_0100472	2.0	3.0
1365_0100473	2.0	3.0
1365_0100474	2.0	3.0
1365_0100475	2.0	3.0
1365_0100476	2.0	3.0
1365_0100477	2.0	3.0
1365_0100478	2.0	3.0
1365_0100479	2.0	3.0
1365_0100480	2.0	3.0
1365_0100481	2.0	3.0
1365_0100482	2.0	2.0
1385_0000011	0.0	1.0
1385_0000012	1.0	2.0
1385_0000013	1.0	2.0
1385_0000016	1.0	1.0
1385_0000017	1.0	1.0
1385_0000020	1.0	1.0
1385_0000021	1.0	2.0
1385_0000022	1.0	2.0
1385_0000023	1.0	2.0
1385_0000033	1.0	2.0
1385_0000034	1.0	2.0
1385_0000035	1.0	2.0
1385_0000036	1.0	2.0
1385_0000037	1.0	2.0
1385_0000038	1.0	2.0
1385_0000039	1.0	1.0
1385_0000040	1.0	1.0
1385_0000041	1.0	2.0
1385_0000042	1.0	2.0
1385_0000043	1.0	1.0
1385_0000044	2.0	2.0
1385_0000045	2.0	2.0
1385_0000047	1.0	2.0
1385_0000048	1.0	2.0
1385_0000049	1.0	2.0
1385_0000050	1.0	1.0
1385_0000051	2.0	2.0
1385_0000052	1.0	1.0
1385_0000053	1.0	2.0
1385_0000054	2.0	2.0
1385_0000057	1.0	2.0
1385_0000058	1.0	2.0
1385_0000059	1.0	2.0
1385_0000095	1.0	1.0
1385_0000097	2.0	2.0
1385_0000098	2.0	1.0
1385_0000099	1.0	2.0
1385_0000100	1.0	1.0
1385_0000101	1.0	1.0
1385_0000102	2.0	2.0
1385_0000103	1.0	1.0
1385_0000104	2.0	2.0
1385_0000114	2.0	2.0
1385_0000119	1.0	2.0
1385_0000120	0.0	1.0
1385_0000122	1.0	2.0
1385_0000123	1.0	2.0
1385_0000124	2.0	2.0
1385_0000125	2.0	2.0
1385_0000126	1.0	2.0
1385_0000127	2.0	2.0
1385_0000128	1.0	2.0
1385_0000129	1.0	2.0
1385_0000130	1.0	1.0
1385_0001103	1.0	1.0
1385_0001104	1.0	1.0
1385_0001105	1.0	2.0
1385_0001107	1.0	2.0
1385_0001108	1.0	2.0
1385_0001109	1.0	2.0
1385_0001110	2.0	2.0
1385_0001111	2.0	2.0
1385_0001112	2.0	2.0
1385_0001113	1.0	1.0
1385_0001118	2.0	2.0
1385_0001119	2.0	2.0
1385_0001120	2.0	1.0
1385_0001121	2.0	2.0
1385_0001122	2.0	2.0
1385_0001123	2.0	2.0
1385_0001124	1.0	1.0
1385_0001125	1.0	2.0
1385_0001126	0.0	1.0
1385_0001127	2.0	2.0
1385_0001128	1.0	2.0
1385_0001129	1.0	2.0
1385_0001130	1.0	1.0
1385_0001131	1.0	1.0
1385_0001132	1.0	1.0
1385_0001133	2.0	2.0
1385_0001134	1.0	2.0
1385_0001135	1.0	2.0
1385_0001136	1.0	2.0
1385_0001137	1.0	2.0
1385_0001138	1.0	2.0
1385_0001147	1.0	2.0
1385_0001148	2.0	2.0
1385_0001149	2.0	2.0
1385_0001150	1.0	1.0
1385_0001151	2.0	2.0
1385_0001152	2.0	2.0
1385_0001153	2.0	2.0
1385_0001154	1.0	2.0
1385_0001155	1.0	1.0
1385_0001156	1.0	1.0
1385_0001157	1.0	2.0
1385_0001158	1.0	2.0
1385_0001159	1.0	2.0
1385_0001160	1.0	2.0
1385_0001161	1.0	2.0
1385_0001162	1.0	2.0
1385_0001163	1.0	2.0
1385_0001164	1.0	2.0
1385_0001165	1.0	2.0
1385_0001166	1.0	2.0
1385_0001167	1.0	2.0
1385_0001169	1.0	1.0
1385_0001170	1.0	1.0
1385_0001171	0.0	1.0
1385_0001172	1.0	1.0
1385_0001173	0.0	1.0
1385_0001174	1.0	1.0
1385_0001175	0.0	1.0
1385_0001178	1.0	1.0
1385_0001188	1.0	2.0
1385_0001189	1.0	2.0
1385_0001190	1.0	2.0
1385_0001191	1.0	2.0
1385_0001192	1.0	2.0
1385_0001193	1.0	2.0
1385_0001194	1.0	2.0
1385_0001195	2.0	2.0
1385_0001196	1.0	2.0
1385_0001197	1.0	2.0
1385_0001198	1.0	2.0
1385_0001199	1.0	2.0
1385_0001501	1.0	1.0
1385_0001503	1.0	2.0
1385_0001522	0.0	1.0
1385_0001523	1.0	2.0
1385_0001524	1.0	2.0
1385_0001525	1.0	2.0
1385_0001526	0.0	1.0
1385_0001527	2.0	1.0
1385_0001528	1.0	2.0
1385_0001712	1.0	2.0
1385_0001714	0.0	1.0
1385_0001715	1.0	2.0
1385_0001716	1.0	2.0
1385_0001717	1.0	2.0
1385_0001718	0.0	1.0
1385_0001719	1.0	1.0
1385_0001720	0.0	1.0
1385_0001723	0.0	1.0
1385_0001724	1.0	2.0
1385_0001725	1.0	1.0
1385_0001726	1.0	2.0
1385_0001727	0.0	2.0
1385_0001728	1.0	2.0
1385_0001729	1.0	2.0
1385_0001730	1.0	2.0
1385_0001732	1.0	2.0
1385_0001733	1.0	2.0
1385_0001734	1.0	2.0
1385_0001736	1.0	2.0
1385_0001737	1.0	2.0
1385_0001738	0.0	1.0
1385_0001739	1.0	2.0
1385_0001740	1.0	2.0
1385_0001741	0.0	1.0
1385_0001742	0.0	1.0
1385_0001744	0.0	2.0
1385_0001746	1.0	2.0
1385_0001747	1.0	2.0
1385_0001748	1.0	2.0
1385_0001749	1.0	2.0
1385_0001750	0.0	1.0
1385_0001751	1.0	2.0
1385_0001752	1.0	2.0
1385_0001753	1.0	2.0
1385_0001754	1.0	2.0
1385_0001756	1.0	2.0
1385_0001757	1.0	1.0
1385_0001758	1.0	1.0
1385_0001759	1.0	2.0
1385_0001760	1.0	2.0
1385_0001761	1.0	1.0
1385_0001762	1.0	2.0
1385_0001764	1.0	2.0
1385_0001765	0.0	1.0
1385_0001766	2.0	2.0
1385_0001767	1.0	2.0
1385_0001768	2.0	2.0
1385_0001771	1.0	2.0
1385_0001772	1.0	2.0
1385_0001773	0.0	1.0
1385_0001774	0.0	1.0
1385_0001775	1.0	2.0
1385_0001785	1.0	2.0
1385_0001786	1.0	2.0
1385_0001787	0.0	1.0
1385_0001788	1.0	2.0
1385_0001789	1.0	2.0
1385_0001790	1.0	2.0
1385_0001791	1.0	2.0
1385_0001792	1.0	2.0
1385_0001793	1.0	2.0
1385_0001794	1.0	2.0
1385_0001795	0.0	1.0
1385_0001796	1.0	2.0
1385_0001798	1.0	2.0
1385_0001799	2.0	2.0
1385_0001800	1.0	2.0
1395_0000333	1.0	2.0
1395_0000337	1.0	1.0
1395_0000338	2.0	2.0
1395_0000340	2.0	2.0
1395_0000341	2.0	2.0
1395_0000353	1.0	2.0
1395_0000354	1.0	2.0
1395_0000355	2.0	2.0
1395_0000356	1.0	1.0
1395_0000357	3.0	2.0
1395_0000359	2.0	2.0
1395_0000360	3.0	2.0
1395_0000361	2.0	2.0
1395_0000364	2.0	2.0
1395_0000365	2.0	2.0
1395_0000366	2.0	2.0
1395_0000368	0.0	1.0
1395_0000369	2.0	2.0
1395_0000376	2.0	2.0
1395_0000378	2.0	2.0
1395_0000379	2.0	2.0
1395_0000380	2.0	2.0
1395_0000383	2.0	2.0
1395_0000387	3.0	2.0
1395_0000388	2.0	2.0
1395_0000389	1.0	1.0
1395_0000390	1.0	1.0
1395_0000391	3.0	2.0
1395_0000392	2.0	2.0
1395_0000396	2.0	2.0
1395_0000398	2.0	2.0
1395_0000399	2.0	2.0
1395_0000402	2.0	2.0
1395_0000403	2.0	2.0
1395_0000404	2.0	2.0
1395_0000409	2.0	2.0
1395_0000413	2.0	2.0
1395_0000414	2.0	2.0
1395_0000415	2.0	2.0
1395_0000432	2.0	2.0
1395_0000438	3.0	3.0
1395_0000443	2.0	2.0
1395_0000446	2.0	3.0
1395_0000447	2.0	2.0
1395_0000448	2.0	2.0
1395_0000449	2.0	2.0
1395_0000450	1.0	2.0
1395_0000451	2.0	2.0
1395_0000452	1.0	1.0
1395_0000454	2.0	2.0
1395_0000455	2.0	2.0
1395_0000458	1.0	2.0
1395_0000460	1.0	2.0
1395_0000462	2.0	2.0
1395_0000465	1.0	2.0
1395_0000469	2.0	2.0
1395_0000470	2.0	2.0
1395_0000471	2.0	2.0
1395_0000499	2.0	2.0
1395_0000500	1.0	1.0
1395_0000504	2.0	2.0
1395_0000512	2.0	2.0
1395_0000513	2.0	2.0
1395_0000514	3.0	2.0
1395_0000515	2.0	2.0
1395_0000516	1.0	1.0
1395_0000518	2.0	2.0
1395_0000525	2.0	2.0
1395_0000526	1.0	2.0
1395_0000527	1.0	1.0
1395_0000528	2.0	2.0
1395_0000529	2.0	2.0
1395_0000531	2.0	2.0
1395_0000533	2.0	2.0
1395_0000534	2.0	2.0
1395_0000535	1.0	2.0
1395_0000537	2.0	2.0
1395_0000547	2.0	2.0
1395_0000548	2.0	2.0
1395_0000549	2.0	2.0
1395_0000550	2.0	2.0
1395_0000551	2.0	2.0
1395_0000552	2.0	2.0
1395_0000553	2.0	2.0
1395_0000554	2.0	2.0
1395_0000555	1.0	2.0
1395_0000556	1.0	2.0
1395_0000557	2.0	2.0
1395_0000559	2.0	2.0
1395_0000560	2.0	2.0
1395_0000563	2.0	2.0
1395_0000564	2.0	2.0
1395_0000565	1.0	2.0
1395_0000572	1.0	2.0
1395_0000575	1.0	2.0
1395_0000579	1.0	2.0
1395_0000581	1.0	2.0
1395_0000582	0.0	2.0
1395_0000583	1.0	2.0
1395_0000584	1.0	1.0
1395_0000585	1.0	2.0
1395_0000587	0.0	2.0
1395_0000591	0.0	1.0
1395_0000593	1.0	2.0
1395_0000595	1.0	1.0
1395_0000596	2.0	2.0
1395_0000597	1.0	2.0
1395_0000598	1.0	2.0
1395_0000599	1.0	1.0
1395_0000602	1.0	2.0
1395_0000604	0.0	2.0
1395_0000606	0.0	2.0
1395_0000607	0.0	1.0
1395_0000608	1.0	2.0
1395_0000609	1.0	2.0
1395_0000610	2.0	2.0
1395_0000611	1.0	2.0
1395_0000612	0.0	2.0
1395_0000626	2.0	2.0
1395_0000627	1.0	2.0
1395_0000628	1.0	2.0
1395_0000630	1.0	2.0
1395_0000631	1.0	2.0
1395_0000635	1.0	1.0
1395_0000636	1.0	2.0
1395_0000639	1.0	2.0
1395_0000642	1.0	2.0
1395_0000644	1.0	2.0
1395_0000646	1.0	2.0
1395_0000649	1.0	2.0
1395_0001010	1.0	2.0
1395_0001013	1.0	2.0
1395_0001015	1.0	2.0
1395_0001016	1.0	2.0
1395_0001017	1.0	2.0
1395_0001019	1.0	1.0
1395_0001020	1.0	2.0
1395_0001021	1.0	2.0
1395_0001022	1.0	2.0
1395_0001023	1.0	2.0
1395_0001024	2.0	2.0
1395_0001028	1.0	2.0
1395_0001033	1.0	2.0
1395_0001034	1.0	2.0
1395_0001040	0.0	1.0
1395_0001045	2.0	2.0
1395_0001058	1.0	2.0
1395_0001060	1.0	2.0
1395_0001061	2.0	3.0
1395_0001064	2.0	2.0
1395_0001065	1.0	2.0
1395_0001066	1.0	2.0
1395_0001067	1.0	2.0
1395_0001068	1.0	2.0
1395_0001069	2.0	2.0
1395_0001070	2.0	2.0
1395_0001071	2.0	1.0
1395_0001073	1.0	2.0
1395_0001074	1.0	2.0
1395_0001075	1.0	2.0
1395_0001076	1.0	2.0
1395_0001078	0.0	2.0
1395_0001080	2.0	2.0
1395_0001084	1.0	2.0
1395_0001090	1.0	2.0
1395_0001093	1.0	2.0
1395_0001101	2.0	2.0
1395_0001103	1.0	2.0
1395_0001104	1.0	2.0
1395_0001108	1.0	1.0
1395_0001109	1.0	2.0
1395_0001114	0.0	2.0
1395_0001115	2.0	2.0
1395_0001116	2.0	2.0
1395_0001117	1.0	2.0
1395_0001118	1.0	2.0
1395_0001119	2.0	2.0
1395_0001120	1.0	2.0
1395_0001121	0.0	2.0
1395_0001122	0.0	1.0
1395_0001123	1.0	2.0
1395_0001124	1.0	1.0
1395_0001126	1.0	2.0
1395_0001131	0.0	1.0
1395_0001132	2.0	2.0
1395_0001133	1.0	2.0
1395_0001141	2.0	2.0
1395_0001145	2.0	3.0
1395_0001146	0.0	2.0
1395_0001147	2.0	2.0
1395_0001149	1.0	2.0
1395_0001150	0.0	2.0
1395_0001158	1.0	2.0
1395_0001160	1.0	2.0
1395_0001161	1.0	2.0
1395_0001164	2.0	2.0
1395_0001167	1.0	2.0
1395_0001169	2.0	2.0
1395_0001170	1.0	2.0
1395_0001171	1.0	2.0
Language = IT, Weighted F1-score = 0.5010428998592051, Dimension = Vocabularyrange

Train on DE, test on  CZ
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
144.90552995391704 65.35717405024758
LANGUAGE: IT
nan nan
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']


======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 1.32
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.28
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.60      0.24      0.34       131
         2.0       0.42      0.37      0.39       182
         3.0       0.42      0.93      0.58       100
         4.0       0.00      0.00      0.00        16

    accuracy                           0.44       434
   macro avg       0.29      0.31      0.26       434
weighted avg       0.45      0.44      0.40       434

[[ 0  2  3  0  0]
 [ 0 31 83 17  0]
 [ 0 18 67 97  0]
 [ 0  1  6 93  0]
 [ 0  0  0 16  0]]
0.39973897364899846
434 434 434



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 1.02
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.39
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.60      0.24      0.35       131
         2.0       0.44      0.45      0.44       182
         3.0       0.46      0.92      0.62       100
         4.0       0.00      0.00      0.00        16

    accuracy                           0.47       434
   macro avg       0.30      0.32      0.28       434
weighted avg       0.47      0.47      0.43       434

[[ 0  2  3  0  0]
 [ 0 32 92  7  0]
 [ 0 18 81 83  0]
 [ 0  1  7 92  0]
 [ 0  0  0 16  0]]
0.4333831014424843
434 434 434



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 0.92
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.53
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.67      0.20      0.31       131
         2.0       0.45      0.45      0.45       182
         3.0       0.43      0.93      0.59       100
         4.0       0.00      0.00      0.00        16

    accuracy                           0.46       434
   macro avg       0.31      0.31      0.27       434
weighted avg       0.49      0.46      0.42       434

[[ 0  2  3  0  0]
 [ 0 26 91 14  0]
 [ 0 10 81 91  0]
 [ 0  1  6 93  0]
 [ 0  0  0 16  0]]
0.4159663537349558
434 434 434



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 0.82
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.54
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.65      0.17      0.27       131
         2.0       0.45      0.47      0.46       182
         3.0       0.44      0.92      0.60       100
         4.0       0.00      0.00      0.00        16

    accuracy                           0.46       434
   macro avg       0.31      0.31      0.26       434
weighted avg       0.49      0.46      0.41       434

[[ 0  2  3  0  0]
 [ 0 22 96 13  0]
 [ 0  9 86 87  0]
 [ 0  1  7 92  0]
 [ 0  0  0 16  0]]
0.41100029454597087
434 434 434
Filename	True Label	Prediction
0601	1.0	2.0
0602	1.0	2.0
0603	2.0	2.0
0604	2.0	3.0
0605	2.0	2.0
0606	2.0	2.0
0607	3.0	2.0
0608	1.0	2.0
0609	2.0	2.0
0610	1.0	3.0
0611	2.0	2.0
0612	1.0	2.0
0613	1.0	2.0
0614	2.0	3.0
0615	1.0	2.0
0616	1.0	2.0
0617	1.0	2.0
0618	1.0	2.0
0619	1.0	2.0
0620	1.0	2.0
0621	1.0	2.0
0622	1.0	2.0
0623	1.0	2.0
0624	2.0	2.0
0625	1.0	2.0
0626	2.0	2.0
0627	2.0	2.0
0628	2.0	2.0
0629	2.0	2.0
0630	1.0	1.0
0631	2.0	2.0
0632	1.0	2.0
0633	2.0	2.0
0634	3.0	2.0
0635	2.0	2.0
0636	2.0	2.0
0637	2.0	2.0
0638	2.0	2.0
0639	1.0	2.0
0640	2.0	2.0
0641	1.0	2.0
0642	1.0	2.0
0643	2.0	2.0
0644	1.0	2.0
0645	2.0	2.0
0714	2.0	2.0
0715	2.0	2.0
0716	2.0	2.0
0717	1.0	2.0
0718	2.0	2.0
0719	2.0	2.0
0720	1.0	2.0
0721	2.0	2.0
0722	2.0	2.0
0723	2.0	2.0
0724	3.0	2.0
0725	1.0	2.0
0801	2.0	2.0
0802	2.0	2.0
0803	2.0	2.0
0804	1.0	2.0
0805	2.0	2.0
0806	2.0	2.0
0807	2.0	2.0
0808	2.0	2.0
0809	2.0	2.0
0810	1.0	2.0
0811	2.0	2.0
0812	1.0	2.0
0813	1.0	2.0
0814	2.0	2.0
0815	2.0	2.0
0816	3.0	2.0
0817	1.0	2.0
0818	1.0	2.0
0819	3.0	2.0
0820	2.0	1.0
0821	1.0	2.0
0822	1.0	2.0
0823	1.0	2.0
0824	1.0	2.0
0825	2.0	2.0
0826	2.0	2.0
0827	2.0	2.0
0828	2.0	2.0
0829	1.0	2.0
0901	2.0	2.0
0902	2.0	2.0
0903	1.0	2.0
0904	1.0	2.0
0905	2.0	2.0
0906	2.0	2.0
0907	2.0	2.0
0910	1.0	2.0
0911	1.0	2.0
0912	1.0	2.0
0913	2.0	2.0
0914	1.0	2.0
0915	3.0	2.0
0916	2.0	2.0
0917	1.0	2.0
0918	1.0	2.0
0919	2.0	2.0
0920	2.0	3.0
0921	1.0	2.0
0922	1.0	2.0
0923	2.0	2.0
0924	1.0	2.0
0925	2.0	2.0
0926	2.0	2.0
0927	2.0	2.0
0928	1.0	2.0
0929	1.0	2.0
0930	2.0	2.0
1001	1.0	2.0
1002	2.0	2.0
1003	1.0	2.0
1004	1.0	2.0
1005	1.0	2.0
1006	2.0	2.0
1007	2.0	2.0
1008	1.0	2.0
1009	2.0	2.0
1010	2.0	2.0
1014	2.0	2.0
1015	1.0	2.0
1016	2.0	2.0
1017	2.0	2.0
1018	2.0	2.0
1019	2.0	2.0
1020	2.0	3.0
1021	2.0	2.0
1022	2.0	2.0
1023	2.0	3.0
1111	1.0	2.0
1112	1.0	2.0
1113	2.0	3.0
1114	3.0	3.0
1115	2.0	3.0
1116	2.0	3.0
1117	1.0	2.0
9999	1.0	2.0
BER0609003	3.0	3.0
BER0611003	2.0	3.0
BER0611005	3.0	3.0
BER0611006	3.0	3.0
BER0611007	3.0	3.0
KYJ0611003A	1.0	2.0
KYJ0611004A	1.0	1.0
KYJ0611005A	0.0	1.0
KYJ0611005B	1.0	2.0
KYJ0611006A	1.0	1.0
KYJ0611006B	0.0	2.0
KYJ0611009A	1.0	1.0
KYJ0611009B	0.0	2.0
LIB0611001A	1.0	1.0
LIB0611001B	1.0	2.0
LIB0611002A	1.0	2.0
LIB0611002B	1.0	3.0
LIB0611003A	1.0	1.0
LIB0611004A	1.0	1.0
LIB0611004B	1.0	3.0
LIB0611011	2.0	2.0
LON0610002A	1.0	2.0
LON0610002B	2.0	3.0
LON0611002A	1.0	1.0
LON0611002B	0.0	2.0
LON0611003	3.0	3.0
LON0611004A	0.0	1.0
LON0611004B	1.0	2.0
MOS0509001	2.0	3.0
MOS0509004	2.0	2.0
MOS0611012	2.0	3.0
MOS0611013	2.0	3.0
MOS0611014	2.0	3.0
MOS0611015	2.0	3.0
PAR1011008A	1.0	2.0
PAR1011009A	2.0	2.0
PAR1011009B	1.0	3.0
PAR1011013	2.0	3.0
PAR1011014	3.0	3.0
PAR1011015	2.0	3.0
PAR1011016	2.0	3.0
PAR1011017	3.0	3.0
PAR1011018	3.0	3.0
PHA0111001A	1.0	1.0
PHA0111001B	1.0	2.0
PHA0111002A	2.0	1.0
PHA0111002B	3.0	3.0
PHA0111003A	1.0	1.0
PHA0111003B	1.0	2.0
PHA0111004A	1.0	1.0
PHA0111004B	1.0	2.0
PHA0111005A	1.0	1.0
PHA0111005B	1.0	3.0
PHA0111010	4.0	3.0
PHA0111011	3.0	3.0
PHA0111012	2.0	3.0
PHA0111014	1.0	3.0
PHA0111015	4.0	3.0
PHA0111016	4.0	3.0
PHA0111018	2.0	3.0
PHA0112002A	2.0	1.0
PHA0112002B	1.0	3.0
PHA0112003A	2.0	1.0
PHA0112003B	1.0	2.0
PHA0112006A	3.0	2.0
PHA0112006B	3.0	3.0
PHA0112007A	2.0	1.0
PHA0112007B	1.0	2.0
PHA0112009A	2.0	2.0
PHA0112009B	1.0	2.0
PHA0112012A	2.0	2.0
PHA0112012B	1.0	3.0
PHA0209001	2.0	3.0
PHA0209008	2.0	2.0
PHA0209013	1.0	2.0
PHA0209024	3.0	3.0
PHA0209026	3.0	3.0
PHA0209028	2.0	3.0
PHA0209031	4.0	3.0
PHA0209034	2.0	3.0
PHA0209038	4.0	3.0
PHA0209039	3.0	3.0
PHA0210001	1.0	2.0
PHA0210004	2.0	2.0
PHA0210007	2.0	3.0
PHA0210008	2.0	2.0
PHA0411008A	2.0	1.0
PHA0411008B	1.0	2.0
PHA0411009A	1.0	1.0
PHA0411009B	1.0	3.0
PHA0411010A	1.0	1.0
PHA0411010B	1.0	2.0
PHA0411011A	1.0	1.0
PHA0411011B	1.0	3.0
PHA0411012A	1.0	1.0
PHA0411012B	1.0	2.0
PHA0411027	2.0	3.0
PHA0411028	2.0	3.0
PHA0411029	3.0	3.0
PHA0411030	3.0	3.0
PHA0411031	3.0	3.0
PHA0411032	2.0	3.0
PHA0411033	2.0	3.0
PHA0411034	2.0	2.0
PHA0411035	3.0	3.0
PHA0411036	3.0	3.0
PHA0411037	2.0	3.0
PHA0411038	3.0	3.0
PHA0411039	3.0	3.0
PHA0411041	3.0	3.0
PHA0411042	2.0	3.0
PHA0411043	3.0	3.0
PHA0411044	3.0	3.0
PHA0411045	2.0	3.0
PHA0411047	3.0	3.0
PHA0411051	3.0	3.0
PHA0411053	3.0	3.0
PHA0411054	3.0	3.0
PHA0411055	3.0	3.0
PHA0411056	4.0	3.0
PHA0411058	3.0	3.0
PHA0411059	2.0	3.0
PHA0411060	3.0	3.0
PHA0411061	2.0	3.0
PHA0411062	2.0	3.0
PHA0509002	1.0	2.0
PHA0509007	1.0	3.0
PHA0509013	1.0	2.0
PHA0509015	3.0	3.0
PHA0509017	3.0	3.0
PHA0509018	3.0	3.0
PHA0509019	2.0	3.0
PHA0509020	3.0	3.0
PHA0509021	3.0	3.0
PHA0509022	4.0	3.0
PHA0509024	3.0	3.0
PHA0509025	4.0	3.0
PHA0509026	4.0	3.0
PHA0509027	2.0	3.0
PHA0509028	3.0	3.0
PHA0509030	3.0	3.0
PHA0509031	2.0	3.0
PHA0509032	2.0	3.0
PHA0509033	2.0	3.0
PHA0509034	2.0	3.0
PHA0509035	3.0	3.0
PHA0509036	2.0	3.0
PHA0509037	3.0	3.0
PHA0509038	2.0	3.0
PHA0509039	2.0	3.0
PHA0509040	2.0	3.0
PHA0509041	3.0	3.0
PHA0509042	3.0	3.0
PHA0509043	3.0	3.0
PHA0509044	2.0	3.0
PHA0509045	2.0	3.0
PHA0510002A	2.0	1.0
PHA0510002B	1.0	2.0
PHA0510003A	2.0	2.0
PHA0510003B	1.0	2.0
PHA0510004A	1.0	1.0
PHA0510004B	1.0	2.0
PHA0510010A	2.0	1.0
PHA0510010B	1.0	1.0
PHA0510013A	1.0	2.0
PHA0510013B	1.0	2.0
PHA0510023	4.0	3.0
PHA0510027	2.0	3.0
PHA0510029	3.0	3.0
PHA0510030	2.0	3.0
PHA0510031	3.0	3.0
PHA0510032	2.0	3.0
PHA0510034	3.0	3.0
PHA0510035	2.0	3.0
PHA0510036	3.0	3.0
PHA0510037	2.0	3.0
PHA0510038	3.0	3.0
PHA0510039	2.0	3.0
PHA0510040	2.0	3.0
PHA0510046	2.0	3.0
PHA0510047	2.0	3.0
PHA0510048	3.0	3.0
PHA0510049	2.0	3.0
PHA0510050	2.0	3.0
PHA0610005A	1.0	1.0
PHA0610005B	1.0	2.0
PHA0610006A	2.0	2.0
PHA0610006B	1.0	3.0
PHA0610007A	3.0	1.0
PHA0610007B	1.0	2.0
PHA0610015	2.0	3.0
PHA0610016	3.0	3.0
PHA0610017	3.0	3.0
PHA0610018	3.0	3.0
PHA0610019A	2.0	2.0
PHA0610019B	2.0	3.0
PHA0610025	3.0	3.0
PHA0610026	3.0	3.0
PHA0709008	3.0	3.0
PHA0710009	2.0	3.0
PHA0710010	2.0	3.0
PHA0710011	3.0	3.0
PHA0710012	3.0	3.0
PHA0710013	4.0	3.0
PHA0710014	3.0	3.0
PHA0710015	3.0	3.0
PHA0710016	3.0	3.0
PHA0710017	3.0	3.0
PHA0710018	3.0	3.0
PHA0710019	4.0	3.0
PHA0710021	4.0	3.0
PHA0809009	2.0	3.0
PHA0809010	3.0	3.0
PHA0810001	2.0	3.0
PHA0810002	2.0	3.0
PHA0810003	2.0	3.0
PHA0810004	3.0	3.0
PHA0810006	2.0	3.0
PHA0810008	2.0	3.0
PHA0810009	3.0	3.0
PHA0810010	3.0	3.0
PHA0810011	2.0	3.0
PHA0810012	2.0	3.0
PHA0810015	2.0	3.0
PHA0811010	2.0	3.0
PHA0811012	4.0	3.0
PHA0811013	3.0	3.0
PHA0811014	2.0	3.0
PHA0811016	3.0	3.0
PHA0811017	3.0	3.0
PHA0811019	3.0	3.0
PHA0811020	2.0	3.0
PHA1109001	2.0	3.0
PHA1109002	3.0	3.0
PHA1109003	2.0	2.0
PHA1109004	3.0	3.0
PHA1109005	3.0	3.0
PHA1109006	2.0	3.0
PHA1109007	3.0	3.0
PHA1109008	1.0	2.0
PHA1109023	2.0	2.0
PHA1109024	4.0	3.0
PHA1109025	1.0	2.0
PHA1109026	3.0	3.0
PHA1109027	4.0	3.0
PHA1109028	2.0	3.0
PHA1110001A	2.0	2.0
PHA1110001B	1.0	2.0
PHA1110002A	2.0	2.0
PHA1110002B	2.0	3.0
PHA1110003A	1.0	2.0
PHA1110003B	1.0	3.0
PHA1110004A	2.0	2.0
PHA1110013	2.0	3.0
PHA1110014	3.0	3.0
PHA1110015	3.0	3.0
PHA1110016	2.0	3.0
PHA1110017	2.0	3.0
PHA1110019	2.0	3.0
PHA1110021	3.0	3.0
PHA1110022	3.0	3.0
PHA1111001A	2.0	2.0
PHA1111001B	1.0	2.0
PHA1111002A	1.0	1.0
PHA1111002B	1.0	2.0
PHA1111003A	1.0	2.0
PHA1111003B	1.0	2.0
PHA1111004A	2.0	2.0
PHA1111004B	1.0	2.0
PHA1111006A	1.0	1.0
PHA1111006B	1.0	2.0
PHA1111008A	2.0	1.0
PHA1111008B	1.0	2.0
PHA1111009A	1.0	1.0
ST071122B	1.0	2.0
TI071122B	2.0	3.0
VAR0209036	3.0	3.0
VAR0909003	3.0	3.0
VAR0909004	3.0	3.0
VAR0909005	2.0	3.0
VAR0909006	2.0	3.0
VAR0909007	3.0	3.0
VAR0909008	3.0	3.0
VAR0909009	3.0	3.0
VAR0909010	3.0	3.0
VAR0910004	3.0	3.0
VAR0910005	3.0	3.0
VAR0910006	2.0	3.0
VAR0910007	2.0	3.0
VAR0910009	3.0	3.0
VAR0910010	3.0	3.0
VAR0910011	2.0	3.0
Language = CZ, Weighted F1-score = 0.41100029454597087, Dimension = Vocabularycontrol

Train on DE, test on  IT
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
nan nan
LANGUAGE: IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']


======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 1.36
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.09
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        70
         1.0       0.51      0.73      0.60       204
         2.0       0.59      0.37      0.45       327
         3.0       0.57      0.88      0.69       196
         4.0       0.00      0.00      0.00         3

    accuracy                           0.55       800
   macro avg       0.33      0.40      0.35       800
weighted avg       0.51      0.55      0.51       800

[[  0  56  14   0   0]
 [  0 148  50   6   0]
 [  0  87 120 120   0]
 [  0   2  21 173   0]
 [  0   0   0   3   0]]
0.5064906071389231
800 800 800



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 1.07
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.02
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        70
         1.0       0.49      0.74      0.59       204
         2.0       0.54      0.53      0.54       327
         3.0       0.65      0.58      0.61       196
         4.0       0.00      0.00      0.00         3

    accuracy                           0.55       800
   macro avg       0.34      0.37      0.35       800
weighted avg       0.51      0.55      0.52       800

[[  0  55  15   0   0]
 [  0 150  54   0   0]
 [  0  96 174  57   0]
 [  0   5  78 113   0]
 [  0   0   0   3   0]]
0.5195680894308944
800 800 800



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 0.95
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        70
         1.0       0.49      0.75      0.60       204
         2.0       0.56      0.48      0.52       327
         3.0       0.63      0.68      0.66       196
         4.0       0.00      0.00      0.00         3

    accuracy                           0.56       800
   macro avg       0.34      0.38      0.35       800
weighted avg       0.51      0.56      0.53       800

[[  0  58  12   0   0]
 [  0 153  51   0   0]
 [  0  95 158  74   0]
 [  0   4  59 133   0]
 [  0   0   0   3   0]]
0.5251190016470043
800 800 800



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 0.87
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.02
              precision    recall  f1-score   support

         0.0       1.00      0.03      0.06        70
         1.0       0.49      0.78      0.60       204
         2.0       0.56      0.51      0.54       327
         3.0       0.67      0.60      0.63       196
         4.0       0.00      0.00      0.00         3

    accuracy                           0.56       800
   macro avg       0.54      0.38      0.37       800
weighted avg       0.61      0.56      0.53       800

[[  2  57  11   0   0]
 [  0 159  45   0   0]
 [  0 104 168  55   0]
 [  0   5  74 117   0]
 [  0   0   0   3   0]]
0.5324226379506445
800 800 800
Filename	True Label	Prediction
1325_1001008	3.0	3.0
1325_1001009	3.0	2.0
1325_1001010	3.0	3.0
1325_1001011	3.0	3.0
1325_1001012	3.0	3.0
1325_1001013	3.0	2.0
1325_1001014	3.0	3.0
1325_1001015	2.0	3.0
1325_1001016	2.0	3.0
1325_1001017	3.0	3.0
1325_1001018	2.0	3.0
1325_1001019	3.0	3.0
1325_1001020	2.0	2.0
1325_1001021	3.0	3.0
1325_1001022	3.0	2.0
1325_1001023	3.0	2.0
1325_1001024	2.0	3.0
1325_1001025	2.0	3.0
1325_1001027	3.0	3.0
1325_1001028	3.0	3.0
1325_1001029	3.0	3.0
1325_1001032	2.0	3.0
1325_1001033	3.0	3.0
1325_1001035	3.0	3.0
1325_1001036	3.0	3.0
1325_1001037	3.0	2.0
1325_1001039	3.0	3.0
1325_1001040	3.0	3.0
1325_1001041	3.0	3.0
1325_1001042	3.0	3.0
1325_1001043	3.0	3.0
1325_1001044	3.0	3.0
1325_1001045	3.0	2.0
1325_1001046	2.0	3.0
1325_1001047	2.0	2.0
1325_1001048	2.0	3.0
1325_1001050	3.0	3.0
1325_1001051	3.0	3.0
1325_1001052	2.0	2.0
1325_1001053	2.0	2.0
1325_1001054	3.0	2.0
1325_1001055	3.0	3.0
1325_1001056	2.0	3.0
1325_1001057	2.0	2.0
1325_1001058	3.0	3.0
1325_1001059	2.0	2.0
1325_1001062	3.0	3.0
1325_1001063	2.0	3.0
1325_1001075	2.0	3.0
1325_1001076	2.0	3.0
1325_1001077	3.0	2.0
1325_1001078	3.0	3.0
1325_1001079	3.0	3.0
1325_1001080	3.0	3.0
1325_1001081	2.0	3.0
1325_1001082	2.0	3.0
1325_1001083	2.0	3.0
1325_1001084	3.0	2.0
1325_1001085	3.0	2.0
1325_1001086	3.0	3.0
1325_1001087	3.0	3.0
1325_1001088	2.0	2.0
1325_1001089	2.0	2.0
1325_1001090	3.0	2.0
1325_1001091	2.0	3.0
1325_1001092	2.0	2.0
1325_1001093	2.0	3.0
1325_1001094	2.0	2.0
1325_1001095	2.0	3.0
1325_1001096	2.0	2.0
1325_1001097	1.0	2.0
1325_1001098	2.0	3.0
1325_1001099	3.0	3.0
1325_1001100	3.0	3.0
1325_1001101	3.0	3.0
1325_1001107	3.0	3.0
1325_1001108	3.0	3.0
1325_1001109	2.0	2.0
1325_1001110	3.0	3.0
1325_1001111	3.0	3.0
1325_1001113	3.0	3.0
1325_1001119	3.0	2.0
1325_1001120	3.0	3.0
1325_1001121	3.0	3.0
1325_1001122	2.0	2.0
1325_1001123	3.0	3.0
1325_1001124	3.0	2.0
1325_1001125	3.0	3.0
1325_1001126	2.0	2.0
1325_1001127	3.0	2.0
1325_1001128	3.0	2.0
1325_1001129	2.0	3.0
1325_1001130	3.0	2.0
1325_1001131	3.0	2.0
1325_1001132	3.0	3.0
1325_1001133	3.0	3.0
1325_1001134	3.0	3.0
1325_1001135	3.0	3.0
1325_1001136	2.0	3.0
1325_1001138	2.0	3.0
1325_1001139	3.0	3.0
1325_1001141	2.0	3.0
1325_1001142	2.0	3.0
1325_1001143	3.0	3.0
1325_1001144	3.0	2.0
1325_1001152	3.0	3.0
1325_1001153	3.0	2.0
1325_1001154	3.0	3.0
1325_1001155	3.0	3.0
1325_1001156	2.0	2.0
1325_1001157	2.0	3.0
1325_1001158	3.0	3.0
1325_1001159	3.0	3.0
1325_1001160	3.0	3.0
1325_1001161	2.0	3.0
1325_1001162	2.0	2.0
1325_1001163	2.0	3.0
1325_1001164	3.0	3.0
1325_1001165	2.0	2.0
1325_1001166	3.0	3.0
1325_1001167	3.0	3.0
1325_1001168	2.0	3.0
1325_1001169	3.0	3.0
1325_1001170	3.0	3.0
1325_9000059	2.0	3.0
1325_9000087	2.0	2.0
1325_9000088	2.0	3.0
1325_9000089	3.0	3.0
1325_9000090	3.0	3.0
1325_9000095	2.0	3.0
1325_9000099	3.0	3.0
1325_9000102	3.0	1.0
1325_9000104	3.0	2.0
1325_9000105	2.0	2.0
1325_9000106	3.0	3.0
1325_9000107	3.0	3.0
1325_9000136	3.0	3.0
1325_9000137	3.0	3.0
1325_9000138	4.0	3.0
1325_9000139	3.0	3.0
1325_9000140	3.0	3.0
1325_9000143	3.0	3.0
1325_9000144	3.0	3.0
1325_9000152	3.0	3.0
1325_9000185	3.0	3.0
1325_9000186	3.0	3.0
1325_9000187	3.0	3.0
1325_9000188	3.0	3.0
1325_9000209	3.0	2.0
1325_9000210	2.0	3.0
1325_9000211	2.0	3.0
1325_9000213	3.0	2.0
1325_9000214	3.0	2.0
1325_9000215	3.0	3.0
1325_9000237	3.0	3.0
1325_9000239	3.0	2.0
1325_9000240	2.0	3.0
1325_9000241	3.0	3.0
1325_9000278	3.0	3.0
1325_9000279	3.0	3.0
1325_9000296	2.0	3.0
1325_9000302	2.0	3.0
1325_9000303	2.0	2.0
1325_9000304	3.0	2.0
1325_9000314	2.0	2.0
1325_9000315	2.0	2.0
1325_9000316	3.0	3.0
1325_9000317	3.0	3.0
1325_9000318	3.0	3.0
1325_9000319	2.0	3.0
1325_9000320	3.0	2.0
1325_9000321	3.0	3.0
1325_9000322	3.0	3.0
1325_9000323	2.0	3.0
1325_9000503	3.0	3.0
1325_9000504	3.0	3.0
1325_9000505	3.0	3.0
1325_9000533	3.0	3.0
1325_9000534	3.0	3.0
1325_9000536	3.0	3.0
1325_9000554	2.0	2.0
1325_9000601	3.0	3.0
1325_9000602	4.0	3.0
1325_9000611	2.0	3.0
1325_9000612	2.0	2.0
1325_9000674	3.0	3.0
1325_9000675	3.0	2.0
1325_9000676	3.0	2.0
1325_9000677	3.0	2.0
1325_9000678	3.0	3.0
1325_9000684	3.0	3.0
1325_9000685	4.0	3.0
1325_9000686	2.0	3.0
1325_9000700	3.0	3.0
1325_9000750	3.0	1.0
1365_0100002	3.0	1.0
1365_0100003	2.0	1.0
1365_0100004	2.0	2.0
1365_0100005	1.0	1.0
1365_0100006	2.0	2.0
1365_0100007	2.0	1.0
1365_0100008	2.0	2.0
1365_0100009	2.0	1.0
1365_0100010	2.0	1.0
1365_0100011	2.0	2.0
1365_0100012	2.0	1.0
1365_0100013	2.0	2.0
1365_0100014	2.0	1.0
1365_0100015	1.0	1.0
1365_0100016	2.0	2.0
1365_0100017	2.0	2.0
1365_0100018	2.0	1.0
1365_0100019	2.0	1.0
1365_0100020	2.0	2.0
1365_0100021	2.0	1.0
1365_0100022	2.0	2.0
1365_0100023	2.0	2.0
1365_0100024	2.0	1.0
1365_0100026	2.0	1.0
1365_0100027	3.0	2.0
1365_0100028	2.0	1.0
1365_0100029	1.0	1.0
1365_0100030	2.0	1.0
1365_0100031	2.0	1.0
1365_0100051	2.0	2.0
1365_0100056	2.0	2.0
1365_0100057	2.0	3.0
1365_0100058	3.0	3.0
1365_0100061	3.0	2.0
1365_0100063	3.0	3.0
1365_0100064	3.0	2.0
1365_0100065	1.0	1.0
1365_0100066	2.0	2.0
1365_0100067	2.0	2.0
1365_0100069	3.0	2.0
1365_0100070	2.0	2.0
1365_0100071	3.0	2.0
1365_0100072	2.0	2.0
1365_0100073	3.0	2.0
1365_0100074	2.0	2.0
1365_0100079	2.0	2.0
1365_0100080	2.0	2.0
1365_0100092	2.0	2.0
1365_0100093	2.0	1.0
1365_0100094	2.0	2.0
1365_0100095	2.0	1.0
1365_0100096	2.0	2.0
1365_0100097	2.0	2.0
1365_0100098	2.0	2.0
1365_0100099	2.0	2.0
1365_0100100	3.0	3.0
1365_0100101	2.0	2.0
1365_0100102	2.0	2.0
1365_0100103	3.0	3.0
1365_0100104	2.0	3.0
1365_0100105	3.0	2.0
1365_0100106	2.0	2.0
1365_0100107	3.0	3.0
1365_0100116	3.0	2.0
1365_0100117	3.0	2.0
1365_0100118	2.0	2.0
1365_0100119	3.0	3.0
1365_0100120	3.0	3.0
1365_0100121	3.0	2.0
1365_0100123	2.0	2.0
1365_0100125	3.0	2.0
1365_0100133	2.0	2.0
1365_0100134	2.0	1.0
1365_0100135	2.0	1.0
1365_0100136	2.0	2.0
1365_0100137	2.0	1.0
1365_0100138	2.0	1.0
1365_0100139	2.0	2.0
1365_0100145	3.0	2.0
1365_0100146	3.0	2.0
1365_0100147	3.0	2.0
1365_0100148	3.0	2.0
1365_0100151	2.0	1.0
1365_0100162	3.0	3.0
1365_0100163	3.0	3.0
1365_0100164	3.0	3.0
1365_0100165	3.0	2.0
1365_0100166	2.0	2.0
1365_0100167	2.0	2.0
1365_0100168	3.0	1.0
1365_0100169	2.0	2.0
1365_0100170	2.0	3.0
1365_0100171	2.0	2.0
1365_0100172	2.0	2.0
1365_0100173	2.0	1.0
1365_0100174	2.0	1.0
1365_0100175	2.0	1.0
1365_0100176	2.0	2.0
1365_0100177	3.0	2.0
1365_0100178	2.0	2.0
1365_0100179	2.0	2.0
1365_0100180	2.0	1.0
1365_0100181	2.0	2.0
1365_0100182	2.0	2.0
1365_0100183	2.0	2.0
1365_0100184	2.0	2.0
1365_0100185	2.0	1.0
1365_0100186	2.0	2.0
1365_0100187	3.0	2.0
1365_0100188	2.0	2.0
1365_0100190	3.0	1.0
1365_0100191	2.0	2.0
1365_0100192	3.0	3.0
1365_0100194	3.0	2.0
1365_0100195	2.0	1.0
1365_0100196	2.0	2.0
1365_0100198	2.0	1.0
1365_0100199	2.0	2.0
1365_0100200	3.0	2.0
1365_0100201	2.0	2.0
1365_0100202	2.0	1.0
1365_0100203	2.0	2.0
1365_0100204	2.0	1.0
1365_0100205	2.0	1.0
1365_0100211	3.0	2.0
1365_0100212	3.0	3.0
1365_0100213	2.0	1.0
1365_0100215	2.0	2.0
1365_0100217	3.0	2.0
1365_0100218	3.0	2.0
1365_0100219	2.0	3.0
1365_0100220	3.0	2.0
1365_0100221	2.0	2.0
1365_0100222	2.0	3.0
1365_0100223	2.0	2.0
1365_0100224	3.0	3.0
1365_0100225	3.0	2.0
1365_0100226	3.0	2.0
1365_0100227	3.0	2.0
1365_0100228	2.0	2.0
1365_0100229	2.0	3.0
1365_0100230	2.0	3.0
1365_0100231	2.0	2.0
1365_0100232	2.0	2.0
1365_0100233	3.0	3.0
1365_0100251	3.0	3.0
1365_0100252	3.0	2.0
1365_0100253	2.0	2.0
1365_0100255	2.0	2.0
1365_0100256	2.0	1.0
1365_0100257	2.0	2.0
1365_0100258	2.0	2.0
1365_0100259	2.0	2.0
1365_0100260	2.0	2.0
1365_0100261	2.0	1.0
1365_0100262	3.0	2.0
1365_0100263	3.0	3.0
1365_0100265	3.0	2.0
1365_0100266	2.0	2.0
1365_0100267	2.0	3.0
1365_0100268	2.0	2.0
1365_0100269	2.0	2.0
1365_0100270	2.0	2.0
1365_0100274	3.0	3.0
1365_0100275	3.0	2.0
1365_0100276	3.0	3.0
1365_0100277	3.0	2.0
1365_0100278	3.0	2.0
1365_0100279	2.0	2.0
1365_0100280	1.0	2.0
1365_0100281	2.0	2.0
1365_0100282	2.0	2.0
1365_0100285	2.0	2.0
1365_0100286	2.0	1.0
1365_0100287	2.0	2.0
1365_0100288	2.0	1.0
1365_0100289	2.0	1.0
1365_0100290	2.0	2.0
1365_0100299	3.0	2.0
1365_0100447	3.0	2.0
1365_0100448	2.0	2.0
1365_0100451	3.0	2.0
1365_0100455	3.0	3.0
1365_0100456	3.0	2.0
1365_0100457	3.0	3.0
1365_0100458	2.0	2.0
1365_0100459	3.0	2.0
1365_0100461	3.0	3.0
1365_0100469	2.0	2.0
1365_0100470	2.0	3.0
1365_0100471	2.0	3.0
1365_0100472	2.0	2.0
1365_0100473	2.0	2.0
1365_0100474	2.0	3.0
1365_0100475	2.0	3.0
1365_0100476	2.0	2.0
1365_0100477	2.0	3.0
1365_0100478	2.0	2.0
1365_0100479	2.0	3.0
1365_0100480	2.0	2.0
1365_0100481	2.0	3.0
1365_0100482	2.0	2.0
1385_0000011	1.0	1.0
1385_0000012	1.0	1.0
1385_0000013	1.0	1.0
1385_0000016	1.0	1.0
1385_0000017	1.0	1.0
1385_0000020	1.0	1.0
1385_0000021	1.0	1.0
1385_0000022	1.0	1.0
1385_0000023	1.0	1.0
1385_0000033	2.0	1.0
1385_0000034	1.0	1.0
1385_0000035	1.0	1.0
1385_0000036	1.0	1.0
1385_0000037	1.0	2.0
1385_0000038	1.0	1.0
1385_0000039	1.0	1.0
1385_0000040	1.0	1.0
1385_0000041	1.0	1.0
1385_0000042	1.0	1.0
1385_0000043	1.0	1.0
1385_0000044	2.0	1.0
1385_0000045	2.0	1.0
1385_0000047	1.0	1.0
1385_0000048	1.0	1.0
1385_0000049	1.0	1.0
1385_0000050	1.0	1.0
1385_0000051	2.0	1.0
1385_0000052	1.0	1.0
1385_0000053	2.0	1.0
1385_0000054	2.0	1.0
1385_0000057	1.0	1.0
1385_0000058	1.0	1.0
1385_0000059	2.0	1.0
1385_0000095	1.0	1.0
1385_0000097	2.0	1.0
1385_0000098	2.0	1.0
1385_0000099	1.0	1.0
1385_0000100	2.0	1.0
1385_0000101	1.0	1.0
1385_0000102	2.0	2.0
1385_0000103	2.0	1.0
1385_0000104	2.0	1.0
1385_0000114	2.0	1.0
1385_0000119	1.0	1.0
1385_0000120	0.0	1.0
1385_0000122	1.0	1.0
1385_0000123	1.0	1.0
1385_0000124	1.0	1.0
1385_0000125	2.0	1.0
1385_0000126	1.0	1.0
1385_0000127	2.0	1.0
1385_0000128	1.0	1.0
1385_0000129	2.0	1.0
1385_0000130	2.0	1.0
1385_0001103	2.0	1.0
1385_0001104	1.0	1.0
1385_0001105	1.0	1.0
1385_0001107	2.0	1.0
1385_0001108	2.0	2.0
1385_0001109	2.0	1.0
1385_0001110	2.0	1.0
1385_0001111	2.0	1.0
1385_0001112	2.0	2.0
1385_0001113	1.0	1.0
1385_0001118	2.0	1.0
1385_0001119	2.0	1.0
1385_0001120	2.0	1.0
1385_0001121	2.0	1.0
1385_0001122	2.0	1.0
1385_0001123	2.0	1.0
1385_0001124	2.0	1.0
1385_0001125	2.0	1.0
1385_0001126	0.0	1.0
1385_0001127	2.0	2.0
1385_0001128	1.0	1.0
1385_0001129	1.0	1.0
1385_0001130	1.0	1.0
1385_0001131	1.0	1.0
1385_0001132	1.0	1.0
1385_0001133	2.0	2.0
1385_0001134	1.0	1.0
1385_0001135	1.0	1.0
1385_0001136	1.0	1.0
1385_0001137	2.0	1.0
1385_0001138	1.0	1.0
1385_0001147	1.0	1.0
1385_0001148	2.0	2.0
1385_0001149	2.0	1.0
1385_0001150	1.0	1.0
1385_0001151	2.0	2.0
1385_0001152	2.0	1.0
1385_0001153	2.0	1.0
1385_0001154	1.0	1.0
1385_0001155	1.0	1.0
1385_0001156	1.0	1.0
1385_0001157	1.0	1.0
1385_0001158	1.0	1.0
1385_0001159	1.0	1.0
1385_0001160	1.0	2.0
1385_0001161	1.0	1.0
1385_0001162	1.0	1.0
1385_0001163	1.0	1.0
1385_0001164	1.0	1.0
1385_0001165	1.0	2.0
1385_0001166	1.0	1.0
1385_0001167	1.0	1.0
1385_0001169	1.0	1.0
1385_0001170	0.0	1.0
1385_0001171	0.0	1.0
1385_0001172	0.0	1.0
1385_0001173	0.0	1.0
1385_0001174	0.0	1.0
1385_0001175	0.0	1.0
1385_0001178	0.0	1.0
1385_0001188	0.0	1.0
1385_0001189	0.0	1.0
1385_0001190	0.0	1.0
1385_0001191	1.0	1.0
1385_0001192	1.0	1.0
1385_0001193	1.0	1.0
1385_0001194	1.0	1.0
1385_0001195	2.0	2.0
1385_0001196	1.0	1.0
1385_0001197	1.0	1.0
1385_0001198	1.0	2.0
1385_0001199	1.0	1.0
1385_0001501	1.0	1.0
1385_0001503	1.0	1.0
1385_0001522	1.0	1.0
1385_0001523	1.0	2.0
1385_0001524	1.0	2.0
1385_0001525	1.0	2.0
1385_0001526	0.0	1.0
1385_0001527	2.0	1.0
1385_0001528	1.0	2.0
1385_0001712	1.0	2.0
1385_0001714	1.0	1.0
1385_0001715	0.0	1.0
1385_0001716	1.0	1.0
1385_0001717	1.0	2.0
1385_0001718	0.0	1.0
1385_0001719	1.0	1.0
1385_0001720	0.0	1.0
1385_0001723	0.0	1.0
1385_0001724	1.0	2.0
1385_0001725	1.0	1.0
1385_0001726	1.0	1.0
1385_0001727	0.0	1.0
1385_0001728	1.0	1.0
1385_0001729	1.0	1.0
1385_0001730	2.0	2.0
1385_0001732	0.0	1.0
1385_0001733	1.0	1.0
1385_0001734	1.0	1.0
1385_0001736	1.0	1.0
1385_0001737	2.0	1.0
1385_0001738	0.0	0.0
1385_0001739	1.0	1.0
1385_0001740	1.0	1.0
1385_0001741	0.0	1.0
1385_0001742	0.0	0.0
1385_0001744	0.0	1.0
1385_0001746	1.0	1.0
1385_0001747	1.0	1.0
1385_0001748	1.0	1.0
1385_0001749	1.0	1.0
1385_0001750	0.0	1.0
1385_0001751	0.0	2.0
1385_0001752	1.0	1.0
1385_0001753	1.0	1.0
1385_0001754	1.0	1.0
1385_0001756	2.0	1.0
1385_0001757	1.0	1.0
1385_0001758	0.0	1.0
1385_0001759	0.0	1.0
1385_0001760	1.0	1.0
1385_0001761	0.0	1.0
1385_0001762	2.0	1.0
1385_0001764	1.0	1.0
1385_0001765	0.0	1.0
1385_0001766	2.0	1.0
1385_0001767	0.0	1.0
1385_0001768	2.0	1.0
1385_0001771	1.0	1.0
1385_0001772	1.0	1.0
1385_0001773	0.0	1.0
1385_0001774	0.0	1.0
1385_0001775	0.0	1.0
1385_0001785	1.0	1.0
1385_0001786	1.0	1.0
1385_0001787	1.0	1.0
1385_0001788	1.0	1.0
1385_0001789	1.0	1.0
1385_0001790	1.0	1.0
1385_0001791	0.0	1.0
1385_0001792	1.0	1.0
1385_0001793	0.0	1.0
1385_0001794	1.0	1.0
1385_0001795	0.0	1.0
1385_0001796	1.0	1.0
1385_0001798	1.0	1.0
1385_0001799	1.0	1.0
1385_0001800	1.0	1.0
1395_0000333	1.0	1.0
1395_0000337	1.0	1.0
1395_0000338	2.0	1.0
1395_0000340	2.0	2.0
1395_0000341	2.0	1.0
1395_0000353	1.0	1.0
1395_0000354	1.0	1.0
1395_0000355	2.0	1.0
1395_0000356	1.0	1.0
1395_0000357	3.0	2.0
1395_0000359	2.0	2.0
1395_0000360	3.0	2.0
1395_0000361	2.0	2.0
1395_0000364	2.0	2.0
1395_0000365	3.0	2.0
1395_0000366	2.0	1.0
1395_0000368	1.0	1.0
1395_0000369	2.0	1.0
1395_0000376	2.0	2.0
1395_0000378	2.0	1.0
1395_0000379	2.0	1.0
1395_0000380	2.0	2.0
1395_0000383	2.0	2.0
1395_0000387	3.0	2.0
1395_0000388	2.0	2.0
1395_0000389	1.0	1.0
1395_0000390	1.0	1.0
1395_0000391	3.0	2.0
1395_0000392	2.0	1.0
1395_0000396	1.0	1.0
1395_0000398	2.0	2.0
1395_0000399	2.0	1.0
1395_0000402	1.0	1.0
1395_0000403	2.0	2.0
1395_0000404	2.0	2.0
1395_0000409	2.0	2.0
1395_0000413	2.0	2.0
1395_0000414	2.0	2.0
1395_0000415	1.0	1.0
1395_0000432	2.0	2.0
1395_0000438	2.0	2.0
1395_0000443	2.0	2.0
1395_0000446	2.0	2.0
1395_0000447	2.0	2.0
1395_0000448	2.0	1.0
1395_0000449	2.0	2.0
1395_0000450	1.0	1.0
1395_0000451	1.0	2.0
1395_0000452	1.0	1.0
1395_0000454	2.0	2.0
1395_0000455	2.0	1.0
1395_0000458	2.0	1.0
1395_0000460	1.0	1.0
1395_0000462	2.0	1.0
1395_0000465	1.0	1.0
1395_0000469	1.0	1.0
1395_0000470	2.0	1.0
1395_0000471	2.0	2.0
1395_0000499	1.0	2.0
1395_0000500	2.0	1.0
1395_0000504	2.0	2.0
1395_0000512	2.0	1.0
1395_0000513	2.0	2.0
1395_0000514	3.0	2.0
1395_0000515	2.0	2.0
1395_0000516	1.0	1.0
1395_0000518	2.0	2.0
1395_0000525	2.0	1.0
1395_0000526	1.0	1.0
1395_0000527	1.0	1.0
1395_0000528	2.0	1.0
1395_0000529	2.0	1.0
1395_0000531	2.0	1.0
1395_0000533	3.0	2.0
1395_0000534	2.0	1.0
1395_0000535	1.0	1.0
1395_0000537	2.0	2.0
1395_0000547	2.0	2.0
1395_0000548	2.0	2.0
1395_0000549	2.0	2.0
1395_0000550	2.0	2.0
1395_0000551	2.0	2.0
1395_0000552	2.0	2.0
1395_0000553	2.0	1.0
1395_0000554	2.0	2.0
1395_0000555	1.0	1.0
1395_0000556	1.0	1.0
1395_0000557	2.0	2.0
1395_0000559	2.0	1.0
1395_0000560	2.0	2.0
1395_0000563	2.0	2.0
1395_0000564	1.0	1.0
1395_0000565	2.0	2.0
1395_0000572	1.0	1.0
1395_0000575	1.0	2.0
1395_0000579	1.0	1.0
1395_0000581	2.0	2.0
1395_0000582	0.0	1.0
1395_0000583	1.0	2.0
1395_0000584	0.0	1.0
1395_0000585	0.0	2.0
1395_0000587	0.0	1.0
1395_0000591	0.0	1.0
1395_0000593	0.0	2.0
1395_0000595	0.0	1.0
1395_0000596	2.0	2.0
1395_0000597	1.0	2.0
1395_0000598	0.0	2.0
1395_0000599	1.0	1.0
1395_0000602	1.0	1.0
1395_0000604	0.0	1.0
1395_0000606	0.0	1.0
1395_0000607	0.0	1.0
1395_0000608	0.0	1.0
1395_0000609	1.0	1.0
1395_0000610	2.0	2.0
1395_0000611	0.0	1.0
1395_0000612	0.0	2.0
1395_0000626	1.0	2.0
1395_0000627	1.0	1.0
1395_0000628	0.0	1.0
1395_0000630	0.0	2.0
1395_0000631	0.0	2.0
1395_0000635	0.0	1.0
1395_0000636	0.0	1.0
1395_0000639	0.0	1.0
1395_0000642	1.0	2.0
1395_0000644	1.0	2.0
1395_0000646	1.0	1.0
1395_0000649	1.0	2.0
1395_0001010	1.0	1.0
1395_0001013	1.0	2.0
1395_0001015	1.0	2.0
1395_0001016	1.0	1.0
1395_0001017	1.0	2.0
1395_0001019	1.0	1.0
1395_0001020	1.0	1.0
1395_0001021	1.0	1.0
1395_0001022	1.0	1.0
1395_0001023	1.0	2.0
1395_0001024	1.0	1.0
1395_0001028	1.0	2.0
1395_0001033	1.0	2.0
1395_0001034	1.0	1.0
1395_0001040	0.0	1.0
1395_0001045	2.0	2.0
1395_0001058	1.0	2.0
1395_0001060	2.0	2.0
1395_0001061	2.0	2.0
1395_0001064	2.0	1.0
1395_0001065	1.0	2.0
1395_0001066	1.0	1.0
1395_0001067	0.0	1.0
1395_0001068	0.0	2.0
1395_0001069	2.0	2.0
1395_0001070	2.0	2.0
1395_0001071	2.0	1.0
1395_0001073	2.0	2.0
1395_0001074	1.0	1.0
1395_0001075	0.0	2.0
1395_0001076	0.0	2.0
1395_0001078	1.0	2.0
1395_0001080	2.0	2.0
1395_0001084	1.0	2.0
1395_0001090	1.0	2.0
1395_0001093	1.0	2.0
1395_0001101	1.0	2.0
1395_0001103	1.0	2.0
1395_0001104	0.0	1.0
1395_0001108	0.0	1.0
1395_0001109	0.0	2.0
1395_0001114	1.0	2.0
1395_0001115	1.0	1.0
1395_0001116	2.0	1.0
1395_0001117	1.0	1.0
1395_0001118	0.0	1.0
1395_0001119	2.0	2.0
1395_0001120	1.0	1.0
1395_0001121	0.0	1.0
1395_0001122	0.0	1.0
1395_0001123	1.0	2.0
1395_0001124	0.0	1.0
1395_0001126	1.0	1.0
1395_0001131	0.0	1.0
1395_0001132	2.0	2.0
1395_0001133	1.0	2.0
1395_0001141	2.0	2.0
1395_0001145	1.0	2.0
1395_0001146	0.0	1.0
1395_0001147	1.0	2.0
1395_0001149	1.0	2.0
1395_0001150	1.0	2.0
1395_0001158	1.0	1.0
1395_0001160	1.0	1.0
1395_0001161	1.0	1.0
1395_0001164	2.0	2.0
1395_0001167	1.0	2.0
1395_0001169	2.0	1.0
1395_0001170	1.0	2.0
1395_0001171	1.0	1.0
Language = IT, Weighted F1-score = 0.5324226379506445, Dimension = Vocabularycontrol

Train on DE, test on  CZ
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
144.90552995391704 65.35717405024758
LANGUAGE: IT
nan nan
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']


======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 1.32
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 0.77
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.86      0.12      0.21       101
         2.0       0.59      0.78      0.67       171
         3.0       0.78      0.97      0.86       156
         4.0       0.00      0.00      0.00         5

    accuracy                           0.68       434
   macro avg       0.45      0.37      0.35       434
weighted avg       0.71      0.68      0.62       434

[[  0   1   0   0   0]
 [  0  12  87   2   0]
 [  0   1 133  37   0]
 [  0   0   4 152   0]
 [  0   0   0   5   0]]
0.6243317142317621
434 434 434



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 0.97
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.01
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.79      0.15      0.25       101
         2.0       0.53      0.54      0.54       171
         3.0       0.64      0.99      0.78       156
         4.0       0.00      0.00      0.00         5

    accuracy                           0.60       434
   macro avg       0.39      0.34      0.31       434
weighted avg       0.62      0.60      0.55       434

[[  0   1   0   0   0]
 [  0  15  79   7   0]
 [  0   3  93  75   0]
 [  0   0   2 154   0]
 [  0   0   0   5   0]]
0.5494677699455998
434 434 434



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 0.85
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 0.84
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.82      0.18      0.29       101
         2.0       0.61      0.79      0.69       171
         3.0       0.80      0.97      0.88       156
         4.0       0.00      0.00      0.00         5

    accuracy                           0.70       434
   macro avg       0.44      0.39      0.37       434
weighted avg       0.72      0.70      0.65       434

[[  0   1   0   0   0]
 [  0  18  82   1   0]
 [  0   3 135  33   0]
 [  0   0   4 152   0]
 [  0   0   0   5   0]]
0.6544013266149945
434 434 434



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 0.76
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 0.83
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.87      0.13      0.22       101
         2.0       0.61      0.82      0.70       171
         3.0       0.81      0.97      0.88       156
         4.0       0.00      0.00      0.00         5

    accuracy                           0.70       434
   macro avg       0.46      0.38      0.36       434
weighted avg       0.73      0.70      0.64       434

[[  0   1   0   0   0]
 [  0  13  87   1   0]
 [  0   1 141  29   0]
 [  0   0   5 151   0]
 [  0   0   0   5   0]]
0.6445937860322786
434 434 434
Filename	True Label	Prediction
0601	2.0	2.0
0602	2.0	3.0
0603	2.0	3.0
0604	2.0	2.0
0605	2.0	2.0
0606	2.0	2.0
0607	2.0	2.0
0608	1.0	2.0
0609	2.0	2.0
0610	2.0	2.0
0611	2.0	2.0
0612	2.0	2.0
0613	2.0	2.0
0614	2.0	2.0
0615	2.0	2.0
0616	2.0	2.0
0617	2.0	2.0
0618	2.0	2.0
0619	2.0	2.0
0620	2.0	2.0
0621	2.0	2.0
0622	2.0	2.0
0623	2.0	2.0
0624	2.0	2.0
0625	1.0	2.0
0626	2.0	2.0
0627	2.0	2.0
0628	2.0	2.0
0629	2.0	2.0
0630	1.0	2.0
0631	2.0	2.0
0632	1.0	2.0
0633	2.0	2.0
0634	2.0	2.0
0635	2.0	2.0
0636	2.0	2.0
0637	2.0	2.0
0638	2.0	2.0
0639	2.0	2.0
0640	2.0	2.0
0641	2.0	2.0
0642	2.0	2.0
0643	2.0	2.0
0644	2.0	2.0
0645	2.0	2.0
0714	2.0	2.0
0715	2.0	2.0
0716	2.0	2.0
0717	2.0	2.0
0718	2.0	2.0
0719	2.0	2.0
0720	2.0	2.0
0721	2.0	2.0
0722	2.0	2.0
0723	1.0	2.0
0724	3.0	2.0
0725	2.0	2.0
0801	1.0	2.0
0802	1.0	2.0
0803	1.0	2.0
0804	2.0	2.0
0805	2.0	2.0
0806	2.0	2.0
0807	2.0	2.0
0808	2.0	2.0
0809	2.0	2.0
0810	2.0	2.0
0811	2.0	2.0
0812	1.0	2.0
0813	2.0	2.0
0814	1.0	2.0
0815	2.0	2.0
0816	2.0	2.0
0817	2.0	2.0
0818	1.0	2.0
0819	3.0	2.0
0820	1.0	2.0
0821	2.0	2.0
0822	2.0	2.0
0823	2.0	2.0
0824	2.0	2.0
0825	2.0	2.0
0826	2.0	2.0
0827	2.0	2.0
0828	2.0	2.0
0829	2.0	3.0
0901	3.0	2.0
0902	2.0	2.0
0903	2.0	2.0
0904	1.0	2.0
0905	2.0	2.0
0906	2.0	2.0
0907	2.0	2.0
0910	1.0	2.0
0911	2.0	2.0
0912	2.0	2.0
0913	2.0	2.0
0914	2.0	2.0
0915	2.0	2.0
0916	1.0	2.0
0917	2.0	2.0
0918	2.0	2.0
0919	2.0	2.0
0920	2.0	3.0
0921	2.0	2.0
0922	1.0	2.0
0923	2.0	2.0
0924	1.0	2.0
0925	2.0	2.0
0926	2.0	2.0
0927	2.0	2.0
0928	2.0	2.0
0929	1.0	2.0
0930	2.0	2.0
1001	2.0	2.0
1002	2.0	2.0
1003	2.0	2.0
1004	2.0	2.0
1005	2.0	2.0
1006	2.0	2.0
1007	2.0	2.0
1008	2.0	2.0
1009	2.0	2.0
1010	1.0	2.0
1014	2.0	2.0
1015	2.0	2.0
1016	2.0	2.0
1017	2.0	2.0
1018	2.0	2.0
1019	2.0	2.0
1020	2.0	2.0
1021	2.0	2.0
1022	2.0	2.0
1023	2.0	3.0
1111	2.0	2.0
1112	2.0	2.0
1113	2.0	2.0
1114	2.0	2.0
1115	2.0	3.0
1116	2.0	2.0
1117	2.0	2.0
9999	1.0	2.0
BER0609003	3.0	3.0
BER0611003	3.0	3.0
BER0611005	3.0	2.0
BER0611006	3.0	3.0
BER0611007	3.0	3.0
KYJ0611003A	1.0	2.0
KYJ0611004A	1.0	2.0
KYJ0611005A	1.0	2.0
KYJ0611005B	1.0	2.0
KYJ0611006A	1.0	1.0
KYJ0611006B	1.0	2.0
KYJ0611009A	2.0	2.0
KYJ0611009B	1.0	2.0
LIB0611001A	1.0	2.0
LIB0611001B	1.0	2.0
LIB0611002A	1.0	2.0
LIB0611002B	1.0	2.0
LIB0611003A	2.0	2.0
LIB0611004A	1.0	1.0
LIB0611004B	2.0	2.0
LIB0611011	2.0	3.0
LON0610002A	2.0	2.0
LON0610002B	1.0	2.0
LON0611002A	1.0	1.0
LON0611002B	1.0	2.0
LON0611003	3.0	3.0
LON0611004A	1.0	2.0
LON0611004B	1.0	2.0
MOS0509001	2.0	3.0
MOS0509004	2.0	3.0
MOS0611012	3.0	3.0
MOS0611013	3.0	3.0
MOS0611014	1.0	3.0
MOS0611015	3.0	3.0
PAR1011008A	2.0	2.0
PAR1011009A	2.0	2.0
PAR1011009B	1.0	2.0
PAR1011013	3.0	3.0
PAR1011014	3.0	3.0
PAR1011015	2.0	3.0
PAR1011016	3.0	3.0
PAR1011017	3.0	3.0
PAR1011018	3.0	3.0
PHA0111001A	1.0	2.0
PHA0111001B	1.0	2.0
PHA0111002A	1.0	2.0
PHA0111002B	2.0	2.0
PHA0111003A	1.0	1.0
PHA0111003B	1.0	2.0
PHA0111004A	1.0	1.0
PHA0111004B	1.0	2.0
PHA0111005A	2.0	2.0
PHA0111005B	2.0	2.0
PHA0111010	3.0	3.0
PHA0111011	3.0	3.0
PHA0111012	3.0	3.0
PHA0111014	2.0	3.0
PHA0111015	4.0	3.0
PHA0111016	3.0	3.0
PHA0111018	2.0	3.0
PHA0112002A	1.0	2.0
PHA0112002B	2.0	2.0
PHA0112003A	1.0	1.0
PHA0112003B	1.0	2.0
PHA0112006A	3.0	2.0
PHA0112006B	2.0	2.0
PHA0112007A	1.0	2.0
PHA0112007B	1.0	2.0
PHA0112009A	2.0	2.0
PHA0112009B	2.0	2.0
PHA0112012A	1.0	2.0
PHA0112012B	1.0	2.0
PHA0209001	2.0	2.0
PHA0209008	1.0	2.0
PHA0209013	1.0	2.0
PHA0209024	3.0	3.0
PHA0209026	3.0	3.0
PHA0209028	3.0	3.0
PHA0209031	3.0	3.0
PHA0209034	3.0	3.0
PHA0209038	3.0	3.0
PHA0209039	3.0	3.0
PHA0210001	1.0	2.0
PHA0210004	1.0	2.0
PHA0210007	1.0	2.0
PHA0210008	1.0	2.0
PHA0411008A	2.0	2.0
PHA0411008B	1.0	2.0
PHA0411009A	1.0	2.0
PHA0411009B	2.0	2.0
PHA0411010A	0.0	1.0
PHA0411010B	1.0	2.0
PHA0411011A	1.0	1.0
PHA0411011B	1.0	2.0
PHA0411012A	2.0	2.0
PHA0411012B	1.0	2.0
PHA0411027	3.0	3.0
PHA0411028	2.0	3.0
PHA0411029	3.0	3.0
PHA0411030	3.0	3.0
PHA0411031	3.0	3.0
PHA0411032	3.0	3.0
PHA0411033	3.0	3.0
PHA0411034	3.0	3.0
PHA0411035	3.0	3.0
PHA0411036	3.0	3.0
PHA0411037	2.0	3.0
PHA0411038	3.0	3.0
PHA0411039	3.0	3.0
PHA0411041	3.0	3.0
PHA0411042	3.0	3.0
PHA0411043	3.0	3.0
PHA0411044	4.0	3.0
PHA0411045	3.0	3.0
PHA0411047	3.0	3.0
PHA0411051	3.0	3.0
PHA0411053	3.0	3.0
PHA0411054	3.0	3.0
PHA0411055	3.0	3.0
PHA0411056	3.0	3.0
PHA0411058	3.0	3.0
PHA0411059	3.0	3.0
PHA0411060	3.0	3.0
PHA0411061	3.0	3.0
PHA0411062	3.0	3.0
PHA0509002	1.0	1.0
PHA0509007	1.0	2.0
PHA0509013	1.0	2.0
PHA0509015	3.0	3.0
PHA0509017	3.0	3.0
PHA0509018	3.0	3.0
PHA0509019	3.0	3.0
PHA0509020	3.0	3.0
PHA0509021	2.0	3.0
PHA0509022	3.0	3.0
PHA0509024	3.0	3.0
PHA0509025	3.0	3.0
PHA0509026	3.0	3.0
PHA0509027	2.0	3.0
PHA0509028	3.0	3.0
PHA0509030	3.0	3.0
PHA0509031	2.0	3.0
PHA0509032	3.0	3.0
PHA0509033	2.0	3.0
PHA0509034	2.0	3.0
PHA0509035	3.0	3.0
PHA0509036	3.0	3.0
PHA0509037	3.0	3.0
PHA0509038	2.0	3.0
PHA0509039	3.0	3.0
PHA0509040	3.0	3.0
PHA0509041	3.0	3.0
PHA0509042	3.0	3.0
PHA0509043	3.0	3.0
PHA0509044	3.0	3.0
PHA0509045	3.0	3.0
PHA0510002A	1.0	2.0
PHA0510002B	2.0	2.0
PHA0510003A	1.0	2.0
PHA0510003B	1.0	2.0
PHA0510004A	1.0	2.0
PHA0510004B	1.0	2.0
PHA0510010A	1.0	2.0
PHA0510010B	1.0	1.0
PHA0510013A	2.0	2.0
PHA0510013B	1.0	2.0
PHA0510023	3.0	3.0
PHA0510027	3.0	3.0
PHA0510029	3.0	3.0
PHA0510030	3.0	3.0
PHA0510031	3.0	3.0
PHA0510032	3.0	3.0
PHA0510034	3.0	3.0
PHA0510035	3.0	3.0
PHA0510036	3.0	3.0
PHA0510037	2.0	2.0
PHA0510038	3.0	3.0
PHA0510039	3.0	3.0
PHA0510040	3.0	3.0
PHA0510046	2.0	3.0
PHA0510047	2.0	3.0
PHA0510048	3.0	3.0
PHA0510049	3.0	3.0
PHA0510050	3.0	3.0
PHA0610005A	1.0	2.0
PHA0610005B	1.0	1.0
PHA0610006A	1.0	2.0
PHA0610006B	1.0	2.0
PHA0610007A	1.0	1.0
PHA0610007B	1.0	2.0
PHA0610015	3.0	3.0
PHA0610016	3.0	3.0
PHA0610017	3.0	3.0
PHA0610018	3.0	3.0
PHA0610019A	1.0	2.0
PHA0610019B	2.0	2.0
PHA0610025	3.0	3.0
PHA0610026	3.0	3.0
PHA0709008	3.0	3.0
PHA0710009	3.0	3.0
PHA0710010	3.0	3.0
PHA0710011	3.0	3.0
PHA0710012	3.0	3.0
PHA0710013	3.0	3.0
PHA0710014	3.0	3.0
PHA0710015	3.0	3.0
PHA0710016	3.0	3.0
PHA0710017	3.0	3.0
PHA0710018	3.0	3.0
PHA0710019	3.0	3.0
PHA0710021	4.0	3.0
PHA0809009	3.0	3.0
PHA0809010	2.0	3.0
PHA0810001	3.0	3.0
PHA0810002	3.0	3.0
PHA0810003	3.0	3.0
PHA0810004	3.0	3.0
PHA0810006	3.0	3.0
PHA0810008	3.0	3.0
PHA0810009	3.0	3.0
PHA0810010	3.0	3.0
PHA0810011	3.0	3.0
PHA0810012	3.0	3.0
PHA0810015	3.0	3.0
PHA0811010	3.0	3.0
PHA0811012	3.0	3.0
PHA0811013	3.0	3.0
PHA0811014	3.0	3.0
PHA0811016	3.0	3.0
PHA0811017	4.0	3.0
PHA0811019	3.0	3.0
PHA0811020	2.0	3.0
PHA1109001	2.0	2.0
PHA1109002	3.0	3.0
PHA1109003	2.0	3.0
PHA1109004	3.0	3.0
PHA1109005	3.0	3.0
PHA1109006	2.0	3.0
PHA1109007	2.0	3.0
PHA1109008	1.0	1.0
PHA1109023	1.0	2.0
PHA1109024	3.0	3.0
PHA1109025	1.0	2.0
PHA1109026	3.0	3.0
PHA1109027	3.0	3.0
PHA1109028	3.0	3.0
PHA1110001A	2.0	2.0
PHA1110001B	2.0	2.0
PHA1110002A	2.0	2.0
PHA1110002B	2.0	2.0
PHA1110003A	1.0	2.0
PHA1110003B	1.0	2.0
PHA1110004A	1.0	2.0
PHA1110013	3.0	3.0
PHA1110014	3.0	3.0
PHA1110015	3.0	3.0
PHA1110016	2.0	3.0
PHA1110017	3.0	3.0
PHA1110019	3.0	3.0
PHA1110021	3.0	3.0
PHA1110022	4.0	3.0
PHA1111001A	2.0	2.0
PHA1111001B	1.0	2.0
PHA1111002A	1.0	1.0
PHA1111002B	1.0	2.0
PHA1111003A	1.0	2.0
PHA1111003B	1.0	2.0
PHA1111004A	1.0	2.0
PHA1111004B	1.0	2.0
PHA1111006A	1.0	2.0
PHA1111006B	2.0	2.0
PHA1111008A	2.0	1.0
PHA1111008B	1.0	2.0
PHA1111009A	1.0	2.0
ST071122B	1.0	2.0
TI071122B	1.0	2.0
VAR0209036	2.0	3.0
VAR0909003	3.0	3.0
VAR0909004	3.0	3.0
VAR0909005	3.0	3.0
VAR0909006	3.0	3.0
VAR0909007	3.0	3.0
VAR0909008	3.0	3.0
VAR0909009	3.0	3.0
VAR0909010	3.0	3.0
VAR0910004	3.0	3.0
VAR0910005	3.0	3.0
VAR0910006	3.0	3.0
VAR0910007	3.0	3.0
VAR0910009	3.0	3.0
VAR0910010	3.0	3.0
VAR0910011	3.0	3.0
Language = CZ, Weighted F1-score = 0.6445937860322786, Dimension = CoherenceCohesion

Train on DE, test on  IT
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
nan nan
LANGUAGE: IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']


======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 1.26
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.05
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00       109
         1.0       0.66      0.78      0.71       334
         2.0       0.64      0.69      0.66       300
         3.0       0.29      0.40      0.34        57

    accuracy                           0.61       800
   macro avg       0.40      0.47      0.43       800
weighted avg       0.53      0.61      0.57       800

[[  0  95  14   0]
 [  0 259  70   5]
 [  0  40 208  52]
 [  0   0  34  23]]
0.5701919433118129
800 800 800



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 0.96
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.27
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00       109
         1.0       0.66      0.38      0.49       334
         2.0       0.42      0.62      0.50       300
         3.0       0.24      0.67      0.36        57

    accuracy                           0.44       800
   macro avg       0.33      0.42      0.33       800
weighted avg       0.45      0.44      0.42       800

[[  0  63  46   0]
 [  0 128 198   8]
 [  0   2 187 111]
 [  0   0  19  38]]
0.4151120874638671
800 800 800



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 0.83
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.28
              precision    recall  f1-score   support

         0.0       1.00      0.02      0.04       109
         1.0       0.63      0.22      0.33       334
         2.0       0.43      0.91      0.59       300
         3.0       0.37      0.33      0.35        57

    accuracy                           0.46       800
   macro avg       0.61      0.37      0.32       800
weighted avg       0.59      0.46      0.39       800

[[  2  43  64   0]
 [  0  74 255   5]
 [  0   0 272  28]
 [  0   0  38  19]]
0.3863469693552706
800 800 800



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.73
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.07
              precision    recall  f1-score   support

         0.0       1.00      0.02      0.04       109
         1.0       0.67      0.52      0.58       334
         2.0       0.54      0.82      0.65       300
         3.0       0.34      0.47      0.39        57

    accuracy                           0.56       800
   macro avg       0.64      0.46      0.42       800
weighted avg       0.64      0.56      0.52       800

[[  2  79  28   0]
 [  0 173 155   6]
 [  0   7 246  47]
 [  0   0  30  27]]
0.5196771927364466
800 800 800
Filename	True Label	Prediction
1325_1001008	2.0	2.0
1325_1001009	3.0	2.0
1325_1001010	2.0	2.0
1325_1001011	2.0	2.0
1325_1001012	2.0	3.0
1325_1001013	2.0	2.0
1325_1001014	3.0	2.0
1325_1001015	2.0	2.0
1325_1001016	2.0	2.0
1325_1001017	2.0	2.0
1325_1001018	2.0	3.0
1325_1001019	2.0	2.0
1325_1001020	2.0	2.0
1325_1001021	2.0	2.0
1325_1001022	2.0	2.0
1325_1001023	2.0	2.0
1325_1001024	2.0	2.0
1325_1001025	2.0	2.0
1325_1001027	3.0	2.0
1325_1001028	2.0	3.0
1325_1001029	2.0	3.0
1325_1001032	2.0	2.0
1325_1001033	2.0	3.0
1325_1001035	3.0	3.0
1325_1001036	2.0	2.0
1325_1001037	2.0	2.0
1325_1001039	3.0	3.0
1325_1001040	2.0	3.0
1325_1001041	3.0	3.0
1325_1001042	2.0	3.0
1325_1001043	2.0	2.0
1325_1001044	2.0	2.0
1325_1001045	2.0	2.0
1325_1001046	2.0	2.0
1325_1001047	1.0	2.0
1325_1001048	2.0	2.0
1325_1001050	2.0	2.0
1325_1001051	2.0	2.0
1325_1001052	2.0	2.0
1325_1001053	1.0	2.0
1325_1001054	2.0	2.0
1325_1001055	2.0	3.0
1325_1001056	2.0	3.0
1325_1001057	1.0	2.0
1325_1001058	2.0	2.0
1325_1001059	2.0	2.0
1325_1001062	2.0	3.0
1325_1001063	2.0	3.0
1325_1001075	1.0	2.0
1325_1001076	2.0	3.0
1325_1001077	2.0	2.0
1325_1001078	2.0	2.0
1325_1001079	2.0	2.0
1325_1001080	2.0	2.0
1325_1001081	2.0	2.0
1325_1001082	2.0	2.0
1325_1001083	2.0	2.0
1325_1001084	2.0	2.0
1325_1001085	2.0	2.0
1325_1001086	2.0	2.0
1325_1001087	2.0	2.0
1325_1001088	2.0	2.0
1325_1001089	2.0	2.0
1325_1001090	2.0	2.0
1325_1001091	2.0	2.0
1325_1001092	2.0	2.0
1325_1001093	2.0	2.0
1325_1001094	2.0	2.0
1325_1001095	2.0	2.0
1325_1001096	2.0	2.0
1325_1001097	0.0	2.0
1325_1001098	2.0	2.0
1325_1001099	3.0	2.0
1325_1001100	2.0	2.0
1325_1001101	3.0	3.0
1325_1001107	2.0	3.0
1325_1001108	3.0	3.0
1325_1001109	2.0	2.0
1325_1001110	2.0	3.0
1325_1001111	3.0	3.0
1325_1001113	3.0	3.0
1325_1001119	2.0	2.0
1325_1001120	2.0	3.0
1325_1001121	2.0	2.0
1325_1001122	2.0	2.0
1325_1001123	2.0	3.0
1325_1001124	2.0	2.0
1325_1001125	3.0	3.0
1325_1001126	2.0	2.0
1325_1001127	3.0	2.0
1325_1001128	2.0	2.0
1325_1001129	1.0	3.0
1325_1001130	2.0	2.0
1325_1001131	2.0	3.0
1325_1001132	2.0	3.0
1325_1001133	2.0	2.0
1325_1001134	2.0	3.0
1325_1001135	2.0	3.0
1325_1001136	2.0	2.0
1325_1001138	2.0	3.0
1325_1001139	2.0	2.0
1325_1001141	2.0	3.0
1325_1001142	2.0	3.0
1325_1001143	2.0	3.0
1325_1001144	2.0	2.0
1325_1001152	2.0	2.0
1325_1001153	2.0	2.0
1325_1001154	3.0	2.0
1325_1001155	2.0	3.0
1325_1001156	2.0	2.0
1325_1001157	2.0	2.0
1325_1001158	2.0	2.0
1325_1001159	2.0	2.0
1325_1001160	2.0	3.0
1325_1001161	2.0	3.0
1325_1001162	2.0	2.0
1325_1001163	2.0	2.0
1325_1001164	2.0	2.0
1325_1001165	2.0	2.0
1325_1001166	2.0	2.0
1325_1001167	2.0	2.0
1325_1001168	2.0	3.0
1325_1001169	2.0	3.0
1325_1001170	2.0	2.0
1325_9000059	2.0	2.0
1325_9000087	2.0	2.0
1325_9000088	2.0	2.0
1325_9000089	2.0	2.0
1325_9000090	2.0	2.0
1325_9000095	2.0	2.0
1325_9000099	3.0	2.0
1325_9000102	2.0	2.0
1325_9000104	2.0	2.0
1325_9000105	2.0	2.0
1325_9000106	2.0	3.0
1325_9000107	2.0	3.0
1325_9000136	2.0	3.0
1325_9000137	2.0	2.0
1325_9000138	3.0	3.0
1325_9000139	2.0	2.0
1325_9000140	3.0	3.0
1325_9000143	3.0	3.0
1325_9000144	3.0	2.0
1325_9000152	2.0	2.0
1325_9000185	3.0	3.0
1325_9000186	3.0	3.0
1325_9000187	2.0	3.0
1325_9000188	2.0	2.0
1325_9000209	2.0	3.0
1325_9000210	1.0	3.0
1325_9000211	2.0	2.0
1325_9000213	3.0	2.0
1325_9000214	3.0	2.0
1325_9000215	3.0	3.0
1325_9000237	2.0	3.0
1325_9000239	2.0	2.0
1325_9000240	3.0	2.0
1325_9000241	3.0	3.0
1325_9000278	3.0	2.0
1325_9000279	3.0	3.0
1325_9000296	1.0	3.0
1325_9000302	2.0	2.0
1325_9000303	2.0	2.0
1325_9000304	2.0	2.0
1325_9000314	2.0	2.0
1325_9000315	2.0	2.0
1325_9000316	2.0	2.0
1325_9000317	3.0	3.0
1325_9000318	3.0	2.0
1325_9000319	2.0	3.0
1325_9000320	3.0	2.0
1325_9000321	3.0	3.0
1325_9000322	3.0	2.0
1325_9000323	2.0	2.0
1325_9000503	3.0	3.0
1325_9000504	2.0	2.0
1325_9000505	3.0	3.0
1325_9000533	2.0	2.0
1325_9000534	2.0	3.0
1325_9000536	3.0	2.0
1325_9000554	2.0	2.0
1325_9000601	2.0	3.0
1325_9000602	3.0	3.0
1325_9000611	2.0	3.0
1325_9000612	2.0	2.0
1325_9000674	3.0	3.0
1325_9000675	2.0	2.0
1325_9000676	3.0	2.0
1325_9000677	2.0	2.0
1325_9000678	3.0	3.0
1325_9000684	3.0	2.0
1325_9000685	3.0	2.0
1325_9000686	2.0	2.0
1325_9000700	2.0	2.0
1325_9000750	3.0	3.0
1365_0100002	2.0	2.0
1365_0100003	1.0	1.0
1365_0100004	2.0	2.0
1365_0100005	1.0	2.0
1365_0100006	2.0	2.0
1365_0100007	1.0	1.0
1365_0100008	2.0	2.0
1365_0100009	1.0	1.0
1365_0100010	1.0	1.0
1365_0100011	2.0	2.0
1365_0100012	1.0	1.0
1365_0100013	2.0	2.0
1365_0100014	2.0	2.0
1365_0100015	2.0	1.0
1365_0100016	2.0	2.0
1365_0100017	2.0	2.0
1365_0100018	2.0	2.0
1365_0100019	1.0	1.0
1365_0100020	2.0	2.0
1365_0100021	2.0	2.0
1365_0100022	2.0	2.0
1365_0100023	1.0	2.0
1365_0100024	1.0	2.0
1365_0100026	1.0	1.0
1365_0100027	2.0	2.0
1365_0100028	1.0	2.0
1365_0100029	1.0	1.0
1365_0100030	1.0	1.0
1365_0100031	2.0	2.0
1365_0100051	0.0	2.0
1365_0100056	2.0	2.0
1365_0100057	2.0	3.0
1365_0100058	2.0	2.0
1365_0100061	3.0	2.0
1365_0100063	2.0	2.0
1365_0100064	2.0	2.0
1365_0100065	1.0	1.0
1365_0100066	1.0	2.0
1365_0100067	1.0	2.0
1365_0100069	1.0	2.0
1365_0100070	2.0	2.0
1365_0100071	2.0	2.0
1365_0100072	2.0	2.0
1365_0100073	2.0	2.0
1365_0100074	2.0	2.0
1365_0100079	2.0	2.0
1365_0100080	2.0	2.0
1365_0100092	2.0	2.0
1365_0100093	1.0	1.0
1365_0100094	2.0	2.0
1365_0100095	2.0	2.0
1365_0100096	2.0	2.0
1365_0100097	2.0	2.0
1365_0100098	1.0	2.0
1365_0100099	1.0	2.0
1365_0100100	2.0	3.0
1365_0100101	2.0	2.0
1365_0100102	2.0	2.0
1365_0100103	2.0	2.0
1365_0100104	2.0	2.0
1365_0100105	3.0	2.0
1365_0100106	2.0	2.0
1365_0100107	2.0	3.0
1365_0100116	2.0	2.0
1365_0100117	2.0	2.0
1365_0100118	2.0	2.0
1365_0100119	3.0	2.0
1365_0100120	3.0	3.0
1365_0100121	2.0	2.0
1365_0100123	2.0	2.0
1365_0100125	2.0	2.0
1365_0100133	2.0	2.0
1365_0100134	2.0	2.0
1365_0100135	2.0	2.0
1365_0100136	1.0	2.0
1365_0100137	1.0	2.0
1365_0100138	2.0	1.0
1365_0100139	1.0	2.0
1365_0100145	2.0	2.0
1365_0100146	2.0	2.0
1365_0100147	2.0	2.0
1365_0100148	2.0	2.0
1365_0100151	1.0	1.0
1365_0100162	2.0	2.0
1365_0100163	2.0	3.0
1365_0100164	2.0	3.0
1365_0100165	2.0	2.0
1365_0100166	1.0	2.0
1365_0100167	1.0	2.0
1365_0100168	2.0	2.0
1365_0100169	1.0	2.0
1365_0100170	2.0	2.0
1365_0100171	1.0	2.0
1365_0100172	1.0	2.0
1365_0100173	1.0	2.0
1365_0100174	1.0	2.0
1365_0100175	1.0	1.0
1365_0100176	2.0	2.0
1365_0100177	2.0	2.0
1365_0100178	1.0	2.0
1365_0100179	1.0	2.0
1365_0100180	1.0	1.0
1365_0100181	1.0	2.0
1365_0100182	2.0	2.0
1365_0100183	2.0	2.0
1365_0100184	1.0	2.0
1365_0100185	1.0	1.0
1365_0100186	1.0	2.0
1365_0100187	2.0	2.0
1365_0100188	2.0	2.0
1365_0100190	2.0	2.0
1365_0100191	2.0	2.0
1365_0100192	3.0	3.0
1365_0100194	2.0	2.0
1365_0100195	1.0	2.0
1365_0100196	1.0	2.0
1365_0100198	1.0	2.0
1365_0100199	2.0	2.0
1365_0100200	2.0	2.0
1365_0100201	1.0	2.0
1365_0100202	1.0	2.0
1365_0100203	2.0	2.0
1365_0100204	1.0	2.0
1365_0100205	2.0	1.0
1365_0100211	3.0	2.0
1365_0100212	3.0	3.0
1365_0100213	1.0	2.0
1365_0100215	2.0	2.0
1365_0100217	3.0	2.0
1365_0100218	2.0	2.0
1365_0100219	2.0	2.0
1365_0100220	2.0	2.0
1365_0100221	3.0	2.0
1365_0100222	2.0	2.0
1365_0100223	2.0	2.0
1365_0100224	2.0	3.0
1365_0100225	2.0	2.0
1365_0100226	2.0	2.0
1365_0100227	2.0	2.0
1365_0100228	1.0	2.0
1365_0100229	2.0	2.0
1365_0100230	2.0	3.0
1365_0100231	2.0	2.0
1365_0100232	1.0	2.0
1365_0100233	2.0	2.0
1365_0100251	2.0	2.0
1365_0100252	2.0	2.0
1365_0100253	1.0	2.0
1365_0100255	1.0	2.0
1365_0100256	2.0	2.0
1365_0100257	2.0	2.0
1365_0100258	2.0	2.0
1365_0100259	2.0	2.0
1365_0100260	2.0	2.0
1365_0100261	2.0	2.0
1365_0100262	2.0	2.0
1365_0100263	3.0	2.0
1365_0100265	2.0	2.0
1365_0100266	2.0	2.0
1365_0100267	2.0	2.0
1365_0100268	1.0	2.0
1365_0100269	2.0	2.0
1365_0100270	2.0	2.0
1365_0100274	2.0	3.0
1365_0100275	2.0	2.0
1365_0100276	3.0	2.0
1365_0100277	3.0	2.0
1365_0100278	2.0	2.0
1365_0100279	2.0	2.0
1365_0100280	1.0	2.0
1365_0100281	2.0	2.0
1365_0100282	2.0	2.0
1365_0100285	2.0	2.0
1365_0100286	1.0	2.0
1365_0100287	1.0	2.0
1365_0100288	1.0	1.0
1365_0100289	2.0	2.0
1365_0100290	1.0	2.0
1365_0100299	2.0	2.0
1365_0100447	2.0	2.0
1365_0100448	1.0	2.0
1365_0100451	2.0	2.0
1365_0100455	2.0	2.0
1365_0100456	2.0	2.0
1365_0100457	2.0	2.0
1365_0100458	2.0	2.0
1365_0100459	2.0	2.0
1365_0100461	2.0	2.0
1365_0100469	2.0	2.0
1365_0100470	1.0	3.0
1365_0100471	1.0	3.0
1365_0100472	2.0	2.0
1365_0100473	2.0	2.0
1365_0100474	2.0	2.0
1365_0100475	2.0	2.0
1365_0100476	2.0	2.0
1365_0100477	1.0	3.0
1365_0100478	1.0	2.0
1365_0100479	2.0	2.0
1365_0100480	1.0	2.0
1365_0100481	1.0	2.0
1365_0100482	2.0	2.0
1385_0000011	0.0	1.0
1385_0000012	1.0	1.0
1385_0000013	1.0	1.0
1385_0000016	1.0	1.0
1385_0000017	1.0	1.0
1385_0000020	1.0	1.0
1385_0000021	1.0	1.0
1385_0000022	1.0	2.0
1385_0000023	1.0	2.0
1385_0000033	1.0	1.0
1385_0000034	1.0	1.0
1385_0000035	1.0	2.0
1385_0000036	1.0	1.0
1385_0000037	1.0	2.0
1385_0000038	1.0	1.0
1385_0000039	1.0	1.0
1385_0000040	1.0	1.0
1385_0000041	1.0	1.0
1385_0000042	1.0	1.0
1385_0000043	1.0	1.0
1385_0000044	1.0	1.0
1385_0000045	1.0	1.0
1385_0000047	1.0	2.0
1385_0000048	1.0	1.0
1385_0000049	1.0	1.0
1385_0000050	1.0	1.0
1385_0000051	1.0	2.0
1385_0000052	1.0	1.0
1385_0000053	1.0	1.0
1385_0000054	1.0	1.0
1385_0000057	1.0	1.0
1385_0000058	1.0	1.0
1385_0000059	1.0	1.0
1385_0000095	1.0	1.0
1385_0000097	1.0	1.0
1385_0000098	1.0	1.0
1385_0000099	1.0	1.0
1385_0000100	1.0	1.0
1385_0000101	1.0	1.0
1385_0000102	1.0	2.0
1385_0000103	1.0	1.0
1385_0000104	1.0	1.0
1385_0000114	1.0	2.0
1385_0000119	1.0	1.0
1385_0000120	0.0	1.0
1385_0000122	1.0	1.0
1385_0000123	1.0	1.0
1385_0000124	1.0	1.0
1385_0000125	1.0	2.0
1385_0000126	1.0	1.0
1385_0000127	1.0	1.0
1385_0000128	1.0	1.0
1385_0000129	1.0	1.0
1385_0000130	1.0	1.0
1385_0001103	1.0	1.0
1385_0001104	0.0	1.0
1385_0001105	1.0	1.0
1385_0001107	1.0	1.0
1385_0001108	1.0	2.0
1385_0001109	1.0	1.0
1385_0001110	1.0	1.0
1385_0001111	1.0	1.0
1385_0001112	1.0	1.0
1385_0001113	1.0	1.0
1385_0001118	1.0	1.0
1385_0001119	1.0	1.0
1385_0001120	1.0	1.0
1385_0001121	1.0	2.0
1385_0001122	1.0	1.0
1385_0001123	1.0	1.0
1385_0001124	1.0	1.0
1385_0001125	1.0	1.0
1385_0001126	0.0	1.0
1385_0001127	1.0	2.0
1385_0001128	0.0	2.0
1385_0001129	0.0	1.0
1385_0001130	1.0	1.0
1385_0001131	1.0	1.0
1385_0001132	1.0	1.0
1385_0001133	1.0	2.0
1385_0001134	1.0	1.0
1385_0001135	1.0	1.0
1385_0001136	1.0	1.0
1385_0001137	1.0	1.0
1385_0001138	1.0	1.0
1385_0001147	1.0	1.0
1385_0001148	1.0	1.0
1385_0001149	1.0	1.0
1385_0001150	1.0	1.0
1385_0001151	1.0	2.0
1385_0001152	1.0	2.0
1385_0001153	2.0	2.0
1385_0001154	1.0	1.0
1385_0001155	1.0	1.0
1385_0001156	1.0	1.0
1385_0001157	1.0	1.0
1385_0001158	1.0	1.0
1385_0001159	1.0	1.0
1385_0001160	1.0	2.0
1385_0001161	1.0	1.0
1385_0001162	1.0	1.0
1385_0001163	1.0	2.0
1385_0001164	1.0	1.0
1385_0001165	1.0	2.0
1385_0001166	1.0	1.0
1385_0001167	0.0	1.0
1385_0001169	1.0	1.0
1385_0001170	0.0	1.0
1385_0001171	0.0	1.0
1385_0001172	0.0	1.0
1385_0001173	0.0	1.0
1385_0001174	0.0	1.0
1385_0001175	0.0	1.0
1385_0001178	0.0	1.0
1385_0001188	1.0	1.0
1385_0001189	0.0	1.0
1385_0001190	0.0	1.0
1385_0001191	1.0	1.0
1385_0001192	1.0	1.0
1385_0001193	1.0	1.0
1385_0001194	1.0	1.0
1385_0001195	1.0	2.0
1385_0001196	0.0	1.0
1385_0001197	0.0	1.0
1385_0001198	2.0	2.0
1385_0001199	1.0	1.0
1385_0001501	0.0	1.0
1385_0001503	1.0	1.0
1385_0001522	0.0	1.0
1385_0001523	1.0	2.0
1385_0001524	0.0	1.0
1385_0001525	1.0	2.0
1385_0001526	0.0	1.0
1385_0001527	1.0	1.0
1385_0001528	1.0	2.0
1385_0001712	1.0	2.0
1385_0001714	1.0	1.0
1385_0001715	1.0	1.0
1385_0001716	1.0	1.0
1385_0001717	1.0	2.0
1385_0001718	0.0	1.0
1385_0001719	1.0	1.0
1385_0001720	0.0	1.0
1385_0001723	0.0	1.0
1385_0001724	2.0	2.0
1385_0001725	0.0	1.0
1385_0001726	1.0	2.0
1385_0001727	0.0	1.0
1385_0001728	0.0	1.0
1385_0001729	1.0	2.0
1385_0001730	1.0	2.0
1385_0001732	0.0	1.0
1385_0001733	0.0	1.0
1385_0001734	0.0	1.0
1385_0001736	1.0	2.0
1385_0001737	1.0	1.0
1385_0001738	0.0	0.0
1385_0001739	1.0	2.0
1385_0001740	1.0	1.0
1385_0001741	0.0	1.0
1385_0001742	0.0	0.0
1385_0001744	0.0	1.0
1385_0001746	1.0	1.0
1385_0001747	0.0	2.0
1385_0001748	2.0	2.0
1385_0001749	0.0	1.0
1385_0001750	0.0	1.0
1385_0001751	0.0	2.0
1385_0001752	1.0	1.0
1385_0001753	1.0	1.0
1385_0001754	0.0	1.0
1385_0001756	1.0	1.0
1385_0001757	1.0	1.0
1385_0001758	0.0	1.0
1385_0001759	1.0	1.0
1385_0001760	1.0	1.0
1385_0001761	0.0	1.0
1385_0001762	1.0	1.0
1385_0001764	0.0	1.0
1385_0001765	0.0	1.0
1385_0001766	1.0	2.0
1385_0001767	0.0	2.0
1385_0001768	1.0	1.0
1385_0001771	0.0	1.0
1385_0001772	1.0	1.0
1385_0001773	0.0	1.0
1385_0001774	0.0	1.0
1385_0001775	1.0	1.0
1385_0001785	0.0	1.0
1385_0001786	1.0	1.0
1385_0001787	0.0	1.0
1385_0001788	0.0	1.0
1385_0001789	1.0	1.0
1385_0001790	0.0	1.0
1385_0001791	1.0	1.0
1385_0001792	1.0	2.0
1385_0001793	0.0	2.0
1385_0001794	1.0	1.0
1385_0001795	0.0	1.0
1385_0001796	1.0	1.0
1385_0001798	1.0	1.0
1385_0001799	1.0	2.0
1385_0001800	1.0	1.0
1395_0000333	1.0	2.0
1395_0000337	0.0	1.0
1395_0000338	1.0	1.0
1395_0000340	1.0	1.0
1395_0000341	1.0	1.0
1395_0000353	1.0	1.0
1395_0000354	1.0	1.0
1395_0000355	1.0	2.0
1395_0000356	1.0	1.0
1395_0000357	2.0	2.0
1395_0000359	1.0	2.0
1395_0000360	2.0	2.0
1395_0000361	1.0	2.0
1395_0000364	1.0	2.0
1395_0000365	2.0	2.0
1395_0000366	2.0	2.0
1395_0000368	0.0	1.0
1395_0000369	2.0	2.0
1395_0000376	2.0	2.0
1395_0000378	1.0	1.0
1395_0000379	1.0	1.0
1395_0000380	1.0	2.0
1395_0000383	1.0	2.0
1395_0000387	3.0	2.0
1395_0000388	2.0	2.0
1395_0000389	0.0	1.0
1395_0000390	1.0	1.0
1395_0000391	3.0	2.0
1395_0000392	1.0	2.0
1395_0000396	1.0	1.0
1395_0000398	2.0	2.0
1395_0000399	1.0	1.0
1395_0000402	1.0	1.0
1395_0000403	1.0	2.0
1395_0000404	1.0	2.0
1395_0000409	2.0	2.0
1395_0000413	1.0	2.0
1395_0000414	1.0	2.0
1395_0000415	1.0	1.0
1395_0000432	1.0	2.0
1395_0000438	2.0	2.0
1395_0000443	2.0	2.0
1395_0000446	2.0	2.0
1395_0000447	1.0	2.0
1395_0000448	1.0	1.0
1395_0000449	2.0	2.0
1395_0000450	1.0	1.0
1395_0000451	1.0	1.0
1395_0000452	1.0	1.0
1395_0000454	1.0	2.0
1395_0000455	1.0	1.0
1395_0000458	1.0	1.0
1395_0000460	1.0	1.0
1395_0000462	1.0	1.0
1395_0000465	1.0	1.0
1395_0000469	1.0	1.0
1395_0000470	1.0	2.0
1395_0000471	1.0	2.0
1395_0000499	1.0	2.0
1395_0000500	1.0	1.0
1395_0000504	1.0	2.0
1395_0000512	1.0	2.0
1395_0000513	2.0	1.0
1395_0000514	2.0	2.0
1395_0000515	2.0	1.0
1395_0000516	1.0	1.0
1395_0000518	1.0	2.0
1395_0000525	1.0	1.0
1395_0000526	1.0	1.0
1395_0000527	0.0	1.0
1395_0000528	1.0	1.0
1395_0000529	1.0	1.0
1395_0000531	1.0	2.0
1395_0000533	2.0	2.0
1395_0000534	1.0	2.0
1395_0000535	1.0	1.0
1395_0000537	1.0	2.0
1395_0000547	1.0	2.0
1395_0000548	1.0	2.0
1395_0000549	1.0	2.0
1395_0000550	1.0	2.0
1395_0000551	2.0	2.0
1395_0000552	1.0	2.0
1395_0000553	1.0	1.0
1395_0000554	1.0	2.0
1395_0000555	1.0	1.0
1395_0000556	1.0	2.0
1395_0000557	2.0	2.0
1395_0000559	1.0	2.0
1395_0000560	2.0	2.0
1395_0000563	1.0	2.0
1395_0000564	1.0	1.0
1395_0000565	1.0	2.0
1395_0000572	1.0	1.0
1395_0000575	1.0	2.0
1395_0000579	1.0	1.0
1395_0000581	1.0	2.0
1395_0000582	0.0	1.0
1395_0000583	1.0	2.0
1395_0000584	0.0	1.0
1395_0000585	0.0	2.0
1395_0000587	0.0	1.0
1395_0000591	0.0	1.0
1395_0000593	0.0	2.0
1395_0000595	0.0	1.0
1395_0000596	2.0	1.0
1395_0000597	1.0	2.0
1395_0000598	1.0	2.0
1395_0000599	1.0	1.0
1395_0000602	1.0	1.0
1395_0000604	0.0	1.0
1395_0000606	0.0	1.0
1395_0000607	0.0	1.0
1395_0000608	0.0	2.0
1395_0000609	1.0	2.0
1395_0000610	1.0	2.0
1395_0000611	0.0	1.0
1395_0000612	1.0	2.0
1395_0000626	1.0	2.0
1395_0000627	1.0	1.0
1395_0000628	0.0	2.0
1395_0000630	0.0	1.0
1395_0000631	0.0	2.0
1395_0000635	0.0	1.0
1395_0000636	0.0	1.0
1395_0000639	0.0	2.0
1395_0000642	0.0	2.0
1395_0000644	1.0	2.0
1395_0000646	0.0	1.0
1395_0000649	2.0	2.0
1395_0001010	1.0	2.0
1395_0001013	1.0	2.0
1395_0001015	0.0	2.0
1395_0001016	1.0	1.0
1395_0001017	0.0	2.0
1395_0001019	0.0	1.0
1395_0001020	1.0	2.0
1395_0001021	1.0	2.0
1395_0001022	1.0	2.0
1395_0001023	1.0	2.0
1395_0001024	1.0	2.0
1395_0001028	1.0	2.0
1395_0001033	1.0	2.0
1395_0001034	0.0	1.0
1395_0001040	0.0	1.0
1395_0001045	1.0	2.0
1395_0001058	1.0	2.0
1395_0001060	1.0	2.0
1395_0001061	1.0	2.0
1395_0001064	1.0	1.0
1395_0001065	0.0	2.0
1395_0001066	0.0	1.0
1395_0001067	0.0	1.0
1395_0001068	0.0	2.0
1395_0001069	2.0	1.0
1395_0001070	1.0	2.0
1395_0001071	1.0	1.0
1395_0001073	1.0	2.0
1395_0001074	1.0	2.0
1395_0001075	0.0	2.0
1395_0001076	1.0	2.0
1395_0001078	0.0	2.0
1395_0001080	1.0	2.0
1395_0001084	0.0	2.0
1395_0001090	1.0	2.0
1395_0001093	0.0	2.0
1395_0001101	1.0	2.0
1395_0001103	0.0	2.0
1395_0001104	0.0	2.0
1395_0001108	0.0	1.0
1395_0001109	0.0	1.0
1395_0001114	0.0	2.0
1395_0001115	1.0	2.0
1395_0001116	1.0	2.0
1395_0001117	0.0	1.0
1395_0001118	0.0	1.0
1395_0001119	1.0	2.0
1395_0001120	0.0	1.0
1395_0001121	0.0	1.0
1395_0001122	0.0	1.0
1395_0001123	0.0	2.0
1395_0001124	1.0	1.0
1395_0001126	1.0	1.0
1395_0001131	0.0	1.0
1395_0001132	2.0	2.0
1395_0001133	1.0	2.0
1395_0001141	1.0	2.0
1395_0001145	1.0	2.0
1395_0001146	0.0	1.0
1395_0001147	0.0	2.0
1395_0001149	0.0	2.0
1395_0001150	0.0	1.0
1395_0001158	2.0	2.0
1395_0001160	1.0	2.0
1395_0001161	1.0	2.0
1395_0001164	1.0	2.0
1395_0001167	2.0	2.0
1395_0001169	1.0	2.0
1395_0001170	1.0	2.0
1395_0001171	1.0	1.0
Language = IT, Weighted F1-score = 0.5196771927364466, Dimension = CoherenceCohesion

Train on DE, test on  CZ
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
144.90552995391704 65.35717405024758
LANGUAGE: IT
nan nan
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']


======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 1.27
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 2.47
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00       261
         1.0       0.51      0.41      0.45       106
         2.0       0.35      0.29      0.32        63
         3.0       0.01      0.75      0.02         4

    accuracy                           0.15       434
   macro avg       0.22      0.36      0.20       434
weighted avg       0.18      0.15      0.16       434

[[  0  30  15 216]
 [  0  43  18  45]
 [  0  10  18  35]
 [  0   1   0   3]]
0.1565734841735447
434 434 434



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 1.02
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 3.57
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00       261
         1.0       0.71      0.23      0.34       106
         2.0       0.28      0.92      0.43        63
         3.0       0.00      0.00      0.00         4

    accuracy                           0.19       434
   macro avg       0.25      0.29      0.19       434
weighted avg       0.21      0.19      0.15       434

[[  0  10  74 177]
 [  0  24  68  14]
 [  0   0  58   5]
 [  0   0   4   0]]
0.14680563055232967
434 434 434



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36

  Average training loss: 0.92
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 4.32
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00       261
         1.0       0.72      0.22      0.33       106
         2.0       0.16      0.19      0.17        63
         3.0       0.01      0.75      0.02         4

    accuracy                           0.09       434
   macro avg       0.22      0.29      0.13       434
weighted avg       0.20      0.09      0.11       434

[[  0   9  44 208]
 [  0  23  20  63]
 [  0   0  12  51]
 [  0   0   1   3]]
0.10646608677788204
434 434 434



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 0.86
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 4.42
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00       261
         1.0       0.76      0.18      0.29       106
         2.0       0.19      0.33      0.24        63
         3.0       0.01      0.75      0.02         4

    accuracy                           0.10       434
   macro avg       0.24      0.32      0.14       434
weighted avg       0.21      0.10      0.11       434

[[  0   6  58 197]
 [  0  19  33  54]
 [  0   0  21  42]
 [  0   0   1   3]]
0.10567323159480263
434 434 434
Filename	True Label	Prediction
0601	1.0	3.0
0602	1.0	3.0
0603	2.0	3.0
0604	1.0	3.0
0605	2.0	3.0
0606	2.0	3.0
0607	2.0	3.0
0608	0.0	3.0
0609	1.0	3.0
0610	1.0	3.0
0611	1.0	3.0
0612	2.0	2.0
0613	0.0	2.0
0614	2.0	3.0
0615	1.0	2.0
0616	2.0	3.0
0617	0.0	3.0
0618	2.0	2.0
0619	2.0	2.0
0620	1.0	3.0
0621	2.0	3.0
0622	1.0	2.0
0623	0.0	3.0
0624	2.0	3.0
0625	1.0	2.0
0626	2.0	2.0
0627	2.0	3.0
0628	2.0	2.0
0629	2.0	2.0
0630	0.0	2.0
0631	2.0	2.0
0632	1.0	2.0
0633	2.0	2.0
0634	3.0	3.0
0635	2.0	2.0
0636	2.0	3.0
0637	2.0	3.0
0638	1.0	2.0
0639	1.0	3.0
0640	2.0	3.0
0641	1.0	2.0
0642	2.0	3.0
0643	1.0	3.0
0644	1.0	3.0
0645	2.0	3.0
0714	2.0	3.0
0715	2.0	2.0
0716	2.0	3.0
0717	1.0	2.0
0718	1.0	3.0
0719	1.0	2.0
0720	2.0	3.0
0721	2.0	3.0
0722	2.0	3.0
0723	2.0	3.0
0724	2.0	3.0
0725	1.0	3.0
0801	1.0	3.0
0802	1.0	2.0
0803	1.0	2.0
0804	2.0	3.0
0805	2.0	3.0
0806	2.0	3.0
0807	2.0	3.0
0808	1.0	2.0
0809	1.0	3.0
0810	1.0	2.0
0811	2.0	3.0
0812	1.0	2.0
0813	2.0	3.0
0814	1.0	2.0
0815	2.0	3.0
0816	3.0	3.0
0817	1.0	3.0
0818	1.0	3.0
0819	3.0	3.0
0820	1.0	2.0
0821	2.0	3.0
0822	1.0	3.0
0823	1.0	3.0
0824	1.0	3.0
0825	1.0	3.0
0826	2.0	3.0
0827	2.0	2.0
0828	1.0	2.0
0829	1.0	3.0
0901	2.0	3.0
0902	2.0	2.0
0903	1.0	3.0
0904	1.0	2.0
0905	1.0	3.0
0906	2.0	3.0
0907	2.0	3.0
0910	2.0	2.0
0911	2.0	2.0
0912	2.0	3.0
0913	2.0	3.0
0914	1.0	2.0
0915	2.0	3.0
0916	1.0	3.0
0917	2.0	2.0
0918	1.0	3.0
0919	2.0	2.0
0920	2.0	3.0
0921	2.0	2.0
0922	1.0	3.0
0923	2.0	3.0
0924	1.0	3.0
0925	2.0	3.0
0926	2.0	3.0
0927	2.0	2.0
0928	2.0	3.0
0929	0.0	2.0
0930	1.0	3.0
1001	1.0	3.0
1002	1.0	3.0
1003	1.0	3.0
1004	1.0	3.0
1005	1.0	3.0
1006	1.0	3.0
1007	1.0	3.0
1008	1.0	3.0
1009	2.0	3.0
1010	1.0	3.0
1014	1.0	3.0
1015	1.0	3.0
1016	1.0	3.0
1017	1.0	3.0
1018	1.0	3.0
1019	1.0	3.0
1020	1.0	3.0
1021	1.0	3.0
1022	1.0	3.0
1023	1.0	3.0
1111	1.0	3.0
1112	1.0	3.0
1113	1.0	3.0
1114	1.0	3.0
1115	1.0	3.0
1116	1.0	3.0
1117	1.0	3.0
9999	0.0	2.0
BER0609003	0.0	3.0
BER0611003	0.0	3.0
BER0611005	0.0	3.0
BER0611006	0.0	3.0
BER0611007	0.0	3.0
KYJ0611003A	1.0	1.0
KYJ0611004A	1.0	1.0
KYJ0611005A	1.0	1.0
KYJ0611005B	0.0	3.0
KYJ0611006A	1.0	1.0
KYJ0611006B	0.0	2.0
KYJ0611009A	1.0	2.0
KYJ0611009B	0.0	3.0
LIB0611001A	0.0	2.0
LIB0611001B	0.0	2.0
LIB0611002A	0.0	2.0
LIB0611002B	0.0	2.0
LIB0611003A	1.0	2.0
LIB0611004A	1.0	1.0
LIB0611004B	0.0	3.0
LIB0611011	0.0	3.0
LON0610002A	1.0	2.0
LON0610002B	0.0	2.0
LON0611002A	1.0	1.0
LON0611002B	0.0	2.0
LON0611003	0.0	3.0
LON0611004A	1.0	1.0
LON0611004B	0.0	2.0
MOS0509001	0.0	3.0
MOS0509004	0.0	3.0
MOS0611012	0.0	3.0
MOS0611013	0.0	3.0
MOS0611014	0.0	3.0
MOS0611015	0.0	3.0
PAR1011008A	2.0	2.0
PAR1011009A	2.0	2.0
PAR1011009B	0.0	3.0
PAR1011013	0.0	3.0
PAR1011014	0.0	3.0
PAR1011015	0.0	3.0
PAR1011016	0.0	3.0
PAR1011017	0.0	3.0
PAR1011018	0.0	3.0
PHA0111001A	1.0	1.0
PHA0111001B	0.0	2.0
PHA0111002A	0.0	2.0
PHA0111002B	0.0	3.0
PHA0111003A	1.0	1.0
PHA0111003B	0.0	2.0
PHA0111004A	1.0	1.0
PHA0111004B	0.0	2.0
PHA0111005A	1.0	1.0
PHA0111005B	0.0	2.0
PHA0111010	0.0	3.0
PHA0111011	0.0	3.0
PHA0111012	0.0	3.0
PHA0111014	0.0	3.0
PHA0111015	0.0	3.0
PHA0111016	0.0	3.0
PHA0111018	0.0	3.0
PHA0112002A	1.0	2.0
PHA0112002B	0.0	3.0
PHA0112003A	1.0	1.0
PHA0112003B	0.0	2.0
PHA0112006A	3.0	2.0
PHA0112006B	0.0	3.0
PHA0112007A	1.0	2.0
PHA0112007B	0.0	2.0
PHA0112009A	2.0	2.0
PHA0112009B	0.0	2.0
PHA0112012A	1.0	2.0
PHA0112012B	0.0	3.0
PHA0209001	0.0	3.0
PHA0209008	0.0	2.0
PHA0209013	0.0	2.0
PHA0209024	0.0	3.0
PHA0209026	0.0	3.0
PHA0209028	0.0	3.0
PHA0209031	0.0	3.0
PHA0209034	0.0	3.0
PHA0209038	0.0	3.0
PHA0209039	0.0	3.0
PHA0210001	0.0	2.0
PHA0210004	0.0	2.0
PHA0210007	0.0	3.0
PHA0210008	0.0	2.0
PHA0411008A	0.0	1.0
PHA0411008B	0.0	2.0
PHA0411009A	1.0	2.0
PHA0411009B	0.0	2.0
PHA0411010A	1.0	1.0
PHA0411010B	0.0	2.0
PHA0411011A	1.0	1.0
PHA0411011B	0.0	2.0
PHA0411012A	1.0	1.0
PHA0411012B	0.0	2.0
PHA0411027	0.0	3.0
PHA0411028	0.0	3.0
PHA0411029	0.0	3.0
PHA0411030	0.0	3.0
PHA0411031	0.0	3.0
PHA0411032	0.0	3.0
PHA0411033	0.0	3.0
PHA0411034	0.0	2.0
PHA0411035	0.0	3.0
PHA0411036	0.0	3.0
PHA0411037	0.0	3.0
PHA0411038	0.0	3.0
PHA0411039	0.0	3.0
PHA0411041	0.0	3.0
PHA0411042	0.0	3.0
PHA0411043	0.0	3.0
PHA0411044	0.0	3.0
PHA0411045	0.0	3.0
PHA0411047	0.0	3.0
PHA0411051	0.0	3.0
PHA0411053	0.0	3.0
PHA0411054	0.0	3.0
PHA0411055	0.0	3.0
PHA0411056	0.0	3.0
PHA0411058	0.0	3.0
PHA0411059	0.0	3.0
PHA0411060	0.0	3.0
PHA0411061	0.0	3.0
PHA0411062	0.0	3.0
PHA0509002	0.0	2.0
PHA0509007	0.0	3.0
PHA0509013	0.0	2.0
PHA0509015	0.0	3.0
PHA0509017	0.0	3.0
PHA0509018	0.0	3.0
PHA0509019	0.0	3.0
PHA0509020	0.0	3.0
PHA0509021	0.0	3.0
PHA0509022	0.0	3.0
PHA0509024	0.0	3.0
PHA0509025	0.0	3.0
PHA0509026	0.0	3.0
PHA0509027	0.0	3.0
PHA0509028	0.0	3.0
PHA0509030	0.0	3.0
PHA0509031	0.0	3.0
PHA0509032	0.0	3.0
PHA0509033	0.0	3.0
PHA0509034	0.0	3.0
PHA0509035	0.0	3.0
PHA0509036	0.0	3.0
PHA0509037	0.0	3.0
PHA0509038	0.0	3.0
PHA0509039	0.0	3.0
PHA0509040	0.0	3.0
PHA0509041	0.0	3.0
PHA0509042	0.0	3.0
PHA0509043	0.0	3.0
PHA0509044	0.0	3.0
PHA0509045	0.0	3.0
PHA0510002A	0.0	2.0
PHA0510002B	0.0	2.0
PHA0510003A	0.0	2.0
PHA0510003B	0.0	2.0
PHA0510004A	0.0	1.0
PHA0510004B	0.0	2.0
PHA0510010A	1.0	1.0
PHA0510010B	0.0	1.0
PHA0510013A	1.0	2.0
PHA0510013B	0.0	2.0
PHA0510023	0.0	3.0
PHA0510027	0.0	3.0
PHA0510029	0.0	3.0
PHA0510030	0.0	3.0
PHA0510031	0.0	3.0
PHA0510032	0.0	3.0
PHA0510034	0.0	3.0
PHA0510035	0.0	3.0
PHA0510036	0.0	3.0
PHA0510037	0.0	3.0
PHA0510038	0.0	3.0
PHA0510039	0.0	3.0
PHA0510040	0.0	3.0
PHA0510046	0.0	3.0
PHA0510047	0.0	3.0
PHA0510048	0.0	3.0
PHA0510049	0.0	3.0
PHA0510050	0.0	3.0
PHA0610005A	1.0	1.0
PHA0610005B	0.0	2.0
PHA0610006A	0.0	2.0
PHA0610006B	0.0	3.0
PHA0610007A	0.0	1.0
PHA0610007B	0.0	2.0
PHA0610015	0.0	3.0
PHA0610016	0.0	3.0
PHA0610017	0.0	3.0
PHA0610018	0.0	3.0
PHA0610019A	0.0	2.0
PHA0610019B	0.0	2.0
PHA0610025	0.0	3.0
PHA0610026	0.0	3.0
PHA0709008	0.0	3.0
PHA0710009	0.0	3.0
PHA0710010	0.0	3.0
PHA0710011	0.0	3.0
PHA0710012	0.0	3.0
PHA0710013	0.0	3.0
PHA0710014	0.0	3.0
PHA0710015	0.0	3.0
PHA0710016	0.0	3.0
PHA0710017	0.0	3.0
PHA0710018	0.0	3.0
PHA0710019	0.0	3.0
PHA0710021	0.0	3.0
PHA0809009	0.0	3.0
PHA0809010	0.0	3.0
PHA0810001	0.0	3.0
PHA0810002	0.0	3.0
PHA0810003	0.0	3.0
PHA0810004	0.0	3.0
PHA0810006	0.0	3.0
PHA0810008	0.0	3.0
PHA0810009	0.0	3.0
PHA0810010	0.0	3.0
PHA0810011	0.0	3.0
PHA0810012	0.0	3.0
PHA0810015	0.0	3.0
PHA0811010	0.0	3.0
PHA0811012	0.0	3.0
PHA0811013	0.0	3.0
PHA0811014	0.0	3.0
PHA0811016	0.0	3.0
PHA0811017	0.0	3.0
PHA0811019	0.0	3.0
PHA0811020	0.0	3.0
PHA1109001	0.0	3.0
PHA1109002	0.0	3.0
PHA1109003	0.0	3.0
PHA1109004	0.0	3.0
PHA1109005	0.0	3.0
PHA1109006	0.0	3.0
PHA1109007	0.0	3.0
PHA1109008	0.0	2.0
PHA1109023	0.0	2.0
PHA1109024	0.0	3.0
PHA1109025	0.0	2.0
PHA1109026	0.0	3.0
PHA1109027	0.0	3.0
PHA1109028	0.0	3.0
PHA1110001A	1.0	2.0
PHA1110001B	0.0	2.0
PHA1110002A	1.0	2.0
PHA1110002B	0.0	2.0
PHA1110003A	1.0	2.0
PHA1110003B	0.0	2.0
PHA1110004A	1.0	2.0
PHA1110013	0.0	3.0
PHA1110014	0.0	3.0
PHA1110015	0.0	3.0
PHA1110016	0.0	3.0
PHA1110017	0.0	3.0
PHA1110019	0.0	3.0
PHA1110021	0.0	3.0
PHA1110022	0.0	3.0
PHA1111001A	1.0	2.0
PHA1111001B	0.0	2.0
PHA1111002A	1.0	1.0
PHA1111002B	0.0	2.0
PHA1111003A	1.0	2.0
PHA1111003B	0.0	2.0
PHA1111004A	1.0	2.0
PHA1111004B	0.0	2.0
PHA1111006A	0.0	1.0
PHA1111006B	0.0	2.0
PHA1111008A	1.0	1.0
PHA1111008B	0.0	2.0
PHA1111009A	0.0	1.0
ST071122B	0.0	2.0
TI071122B	0.0	2.0
VAR0209036	0.0	3.0
VAR0909003	0.0	3.0
VAR0909004	0.0	3.0
VAR0909005	0.0	3.0
VAR0909006	0.0	3.0
VAR0909007	0.0	3.0
VAR0909008	0.0	3.0
VAR0909009	0.0	3.0
VAR0909010	0.0	3.0
VAR0910004	0.0	3.0
VAR0910005	0.0	3.0
VAR0910006	0.0	3.0
VAR0910007	0.0	3.0
VAR0910009	0.0	3.0
VAR0910010	0.0	3.0
VAR0910011	0.0	3.0
Language = CZ, Weighted F1-score = 0.10567323159480263, Dimension = Sociolinguisticappropriateness

Train on DE, test on  IT
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
nan nan
LANGUAGE: IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']


======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 1.24
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.51
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        56
         1.0       0.63      0.34      0.44       336
         2.0       0.33      0.23      0.27       372
         3.0       0.10      0.97      0.18        36

    accuracy                           0.29       800
   macro avg       0.26      0.39      0.22       800
weighted avg       0.42      0.29      0.32       800

[[  0  36  17   3]
 [  0 115 154  67]
 [  0  30  84 258]
 [  0   1   0  35]]
0.3189745247639985
800 800 800



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 1.00
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.48
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        56
         1.0       0.61      0.40      0.48       336
         2.0       0.34      0.21      0.26       372
         3.0       0.10      0.97      0.18        36

    accuracy                           0.31       800
   macro avg       0.26      0.40      0.23       800
weighted avg       0.42      0.31      0.33       800

[[  0  41  12   3]
 [  0 134 138  64]
 [  0  42  78 252]
 [  0   1   0  35]]
0.33215381838378233
800 800 800



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 0.89
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.66
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        56
         1.0       0.63      0.42      0.50       336
         2.0       0.33      0.23      0.27       372
         3.0       0.10      0.92      0.18        36

    accuracy                           0.32       800
   macro avg       0.27      0.39      0.24       800
weighted avg       0.42      0.32      0.34       800

[[  0  40  14   2]
 [  0 140 153  43]
 [  0  41  84 247]
 [  0   1   2  33]]
0.34397183498644746
800 800 800



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35

  Average training loss: 0.82
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.80
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        56
         1.0       0.63      0.33      0.43       336
         2.0       0.31      0.24      0.27       372
         3.0       0.10      0.92      0.18        36

    accuracy                           0.29       800
   macro avg       0.26      0.37      0.22       800
weighted avg       0.41      0.29      0.32       800

[[  0  37  17   2]
 [  0 112 173  51]
 [  0  29  88 255]
 [  0   1   2  33]]
0.3160790681280041
800 800 800
Filename	True Label	Prediction
1325_1001008	2.0	3.0
1325_1001009	2.0	3.0
1325_1001010	2.0	3.0
1325_1001011	3.0	3.0
1325_1001012	2.0	3.0
1325_1001013	3.0	3.0
1325_1001014	2.0	3.0
1325_1001015	2.0	3.0
1325_1001016	1.0	3.0
1325_1001017	1.0	3.0
1325_1001018	2.0	3.0
1325_1001019	2.0	3.0
1325_1001020	2.0	3.0
1325_1001021	2.0	3.0
1325_1001022	2.0	3.0
1325_1001023	2.0	3.0
1325_1001024	2.0	3.0
1325_1001025	2.0	3.0
1325_1001027	2.0	3.0
1325_1001028	1.0	3.0
1325_1001029	2.0	3.0
1325_1001032	2.0	3.0
1325_1001033	2.0	3.0
1325_1001035	3.0	3.0
1325_1001036	2.0	3.0
1325_1001037	2.0	3.0
1325_1001039	2.0	3.0
1325_1001040	2.0	3.0
1325_1001041	3.0	3.0
1325_1001042	2.0	3.0
1325_1001043	2.0	3.0
1325_1001044	2.0	3.0
1325_1001045	2.0	3.0
1325_1001046	1.0	3.0
1325_1001047	2.0	2.0
1325_1001048	2.0	3.0
1325_1001050	2.0	3.0
1325_1001051	1.0	3.0
1325_1001052	2.0	3.0
1325_1001053	2.0	3.0
1325_1001054	3.0	3.0
1325_1001055	2.0	3.0
1325_1001056	2.0	3.0
1325_1001057	2.0	3.0
1325_1001058	2.0	3.0
1325_1001059	2.0	3.0
1325_1001062	2.0	3.0
1325_1001063	1.0	3.0
1325_1001075	2.0	2.0
1325_1001076	2.0	3.0
1325_1001077	2.0	3.0
1325_1001078	2.0	3.0
1325_1001079	2.0	3.0
1325_1001080	2.0	3.0
1325_1001081	2.0	3.0
1325_1001082	2.0	3.0
1325_1001083	2.0	3.0
1325_1001084	2.0	3.0
1325_1001085	2.0	3.0
1325_1001086	2.0	3.0
1325_1001087	2.0	3.0
1325_1001088	2.0	3.0
1325_1001089	2.0	3.0
1325_1001090	2.0	3.0
1325_1001091	2.0	3.0
1325_1001092	2.0	2.0
1325_1001093	2.0	3.0
1325_1001094	2.0	2.0
1325_1001095	2.0	3.0
1325_1001096	2.0	3.0
1325_1001097	1.0	2.0
1325_1001098	2.0	3.0
1325_1001099	3.0	3.0
1325_1001100	2.0	3.0
1325_1001101	3.0	3.0
1325_1001107	2.0	3.0
1325_1001108	2.0	3.0
1325_1001109	2.0	3.0
1325_1001110	2.0	3.0
1325_1001111	3.0	3.0
1325_1001113	3.0	3.0
1325_1001119	2.0	3.0
1325_1001120	2.0	3.0
1325_1001121	2.0	3.0
1325_1001122	2.0	3.0
1325_1001123	2.0	3.0
1325_1001124	1.0	2.0
1325_1001125	2.0	3.0
1325_1001126	2.0	3.0
1325_1001127	2.0	3.0
1325_1001128	2.0	3.0
1325_1001129	1.0	3.0
1325_1001130	2.0	3.0
1325_1001131	2.0	3.0
1325_1001132	2.0	3.0
1325_1001133	2.0	3.0
1325_1001134	2.0	3.0
1325_1001135	2.0	3.0
1325_1001136	2.0	3.0
1325_1001138	2.0	3.0
1325_1001139	2.0	3.0
1325_1001141	1.0	3.0
1325_1001142	1.0	3.0
1325_1001143	2.0	3.0
1325_1001144	2.0	3.0
1325_1001152	2.0	3.0
1325_1001153	2.0	3.0
1325_1001154	3.0	3.0
1325_1001155	2.0	3.0
1325_1001156	2.0	3.0
1325_1001157	2.0	3.0
1325_1001158	2.0	3.0
1325_1001159	2.0	3.0
1325_1001160	2.0	3.0
1325_1001161	2.0	3.0
1325_1001162	2.0	3.0
1325_1001163	1.0	3.0
1325_1001164	2.0	3.0
1325_1001165	1.0	2.0
1325_1001166	2.0	3.0
1325_1001167	2.0	3.0
1325_1001168	1.0	3.0
1325_1001169	2.0	3.0
1325_1001170	2.0	3.0
1325_9000059	3.0	3.0
1325_9000087	2.0	3.0
1325_9000088	2.0	3.0
1325_9000089	2.0	3.0
1325_9000090	2.0	3.0
1325_9000095	2.0	3.0
1325_9000099	2.0	3.0
1325_9000102	2.0	1.0
1325_9000104	2.0	3.0
1325_9000105	2.0	3.0
1325_9000106	2.0	3.0
1325_9000107	3.0	3.0
1325_9000136	2.0	3.0
1325_9000137	2.0	3.0
1325_9000138	2.0	3.0
1325_9000139	2.0	3.0
1325_9000140	3.0	3.0
1325_9000143	3.0	3.0
1325_9000144	3.0	3.0
1325_9000152	2.0	3.0
1325_9000185	3.0	3.0
1325_9000186	3.0	3.0
1325_9000187	2.0	3.0
1325_9000188	2.0	3.0
1325_9000209	2.0	3.0
1325_9000210	1.0	3.0
1325_9000211	2.0	3.0
1325_9000213	3.0	2.0
1325_9000214	2.0	3.0
1325_9000215	2.0	3.0
1325_9000237	2.0	3.0
1325_9000239	3.0	3.0
1325_9000240	2.0	3.0
1325_9000241	3.0	3.0
1325_9000278	3.0	3.0
1325_9000279	2.0	3.0
1325_9000296	2.0	3.0
1325_9000302	2.0	3.0
1325_9000303	2.0	3.0
1325_9000304	2.0	3.0
1325_9000314	2.0	3.0
1325_9000315	1.0	2.0
1325_9000316	2.0	3.0
1325_9000317	2.0	3.0
1325_9000318	2.0	3.0
1325_9000319	2.0	3.0
1325_9000320	3.0	3.0
1325_9000321	3.0	3.0
1325_9000322	2.0	3.0
1325_9000323	1.0	3.0
1325_9000503	3.0	3.0
1325_9000504	2.0	3.0
1325_9000505	2.0	3.0
1325_9000533	2.0	3.0
1325_9000534	1.0	3.0
1325_9000536	3.0	3.0
1325_9000554	2.0	3.0
1325_9000601	2.0	3.0
1325_9000602	2.0	3.0
1325_9000611	2.0	3.0
1325_9000612	1.0	3.0
1325_9000674	3.0	3.0
1325_9000675	2.0	3.0
1325_9000676	2.0	3.0
1325_9000677	2.0	3.0
1325_9000678	3.0	3.0
1325_9000684	2.0	3.0
1325_9000685	3.0	3.0
1325_9000686	2.0	3.0
1325_9000700	2.0	3.0
1325_9000750	3.0	1.0
1365_0100002	1.0	2.0
1365_0100003	2.0	2.0
1365_0100004	1.0	3.0
1365_0100005	2.0	2.0
1365_0100006	2.0	3.0
1365_0100007	1.0	1.0
1365_0100008	1.0	2.0
1365_0100009	2.0	1.0
1365_0100010	1.0	1.0
1365_0100011	2.0	2.0
1365_0100012	2.0	1.0
1365_0100013	2.0	3.0
1365_0100014	2.0	2.0
1365_0100015	2.0	2.0
1365_0100016	1.0	3.0
1365_0100017	2.0	3.0
1365_0100018	2.0	2.0
1365_0100019	1.0	1.0
1365_0100020	2.0	2.0
1365_0100021	2.0	2.0
1365_0100022	1.0	2.0
1365_0100023	1.0	2.0
1365_0100024	1.0	1.0
1365_0100026	2.0	1.0
1365_0100027	2.0	2.0
1365_0100028	2.0	2.0
1365_0100029	2.0	1.0
1365_0100030	1.0	2.0
1365_0100031	1.0	1.0
1365_0100051	1.0	2.0
1365_0100056	1.0	3.0
1365_0100057	1.0	3.0
1365_0100058	2.0	3.0
1365_0100061	3.0	3.0
1365_0100063	2.0	3.0
1365_0100064	2.0	3.0
1365_0100065	1.0	1.0
1365_0100066	1.0	2.0
1365_0100067	1.0	3.0
1365_0100069	2.0	3.0
1365_0100070	2.0	3.0
1365_0100071	2.0	3.0
1365_0100072	2.0	3.0
1365_0100073	2.0	3.0
1365_0100074	1.0	3.0
1365_0100079	1.0	2.0
1365_0100080	2.0	3.0
1365_0100092	2.0	2.0
1365_0100093	2.0	2.0
1365_0100094	2.0	2.0
1365_0100095	2.0	2.0
1365_0100096	2.0	3.0
1365_0100097	2.0	2.0
1365_0100098	2.0	3.0
1365_0100099	2.0	3.0
1365_0100100	2.0	3.0
1365_0100101	2.0	3.0
1365_0100102	2.0	3.0
1365_0100103	2.0	3.0
1365_0100104	1.0	3.0
1365_0100105	3.0	2.0
1365_0100106	1.0	3.0
1365_0100107	2.0	3.0
1365_0100116	2.0	3.0
1365_0100117	2.0	3.0
1365_0100118	2.0	3.0
1365_0100119	3.0	3.0
1365_0100120	2.0	3.0
1365_0100121	2.0	3.0
1365_0100123	2.0	3.0
1365_0100125	2.0	3.0
1365_0100133	2.0	2.0
1365_0100134	2.0	2.0
1365_0100135	1.0	2.0
1365_0100136	2.0	3.0
1365_0100137	2.0	2.0
1365_0100138	2.0	1.0
1365_0100139	2.0	3.0
1365_0100145	2.0	3.0
1365_0100146	2.0	2.0
1365_0100147	2.0	2.0
1365_0100148	1.0	3.0
1365_0100151	2.0	1.0
1365_0100162	2.0	3.0
1365_0100163	2.0	3.0
1365_0100164	1.0	3.0
1365_0100165	2.0	3.0
1365_0100166	2.0	2.0
1365_0100167	1.0	3.0
1365_0100168	2.0	2.0
1365_0100169	2.0	2.0
1365_0100170	1.0	3.0
1365_0100171	1.0	2.0
1365_0100172	2.0	2.0
1365_0100173	1.0	2.0
1365_0100174	1.0	2.0
1365_0100175	2.0	2.0
1365_0100176	2.0	3.0
1365_0100177	2.0	3.0
1365_0100178	2.0	3.0
1365_0100179	2.0	2.0
1365_0100180	1.0	2.0
1365_0100181	1.0	2.0
1365_0100182	2.0	3.0
1365_0100183	2.0	3.0
1365_0100184	2.0	3.0
1365_0100185	2.0	2.0
1365_0100186	2.0	3.0
1365_0100187	2.0	3.0
1365_0100188	1.0	3.0
1365_0100190	2.0	1.0
1365_0100191	1.0	3.0
1365_0100192	3.0	3.0
1365_0100194	2.0	3.0
1365_0100195	1.0	2.0
1365_0100196	1.0	3.0
1365_0100198	1.0	2.0
1365_0100199	2.0	3.0
1365_0100200	2.0	3.0
1365_0100201	2.0	2.0
1365_0100202	2.0	2.0
1365_0100203	2.0	2.0
1365_0100204	1.0	1.0
1365_0100205	2.0	1.0
1365_0100211	2.0	3.0
1365_0100212	3.0	3.0
1365_0100213	2.0	1.0
1365_0100215	2.0	2.0
1365_0100217	2.0	3.0
1365_0100218	2.0	3.0
1365_0100219	2.0	3.0
1365_0100220	2.0	3.0
1365_0100221	2.0	3.0
1365_0100222	2.0	3.0
1365_0100223	2.0	3.0
1365_0100224	2.0	3.0
1365_0100225	2.0	3.0
1365_0100226	2.0	3.0
1365_0100227	2.0	3.0
1365_0100228	2.0	3.0
1365_0100229	2.0	3.0
1365_0100230	2.0	3.0
1365_0100231	2.0	3.0
1365_0100232	2.0	3.0
1365_0100233	2.0	3.0
1365_0100251	2.0	3.0
1365_0100252	2.0	3.0
1365_0100253	1.0	2.0
1365_0100255	1.0	2.0
1365_0100256	2.0	2.0
1365_0100257	2.0	3.0
1365_0100258	2.0	2.0
1365_0100259	2.0	3.0
1365_0100260	1.0	2.0
1365_0100261	2.0	2.0
1365_0100262	2.0	3.0
1365_0100263	3.0	3.0
1365_0100265	2.0	3.0
1365_0100266	1.0	3.0
1365_0100267	2.0	3.0
1365_0100268	2.0	3.0
1365_0100269	2.0	3.0
1365_0100270	2.0	3.0
1365_0100274	2.0	3.0
1365_0100275	2.0	3.0
1365_0100276	2.0	3.0
1365_0100277	2.0	3.0
1365_0100278	2.0	3.0
1365_0100279	2.0	3.0
1365_0100280	1.0	2.0
1365_0100281	2.0	3.0
1365_0100282	2.0	3.0
1365_0100285	2.0	2.0
1365_0100286	1.0	2.0
1365_0100287	2.0	2.0
1365_0100288	2.0	2.0
1365_0100289	2.0	2.0
1365_0100290	2.0	3.0
1365_0100299	1.0	3.0
1365_0100447	2.0	3.0
1365_0100448	2.0	3.0
1365_0100451	2.0	3.0
1365_0100455	2.0	3.0
1365_0100456	2.0	3.0
1365_0100457	2.0	3.0
1365_0100458	2.0	3.0
1365_0100459	2.0	3.0
1365_0100461	2.0	3.0
1365_0100469	2.0	3.0
1365_0100470	2.0	3.0
1365_0100471	2.0	3.0
1365_0100472	2.0	3.0
1365_0100473	2.0	3.0
1365_0100474	2.0	3.0
1365_0100475	2.0	3.0
1365_0100476	2.0	3.0
1365_0100477	2.0	3.0
1365_0100478	2.0	3.0
1365_0100479	2.0	3.0
1365_0100480	2.0	3.0
1365_0100481	2.0	3.0
1365_0100482	2.0	3.0
1385_0000011	1.0	1.0
1385_0000012	1.0	1.0
1385_0000013	0.0	1.0
1385_0000016	1.0	1.0
1385_0000017	1.0	1.0
1385_0000020	2.0	1.0
1385_0000021	2.0	2.0
1385_0000022	0.0	2.0
1385_0000023	1.0	2.0
1385_0000033	1.0	2.0
1385_0000034	1.0	1.0
1385_0000035	1.0	1.0
1385_0000036	1.0	2.0
1385_0000037	0.0	2.0
1385_0000038	2.0	1.0
1385_0000039	1.0	1.0
1385_0000040	1.0	1.0
1385_0000041	2.0	2.0
1385_0000042	2.0	2.0
1385_0000043	2.0	1.0
1385_0000044	2.0	2.0
1385_0000045	2.0	2.0
1385_0000047	2.0	2.0
1385_0000048	2.0	2.0
1385_0000049	2.0	1.0
1385_0000050	2.0	1.0
1385_0000051	2.0	2.0
1385_0000052	2.0	1.0
1385_0000053	1.0	2.0
1385_0000054	2.0	2.0
1385_0000057	2.0	1.0
1385_0000058	2.0	2.0
1385_0000059	2.0	2.0
1385_0000095	1.0	1.0
1385_0000097	1.0	1.0
1385_0000098	1.0	1.0
1385_0000099	1.0	1.0
1385_0000100	1.0	1.0
1385_0000101	1.0	1.0
1385_0000102	1.0	2.0
1385_0000103	1.0	1.0
1385_0000104	1.0	1.0
1385_0000114	2.0	2.0
1385_0000119	2.0	1.0
1385_0000120	0.0	1.0
1385_0000122	1.0	2.0
1385_0000123	1.0	2.0
1385_0000124	2.0	2.0
1385_0000125	1.0	2.0
1385_0000126	1.0	1.0
1385_0000127	1.0	2.0
1385_0000128	1.0	2.0
1385_0000129	2.0	2.0
1385_0000130	2.0	1.0
1385_0001103	1.0	1.0
1385_0001104	1.0	1.0
1385_0001105	1.0	1.0
1385_0001107	2.0	1.0
1385_0001108	1.0	1.0
1385_0001109	1.0	1.0
1385_0001110	1.0	1.0
1385_0001111	2.0	1.0
1385_0001112	1.0	2.0
1385_0001113	1.0	1.0
1385_0001118	1.0	1.0
1385_0001119	1.0	1.0
1385_0001120	1.0	1.0
1385_0001121	1.0	2.0
1385_0001122	1.0	1.0
1385_0001123	1.0	1.0
1385_0001124	1.0	1.0
1385_0001125	1.0	1.0
1385_0001126	1.0	1.0
1385_0001127	1.0	2.0
1385_0001128	1.0	1.0
1385_0001129	1.0	2.0
1385_0001130	1.0	1.0
1385_0001131	1.0	1.0
1385_0001132	1.0	1.0
1385_0001133	1.0	2.0
1385_0001134	1.0	1.0
1385_0001135	1.0	2.0
1385_0001136	1.0	1.0
1385_0001137	1.0	1.0
1385_0001138	1.0	1.0
1385_0001147	1.0	1.0
1385_0001148	1.0	2.0
1385_0001149	2.0	2.0
1385_0001150	2.0	1.0
1385_0001151	2.0	2.0
1385_0001152	2.0	2.0
1385_0001153	2.0	2.0
1385_0001154	2.0	2.0
1385_0001155	2.0	1.0
1385_0001156	2.0	1.0
1385_0001157	2.0	1.0
1385_0001158	2.0	2.0
1385_0001159	2.0	2.0
1385_0001160	1.0	3.0
1385_0001161	2.0	2.0
1385_0001162	1.0	1.0
1385_0001163	1.0	2.0
1385_0001164	1.0	2.0
1385_0001165	1.0	2.0
1385_0001166	1.0	2.0
1385_0001167	1.0	2.0
1385_0001169	1.0	1.0
1385_0001170	0.0	1.0
1385_0001171	1.0	1.0
1385_0001172	1.0	1.0
1385_0001173	0.0	1.0
1385_0001174	1.0	1.0
1385_0001175	1.0	1.0
1385_0001178	0.0	1.0
1385_0001188	1.0	2.0
1385_0001189	0.0	1.0
1385_0001190	0.0	1.0
1385_0001191	1.0	1.0
1385_0001192	0.0	2.0
1385_0001193	1.0	2.0
1385_0001194	1.0	1.0
1385_0001195	2.0	2.0
1385_0001196	1.0	2.0
1385_0001197	1.0	1.0
1385_0001198	1.0	2.0
1385_0001199	0.0	2.0
1385_0001501	0.0	1.0
1385_0001503	1.0	1.0
1385_0001522	1.0	1.0
1385_0001523	1.0	2.0
1385_0001524	1.0	2.0
1385_0001525	1.0	2.0
1385_0001526	0.0	1.0
1385_0001527	1.0	1.0
1385_0001528	1.0	2.0
1385_0001712	1.0	3.0
1385_0001714	0.0	1.0
1385_0001715	0.0	2.0
1385_0001716	1.0	1.0
1385_0001717	1.0	2.0
1385_0001718	1.0	1.0
1385_0001719	1.0	1.0
1385_0001720	0.0	1.0
1385_0001723	0.0	1.0
1385_0001724	1.0	2.0
1385_0001725	1.0	1.0
1385_0001726	0.0	2.0
1385_0001727	0.0	1.0
1385_0001728	1.0	1.0
1385_0001729	1.0	2.0
1385_0001730	1.0	2.0
1385_0001732	1.0	2.0
1385_0001733	1.0	2.0
1385_0001734	1.0	1.0
1385_0001736	1.0	1.0
1385_0001737	2.0	2.0
1385_0001738	0.0	1.0
1385_0001739	0.0	2.0
1385_0001740	1.0	1.0
1385_0001741	0.0	1.0
1385_0001742	0.0	1.0
1385_0001744	0.0	1.0
1385_0001746	1.0	2.0
1385_0001747	0.0	2.0
1385_0001748	1.0	1.0
1385_0001749	0.0	1.0
1385_0001750	0.0	1.0
1385_0001751	1.0	2.0
1385_0001752	0.0	2.0
1385_0001753	1.0	2.0
1385_0001754	0.0	2.0
1385_0001756	1.0	1.0
1385_0001757	1.0	1.0
1385_0001758	1.0	1.0
1385_0001759	1.0	1.0
1385_0001760	0.0	1.0
1385_0001761	1.0	1.0
1385_0001762	1.0	1.0
1385_0001764	1.0	2.0
1385_0001765	0.0	1.0
1385_0001766	2.0	2.0
1385_0001767	1.0	2.0
1385_0001768	1.0	1.0
1385_0001771	1.0	1.0
1385_0001772	0.0	1.0
1385_0001773	1.0	1.0
1385_0001774	0.0	1.0
1385_0001775	1.0	2.0
1385_0001785	0.0	2.0
1385_0001786	1.0	2.0
1385_0001787	0.0	1.0
1385_0001788	0.0	2.0
1385_0001789	1.0	1.0
1385_0001790	1.0	2.0
1385_0001791	1.0	1.0
1385_0001792	1.0	2.0
1385_0001793	1.0	2.0
1385_0001794	1.0	2.0
1385_0001795	1.0	1.0
1385_0001796	0.0	1.0
1385_0001798	1.0	2.0
1385_0001799	1.0	2.0
1385_0001800	1.0	2.0
1395_0000333	1.0	2.0
1395_0000337	1.0	1.0
1395_0000338	1.0	2.0
1395_0000340	1.0	2.0
1395_0000341	1.0	1.0
1395_0000353	1.0	1.0
1395_0000354	0.0	1.0
1395_0000355	1.0	1.0
1395_0000356	1.0	1.0
1395_0000357	2.0	3.0
1395_0000359	1.0	2.0
1395_0000360	2.0	2.0
1395_0000361	1.0	2.0
1395_0000364	1.0	2.0
1395_0000365	2.0	2.0
1395_0000366	2.0	2.0
1395_0000368	0.0	1.0
1395_0000369	2.0	2.0
1395_0000376	2.0	3.0
1395_0000378	1.0	2.0
1395_0000379	1.0	2.0
1395_0000380	1.0	2.0
1395_0000383	1.0	2.0
1395_0000387	3.0	3.0
1395_0000388	1.0	2.0
1395_0000389	1.0	1.0
1395_0000390	1.0	1.0
1395_0000391	2.0	3.0
1395_0000392	1.0	2.0
1395_0000396	1.0	2.0
1395_0000398	1.0	2.0
1395_0000399	1.0	2.0
1395_0000402	1.0	2.0
1395_0000403	1.0	2.0
1395_0000404	1.0	3.0
1395_0000409	2.0	3.0
1395_0000413	1.0	3.0
1395_0000414	1.0	2.0
1395_0000415	1.0	2.0
1395_0000432	1.0	2.0
1395_0000438	2.0	3.0
1395_0000443	1.0	3.0
1395_0000446	2.0	3.0
1395_0000447	1.0	2.0
1395_0000448	1.0	2.0
1395_0000449	1.0	2.0
1395_0000450	1.0	2.0
1395_0000451	2.0	2.0
1395_0000452	1.0	1.0
1395_0000454	2.0	2.0
1395_0000455	1.0	1.0
1395_0000458	1.0	2.0
1395_0000460	1.0	1.0
1395_0000462	2.0	2.0
1395_0000465	1.0	1.0
1395_0000469	1.0	1.0
1395_0000470	1.0	2.0
1395_0000471	1.0	2.0
1395_0000499	1.0	2.0
1395_0000500	1.0	1.0
1395_0000504	1.0	2.0
1395_0000512	2.0	2.0
1395_0000513	1.0	2.0
1395_0000514	2.0	2.0
1395_0000515	1.0	2.0
1395_0000516	1.0	1.0
1395_0000518	2.0	2.0
1395_0000525	2.0	1.0
1395_0000526	1.0	1.0
1395_0000527	1.0	1.0
1395_0000528	2.0	1.0
1395_0000529	2.0	1.0
1395_0000531	2.0	2.0
1395_0000533	2.0	3.0
1395_0000534	1.0	2.0
1395_0000535	1.0	1.0
1395_0000537	1.0	2.0
1395_0000547	1.0	2.0
1395_0000548	1.0	2.0
1395_0000549	1.0	3.0
1395_0000550	1.0	3.0
1395_0000551	2.0	3.0
1395_0000552	2.0	2.0
1395_0000553	1.0	2.0
1395_0000554	2.0	2.0
1395_0000555	2.0	1.0
1395_0000556	1.0	2.0
1395_0000557	2.0	3.0
1395_0000559	1.0	2.0
1395_0000560	1.0	2.0
1395_0000563	2.0	2.0
1395_0000564	1.0	2.0
1395_0000565	1.0	2.0
1395_0000572	1.0	1.0
1395_0000575	1.0	2.0
1395_0000579	1.0	2.0
1395_0000581	1.0	3.0
1395_0000582	1.0	1.0
1395_0000583	1.0	2.0
1395_0000584	0.0	1.0
1395_0000585	1.0	2.0
1395_0000587	0.0	1.0
1395_0000591	0.0	1.0
1395_0000593	1.0	2.0
1395_0000595	0.0	1.0
1395_0000596	2.0	2.0
1395_0000597	1.0	2.0
1395_0000598	1.0	2.0
1395_0000599	1.0	1.0
1395_0000602	1.0	1.0
1395_0000604	0.0	1.0
1395_0000606	1.0	2.0
1395_0000607	0.0	1.0
1395_0000608	1.0	2.0
1395_0000609	0.0	1.0
1395_0000610	2.0	2.0
1395_0000611	1.0	1.0
1395_0000612	1.0	2.0
1395_0000626	0.0	3.0
1395_0000627	1.0	2.0
1395_0000628	1.0	2.0
1395_0000630	1.0	3.0
1395_0000631	1.0	2.0
1395_0000635	1.0	1.0
1395_0000636	1.0	1.0
1395_0000639	0.0	2.0
1395_0000642	1.0	2.0
1395_0000644	1.0	3.0
1395_0000646	1.0	2.0
1395_0000649	1.0	2.0
1395_0001010	2.0	2.0
1395_0001013	1.0	2.0
1395_0001015	1.0	2.0
1395_0001016	1.0	2.0
1395_0001017	1.0	2.0
1395_0001019	1.0	1.0
1395_0001020	1.0	2.0
1395_0001021	1.0	2.0
1395_0001022	1.0	2.0
1395_0001023	0.0	2.0
1395_0001024	0.0	2.0
1395_0001028	1.0	2.0
1395_0001033	1.0	2.0
1395_0001034	1.0	2.0
1395_0001040	1.0	1.0
1395_0001045	2.0	2.0
1395_0001058	1.0	2.0
1395_0001060	1.0	2.0
1395_0001061	1.0	3.0
1395_0001064	1.0	2.0
1395_0001065	1.0	2.0
1395_0001066	1.0	2.0
1395_0001067	1.0	2.0
1395_0001068	0.0	2.0
1395_0001069	1.0	2.0
1395_0001070	1.0	3.0
1395_0001071	1.0	1.0
1395_0001073	1.0	3.0
1395_0001074	1.0	2.0
1395_0001075	1.0	2.0
1395_0001076	1.0	3.0
1395_0001078	1.0	2.0
1395_0001080	2.0	2.0
1395_0001084	1.0	2.0
1395_0001090	1.0	3.0
1395_0001093	1.0	2.0
1395_0001101	1.0	2.0
1395_0001103	1.0	3.0
1395_0001104	1.0	2.0
1395_0001108	1.0	1.0
1395_0001109	1.0	2.0
1395_0001114	1.0	2.0
1395_0001115	1.0	2.0
1395_0001116	2.0	2.0
1395_0001117	1.0	2.0
1395_0001118	1.0	1.0
1395_0001119	1.0	3.0
1395_0001120	1.0	2.0
1395_0001121	0.0	1.0
1395_0001122	1.0	1.0
1395_0001123	0.0	2.0
1395_0001124	0.0	1.0
1395_0001126	1.0	2.0
1395_0001131	1.0	1.0
1395_0001132	1.0	2.0
1395_0001133	1.0	3.0
1395_0001141	1.0	2.0
1395_0001145	1.0	3.0
1395_0001146	0.0	1.0
1395_0001147	0.0	3.0
1395_0001149	1.0	2.0
1395_0001150	1.0	2.0
1395_0001158	1.0	2.0
1395_0001160	1.0	2.0
1395_0001161	1.0	2.0
1395_0001164	2.0	2.0
1395_0001167	1.0	2.0
1395_0001169	1.0	2.0
1395_0001170	1.0	2.0
1395_0001171	1.0	2.0
Language = IT, Weighted F1-score = 0.3160790681280041, Dimension = Sociolinguisticappropriateness

MONOLINGUAL EXPERIMENTS
MONOLINGUAL Experiments with:  DE
130.68810916179336 82.83744971317162
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']
LANGUAGE: DE, 0th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.17
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.81
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        12
         1.0       0.63      0.84      0.72        61
         2.0       0.76      0.58      0.66        66
         3.0       0.75      0.95      0.84        59
         4.0       0.00      0.00      0.00         8

    accuracy                           0.70       206
   macro avg       0.43      0.47      0.44       206
weighted avg       0.64      0.70      0.66       206

[[ 0 12  0  0  0]
 [ 0 51 10  0  0]
 [ 0 17 38 11  0]
 [ 0  1  2 56  0]
 [ 0  0  0  8  0]]
0.6619986094884293
206 206 206



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.86
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.72
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        12
         1.0       0.71      0.64      0.67        61
         2.0       0.67      0.77      0.72        66
         3.0       0.75      0.95      0.84        59
         4.0       0.00      0.00      0.00         8

    accuracy                           0.71       206
   macro avg       0.43      0.47      0.45       206
weighted avg       0.64      0.71      0.67       206

[[ 0 12  0  0  0]
 [ 0 39 22  0  0]
 [ 0  4 51 11  0]
 [ 0  0  3 56  0]
 [ 0  0  0  8  0]]
0.6686365287347407
206 206 206



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.73
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.62
              precision    recall  f1-score   support

         0.0       1.00      0.08      0.15        12
         1.0       0.72      0.84      0.77        61
         2.0       0.78      0.71      0.75        66
         3.0       0.76      0.95      0.84        59
         4.0       0.00      0.00      0.00         8

    accuracy                           0.75       206
   macro avg       0.65      0.52      0.50       206
weighted avg       0.74      0.75      0.72       206

[[ 1 11  0  0  0]
 [ 0 51 10  0  0]
 [ 0  9 47 10  0]
 [ 0  0  3 56  0]
 [ 0  0  0  8  0]]
0.717984578868585
206 206 206



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.61
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.66
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        12
         1.0       0.72      0.70      0.71        61
         2.0       0.71      0.74      0.73        66
         3.0       0.74      0.97      0.84        59
         4.0       0.00      0.00      0.00         8

    accuracy                           0.72       206
   macro avg       0.43      0.48      0.45       206
weighted avg       0.65      0.72      0.68       206

[[ 0 12  0  0  0]
 [ 0 43 18  0  0]
 [ 0  5 49 12  0]
 [ 0  0  2 57  0]
 [ 0  0  0  8  0]]
0.6831182784702851
206 206 206
Filename	True Label	Prediction
1023_0001416	3.0	3.0
1023_0101753	3.0	3.0
1023_0101893	3.0	3.0
1023_0101909	4.0	3.0
1023_0103829	2.0	3.0
1023_0103830	3.0	3.0
1023_0103833	4.0	3.0
1023_0103880	3.0	3.0
1023_0107042	3.0	3.0
1023_0107244	3.0	3.0
1023_0107672	2.0	3.0
1023_0107727	3.0	3.0
1023_0108306	3.0	3.0
1023_0108641	4.0	3.0
1023_0108649	3.0	3.0
1023_0108752	3.0	3.0
1023_0108814	3.0	3.0
1023_0108815	3.0	3.0
1023_0108888	3.0	3.0
1023_0108889	3.0	3.0
1023_0109026	2.0	3.0
1023_0109030	3.0	3.0
1023_0109151	3.0	3.0
1023_0109392	3.0	3.0
1023_0109396	2.0	3.0
1023_0109401	3.0	3.0
1023_0109422	3.0	3.0
1023_0109516	3.0	3.0
1023_0109519	2.0	3.0
1023_0109614	2.0	2.0
1023_0109651	3.0	3.0
1023_0109671	3.0	3.0
1023_0109721	2.0	3.0
1023_0109917	3.0	3.0
1023_0109954	3.0	3.0
1031_0002004	3.0	3.0
1031_0002005	3.0	3.0
1031_0002061	3.0	3.0
1031_0002089	3.0	3.0
1031_0002196	3.0	3.0
1031_0002199	3.0	3.0
1031_0003042	3.0	3.0
1031_0003053	3.0	3.0
1031_0003072	3.0	3.0
1031_0003074	3.0	3.0
1031_0003077	3.0	3.0
1031_0003092	2.0	3.0
1031_0003106	3.0	3.0
1031_0003130	4.0	3.0
1031_0003133	4.0	3.0
1031_0003141	3.0	3.0
1031_0003144	3.0	3.0
1031_0003156	3.0	3.0
1031_0003173	3.0	3.0
1031_0003181	4.0	3.0
1031_0003190	3.0	3.0
1031_0003207	4.0	3.0
1031_0003220	3.0	3.0
1031_0003230	3.0	3.0
1031_0003231	3.0	3.0
1031_0003233	3.0	3.0
1031_0003234	3.0	3.0
1031_0003244	4.0	3.0
1031_0003249	3.0	3.0
1031_0003261	3.0	3.0
1031_0003274	3.0	3.0
1031_0003310	3.0	3.0
1031_0003327	2.0	3.0
1031_0003354	3.0	3.0
1031_0003355	3.0	3.0
1031_0003356	3.0	3.0
1031_0003386	2.0	3.0
1031_0003393	3.0	3.0
1031_0003414	3.0	3.0
1061_0120271	2.0	2.0
1061_0120277	1.0	2.0
1061_0120285	1.0	2.0
1061_0120301	2.0	2.0
1061_0120312	1.0	1.0
1061_0120325	2.0	2.0
1061_0120326	2.0	2.0
1061_0120333	2.0	2.0
1061_0120335	2.0	3.0
1061_0120351	2.0	2.0
1061_0120353	1.0	1.0
1061_0120359	1.0	2.0
1061_0120367	3.0	2.0
1061_0120369	2.0	2.0
1061_0120372	2.0	2.0
1061_0120388	1.0	2.0
1061_0120423	2.0	3.0
1061_0120428	2.0	2.0
1061_0120431	2.0	2.0
1061_0120433	1.0	2.0
1061_0120440	1.0	1.0
1061_0120443	0.0	1.0
1061_0120455	2.0	2.0
1061_0120458	3.0	3.0
1061_0120459	2.0	2.0
1061_0120482	2.0	2.0
1061_0120484	2.0	2.0
1061_0120486	2.0	2.0
1061_0120493	2.0	2.0
1061_0120495	2.0	2.0
1061_0120498	2.0	2.0
1061_0120853	2.0	2.0
1061_0120883	2.0	2.0
1061_0120887	1.0	2.0
1061_1029113	1.0	2.0
1061_1029120	1.0	2.0
1061_1202910	2.0	2.0
1061_1202912	2.0	2.0
1061_1202913	2.0	1.0
1071_0020001	1.0	1.0
1071_0024680	2.0	2.0
1071_0024688	1.0	2.0
1071_0024701	2.0	2.0
1071_0024761	1.0	1.0
1071_0024765	0.0	1.0
1071_0024766	1.0	1.0
1071_0024769	0.0	1.0
1071_0024777	1.0	1.0
1071_0024778	0.0	1.0
1071_0024784	1.0	1.0
1071_0024802	1.0	2.0
1071_0024822	0.0	1.0
1071_0024826	1.0	2.0
1071_0024827	1.0	1.0
1071_0024833	1.0	1.0
1071_0024835	1.0	1.0
1071_0024837	0.0	1.0
1071_0024844	1.0	1.0
1071_0024848	1.0	1.0
1071_0024854	0.0	1.0
1071_0024857	1.0	1.0
1071_0024863	1.0	1.0
1071_0024866	2.0	2.0
1071_0024874	1.0	1.0
1071_0024878	2.0	2.0
1071_0024879	1.0	1.0
1071_0242022	0.0	1.0
1071_0242091	1.0	1.0
1071_0243593	1.0	1.0
1071_0248307	2.0	1.0
1071_0248308	1.0	1.0
1071_0248321	1.0	1.0
1071_0248329	1.0	1.0
1071_0248332	2.0	2.0
1071_0248340	0.0	1.0
1071_0248343	1.0	1.0
1071_0248344	1.0	1.0
1071_0248345	1.0	2.0
1071_0248347	1.0	1.0
1071_0248349	1.0	1.0
1091_0000001	1.0	2.0
1091_0000007	2.0	2.0
1091_0000009	0.0	1.0
1091_0000014	0.0	1.0
1091_0000030	1.0	1.0
1091_0000036	1.0	2.0
1091_0000037	1.0	1.0
1091_0000038	1.0	1.0
1091_0000039	1.0	1.0
1091_0000041	1.0	1.0
1091_0000047	1.0	1.0
1091_0000048	1.0	1.0
1091_0000052	0.0	1.0
1091_0000054	1.0	1.0
1091_0000062	1.0	1.0
1091_0000067	2.0	1.0
1091_0000071	2.0	2.0
1091_0000072	2.0	2.0
1091_0000075	2.0	2.0
1091_0000076	3.0	2.0
1091_0000079	2.0	2.0
1091_0000102	1.0	2.0
1091_0000114	2.0	2.0
1091_0000123	2.0	2.0
1091_0000145	1.0	1.0
1091_0000151	1.0	1.0
1091_0000153	2.0	2.0
1091_0000154	2.0	3.0
1091_0000161	2.0	2.0
1091_0000165	1.0	1.0
1091_0000166	2.0	2.0
1091_0000172	2.0	1.0
1091_0000194	2.0	2.0
1091_0000196	2.0	1.0
1091_0000219	2.0	2.0
1091_0000221	1.0	1.0
1091_0000224	1.0	1.0
1091_0000227	1.0	2.0
1091_0000231	2.0	2.0
1091_0000236	1.0	2.0
1091_0000237	2.0	2.0
1091_0000240	1.0	1.0
1091_0000242	2.0	2.0
1091_0000247	2.0	2.0
1091_0000251	2.0	2.0
1091_0000254	1.0	1.0
1091_0000260	2.0	2.0
1091_0000261	2.0	2.0
1091_0000264	1.0	1.0
1091_0000268	2.0	2.0
1091_0000273	2.0	2.0
1091_0000275	1.0	2.0
LANGUAGE: DE, 1th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.26
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.99
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.68      0.74      0.71        61
         2.0       0.67      0.55      0.61        65
         3.0       0.68      0.98      0.81        59
         4.0       0.00      0.00      0.00         9

    accuracy                           0.68       205
   macro avg       0.41      0.45      0.42       205
weighted avg       0.61      0.68      0.63       205

[[ 0 10  1  0  0]
 [ 0 45 16  0  0]
 [ 0 11 36 18  0]
 [ 0  0  1 58  0]
 [ 0  0  0  9  0]]
0.634555391838574
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.84
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.74
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.72      0.69      0.71        61
         2.0       0.69      0.72      0.71        65
         3.0       0.73      0.98      0.84        59
         4.0       0.00      0.00      0.00         9

    accuracy                           0.72       205
   macro avg       0.43      0.48      0.45       205
weighted avg       0.65      0.72      0.68       205

[[ 0 10  1  0  0]
 [ 0 42 19  0  0]
 [ 0  6 47 12  0]
 [ 0  0  1 58  0]
 [ 0  0  0  9  0]]
0.6760628100098665
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.66
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.78
              precision    recall  f1-score   support

         0.0       1.00      0.27      0.43        11
         1.0       0.78      0.69      0.73        61
         2.0       0.67      0.63      0.65        65
         3.0       0.68      1.00      0.81        59
         4.0       0.00      0.00      0.00         9

    accuracy                           0.71       205
   macro avg       0.63      0.52      0.52       205
weighted avg       0.69      0.71      0.68       205

[[ 3  7  1  0  0]
 [ 0 42 19  0  0]
 [ 0  5 41 19  0]
 [ 0  0  0 59  0]
 [ 0  0  0  9  0]]
0.6793040305456233
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.55
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.71
              precision    recall  f1-score   support

         0.0       0.75      0.27      0.40        11
         1.0       0.76      0.69      0.72        61
         2.0       0.70      0.72      0.71        65
         3.0       0.73      0.98      0.84        59
         4.0       0.00      0.00      0.00         9

    accuracy                           0.73       205
   macro avg       0.59      0.53      0.54       205
weighted avg       0.70      0.73      0.70       205

[[ 3  7  1  0  0]
 [ 1 42 18  0  0]
 [ 0  6 47 12  0]
 [ 0  0  1 58  0]
 [ 0  0  0  9  0]]
0.7046560755099169
205 205 205
Filename	True Label	Prediction
1023_0001418	3.0	3.0
1023_0001422	2.0	2.0
1023_0101683	3.0	3.0
1023_0101684	2.0	3.0
1023_0101690	2.0	3.0
1023_0101691	3.0	3.0
1023_0101693	3.0	3.0
1023_0101695	2.0	2.0
1023_0101700	3.0	3.0
1023_0101843	3.0	3.0
1023_0101847	3.0	3.0
1023_0101851	3.0	3.0
1023_0101852	3.0	3.0
1023_0101853	2.0	3.0
1023_0101854	2.0	2.0
1023_0101897	2.0	3.0
1023_0101901	3.0	3.0
1023_0101907	3.0	3.0
1023_0103823	3.0	3.0
1023_0103831	3.0	3.0
1023_0103838	3.0	3.0
1023_0103883	3.0	3.0
1023_0104203	3.0	3.0
1023_0104206	3.0	3.0
1023_0107729	3.0	3.0
1023_0107740	3.0	3.0
1023_0107780	3.0	3.0
1023_0108305	3.0	3.0
1023_0108510	3.0	3.0
1023_0108520	3.0	3.0
1023_0108751	3.0	3.0
1023_0108753	2.0	2.0
1023_0108890	3.0	3.0
1023_0108935	2.0	3.0
1023_0108955	3.0	3.0
1023_0108992	3.0	3.0
1023_0109038	3.0	3.0
1023_0109039	3.0	3.0
1023_0109518	2.0	2.0
1023_0109606	3.0	3.0
1023_0109946	2.0	3.0
1023_0111896	2.0	2.0
1031_0002006	4.0	3.0
1031_0002079	4.0	3.0
1031_0002087	3.0	3.0
1031_0002187	3.0	3.0
1031_0002198	3.0	3.0
1031_0003023	3.0	3.0
1031_0003048	4.0	3.0
1031_0003063	4.0	3.0
1031_0003071	3.0	3.0
1031_0003078	3.0	3.0
1031_0003088	4.0	3.0
1031_0003091	2.0	3.0
1031_0003132	3.0	3.0
1031_0003140	3.0	3.0
1031_0003165	2.0	3.0
1031_0003179	3.0	3.0
1031_0003183	4.0	3.0
1031_0003186	3.0	3.0
1031_0003187	3.0	3.0
1031_0003191	3.0	3.0
1031_0003214	3.0	3.0
1031_0003219	3.0	3.0
1031_0003225	3.0	3.0
1031_0003226	3.0	3.0
1031_0003239	4.0	3.0
1031_0003260	3.0	3.0
1031_0003273	3.0	3.0
1031_0003330	3.0	3.0
1031_0003336	3.0	3.0
1031_0003339	3.0	3.0
1031_0003352	2.0	3.0
1031_0003353	2.0	3.0
1031_0003357	3.0	3.0
1031_0003365	3.0	3.0
1031_0003369	3.0	3.0
1031_0003387	3.0	3.0
1031_0003388	3.0	3.0
1031_0003389	3.0	3.0
1031_0003391	2.0	3.0
1031_0003407	3.0	3.0
1031_0003409	4.0	3.0
1031_0003415	4.0	3.0
1061_0120272	1.0	1.0
1061_0120273	1.0	2.0
1061_0120279	1.0	2.0
1061_0120283	1.0	1.0
1061_0120286	1.0	1.0
1061_0120290	1.0	2.0
1061_0120291	1.0	1.0
1061_0120295	0.0	2.0
1061_0120300	2.0	2.0
1061_0120304	2.0	2.0
1061_0120308	2.0	2.0
1061_0120313	2.0	1.0
1061_0120316	2.0	2.0
1061_0120327	2.0	2.0
1061_0120330	2.0	2.0
1061_0120346	2.0	2.0
1061_0120348	1.0	1.0
1061_0120354	1.0	1.0
1061_0120355	1.0	1.0
1061_0120357	3.0	2.0
1061_0120373	2.0	2.0
1061_0120375	2.0	1.0
1061_0120376	2.0	2.0
1061_0120382	1.0	2.0
1061_0120384	1.0	1.0
1061_0120386	1.0	2.0
1061_0120390	2.0	3.0
1061_0120404	1.0	1.0
1061_0120410	2.0	2.0
1061_0120430	2.0	2.0
1061_0120432	2.0	2.0
1061_0120449	2.0	2.0
1061_0120450	2.0	2.0
1061_0120457	2.0	2.0
1061_0120487	2.0	2.0
1061_0120488	2.0	2.0
1061_0120491	2.0	2.0
1061_0120496	2.0	2.0
1061_0120499	2.0	2.0
1061_0120500	2.0	2.0
1061_0120857	2.0	2.0
1061_0120876	2.0	2.0
1061_0120877	2.0	2.0
1061_0120888	1.0	2.0
1061_0120889	1.0	1.0
1061_1029118	1.0	2.0
1061_1202916	2.0	2.0
1061_1202918	1.0	2.0
1071_0024687	1.0	1.0
1071_0024690	1.0	2.0
1071_0024691	1.0	2.0
1071_0024699	1.0	1.0
1071_0024702	1.0	2.0
1071_0024704	1.0	1.0
1071_0024705	1.0	2.0
1071_0024714	2.0	2.0
1071_0024759	0.0	1.0
1071_0024768	1.0	1.0
1071_0024774	0.0	0.0
1071_0024797	0.0	1.0
1071_0024808	1.0	1.0
1071_0024814	1.0	1.0
1071_0024816	1.0	1.0
1071_0024818	2.0	1.0
1071_0024834	2.0	2.0
1071_0024845	0.0	1.0
1071_0024855	1.0	1.0
1071_0024859	1.0	1.0
1071_0024860	1.0	1.0
1071_0024861	0.0	1.0
1071_0024862	1.0	1.0
1071_0024865	2.0	1.0
1071_0024873	1.0	1.0
1071_0024875	1.0	1.0
1071_0241831	1.0	2.0
1071_0242013	1.0	1.0
1071_0242092	0.0	0.0
1071_0243501	1.0	1.0
1071_0248302	1.0	0.0
1071_0248311	1.0	1.0
1071_0248312	1.0	1.0
1071_0248313	1.0	2.0
1071_0248319	0.0	1.0
1071_0248320	0.0	0.0
1071_0248323	1.0	1.0
1071_0248327	0.0	1.0
1071_0248334	2.0	2.0
1091_0000002	2.0	2.0
1091_0000011	2.0	2.0
1091_0000015	1.0	2.0
1091_0000016	0.0	1.0
1091_0000023	1.0	1.0
1091_0000025	1.0	1.0
1091_0000029	1.0	1.0
1091_0000033	1.0	1.0
1091_0000034	1.0	1.0
1091_0000044	1.0	2.0
1091_0000045	2.0	2.0
1091_0000049	1.0	1.0
1091_0000086	1.0	1.0
1091_0000095	2.0	1.0
1091_0000125	2.0	2.0
1091_0000126	2.0	2.0
1091_0000144	1.0	1.0
1091_0000174	1.0	1.0
1091_0000197	2.0	2.0
1091_0000198	1.0	2.0
1091_0000204	1.0	2.0
1091_0000206	1.0	1.0
1091_0000207	2.0	2.0
1091_0000208	2.0	1.0
1091_0000212	2.0	2.0
1091_0000213	1.0	1.0
1091_0000215	2.0	2.0
1091_0000216	2.0	2.0
1091_0000218	1.0	1.0
1091_0000225	1.0	1.0
1091_0000244	2.0	2.0
1091_0000248	1.0	1.0
1091_0000257	2.0	2.0
1091_0000266	2.0	2.0
LANGUAGE: DE, 2th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.20
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.96
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.71      0.72      0.72        61
         2.0       0.61      0.48      0.53        65
         3.0       0.60      0.93      0.73        59
         4.0       0.00      0.00      0.00         9

    accuracy                           0.63       205
   macro avg       0.38      0.43      0.40       205
weighted avg       0.58      0.63      0.59       205

[[ 0 11  0  0  0]
 [ 0 44 16  1  0]
 [ 0  7 31 27  0]
 [ 0  0  4 55  0]
 [ 0  0  0  9  0]]
0.5920184789436106
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.83
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.78
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.66      0.90      0.76        61
         2.0       0.68      0.52      0.59        65
         3.0       0.67      0.81      0.73        59
         4.0       0.00      0.00      0.00         9

    accuracy                           0.67       205
   macro avg       0.40      0.45      0.42       205
weighted avg       0.60      0.67      0.63       205

[[ 0 11  0  0  0]
 [ 0 55  6  0  0]
 [ 0 16 34 15  0]
 [ 0  1 10 48  0]
 [ 0  0  0  9  0]]
0.6257007124502045
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.71
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.98
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.76      0.56      0.64        61
         2.0       0.57      0.66      0.61        65
         3.0       0.64      0.92      0.75        59
         4.0       0.00      0.00      0.00         9

    accuracy                           0.64       205
   macro avg       0.39      0.43      0.40       205
weighted avg       0.59      0.64      0.60       205

[[ 0 11  0  0  0]
 [ 0 34 27  0  0]
 [ 0  0 43 22  0]
 [ 0  0  5 54  0]
 [ 0  0  0  9  0]]
0.6015153507330222
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.61
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.80
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.74      0.79      0.76        61
         2.0       0.68      0.62      0.65        65
         3.0       0.65      0.90      0.76        59
         4.0       0.00      0.00      0.00         9

    accuracy                           0.69       205
   macro avg       0.41      0.46      0.43       205
weighted avg       0.62      0.69      0.65       205

[[ 0 11  0  0  0]
 [ 0 48 13  0  0]
 [ 0  6 40 19  0]
 [ 0  0  6 53  0]
 [ 0  0  0  9  0]]
0.6491858678955453
205 205 205
Filename	True Label	Prediction
1023_0001419	3.0	3.0
1023_0001420	3.0	3.0
1023_0101688	3.0	3.0
1023_0101694	3.0	3.0
1023_0101752	3.0	3.0
1023_0101841	2.0	3.0
1023_0101848	2.0	2.0
1023_0101855	2.0	2.0
1023_0101894	2.0	3.0
1023_0101895	3.0	3.0
1023_0101899	2.0	3.0
1023_0101906	2.0	3.0
1023_0103821	3.0	3.0
1023_0103822	2.0	2.0
1023_0103824	3.0	3.0
1023_0103827	3.0	3.0
1023_0103832	2.0	2.0
1023_0103841	3.0	3.0
1023_0104207	2.0	3.0
1023_0107074	3.0	3.0
1023_0107682	2.0	1.0
1023_0107725	2.0	3.0
1023_0107781	3.0	3.0
1023_0107783	3.0	2.0
1023_0107784	2.0	2.0
1023_0108648	3.0	3.0
1023_0108811	3.0	3.0
1023_0108813	3.0	3.0
1023_0108887	2.0	3.0
1023_0108932	2.0	3.0
1023_0108933	3.0	3.0
1023_0108958	2.0	3.0
1023_0109096	3.0	3.0
1023_0109249	3.0	3.0
1023_0109250	2.0	3.0
1023_0109267	2.0	3.0
1023_0109399	2.0	3.0
1023_0109400	3.0	3.0
1023_0109402	2.0	3.0
1023_0109495	3.0	3.0
1023_0109500	2.0	3.0
1023_0109505	3.0	3.0
1023_0109522	3.0	3.0
1023_0109524	3.0	3.0
1023_0109590	2.0	3.0
1023_0109716	3.0	3.0
1023_0109717	3.0	3.0
1023_0109947	2.0	3.0
1031_0001703	4.0	3.0
1031_0001949	3.0	3.0
1031_0001997	3.0	3.0
1031_0002003	3.0	3.0
1031_0002043	4.0	3.0
1031_0002091	3.0	3.0
1031_0002131	3.0	3.0
1031_0002197	3.0	3.0
1031_0003076	4.0	3.0
1031_0003095	3.0	3.0
1031_0003097	3.0	3.0
1031_0003098	4.0	3.0
1031_0003127	4.0	3.0
1031_0003129	3.0	3.0
1031_0003131	3.0	3.0
1031_0003135	3.0	3.0
1031_0003136	4.0	3.0
1031_0003146	4.0	3.0
1031_0003149	3.0	3.0
1031_0003155	3.0	3.0
1031_0003157	4.0	3.0
1031_0003160	3.0	3.0
1031_0003166	2.0	2.0
1031_0003169	3.0	3.0
1031_0003180	4.0	3.0
1031_0003185	3.0	3.0
1031_0003189	3.0	3.0
1031_0003211	2.0	3.0
1031_0003218	3.0	3.0
1031_0003236	3.0	3.0
1031_0003237	3.0	3.0
1031_0003242	3.0	3.0
1031_0003246	3.0	3.0
1031_0003272	3.0	2.0
1031_0003315	3.0	3.0
1031_0003368	3.0	3.0
1061_0012029	3.0	2.0
1061_0120276	2.0	2.0
1061_0120278	2.0	2.0
1061_0120296	2.0	2.0
1061_0120303	1.0	2.0
1061_0120306	3.0	2.0
1061_0120319	2.0	2.0
1061_0120329	2.0	2.0
1061_0120332	1.0	2.0
1061_0120337	2.0	2.0
1061_0120347	1.0	2.0
1061_0120368	2.0	2.0
1061_0120387	1.0	2.0
1061_0120389	2.0	2.0
1061_0120403	3.0	2.0
1061_0120408	2.0	2.0
1061_0120411	3.0	3.0
1061_0120413	1.0	1.0
1061_0120425	2.0	2.0
1061_0120438	2.0	2.0
1061_0120439	1.0	1.0
1061_0120442	2.0	2.0
1061_0120448	3.0	2.0
1061_0120453	2.0	2.0
1061_0120478	2.0	2.0
1061_0120481	3.0	3.0
1061_0120485	2.0	2.0
1061_0120855	1.0	2.0
1061_0120858	2.0	2.0
1061_0120859	2.0	3.0
1061_0120882	3.0	3.0
1061_0120885	2.0	2.0
1061_0120890	1.0	1.0
1061_0120894	2.0	2.0
1061_1029112	3.0	3.0
1061_1029114	1.0	1.0
1061_1029119	1.0	2.0
1061_1202917	1.0	1.0
1071_0024678	1.0	1.0
1071_0024683	0.0	1.0
1071_0024685	2.0	2.0
1071_0024693	1.0	1.0
1071_0024694	1.0	2.0
1071_0024708	1.0	1.0
1071_0024715	1.0	2.0
1071_0024716	1.0	1.0
1071_0024756	1.0	1.0
1071_0024758	2.0	2.0
1071_0024770	1.0	1.0
1071_0024775	0.0	1.0
1071_0024779	1.0	1.0
1071_0024782	0.0	1.0
1071_0024799	2.0	2.0
1071_0024803	1.0	1.0
1071_0024809	1.0	1.0
1071_0024810	1.0	1.0
1071_0024813	0.0	1.0
1071_0024817	1.0	1.0
1071_0024824	1.0	1.0
1071_0024838	0.0	1.0
1071_0024840	1.0	1.0
1071_0024864	0.0	1.0
1071_0242011	1.0	1.0
1071_0242012	1.0	1.0
1071_0242021	1.0	1.0
1071_0242042	1.0	1.0
1071_0242093	0.0	1.0
1071_0243581	1.0	1.0
1071_0243582	1.0	1.0
1071_0243621	2.0	1.0
1071_0243623	1.0	1.0
1071_0248301	1.0	1.0
1071_0248309	2.0	1.0
1071_0248324	0.0	1.0
1071_0248326	1.0	1.0
1071_0248328	0.0	1.0
1071_0248330	2.0	1.0
1071_0248331	1.0	1.0
1071_0248338	1.0	1.0
1071_0248341	1.0	1.0
1071_0248350	1.0	1.0
1091_0000006	0.0	1.0
1091_0000018	2.0	2.0
1091_0000031	1.0	1.0
1091_0000050	1.0	1.0
1091_0000051	0.0	1.0
1091_0000053	1.0	1.0
1091_0000063	1.0	1.0
1091_0000069	1.0	1.0
1091_0000070	1.0	1.0
1091_0000078	1.0	1.0
1091_0000087	1.0	2.0
1091_0000092	2.0	2.0
1091_0000127	1.0	1.0
1091_0000155	2.0	3.0
1091_0000157	2.0	2.0
1091_0000159	2.0	2.0
1091_0000164	1.0	1.0
1091_0000168	2.0	2.0
1091_0000173	2.0	1.0
1091_0000185	1.0	1.0
1091_0000192	1.0	2.0
1091_0000193	1.0	1.0
1091_0000200	1.0	2.0
1091_0000201	1.0	2.0
1091_0000203	1.0	1.0
1091_0000211	2.0	2.0
1091_0000217	1.0	1.0
1091_0000220	2.0	2.0
1091_0000233	1.0	2.0
1091_0000239	2.0	2.0
1091_0000245	2.0	1.0
1091_0000246	2.0	2.0
1091_0000255	1.0	1.0
1091_0000256	2.0	2.0
1091_0000259	2.0	2.0
1091_0000262	1.0	1.0
1091_0000263	2.0	2.0
1091_0000269	2.0	2.0
1091_0000271	1.0	1.0
1091_0000272	1.0	1.0
LANGUAGE: DE, 3th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.10
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.86
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.70      0.68      0.69        62
         2.0       0.63      0.59      0.61        66
         3.0       0.66      0.95      0.78        58
         4.0       0.00      0.00      0.00         8

    accuracy                           0.66       205
   macro avg       0.40      0.44      0.42       205
weighted avg       0.60      0.66      0.63       205

[[ 0 11  0  0  0]
 [ 0 42 20  0  0]
 [ 0  7 39 20  0]
 [ 0  0  3 55  0]
 [ 0  0  0  8  0]]
0.6251487880025295
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.82
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.82
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.72      0.63      0.67        62
         2.0       0.63      0.73      0.68        66
         3.0       0.71      0.91      0.80        58
         4.0       0.00      0.00      0.00         8

    accuracy                           0.68       205
   macro avg       0.41      0.45      0.43       205
weighted avg       0.62      0.68      0.65       205

[[ 0 11  0  0  0]
 [ 0 39 23  0  0]
 [ 0  4 48 14  0]
 [ 0  0  5 53  0]
 [ 0  0  0  8  0]]
0.6465118897173042
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.69
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.72
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.75      0.71      0.73        62
         2.0       0.69      0.80      0.74        66
         3.0       0.75      0.90      0.82        58
         4.0       0.00      0.00      0.00         8

    accuracy                           0.73       205
   macro avg       0.44      0.48      0.46       205
weighted avg       0.66      0.73      0.69       205

[[ 0 11  0  0  0]
 [ 0 44 18  0  0]
 [ 0  4 53  9  0]
 [ 0  0  6 52  0]
 [ 0  0  0  8  0]]
0.6902929219810341
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.59
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.74
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.74      0.69      0.72        62
         2.0       0.67      0.71      0.69        66
         3.0       0.70      0.93      0.80        58
         4.0       0.00      0.00      0.00         8

    accuracy                           0.70       205
   macro avg       0.42      0.47      0.44       205
weighted avg       0.64      0.70      0.67       205

[[ 0 11  0  0  0]
 [ 0 43 19  0  0]
 [ 0  4 47 15  0]
 [ 0  0  4 54  0]
 [ 0  0  0  8  0]]
0.6656145384983262
205 205 205
Filename	True Label	Prediction
1023_0001575	3.0	3.0
1023_0101675	3.0	3.0
1023_0101845	3.0	3.0
1023_0101856	2.0	3.0
1023_0101896	2.0	2.0
1023_0101900	3.0	3.0
1023_0101904	2.0	3.0
1023_0102118	3.0	3.0
1023_0103825	3.0	3.0
1023_0103826	3.0	3.0
1023_0103828	1.0	2.0
1023_0103836	3.0	3.0
1023_0103839	3.0	3.0
1023_0103843	2.0	2.0
1023_0103844	4.0	3.0
1023_0104209	3.0	3.0
1023_0107075	2.0	3.0
1023_0107726	3.0	3.0
1023_0107773	2.0	3.0
1023_0107787	2.0	3.0
1023_0108304	3.0	3.0
1023_0108423	3.0	2.0
1023_0108518	3.0	3.0
1023_0108766	2.0	3.0
1023_0108810	3.0	3.0
1023_0108885	2.0	2.0
1023_0108908	3.0	3.0
1023_0108931	3.0	3.0
1023_0108934	3.0	3.0
1023_0109022	3.0	3.0
1023_0109029	1.0	2.0
1023_0109033	4.0	3.0
1023_0109192	3.0	3.0
1023_0109248	2.0	3.0
1023_0109391	2.0	3.0
1023_0109496	3.0	3.0
1023_0109515	3.0	3.0
1023_0109528	3.0	3.0
1023_0109588	3.0	3.0
1023_0109591	3.0	3.0
1023_0109878	2.0	3.0
1023_0109890	3.0	3.0
1023_0109914	2.0	3.0
1023_0109945	3.0	3.0
1031_0001950	3.0	3.0
1031_0001998	4.0	3.0
1031_0002002	2.0	3.0
1031_0002010	3.0	3.0
1031_0002011	4.0	3.0
1031_0002032	3.0	3.0
1031_0002042	3.0	3.0
1031_0002083	3.0	3.0
1031_0002084	3.0	3.0
1031_0002085	3.0	3.0
1031_0002086	3.0	3.0
1031_0002088	3.0	3.0
1031_0002092	4.0	3.0
1031_0003013	4.0	3.0
1031_0003043	4.0	3.0
1031_0003052	3.0	3.0
1031_0003054	3.0	3.0
1031_0003161	3.0	3.0
1031_0003162	3.0	3.0
1031_0003164	3.0	3.0
1031_0003167	3.0	3.0
1031_0003170	3.0	3.0
1031_0003172	3.0	3.0
1031_0003174	3.0	3.0
1031_0003182	4.0	3.0
1031_0003206	3.0	3.0
1031_0003212	2.0	3.0
1031_0003216	3.0	3.0
1031_0003240	2.0	3.0
1031_0003245	3.0	3.0
1031_0003262	3.0	3.0
1031_0003314	3.0	3.0
1031_0003338	3.0	3.0
1031_0003359	2.0	3.0
1031_0003383	3.0	3.0
1061_0120274	1.0	1.0
1061_0120275	2.0	2.0
1061_0120280	1.0	1.0
1061_0120281	1.0	2.0
1061_0120282	0.0	1.0
1061_0120284	0.0	1.0
1061_0120287	1.0	2.0
1061_0120288	2.0	2.0
1061_0120289	1.0	2.0
1061_0120297	2.0	2.0
1061_0120298	2.0	2.0
1061_0120309	1.0	1.0
1061_0120310	2.0	2.0
1061_0120311	3.0	2.0
1061_0120314	2.0	2.0
1061_0120315	2.0	1.0
1061_0120317	2.0	2.0
1061_0120318	2.0	2.0
1061_0120320	3.0	3.0
1061_0120323	1.0	2.0
1061_0120331	1.0	1.0
1061_0120341	1.0	1.0
1061_0120345	2.0	2.0
1061_0120350	2.0	2.0
1061_0120352	1.0	1.0
1061_0120356	2.0	2.0
1061_0120358	1.0	2.0
1061_0120361	2.0	2.0
1061_0120370	2.0	2.0
1061_0120391	1.0	1.0
1061_0120394	2.0	2.0
1061_0120405	2.0	2.0
1061_0120406	2.0	2.0
1061_0120414	2.0	2.0
1061_0120421	2.0	2.0
1061_0120424	2.0	2.0
1061_0120427	2.0	2.0
1061_0120456	2.0	2.0
1061_0120483	1.0	2.0
1061_0120489	2.0	2.0
1061_0120490	2.0	2.0
1061_0120494	2.0	2.0
1061_0120497	3.0	3.0
1061_0120856	1.0	2.0
1061_0120878	1.0	1.0
1061_0120880	3.0	3.0
1061_0120881	2.0	3.0
1061_0120886	2.0	2.0
1061_1029115	2.0	2.0
1061_1029116	1.0	2.0
1061_1202911	1.0	2.0
1061_1202914	1.0	1.0
1061_1202915	1.0	2.0
1061_1202919	2.0	1.0
1071_0024689	1.0	1.0
1071_0024712	1.0	1.0
1071_0024757	2.0	2.0
1071_0024762	1.0	1.0
1071_0024763	1.0	1.0
1071_0024773	1.0	1.0
1071_0024776	0.0	1.0
1071_0024783	0.0	1.0
1071_0024798	0.0	1.0
1071_0024804	1.0	1.0
1071_0024807	1.0	1.0
1071_0024812	1.0	1.0
1071_0024815	0.0	1.0
1071_0024831	0.0	1.0
1071_0024843	1.0	1.0
1071_0024846	1.0	1.0
1071_0024847	1.0	2.0
1071_0024851	1.0	1.0
1071_0024853	1.0	1.0
1071_0024867	2.0	2.0
1071_0024871	1.0	1.0
1071_0024876	1.0	1.0
1071_0024877	1.0	1.0
1071_0024881	2.0	2.0
1071_0241832	1.0	1.0
1071_0241833	1.0	1.0
1071_0242041	1.0	1.0
1071_0242072	0.0	1.0
1071_0243592	1.0	1.0
1071_0248303	1.0	1.0
1071_0248305	0.0	1.0
1071_0248310	1.0	1.0
1071_0248314	1.0	1.0
1071_0248315	0.0	1.0
1071_0248333	2.0	1.0
1071_0248346	1.0	1.0
1091_0000004	1.0	1.0
1091_0000005	2.0	2.0
1091_0000010	2.0	2.0
1091_0000021	2.0	2.0
1091_0000022	3.0	2.0
1091_0000061	1.0	1.0
1091_0000064	1.0	1.0
1091_0000068	2.0	2.0
1091_0000073	1.0	1.0
1091_0000074	2.0	1.0
1091_0000101	1.0	1.0
1091_0000116	2.0	2.0
1091_0000148	1.0	1.0
1091_0000156	2.0	2.0
1091_0000158	2.0	2.0
1091_0000163	1.0	1.0
1091_0000167	2.0	2.0
1091_0000169	1.0	2.0
1091_0000171	2.0	2.0
1091_0000190	1.0	2.0
1091_0000195	1.0	1.0
1091_0000202	2.0	2.0
1091_0000210	1.0	1.0
1091_0000222	1.0	2.0
1091_0000223	2.0	2.0
1091_0000229	2.0	2.0
1091_0000230	2.0	2.0
1091_0000235	1.0	1.0
1091_0000238	1.0	2.0
1091_0000249	2.0	2.0
1091_0000250	1.0	2.0
1091_0000252	2.0	2.0
1091_0000253	0.0	1.0
1091_0000267	1.0	2.0
1091_0000270	1.0	1.0
1091_0000276	3.0	2.0
LANGUAGE: DE, 4th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.17
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.99
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        12
         1.0       0.73      0.57      0.64        61
         2.0       0.57      0.53      0.55        66
         3.0       0.58      0.97      0.73        58
         4.0       0.00      0.00      0.00         8

    accuracy                           0.61       205
   macro avg       0.38      0.41      0.38       205
weighted avg       0.57      0.61      0.57       205

[[ 0 12  0  0  0]
 [ 0 35 24  2  0]
 [ 0  1 35 30  0]
 [ 0  0  2 56  0]
 [ 0  0  0  8  0]]
0.5743125993381912
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.85
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.76
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        12
         1.0       0.65      0.89      0.75        61
         2.0       0.79      0.50      0.61        66
         3.0       0.70      0.97      0.81        58
         4.0       0.00      0.00      0.00         8

    accuracy                           0.70       205
   macro avg       0.43      0.47      0.43       205
weighted avg       0.64      0.70      0.65       205

[[ 0 12  0  0  0]
 [ 0 54  7  0  0]
 [ 0 17 33 16  0]
 [ 0  0  2 56  0]
 [ 0  0  0  8  0]]
0.6495404736656063
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.72
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.71
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        12
         1.0       0.74      0.80      0.77        61
         2.0       0.76      0.67      0.71        66
         3.0       0.69      0.97      0.81        58
         4.0       0.00      0.00      0.00         8

    accuracy                           0.73       205
   macro avg       0.44      0.49      0.46       205
weighted avg       0.66      0.73      0.69       205

[[ 0 12  0  0  0]
 [ 0 49 12  0  0]
 [ 0  5 44 17  0]
 [ 0  0  2 56  0]
 [ 0  0  0  8  0]]
0.6860653110672777
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.60
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.71
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        12
         1.0       0.76      0.82      0.79        61
         2.0       0.79      0.70      0.74        66
         3.0       0.69      0.97      0.81        58
         4.0       0.00      0.00      0.00         8

    accuracy                           0.74       205
   macro avg       0.45      0.50      0.47       205
weighted avg       0.68      0.74      0.70       205

[[ 0 12  0  0  0]
 [ 0 50 10  1  0]
 [ 0  4 46 16  0]
 [ 0  0  2 56  0]
 [ 0  0  0  8  0]]
0.7011368338932501
205 205 205
Filename	True Label	Prediction
1023_0001423	2.0	2.0
1023_0101689	2.0	2.0
1023_0101701	2.0	3.0
1023_0101749	3.0	3.0
1023_0101751	3.0	3.0
1023_0101844	2.0	3.0
1023_0101846	4.0	3.0
1023_0101849	3.0	3.0
1023_0101898	3.0	3.0
1023_0102117	3.0	3.0
1023_0103834	3.0	3.0
1023_0103837	3.0	3.0
1023_0103840	3.0	3.0
1023_0103955	3.0	3.0
1023_0106816	3.0	3.0
1023_0107788	3.0	3.0
1023_0108307	3.0	3.0
1023_0108422	3.0	3.0
1023_0108426	2.0	3.0
1023_0108650	3.0	3.0
1023_0108812	2.0	3.0
1023_0108886	3.0	3.0
1023_0108993	3.0	3.0
1023_0109027	2.0	2.0
1023_0109247	3.0	3.0
1023_0109395	2.0	3.0
1023_0109520	2.0	3.0
1023_0109527	3.0	3.0
1023_0109609	2.0	2.0
1023_0109649	3.0	3.0
1023_0109674	3.0	3.0
1023_0109880	3.0	3.0
1023_0109891	3.0	3.0
1023_0109915	2.0	2.0
1023_0109951	2.0	3.0
1031_0001951	2.0	3.0
1031_0002036	4.0	3.0
1031_0002040	4.0	3.0
1031_0002184	3.0	3.0
1031_0002185	3.0	3.0
1031_0002195	3.0	3.0
1031_0002200	2.0	3.0
1031_0003012	3.0	3.0
1031_0003029	3.0	3.0
1031_0003035	3.0	3.0
1031_0003065	3.0	3.0
1031_0003073	4.0	3.0
1031_0003085	3.0	3.0
1031_0003090	3.0	3.0
1031_0003099	3.0	3.0
1031_0003121	3.0	3.0
1031_0003126	3.0	3.0
1031_0003128	4.0	3.0
1031_0003145	3.0	3.0
1031_0003150	3.0	3.0
1031_0003154	3.0	3.0
1031_0003163	3.0	3.0
1031_0003184	4.0	3.0
1031_0003203	2.0	3.0
1031_0003205	3.0	3.0
1031_0003217	4.0	3.0
1031_0003221	2.0	3.0
1031_0003224	3.0	3.0
1031_0003232	2.0	3.0
1031_0003235	3.0	3.0
1031_0003238	3.0	3.0
1031_0003243	3.0	3.0
1031_0003309	3.0	3.0
1031_0003313	3.0	3.0
1031_0003331	3.0	3.0
1031_0003337	3.0	3.0
1031_0003358	4.0	3.0
1031_0003366	3.0	3.0
1031_0003367	3.0	3.0
1031_0003384	2.0	3.0
1031_0003390	3.0	3.0
1031_0003392	3.0	3.0
1031_0003408	2.0	3.0
1031_0003410	3.0	3.0
1031_0003419	3.0	3.0
1061_0120299	2.0	2.0
1061_0120302	1.0	2.0
1061_0120307	2.0	2.0
1061_0120321	2.0	2.0
1061_0120324	2.0	2.0
1061_0120328	1.0	2.0
1061_0120334	2.0	2.0
1061_0120336	1.0	2.0
1061_0120338	1.0	2.0
1061_0120343	2.0	2.0
1061_0120349	1.0	1.0
1061_0120360	3.0	3.0
1061_0120366	3.0	2.0
1061_0120371	3.0	3.0
1061_0120374	2.0	2.0
1061_0120383	2.0	3.0
1061_0120407	3.0	2.0
1061_0120409	2.0	2.0
1061_0120415	2.0	2.0
1061_0120426	2.0	2.0
1061_0120429	2.0	2.0
1061_0120441	2.0	2.0
1061_0120460	2.0	2.0
1061_0120479	2.0	2.0
1061_0120480	2.0	2.0
1061_0120492	2.0	3.0
1061_0120874	2.0	2.0
1061_0120875	3.0	3.0
1061_0120884	2.0	2.0
1061_1029111	2.0	2.0
1061_1029117	1.0	2.0
1071_0024681	2.0	2.0
1071_0024682	2.0	2.0
1071_0024686	2.0	2.0
1071_0024692	2.0	2.0
1071_0024703	1.0	1.0
1071_0024706	1.0	1.0
1071_0024709	2.0	2.0
1071_0024710	1.0	1.0
1071_0024711	1.0	1.0
1071_0024713	1.0	1.0
1071_0024767	2.0	1.0
1071_0024772	0.0	1.0
1071_0024781	1.0	1.0
1071_0024800	1.0	1.0
1071_0024801	1.0	1.0
1071_0024806	1.0	1.0
1071_0024811	1.0	1.0
1071_0024819	1.0	1.0
1071_0024820	0.0	1.0
1071_0024821	1.0	1.0
1071_0024823	1.0	1.0
1071_0024825	1.0	1.0
1071_0024836	1.0	2.0
1071_0024841	0.0	1.0
1071_0024849	0.0	1.0
1071_0024850	1.0	1.0
1071_0024852	0.0	1.0
1071_0024856	1.0	1.0
1071_0024872	1.0	1.0
1071_0242023	1.0	1.0
1071_0242043	0.0	1.0
1071_0242071	0.0	1.0
1071_0242073	1.0	1.0
1071_0243502	1.0	1.0
1071_0243591	1.0	1.0
1071_0243622	1.0	1.0
1071_0248304	1.0	1.0
1071_0248316	1.0	1.0
1071_0248317	0.0	1.0
1071_0248318	0.0	1.0
1071_0248322	1.0	1.0
1071_0248325	0.0	1.0
1071_0248335	1.0	1.0
1071_0248336	1.0	1.0
1071_0248337	1.0	2.0
1071_0248339	1.0	1.0
1071_0248342	1.0	1.0
1071_0248348	1.0	1.0
1091_0000003	2.0	2.0
1091_0000008	2.0	2.0
1091_0000012	1.0	1.0
1091_0000013	1.0	1.0
1091_0000017	2.0	2.0
1091_0000019	1.0	1.0
1091_0000020	1.0	2.0
1091_0000024	1.0	1.0
1091_0000026	1.0	1.0
1091_0000027	2.0	1.0
1091_0000028	0.0	1.0
1091_0000032	2.0	2.0
1091_0000035	1.0	1.0
1091_0000042	0.0	1.0
1091_0000043	1.0	1.0
1091_0000046	1.0	1.0
1091_0000055	2.0	2.0
1091_0000056	2.0	2.0
1091_0000057	1.0	1.0
1091_0000058	2.0	2.0
1091_0000059	2.0	2.0
1091_0000060	2.0	2.0
1091_0000065	2.0	1.0
1091_0000066	1.0	1.0
1091_0000077	1.0	1.0
1091_0000113	2.0	2.0
1091_0000140	1.0	1.0
1091_0000146	1.0	1.0
1091_0000152	1.0	1.0
1091_0000160	1.0	3.0
1091_0000162	1.0	2.0
1091_0000170	2.0	1.0
1091_0000191	2.0	2.0
1091_0000199	2.0	2.0
1091_0000205	2.0	2.0
1091_0000209	1.0	1.0
1091_0000214	1.0	1.0
1091_0000226	1.0	1.0
1091_0000228	2.0	2.0
1091_0000232	2.0	2.0
1091_0000234	2.0	2.0
1091_0000241	1.0	1.0
1091_0000243	1.0	1.0
1091_0000258	2.0	2.0
1091_0000265	1.0	2.0
1091_0000274	2.0	2.0
Averaged weighted F1-scores 0.6807423188534647
130.68810916179336 82.83744971317162
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']
LANGUAGE: DE, 0th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.35
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.07
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        18
         1.0       0.50      0.98      0.66        57
         2.0       0.86      0.36      0.51        70
         3.0       0.69      0.86      0.77        51
         4.0       0.00      0.00      0.00        10

    accuracy                           0.61       206
   macro avg       0.41      0.44      0.39       206
weighted avg       0.60      0.61      0.54       206

[[ 0 18  0  0  0]
 [ 0 56  1  0  0]
 [ 0 35 25 10  0]
 [ 0  4  3 44  0]
 [ 0  0  0 10  0]]
0.5433619586724645
206 206 206



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.03
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.89
              precision    recall  f1-score   support

         0.0       1.00      0.06      0.11        18
         1.0       0.56      0.56      0.56        57
         2.0       0.60      0.67      0.64        70
         3.0       0.66      0.90      0.76        51
         4.0       0.00      0.00      0.00        10

    accuracy                           0.61       206
   macro avg       0.56      0.44      0.41       206
weighted avg       0.61      0.61      0.57       206

[[ 1 16  1  0  0]
 [ 0 32 25  0  0]
 [ 0  9 47 14  0]
 [ 0  0  5 46  0]
 [ 0  0  0 10  0]]
0.5685973582800823
206 206 206



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.91
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.85
              precision    recall  f1-score   support

         0.0       1.00      0.17      0.29        18
         1.0       0.59      0.77      0.67        57
         2.0       0.69      0.54      0.61        70
         3.0       0.64      0.92      0.76        51
         4.0       0.00      0.00      0.00        10

    accuracy                           0.64       206
   macro avg       0.58      0.48      0.46       206
weighted avg       0.64      0.64      0.60       206

[[ 3 14  1  0  0]
 [ 0 44 13  0  0]
 [ 0 16 38 16  0]
 [ 0  1  3 47  0]
 [ 0  0  0 10  0]]
0.6037094537157174
206 206 206



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.78
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.80
              precision    recall  f1-score   support

         0.0       0.88      0.39      0.54        18
         1.0       0.64      0.75      0.69        57
         2.0       0.68      0.63      0.65        70
         3.0       0.68      0.88      0.77        51
         4.0       0.00      0.00      0.00        10

    accuracy                           0.67       206
   macro avg       0.58      0.53      0.53       206
weighted avg       0.65      0.67      0.65       206

[[ 7 10  1  0  0]
 [ 0 43 14  0  0]
 [ 1 14 44 11  0]
 [ 0  0  6 45  0]
 [ 0  0  0 10  0]]
0.6508978864913723
206 206 206
Filename	True Label	Prediction
1023_0001416	4.0	3.0
1023_0001423	2.0	2.0
1023_0101675	3.0	3.0
1023_0101693	3.0	3.0
1023_0101844	2.0	3.0
1023_0101853	2.0	3.0
1023_0101856	2.0	2.0
1023_0101901	3.0	3.0
1023_0102117	2.0	3.0
1023_0102118	3.0	3.0
1023_0103824	3.0	3.0
1023_0103834	4.0	3.0
1023_0103880	3.0	3.0
1023_0103955	3.0	3.0
1023_0107726	2.0	3.0
1023_0107780	3.0	3.0
1023_0108306	3.0	3.0
1023_0108520	2.0	2.0
1023_0108649	3.0	3.0
1023_0108751	2.0	2.0
1023_0108753	2.0	2.0
1023_0108887	2.0	2.0
1023_0108890	3.0	2.0
1023_0108908	2.0	2.0
1023_0108935	2.0	3.0
1023_0108993	3.0	3.0
1023_0109250	2.0	2.0
1023_0109422	3.0	3.0
1023_0109527	3.0	3.0
1023_0109609	2.0	2.0
1023_0109671	3.0	3.0
1023_0109721	2.0	2.0
1023_0109947	2.0	3.0
1031_0001949	3.0	3.0
1031_0001951	2.0	3.0
1031_0002004	3.0	3.0
1031_0002185	3.0	3.0
1031_0002195	3.0	3.0
1031_0002196	4.0	3.0
1031_0003029	3.0	3.0
1031_0003074	3.0	3.0
1031_0003076	4.0	3.0
1031_0003097	4.0	3.0
1031_0003098	4.0	3.0
1031_0003127	4.0	3.0
1031_0003129	3.0	3.0
1031_0003130	4.0	3.0
1031_0003145	3.0	3.0
1031_0003149	3.0	3.0
1031_0003157	4.0	3.0
1031_0003163	3.0	3.0
1031_0003165	2.0	3.0
1031_0003170	2.0	3.0
1031_0003179	3.0	3.0
1031_0003182	4.0	3.0
1031_0003203	2.0	2.0
1031_0003205	3.0	3.0
1031_0003211	2.0	3.0
1031_0003224	3.0	3.0
1031_0003226	3.0	3.0
1031_0003231	3.0	3.0
1031_0003233	2.0	2.0
1031_0003234	3.0	3.0
1031_0003236	3.0	3.0
1031_0003238	3.0	3.0
1031_0003242	3.0	3.0
1031_0003246	3.0	3.0
1031_0003274	3.0	3.0
1031_0003330	3.0	3.0
1031_0003354	3.0	3.0
1031_0003355	3.0	3.0
1031_0003359	3.0	3.0
1031_0003365	3.0	3.0
1031_0003366	3.0	3.0
1031_0003388	3.0	3.0
1031_0003393	3.0	3.0
1031_0003410	3.0	3.0
1031_0003419	3.0	3.0
1061_0120281	1.0	2.0
1061_0120282	0.0	1.0
1061_0120283	0.0	1.0
1061_0120296	1.0	2.0
1061_0120297	1.0	2.0
1061_0120303	0.0	2.0
1061_0120306	3.0	2.0
1061_0120307	2.0	2.0
1061_0120311	3.0	2.0
1061_0120313	1.0	1.0
1061_0120318	2.0	2.0
1061_0120321	2.0	2.0
1061_0120329	2.0	2.0
1061_0120336	1.0	2.0
1061_0120337	2.0	2.0
1061_0120345	2.0	2.0
1061_0120348	1.0	1.0
1061_0120351	2.0	2.0
1061_0120368	2.0	2.0
1061_0120371	3.0	2.0
1061_0120372	1.0	2.0
1061_0120384	1.0	1.0
1061_0120390	2.0	2.0
1061_0120408	2.0	2.0
1061_0120424	2.0	2.0
1061_0120431	2.0	2.0
1061_0120439	2.0	1.0
1061_0120479	2.0	2.0
1061_0120495	2.0	2.0
1061_0120498	2.0	3.0
1061_0120859	2.0	2.0
1061_0120877	2.0	2.0
1061_0120881	2.0	2.0
1061_0120886	2.0	2.0
1061_0120894	2.0	2.0
1061_1029117	1.0	2.0
1061_1029118	1.0	1.0
1061_1202914	1.0	1.0
1061_1202919	2.0	1.0
1071_0024702	1.0	1.0
1071_0024703	1.0	1.0
1071_0024704	1.0	1.0
1071_0024705	1.0	1.0
1071_0024706	1.0	1.0
1071_0024711	1.0	1.0
1071_0024757	2.0	2.0
1071_0024770	1.0	1.0
1071_0024772	0.0	0.0
1071_0024774	0.0	0.0
1071_0024784	1.0	1.0
1071_0024804	1.0	1.0
1071_0024806	1.0	1.0
1071_0024815	0.0	1.0
1071_0024821	1.0	1.0
1071_0024824	1.0	1.0
1071_0024826	1.0	1.0
1071_0024836	1.0	1.0
1071_0024837	0.0	0.0
1071_0024841	0.0	0.0
1071_0024848	1.0	1.0
1071_0024851	2.0	1.0
1071_0024875	1.0	1.0
1071_0241833	1.0	1.0
1071_0242042	0.0	1.0
1071_0242073	1.0	1.0
1071_0242091	1.0	1.0
1071_0242093	0.0	0.0
1071_0243501	2.0	1.0
1071_0243582	0.0	1.0
1071_0243623	1.0	1.0
1071_0248302	0.0	0.0
1071_0248303	0.0	0.0
1071_0248307	2.0	1.0
1071_0248311	1.0	1.0
1071_0248313	1.0	1.0
1071_0248319	0.0	1.0
1071_0248323	0.0	1.0
1071_0248329	1.0	1.0
1071_0248330	2.0	1.0
1071_0248335	1.0	1.0
1071_0248337	1.0	2.0
1071_0248338	1.0	1.0
1071_0248342	0.0	1.0
1071_0248345	1.0	1.0
1071_0248348	2.0	1.0
1071_0248349	1.0	1.0
1071_0248350	2.0	0.0
1091_0000007	3.0	2.0
1091_0000015	2.0	2.0
1091_0000026	1.0	1.0
1091_0000030	0.0	1.0
1091_0000035	1.0	1.0
1091_0000037	1.0	1.0
1091_0000039	1.0	1.0
1091_0000045	1.0	2.0
1091_0000046	2.0	1.0
1091_0000055	1.0	2.0
1091_0000058	2.0	2.0
1091_0000062	2.0	2.0
1091_0000063	1.0	1.0
1091_0000070	2.0	1.0
1091_0000073	3.0	2.0
1091_0000077	2.0	1.0
1091_0000125	2.0	1.0
1091_0000144	1.0	1.0
1091_0000148	1.0	1.0
1091_0000151	0.0	1.0
1091_0000158	2.0	2.0
1091_0000159	2.0	2.0
1091_0000165	1.0	1.0
1091_0000172	2.0	1.0
1091_0000192	1.0	1.0
1091_0000195	1.0	1.0
1091_0000203	1.0	1.0
1091_0000210	2.0	1.0
1091_0000222	2.0	1.0
1091_0000230	2.0	2.0
1091_0000232	2.0	2.0
1091_0000237	2.0	2.0
1091_0000242	1.0	2.0
1091_0000245	1.0	2.0
1091_0000251	2.0	2.0
1091_0000256	1.0	2.0
1091_0000261	1.0	2.0
1091_0000265	2.0	2.0
1091_0000266	2.0	2.0
1091_0000268	2.0	2.0
1091_0000269	1.0	2.0
LANGUAGE: DE, 1th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.26
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.08
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.51      0.70      0.59        57
         2.0       0.45      0.58      0.51        69
         3.0       0.62      0.46      0.53        52
         4.0       0.00      0.00      0.00        10

    accuracy                           0.51       205
   macro avg       0.32      0.35      0.33       205
weighted avg       0.45      0.51      0.47       205

[[ 0 15  2  0  0]
 [ 0 40 17  0  0]
 [ 0 22 40  7  0]
 [ 0  1 27 24  0]
 [ 0  0  2  8  0]]
0.47007602378116586
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.99
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.98
              precision    recall  f1-score   support

         0.0       0.50      0.12      0.19        17
         1.0       0.53      0.74      0.61        57
         2.0       0.52      0.51      0.51        69
         3.0       0.65      0.67      0.66        52
         4.0       0.00      0.00      0.00        10

    accuracy                           0.56       205
   macro avg       0.44      0.41      0.40       205
weighted avg       0.53      0.56      0.53       205

[[ 2 14  1  0  0]
 [ 2 42 13  0  0]
 [ 0 23 35 11  0]
 [ 0  1 16 35  0]
 [ 0  0  2  8  0]]
0.5270308725436422
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.90
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.57      0.51      0.54        57
         2.0       0.50      0.59      0.54        69
         3.0       0.61      0.85      0.71        52
         4.0       0.00      0.00      0.00        10

    accuracy                           0.56       205
   macro avg       0.34      0.39      0.36       205
weighted avg       0.48      0.56      0.51       205

[[ 0 12  5  0  0]
 [ 0 29 28  0  0]
 [ 0 10 41 18  0]
 [ 0  0  8 44  0]
 [ 0  0  0 10  0]]
0.512119685819802
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.82
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.99
              precision    recall  f1-score   support

         0.0       0.43      0.18      0.25        17
         1.0       0.59      0.60      0.59        57
         2.0       0.55      0.57      0.56        69
         3.0       0.62      0.83      0.71        52
         4.0       0.00      0.00      0.00        10

    accuracy                           0.58       205
   macro avg       0.44      0.43      0.42       205
weighted avg       0.54      0.58      0.55       205

[[ 3 11  3  0  0]
 [ 3 34 20  0  0]
 [ 1 13 39 16  0]
 [ 0  0  9 43  0]
 [ 0  0  0 10  0]]
0.5529555251458269
205 205 205
Filename	True Label	Prediction
1023_0001418	2.0	3.0
1023_0101688	3.0	3.0
1023_0101689	1.0	1.0
1023_0101691	3.0	3.0
1023_0101694	3.0	3.0
1023_0101753	3.0	3.0
1023_0101846	4.0	3.0
1023_0101854	2.0	2.0
1023_0103822	2.0	2.0
1023_0103823	3.0	3.0
1023_0103829	2.0	3.0
1023_0103837	3.0	3.0
1023_0103841	3.0	3.0
1023_0103883	2.0	3.0
1023_0107042	3.0	2.0
1023_0107074	3.0	3.0
1023_0107244	2.0	3.0
1023_0107682	2.0	2.0
1023_0107727	4.0	3.0
1023_0107729	3.0	3.0
1023_0107781	2.0	3.0
1023_0107787	2.0	3.0
1023_0107788	2.0	3.0
1023_0108304	3.0	3.0
1023_0108426	2.0	2.0
1023_0108648	3.0	3.0
1023_0108766	2.0	3.0
1023_0108811	3.0	2.0
1023_0108812	2.0	3.0
1023_0108886	3.0	3.0
1023_0109030	3.0	3.0
1023_0109033	3.0	3.0
1023_0109391	2.0	2.0
1023_0109392	2.0	3.0
1023_0109395	2.0	2.0
1023_0109400	3.0	3.0
1023_0109401	2.0	2.0
1023_0109500	2.0	3.0
1023_0109505	2.0	3.0
1023_0109524	3.0	3.0
1023_0109590	3.0	3.0
1023_0109651	3.0	3.0
1023_0109674	2.0	3.0
1023_0109716	3.0	2.0
1023_0109914	2.0	2.0
1023_0109945	4.0	3.0
1023_0109954	3.0	3.0
1023_0111896	2.0	2.0
1031_0002006	4.0	3.0
1031_0002010	2.0	3.0
1031_0002011	3.0	3.0
1031_0002032	3.0	3.0
1031_0002036	4.0	3.0
1031_0002084	3.0	3.0
1031_0002085	3.0	3.0
1031_0002087	3.0	3.0
1031_0002198	3.0	3.0
1031_0003035	3.0	3.0
1031_0003088	4.0	3.0
1031_0003106	3.0	3.0
1031_0003131	3.0	3.0
1031_0003132	3.0	3.0
1031_0003154	3.0	3.0
1031_0003156	3.0	3.0
1031_0003164	3.0	3.0
1031_0003172	3.0	3.0
1031_0003173	3.0	3.0
1031_0003174	4.0	3.0
1031_0003180	3.0	3.0
1031_0003206	3.0	3.0
1031_0003218	3.0	3.0
1031_0003219	3.0	3.0
1031_0003230	2.0	3.0
1031_0003232	2.0	3.0
1031_0003237	3.0	3.0
1031_0003313	4.0	3.0
1031_0003314	3.0	3.0
1031_0003337	3.0	3.0
1031_0003353	3.0	2.0
1031_0003367	4.0	3.0
1031_0003390	3.0	3.0
1031_0003407	3.0	3.0
1031_0003415	4.0	3.0
1061_0012029	3.0	2.0
1061_0120274	1.0	1.0
1061_0120280	1.0	1.0
1061_0120287	1.0	2.0
1061_0120295	0.0	2.0
1061_0120298	1.0	2.0
1061_0120308	3.0	2.0
1061_0120310	2.0	2.0
1061_0120316	2.0	2.0
1061_0120323	1.0	1.0
1061_0120325	2.0	2.0
1061_0120327	2.0	2.0
1061_0120331	1.0	1.0
1061_0120347	2.0	1.0
1061_0120349	1.0	1.0
1061_0120350	2.0	2.0
1061_0120354	1.0	1.0
1061_0120355	1.0	1.0
1061_0120358	1.0	2.0
1061_0120361	2.0	2.0
1061_0120373	2.0	2.0
1061_0120375	2.0	1.0
1061_0120386	0.0	2.0
1061_0120387	1.0	2.0
1061_0120388	1.0	2.0
1061_0120394	2.0	2.0
1061_0120405	2.0	2.0
1061_0120413	1.0	1.0
1061_0120415	2.0	2.0
1061_0120427	1.0	2.0
1061_0120428	2.0	2.0
1061_0120429	2.0	2.0
1061_0120433	1.0	1.0
1061_0120455	2.0	2.0
1061_0120458	3.0	2.0
1061_0120482	1.0	2.0
1061_0120486	2.0	2.0
1061_0120487	2.0	2.0
1061_0120488	2.0	2.0
1061_0120492	2.0	2.0
1061_0120493	1.0	2.0
1061_0120853	1.0	2.0
1061_0120855	1.0	2.0
1061_0120880	2.0	2.0
1061_1029114	1.0	1.0
1061_1029120	1.0	2.0
1061_1202911	0.0	1.0
1061_1202915	1.0	1.0
1071_0024689	1.0	1.0
1071_0024797	0.0	1.0
1071_0024798	1.0	1.0
1071_0024799	2.0	2.0
1071_0024800	1.0	1.0
1071_0024807	0.0	1.0
1071_0024811	1.0	1.0
1071_0024816	1.0	1.0
1071_0024819	1.0	1.0
1071_0024820	0.0	1.0
1071_0024823	1.0	1.0
1071_0024825	0.0	1.0
1071_0024833	1.0	1.0
1071_0024838	0.0	0.0
1071_0024840	1.0	1.0
1071_0024850	0.0	1.0
1071_0024852	0.0	0.0
1071_0024853	1.0	0.0
1071_0024876	1.0	1.0
1071_0242023	1.0	1.0
1071_0242043	0.0	1.0
1071_0242072	0.0	1.0
1071_0243581	0.0	1.0
1071_0243591	1.0	1.0
1071_0243592	1.0	1.0
1071_0243593	1.0	2.0
1071_0248317	0.0	0.0
1071_0248332	2.0	2.0
1071_0248339	2.0	1.0
1091_0000002	2.0	1.0
1091_0000010	3.0	2.0
1091_0000016	0.0	1.0
1091_0000019	1.0	1.0
1091_0000021	1.0	2.0
1091_0000031	1.0	1.0
1091_0000032	1.0	1.0
1091_0000036	1.0	2.0
1091_0000038	1.0	1.0
1091_0000042	1.0	0.0
1091_0000049	1.0	1.0
1091_0000057	2.0	1.0
1091_0000061	2.0	0.0
1091_0000066	2.0	1.0
1091_0000067	2.0	1.0
1091_0000076	2.0	2.0
1091_0000087	2.0	1.0
1091_0000116	2.0	2.0
1091_0000123	2.0	2.0
1091_0000146	1.0	0.0
1091_0000156	2.0	2.0
1091_0000162	1.0	2.0
1091_0000173	2.0	1.0
1091_0000185	2.0	1.0
1091_0000194	1.0	2.0
1091_0000199	2.0	2.0
1091_0000200	1.0	2.0
1091_0000204	2.0	1.0
1091_0000216	1.0	2.0
1091_0000218	2.0	2.0
1091_0000221	2.0	1.0
1091_0000224	1.0	1.0
1091_0000226	1.0	1.0
1091_0000227	0.0	2.0
1091_0000231	1.0	2.0
1091_0000234	3.0	2.0
1091_0000240	1.0	1.0
1091_0000244	2.0	2.0
1091_0000249	2.0	2.0
1091_0000250	1.0	1.0
1091_0000255	0.0	1.0
1091_0000258	2.0	2.0
1091_0000270	2.0	2.0
1091_0000271	2.0	1.0
1091_0000274	1.0	2.0
LANGUAGE: DE, 2th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.30
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.07
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.49      0.95      0.64        57
         2.0       0.50      0.06      0.10        70
         3.0       0.57      0.94      0.71        52
         4.0       0.00      0.00      0.00         9

    accuracy                           0.52       205
   macro avg       0.31      0.39      0.29       205
weighted avg       0.45      0.52      0.39       205

[[ 0 17  0  0  0]
 [ 0 54  2  1  0]
 [ 0 39  4 27  0]
 [ 0  1  2 49  0]
 [ 0  0  0  9  0]]
0.3939018563620897
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.02
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.96
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.51      0.75      0.61        57
         2.0       0.57      0.30      0.39        70
         3.0       0.58      0.94      0.72        52
         4.0       0.00      0.00      0.00         9

    accuracy                           0.55       205
   macro avg       0.33      0.40      0.34       205
weighted avg       0.48      0.55      0.49       205

[[ 0 17  0  0  0]
 [ 0 43 13  1  0]
 [ 0 24 21 25  0]
 [ 0  0  3 49  0]
 [ 0  0  0  9  0]]
0.4864057619322992
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.91
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.91
              precision    recall  f1-score   support

         0.0       0.62      0.47      0.53        17
         1.0       0.56      0.61      0.59        57
         2.0       0.55      0.41      0.47        70
         3.0       0.60      0.88      0.71        52
         4.0       0.00      0.00      0.00         9

    accuracy                           0.58       205
   macro avg       0.46      0.48      0.46       205
weighted avg       0.55      0.58      0.55       205

[[ 8  9  0  0  0]
 [ 4 35 18  0  0]
 [ 1 18 29 22  0]
 [ 0  0  6 46  0]
 [ 0  0  0  9  0]]
0.5497047796609894
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.81
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.92
              precision    recall  f1-score   support

         0.0       0.86      0.35      0.50        17
         1.0       0.56      0.61      0.58        57
         2.0       0.52      0.44      0.48        70
         3.0       0.59      0.85      0.69        52
         4.0       0.00      0.00      0.00         9

    accuracy                           0.57       205
   macro avg       0.50      0.45      0.45       205
weighted avg       0.55      0.57      0.54       205

[[ 6 11  0  0  0]
 [ 1 35 21  0  0]
 [ 0 17 31 22  0]
 [ 0  0  8 44  0]
 [ 0  0  0  9  0]]
0.5422737143785733
205 205 205
Filename	True Label	Prediction
1023_0001420	3.0	3.0
1023_0001575	3.0	2.0
1023_0101700	2.0	3.0
1023_0101841	2.0	3.0
1023_0101843	3.0	3.0
1023_0101845	2.0	3.0
1023_0101897	2.0	3.0
1023_0101906	2.0	3.0
1023_0103825	3.0	3.0
1023_0103826	3.0	3.0
1023_0103830	3.0	3.0
1023_0103838	3.0	3.0
1023_0103843	2.0	2.0
1023_0107672	2.0	3.0
1023_0107784	1.0	2.0
1023_0108650	3.0	3.0
1023_0108752	3.0	3.0
1023_0108810	3.0	3.0
1023_0108889	3.0	3.0
1023_0108934	2.0	3.0
1023_0108958	2.0	3.0
1023_0108992	3.0	3.0
1023_0109027	2.0	2.0
1023_0109039	3.0	3.0
1023_0109096	3.0	3.0
1023_0109267	2.0	3.0
1023_0109396	2.0	3.0
1023_0109496	3.0	3.0
1023_0109519	2.0	2.0
1023_0109520	2.0	3.0
1023_0109528	3.0	2.0
1023_0109591	3.0	2.0
1023_0109614	2.0	2.0
1023_0109649	2.0	3.0
1023_0109880	3.0	3.0
1023_0109915	2.0	2.0
1023_0109917	3.0	3.0
1023_0109951	2.0	3.0
1031_0001950	3.0	3.0
1031_0002040	4.0	3.0
1031_0002079	4.0	3.0
1031_0002083	2.0	3.0
1031_0002088	3.0	3.0
1031_0002091	3.0	3.0
1031_0002092	4.0	3.0
1031_0002187	3.0	3.0
1031_0002199	3.0	3.0
1031_0002200	2.0	3.0
1031_0003023	3.0	3.0
1031_0003042	3.0	3.0
1031_0003043	4.0	3.0
1031_0003052	3.0	3.0
1031_0003053	3.0	3.0
1031_0003054	3.0	3.0
1031_0003063	4.0	3.0
1031_0003078	3.0	3.0
1031_0003135	3.0	3.0
1031_0003141	3.0	3.0
1031_0003150	3.0	3.0
1031_0003155	3.0	3.0
1031_0003161	3.0	3.0
1031_0003183	4.0	3.0
1031_0003185	3.0	3.0
1031_0003186	3.0	3.0
1031_0003187	3.0	3.0
1031_0003191	3.0	3.0
1031_0003214	3.0	3.0
1031_0003220	2.0	3.0
1031_0003221	2.0	3.0
1031_0003244	4.0	3.0
1031_0003245	3.0	3.0
1031_0003249	3.0	3.0
1031_0003260	4.0	3.0
1031_0003272	3.0	2.0
1031_0003310	3.0	3.0
1031_0003327	2.0	3.0
1031_0003331	2.0	3.0
1031_0003356	3.0	3.0
1031_0003358	4.0	3.0
1031_0003368	3.0	3.0
1031_0003389	3.0	3.0
1031_0003408	2.0	3.0
1061_0120272	1.0	2.0
1061_0120273	2.0	2.0
1061_0120277	1.0	2.0
1061_0120278	1.0	2.0
1061_0120284	0.0	0.0
1061_0120286	0.0	1.0
1061_0120289	1.0	1.0
1061_0120291	1.0	1.0
1061_0120299	2.0	2.0
1061_0120300	2.0	1.0
1061_0120314	1.0	2.0
1061_0120315	2.0	1.0
1061_0120320	3.0	2.0
1061_0120326	2.0	2.0
1061_0120332	1.0	2.0
1061_0120346	2.0	2.0
1061_0120353	1.0	1.0
1061_0120356	2.0	2.0
1061_0120366	3.0	2.0
1061_0120383	2.0	3.0
1061_0120404	1.0	1.0
1061_0120411	3.0	3.0
1061_0120426	1.0	2.0
1061_0120441	2.0	2.0
1061_0120442	2.0	2.0
1061_0120448	3.0	2.0
1061_0120480	2.0	2.0
1061_0120481	2.0	3.0
1061_0120485	3.0	2.0
1061_0120490	2.0	2.0
1061_0120491	2.0	2.0
1061_0120494	1.0	2.0
1061_0120857	2.0	2.0
1061_0120874	2.0	2.0
1061_0120875	2.0	2.0
1061_0120883	1.0	1.0
1061_0120884	1.0	2.0
1061_0120885	2.0	2.0
1061_0120890	1.0	1.0
1061_1029116	1.0	2.0
1061_1202913	2.0	1.0
1071_0024681	1.0	2.0
1071_0024682	2.0	2.0
1071_0024683	0.0	1.0
1071_0024685	1.0	2.0
1071_0024686	2.0	2.0
1071_0024708	1.0	1.0
1071_0024713	1.0	1.0
1071_0024714	2.0	1.0
1071_0024716	1.0	1.0
1071_0024765	0.0	1.0
1071_0024766	1.0	1.0
1071_0024767	2.0	1.0
1071_0024769	0.0	1.0
1071_0024773	1.0	1.0
1071_0024776	0.0	0.0
1071_0024783	0.0	0.0
1071_0024810	2.0	1.0
1071_0024814	1.0	1.0
1071_0024844	1.0	1.0
1071_0024846	0.0	0.0
1071_0024854	0.0	1.0
1071_0024859	1.0	1.0
1071_0024862	1.0	1.0
1071_0024863	1.0	1.0
1071_0024867	1.0	1.0
1071_0024872	1.0	1.0
1071_0024878	1.0	2.0
1071_0024879	1.0	1.0
1071_0242013	1.0	1.0
1071_0242041	1.0	1.0
1071_0243621	2.0	2.0
1071_0248309	1.0	1.0
1071_0248310	0.0	1.0
1071_0248312	1.0	1.0
1071_0248320	0.0	0.0
1071_0248322	1.0	1.0
1071_0248328	1.0	1.0
1071_0248331	0.0	1.0
1071_0248336	0.0	1.0
1071_0248341	0.0	0.0
1071_0248343	2.0	1.0
1071_0248346	0.0	1.0
1071_0248347	1.0	0.0
1091_0000003	2.0	1.0
1091_0000005	2.0	2.0
1091_0000009	0.0	1.0
1091_0000011	1.0	1.0
1091_0000018	2.0	2.0
1091_0000025	1.0	1.0
1091_0000029	2.0	1.0
1091_0000043	1.0	1.0
1091_0000051	1.0	1.0
1091_0000052	0.0	1.0
1091_0000064	1.0	1.0
1091_0000065	2.0	1.0
1091_0000071	2.0	2.0
1091_0000075	2.0	2.0
1091_0000079	1.0	2.0
1091_0000095	1.0	1.0
1091_0000102	1.0	1.0
1091_0000113	1.0	2.0
1091_0000140	2.0	1.0
1091_0000145	1.0	1.0
1091_0000157	2.0	2.0
1091_0000161	2.0	2.0
1091_0000170	2.0	1.0
1091_0000193	2.0	1.0
1091_0000207	1.0	2.0
1091_0000209	2.0	2.0
1091_0000211	1.0	1.0
1091_0000214	2.0	1.0
1091_0000219	1.0	2.0
1091_0000220	1.0	1.0
1091_0000223	1.0	2.0
1091_0000228	1.0	2.0
1091_0000233	2.0	2.0
1091_0000238	1.0	2.0
1091_0000241	2.0	1.0
1091_0000248	2.0	2.0
1091_0000253	2.0	1.0
1091_0000257	1.0	2.0
1091_0000276	2.0	1.0
LANGUAGE: DE, 3th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.25
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.99
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        18
         1.0       0.49      0.66      0.56        56
         2.0       0.53      0.53      0.53        70
         3.0       0.63      0.73      0.68        52
         4.0       0.00      0.00      0.00         9

    accuracy                           0.55       205
   macro avg       0.33      0.38      0.35       205
weighted avg       0.48      0.55      0.51       205

[[ 0 18  0  0  0]
 [ 0 37 19  0  0]
 [ 0 20 37 13  0]
 [ 0  0 14 38  0]
 [ 0  0  0  9  0]]
0.5069234247413357
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.98
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.08
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        18
         1.0       0.45      0.61      0.52        56
         2.0       0.41      0.24      0.31        70
         3.0       0.54      0.92      0.68        52
         4.0       0.00      0.00      0.00         9

    accuracy                           0.48       205
   macro avg       0.28      0.35      0.30       205
weighted avg       0.40      0.48      0.42       205

[[ 0 18  0  0  0]
 [ 0 34 20  2  0]
 [ 0 23 17 30  0]
 [ 0  0  4 48  0]
 [ 0  0  0  9  0]]
0.41909462951544746
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.89
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.91
              precision    recall  f1-score   support

         0.0       1.00      0.28      0.43        18
         1.0       0.52      0.59      0.55        56
         2.0       0.52      0.47      0.50        70
         3.0       0.62      0.87      0.72        52
         4.0       0.00      0.00      0.00         9

    accuracy                           0.57       205
   macro avg       0.53      0.44      0.44       205
weighted avg       0.56      0.57      0.54       205

[[ 5 13  0  0  0]
 [ 0 33 23  0  0]
 [ 0 18 33 19  0]
 [ 0  0  7 45  0]
 [ 0  0  0  9  0]]
0.5405020929843166
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.79
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.93
              precision    recall  f1-score   support

         0.0       1.00      0.28      0.43        18
         1.0       0.53      0.57      0.55        56
         2.0       0.54      0.51      0.53        70
         3.0       0.62      0.87      0.72        52
         4.0       0.00      0.00      0.00         9

    accuracy                           0.58       205
   macro avg       0.54      0.45      0.45       205
weighted avg       0.57      0.58      0.55       205

[[ 5 13  0  0  0]
 [ 0 32 24  0  0]
 [ 0 15 36 19  0]
 [ 0  0  7 45  0]
 [ 0  0  0  9  0]]
0.5509802919441116
205 205 205
Filename	True Label	Prediction
1023_0001419	3.0	3.0
1023_0001422	3.0	2.0
1023_0101683	3.0	3.0
1023_0101684	2.0	3.0
1023_0101690	2.0	2.0
1023_0101695	2.0	3.0
1023_0101848	2.0	2.0
1023_0101849	3.0	3.0
1023_0101851	3.0	3.0
1023_0101855	2.0	2.0
1023_0101894	2.0	3.0
1023_0101898	4.0	3.0
1023_0101900	3.0	3.0
1023_0101907	3.0	3.0
1023_0103831	3.0	3.0
1023_0103832	2.0	2.0
1023_0103840	3.0	3.0
1023_0103844	4.0	3.0
1023_0104206	3.0	2.0
1023_0104207	2.0	3.0
1023_0107075	2.0	2.0
1023_0107740	3.0	3.0
1023_0107773	2.0	3.0
1023_0107783	3.0	3.0
1023_0108305	3.0	3.0
1023_0108307	3.0	3.0
1023_0108422	3.0	3.0
1023_0108423	2.0	2.0
1023_0108510	2.0	3.0
1023_0108814	3.0	3.0
1023_0109026	2.0	2.0
1023_0109151	3.0	3.0
1023_0109247	3.0	3.0
1023_0109248	2.0	3.0
1023_0109495	2.0	3.0
1023_0109515	3.0	3.0
1023_0109522	3.0	3.0
1023_0109606	2.0	3.0
1023_0109717	3.0	3.0
1023_0109891	3.0	3.0
1031_0001703	4.0	3.0
1031_0001997	3.0	3.0
1031_0002002	2.0	3.0
1031_0002005	3.0	3.0
1031_0002042	3.0	3.0
1031_0002086	3.0	3.0
1031_0002197	4.0	3.0
1031_0003012	3.0	3.0
1031_0003048	4.0	3.0
1031_0003071	3.0	3.0
1031_0003072	3.0	3.0
1031_0003073	4.0	3.0
1031_0003085	3.0	3.0
1031_0003090	3.0	3.0
1031_0003091	2.0	3.0
1031_0003092	2.0	3.0
1031_0003095	2.0	3.0
1031_0003099	3.0	3.0
1031_0003121	4.0	3.0
1031_0003128	3.0	3.0
1031_0003133	4.0	3.0
1031_0003136	3.0	3.0
1031_0003144	3.0	3.0
1031_0003162	3.0	3.0
1031_0003167	3.0	3.0
1031_0003169	3.0	3.0
1031_0003189	3.0	3.0
1031_0003190	3.0	3.0
1031_0003212	2.0	3.0
1031_0003216	3.0	3.0
1031_0003239	4.0	3.0
1031_0003240	2.0	3.0
1031_0003243	3.0	3.0
1031_0003262	3.0	3.0
1031_0003273	3.0	3.0
1031_0003339	3.0	3.0
1031_0003369	3.0	3.0
1031_0003384	2.0	2.0
1031_0003386	2.0	3.0
1031_0003414	3.0	3.0
1061_0120271	2.0	2.0
1061_0120275	2.0	2.0
1061_0120276	2.0	2.0
1061_0120279	1.0	2.0
1061_0120288	1.0	2.0
1061_0120290	1.0	2.0
1061_0120301	2.0	2.0
1061_0120304	2.0	1.0
1061_0120309	1.0	1.0
1061_0120312	1.0	1.0
1061_0120324	2.0	2.0
1061_0120328	1.0	1.0
1061_0120333	3.0	2.0
1061_0120338	2.0	2.0
1061_0120341	1.0	1.0
1061_0120343	2.0	2.0
1061_0120352	1.0	1.0
1061_0120359	1.0	2.0
1061_0120367	2.0	2.0
1061_0120369	1.0	1.0
1061_0120376	2.0	2.0
1061_0120391	1.0	1.0
1061_0120407	3.0	2.0
1061_0120410	2.0	2.0
1061_0120414	2.0	2.0
1061_0120421	2.0	3.0
1061_0120423	2.0	2.0
1061_0120430	1.0	2.0
1061_0120432	1.0	2.0
1061_0120438	2.0	3.0
1061_0120449	3.0	2.0
1061_0120457	3.0	2.0
1061_0120460	2.0	2.0
1061_0120483	1.0	2.0
1061_0120496	1.0	2.0
1061_0120499	2.0	2.0
1061_0120876	2.0	2.0
1061_0120887	1.0	1.0
1061_1202917	1.0	1.0
1061_1202918	1.0	2.0
1071_0020001	1.0	1.0
1071_0024678	1.0	1.0
1071_0024690	1.0	2.0
1071_0024692	2.0	2.0
1071_0024699	1.0	1.0
1071_0024709	2.0	2.0
1071_0024710	1.0	1.0
1071_0024712	1.0	1.0
1071_0024758	2.0	2.0
1071_0024761	2.0	1.0
1071_0024763	1.0	1.0
1071_0024775	0.0	1.0
1071_0024779	1.0	1.0
1071_0024782	0.0	0.0
1071_0024801	1.0	1.0
1071_0024802	2.0	1.0
1071_0024818	2.0	1.0
1071_0024822	0.0	1.0
1071_0024831	0.0	1.0
1071_0024835	0.0	1.0
1071_0024843	0.0	1.0
1071_0024845	0.0	1.0
1071_0024847	1.0	1.0
1071_0024849	0.0	0.0
1071_0024855	1.0	1.0
1071_0024856	1.0	1.0
1071_0024860	1.0	1.0
1071_0024861	0.0	1.0
1071_0024864	0.0	0.0
1071_0024865	2.0	1.0
1071_0024866	2.0	2.0
1071_0024871	1.0	1.0
1071_0241831	1.0	1.0
1071_0241832	1.0	1.0
1071_0242012	1.0	2.0
1071_0242021	1.0	1.0
1071_0242022	0.0	1.0
1071_0242071	0.0	1.0
1071_0248305	0.0	1.0
1071_0248315	0.0	0.0
1071_0248318	0.0	0.0
1071_0248321	2.0	1.0
1071_0248325	0.0	1.0
1071_0248334	2.0	1.0
1091_0000001	1.0	1.0
1091_0000008	2.0	2.0
1091_0000027	0.0	1.0
1091_0000028	1.0	1.0
1091_0000034	2.0	1.0
1091_0000044	0.0	1.0
1091_0000047	2.0	1.0
1091_0000050	1.0	1.0
1091_0000059	1.0	2.0
1091_0000068	2.0	2.0
1091_0000069	2.0	1.0
1091_0000072	1.0	2.0
1091_0000086	1.0	1.0
1091_0000101	2.0	1.0
1091_0000126	2.0	1.0
1091_0000127	2.0	1.0
1091_0000152	1.0	1.0
1091_0000153	1.0	2.0
1091_0000160	2.0	3.0
1091_0000166	1.0	2.0
1091_0000168	2.0	2.0
1091_0000169	3.0	2.0
1091_0000171	1.0	2.0
1091_0000174	2.0	1.0
1091_0000190	1.0	2.0
1091_0000191	1.0	2.0
1091_0000196	2.0	1.0
1091_0000201	2.0	2.0
1091_0000202	1.0	2.0
1091_0000205	1.0	2.0
1091_0000206	1.0	1.0
1091_0000212	1.0	2.0
1091_0000215	2.0	2.0
1091_0000229	1.0	2.0
1091_0000236	2.0	2.0
1091_0000239	2.0	2.0
1091_0000243	1.0	1.0
1091_0000262	2.0	2.0
1091_0000267	1.0	2.0
1091_0000273	1.0	2.0
1091_0000275	2.0	2.0
LANGUAGE: DE, 4th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.31
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.05
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        18
         1.0       0.51      0.81      0.63        57
         2.0       0.59      0.33      0.42        70
         3.0       0.59      0.88      0.71        51
         4.0       0.00      0.00      0.00         9

    accuracy                           0.56       205
   macro avg       0.34      0.40      0.35       205
weighted avg       0.49      0.56      0.49       205

[[ 0 18  0  0  0]
 [ 0 46 11  0  0]
 [ 0 25 23 22  0]
 [ 0  1  5 45  0]
 [ 0  0  0  9  0]]
0.49442188329214337
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.03
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.93
              precision    recall  f1-score   support

         0.0       0.57      0.22      0.32        18
         1.0       0.53      0.60      0.56        57
         2.0       0.57      0.69      0.62        70
         3.0       0.68      0.67      0.67        51
         4.0       0.00      0.00      0.00         9

    accuracy                           0.59       205
   macro avg       0.47      0.43      0.44       205
weighted avg       0.56      0.59      0.56       205

[[ 4 14  0  0  0]
 [ 3 34 20  0  0]
 [ 0 15 48  7  0]
 [ 0  1 16 34  0]
 [ 0  0  0  9  0]]
0.5647124641510713
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.89
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.92
              precision    recall  f1-score   support

         0.0       0.38      0.28      0.32        18
         1.0       0.55      0.51      0.53        57
         2.0       0.60      0.57      0.58        70
         3.0       0.61      0.86      0.72        51
         4.0       0.00      0.00      0.00         9

    accuracy                           0.58       205
   macro avg       0.43      0.44      0.43       205
weighted avg       0.54      0.58      0.55       205

[[ 5 13  0  0  0]
 [ 7 29 21  0  0]
 [ 1 10 40 19  0]
 [ 0  1  6 44  0]
 [ 0  0  0  9  0]]
0.5523156797768615
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.82
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.96
              precision    recall  f1-score   support

         0.0       0.56      0.28      0.37        18
         1.0       0.56      0.56      0.56        57
         2.0       0.60      0.56      0.58        70
         3.0       0.61      0.88      0.72        51
         4.0       0.00      0.00      0.00         9

    accuracy                           0.59       205
   macro avg       0.47      0.46      0.45       205
weighted avg       0.56      0.59      0.57       205

[[ 5 13  0  0  0]
 [ 4 32 21  0  0]
 [ 0 11 39 20  0]
 [ 0  1  5 45  0]
 [ 0  0  0  9  0]]
0.565029810298103
205 205 205
Filename	True Label	Prediction
1023_0101701	2.0	2.0
1023_0101749	4.0	3.0
1023_0101751	3.0	3.0
1023_0101752	2.0	3.0
1023_0101847	3.0	3.0
1023_0101852	3.0	3.0
1023_0101893	3.0	3.0
1023_0101895	4.0	3.0
1023_0101896	2.0	2.0
1023_0101899	3.0	2.0
1023_0101904	2.0	2.0
1023_0101909	3.0	3.0
1023_0103821	3.0	3.0
1023_0103827	3.0	3.0
1023_0103828	1.0	1.0
1023_0103833	4.0	3.0
1023_0103836	3.0	3.0
1023_0103839	3.0	3.0
1023_0104203	2.0	3.0
1023_0104209	3.0	3.0
1023_0106816	3.0	3.0
1023_0107725	2.0	3.0
1023_0108518	3.0	3.0
1023_0108641	3.0	3.0
1023_0108813	3.0	3.0
1023_0108815	2.0	3.0
1023_0108885	2.0	2.0
1023_0108888	3.0	3.0
1023_0108931	3.0	3.0
1023_0108932	2.0	3.0
1023_0108933	2.0	3.0
1023_0108955	3.0	3.0
1023_0109022	2.0	3.0
1023_0109029	1.0	2.0
1023_0109038	3.0	3.0
1023_0109192	2.0	3.0
1023_0109249	2.0	3.0
1023_0109399	2.0	3.0
1023_0109402	2.0	3.0
1023_0109516	3.0	3.0
1023_0109518	2.0	2.0
1023_0109588	3.0	3.0
1023_0109878	2.0	3.0
1023_0109890	3.0	3.0
1023_0109946	2.0	3.0
1031_0001998	4.0	3.0
1031_0002003	2.0	3.0
1031_0002043	3.0	3.0
1031_0002061	3.0	3.0
1031_0002089	3.0	3.0
1031_0002131	3.0	3.0
1031_0002184	3.0	3.0
1031_0003013	4.0	3.0
1031_0003065	3.0	3.0
1031_0003077	3.0	3.0
1031_0003126	3.0	3.0
1031_0003140	3.0	3.0
1031_0003146	4.0	3.0
1031_0003160	3.0	3.0
1031_0003166	2.0	3.0
1031_0003181	3.0	3.0
1031_0003184	4.0	3.0
1031_0003207	4.0	3.0
1031_0003217	3.0	3.0
1031_0003225	3.0	3.0
1031_0003235	3.0	3.0
1031_0003261	3.0	3.0
1031_0003309	3.0	3.0
1031_0003315	3.0	3.0
1031_0003336	2.0	3.0
1031_0003338	3.0	3.0
1031_0003352	2.0	3.0
1031_0003357	3.0	3.0
1031_0003383	3.0	3.0
1031_0003387	3.0	3.0
1031_0003391	2.0	3.0
1031_0003392	3.0	3.0
1031_0003409	4.0	3.0
1061_0120285	1.0	2.0
1061_0120302	1.0	2.0
1061_0120317	2.0	2.0
1061_0120319	2.0	2.0
1061_0120330	2.0	2.0
1061_0120334	2.0	2.0
1061_0120335	3.0	3.0
1061_0120357	3.0	3.0
1061_0120360	3.0	2.0
1061_0120370	2.0	2.0
1061_0120374	2.0	2.0
1061_0120382	1.0	1.0
1061_0120389	2.0	2.0
1061_0120403	3.0	2.0
1061_0120406	2.0	2.0
1061_0120409	2.0	2.0
1061_0120425	2.0	2.0
1061_0120440	1.0	1.0
1061_0120443	0.0	0.0
1061_0120450	2.0	2.0
1061_0120453	2.0	2.0
1061_0120456	2.0	2.0
1061_0120459	2.0	2.0
1061_0120478	2.0	2.0
1061_0120484	1.0	2.0
1061_0120489	2.0	2.0
1061_0120497	2.0	3.0
1061_0120500	1.0	2.0
1061_0120856	1.0	2.0
1061_0120858	2.0	2.0
1061_0120878	1.0	2.0
1061_0120882	2.0	3.0
1061_0120888	1.0	2.0
1061_0120889	1.0	1.0
1061_1029111	2.0	2.0
1061_1029112	3.0	2.0
1061_1029113	2.0	1.0
1061_1029115	2.0	2.0
1061_1029119	1.0	2.0
1061_1202910	2.0	2.0
1061_1202912	2.0	2.0
1061_1202916	2.0	2.0
1071_0024680	2.0	2.0
1071_0024687	0.0	1.0
1071_0024688	1.0	1.0
1071_0024691	1.0	2.0
1071_0024693	1.0	1.0
1071_0024694	1.0	2.0
1071_0024701	2.0	2.0
1071_0024715	2.0	2.0
1071_0024756	1.0	1.0
1071_0024759	0.0	1.0
1071_0024762	0.0	1.0
1071_0024768	1.0	1.0
1071_0024777	1.0	1.0
1071_0024778	0.0	0.0
1071_0024781	0.0	1.0
1071_0024803	1.0	1.0
1071_0024808	0.0	1.0
1071_0024809	0.0	1.0
1071_0024812	1.0	0.0
1071_0024813	0.0	1.0
1071_0024817	1.0	1.0
1071_0024827	1.0	1.0
1071_0024834	2.0	2.0
1071_0024857	0.0	1.0
1071_0024873	0.0	0.0
1071_0024874	1.0	1.0
1071_0024877	1.0	1.0
1071_0024881	2.0	1.0
1071_0242011	2.0	1.0
1071_0242092	0.0	0.0
1071_0243502	1.0	0.0
1071_0243622	1.0	1.0
1071_0248301	2.0	1.0
1071_0248304	1.0	0.0
1071_0248308	1.0	1.0
1071_0248314	1.0	1.0
1071_0248316	1.0	0.0
1071_0248324	0.0	1.0
1071_0248326	1.0	1.0
1071_0248327	0.0	1.0
1071_0248333	2.0	1.0
1071_0248340	0.0	0.0
1071_0248344	1.0	1.0
1091_0000004	1.0	1.0
1091_0000006	1.0	1.0
1091_0000012	1.0	1.0
1091_0000013	1.0	1.0
1091_0000014	0.0	1.0
1091_0000017	2.0	2.0
1091_0000020	1.0	2.0
1091_0000022	1.0	2.0
1091_0000023	2.0	1.0
1091_0000024	3.0	1.0
1091_0000033	1.0	1.0
1091_0000041	1.0	1.0
1091_0000048	1.0	1.0
1091_0000053	0.0	1.0
1091_0000054	0.0	1.0
1091_0000056	1.0	2.0
1091_0000060	2.0	2.0
1091_0000074	1.0	1.0
1091_0000078	2.0	1.0
1091_0000092	1.0	1.0
1091_0000114	1.0	2.0
1091_0000154	1.0	2.0
1091_0000155	1.0	2.0
1091_0000163	1.0	1.0
1091_0000164	1.0	1.0
1091_0000167	2.0	2.0
1091_0000197	1.0	2.0
1091_0000198	2.0	2.0
1091_0000208	1.0	1.0
1091_0000213	2.0	2.0
1091_0000217	2.0	1.0
1091_0000225	2.0	1.0
1091_0000235	1.0	1.0
1091_0000246	2.0	2.0
1091_0000247	2.0	2.0
1091_0000252	1.0	2.0
1091_0000254	2.0	1.0
1091_0000259	2.0	2.0
1091_0000260	1.0	2.0
1091_0000263	3.0	2.0
1091_0000264	2.0	1.0
1091_0000272	1.0	2.0
Averaged weighted F1-scores 0.5724274456515974
130.68810916179336 82.83744971317162
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
LANGUAGE: DE, 0th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.39
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.09
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.00      0.00      0.00        30
         2.0       0.49      0.73      0.59        64
         3.0       0.46      0.76      0.58        67
         4.0       0.00      0.00      0.00        33
         5.0       0.00      0.00      0.00         8

    accuracy                           0.48       206
   macro avg       0.16      0.25      0.19       206
weighted avg       0.30      0.48      0.37       206

[[ 0  0  3  1  0  0]
 [ 0  0 30  0  0  0]
 [ 0  0 47 17  0  0]
 [ 0  0 16 51  0  0]
 [ 0  0  0 33  0  0]
 [ 0  0  0  8  0  0]]
0.36995227908507483
206 206 206



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.10
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.09
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.50      0.10      0.17        30
         2.0       0.52      0.59      0.55        64
         3.0       0.53      0.76      0.62        67
         4.0       0.50      0.45      0.48        33
         5.0       0.00      0.00      0.00         8

    accuracy                           0.52       206
   macro avg       0.34      0.32      0.30       206
weighted avg       0.49      0.52      0.48       206

[[ 0  2  1  1  0  0]
 [ 0  3 26  1  0  0]
 [ 0  1 38 24  1  0]
 [ 0  0  8 51  8  0]
 [ 0  0  0 18 15  0]
 [ 0  0  0  2  6  0]]
0.4751877041584431
206 206 206



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.96
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.96
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.53      0.57      0.55        30
         2.0       0.63      0.75      0.69        64
         3.0       0.59      0.60      0.59        67
         4.0       0.47      0.42      0.44        33
         5.0       0.00      0.00      0.00         8

    accuracy                           0.58       206
   macro avg       0.37      0.39      0.38       206
weighted avg       0.54      0.58      0.56       206

[[ 0  3  0  1  0  0]
 [ 0 17 13  0  0  0]
 [ 0  8 48  7  1  0]
 [ 0  4 14 40  9  0]
 [ 0  0  1 18 14  0]
 [ 0  0  0  2  6  0]]
0.5568334832976236
206 206 206



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.84
  Training epoch took: 31
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.57      0.40      0.47        30
         2.0       0.61      0.62      0.62        64
         3.0       0.60      0.63      0.61        67
         4.0       0.51      0.76      0.61        33
         5.0       0.00      0.00      0.00         8

    accuracy                           0.58       206
   macro avg       0.38      0.40      0.38       206
weighted avg       0.55      0.58      0.56       206

[[ 0  3  0  1  0  0]
 [ 0 12 17  1  0  0]
 [ 0  5 40 18  1  0]
 [ 0  1  9 42 15  0]
 [ 0  0  0  8 25  0]
 [ 0  0  0  0  8  0]]
0.556817988513369
206 206 206
Filename	True Label	Prediction
1023_0001420	3.0	3.0
1023_0101683	3.0	3.0
1023_0101693	3.0	4.0
1023_0101843	4.0	4.0
1023_0101845	3.0	3.0
1023_0101894	2.0	3.0
1023_0101897	3.0	3.0
1023_0101899	3.0	3.0
1023_0101907	4.0	4.0
1023_0101909	4.0	4.0
1023_0103823	5.0	4.0
1023_0103824	4.0	4.0
1023_0103840	4.0	3.0
1023_0103883	3.0	4.0
1023_0104203	4.0	3.0
1023_0107773	3.0	4.0
1023_0108304	4.0	4.0
1023_0108305	3.0	4.0
1023_0108520	3.0	3.0
1023_0108810	3.0	4.0
1023_0108811	3.0	3.0
1023_0108888	3.0	4.0
1023_0108889	3.0	3.0
1023_0108955	5.0	4.0
1023_0109192	4.0	4.0
1023_0109249	3.0	3.0
1023_0109250	3.0	3.0
1023_0109395	3.0	3.0
1023_0109495	3.0	3.0
1023_0109500	4.0	4.0
1023_0109505	4.0	4.0
1023_0109519	4.0	4.0
1023_0109527	5.0	4.0
1023_0109716	5.0	4.0
1023_0109717	3.0	4.0
1023_0109891	4.0	4.0
1023_0109915	2.0	3.0
1031_0002006	4.0	3.0
1031_0002036	5.0	4.0
1031_0002079	4.0	4.0
1031_0002085	4.0	4.0
1031_0002199	4.0	4.0
1031_0002200	3.0	3.0
1031_0003012	3.0	3.0
1031_0003023	3.0	4.0
1031_0003029	3.0	4.0
1031_0003048	4.0	3.0
1031_0003052	4.0	4.0
1031_0003072	5.0	4.0
1031_0003074	5.0	4.0
1031_0003076	4.0	4.0
1031_0003077	3.0	3.0
1031_0003090	4.0	3.0
1031_0003127	4.0	4.0
1031_0003144	3.0	3.0
1031_0003145	4.0	4.0
1031_0003160	3.0	4.0
1031_0003169	3.0	4.0
1031_0003172	3.0	3.0
1031_0003184	4.0	4.0
1031_0003191	3.0	3.0
1031_0003212	2.0	4.0
1031_0003216	4.0	4.0
1031_0003219	3.0	4.0
1031_0003224	3.0	4.0
1031_0003232	3.0	4.0
1031_0003235	4.0	4.0
1031_0003238	4.0	4.0
1031_0003336	5.0	4.0
1031_0003356	4.0	3.0
1031_0003357	3.0	3.0
1031_0003368	3.0	3.0
1031_0003387	3.0	3.0
1031_0003389	4.0	3.0
1031_0003391	3.0	3.0
1031_0003393	4.0	4.0
1031_0003409	4.0	4.0
1061_0120274	2.0	2.0
1061_0120278	2.0	3.0
1061_0120279	2.0	2.0
1061_0120284	1.0	1.0
1061_0120295	0.0	3.0
1061_0120297	3.0	3.0
1061_0120302	2.0	2.0
1061_0120311	3.0	3.0
1061_0120313	2.0	2.0
1061_0120316	2.0	3.0
1061_0120321	2.0	2.0
1061_0120326	3.0	3.0
1061_0120328	2.0	2.0
1061_0120332	2.0	2.0
1061_0120345	3.0	3.0
1061_0120346	3.0	3.0
1061_0120354	1.0	2.0
1061_0120356	2.0	2.0
1061_0120374	3.0	3.0
1061_0120382	2.0	2.0
1061_0120386	1.0	2.0
1061_0120391	1.0	2.0
1061_0120394	3.0	3.0
1061_0120410	3.0	2.0
1061_0120413	2.0	2.0
1061_0120423	3.0	3.0
1061_0120425	2.0	3.0
1061_0120428	3.0	3.0
1061_0120430	2.0	2.0
1061_0120439	2.0	2.0
1061_0120441	2.0	3.0
1061_0120443	1.0	1.0
1061_0120458	4.0	4.0
1061_0120491	4.0	3.0
1061_0120495	3.0	3.0
1061_0120497	4.0	4.0
1061_0120853	3.0	3.0
1061_0120855	2.0	3.0
1061_0120874	3.0	3.0
1061_0120877	3.0	3.0
1061_0120878	2.0	2.0
1061_0120881	3.0	4.0
1061_0120884	3.0	3.0
1061_0120886	2.0	2.0
1061_0120888	2.0	3.0
1061_0120889	2.0	1.0
1061_1029113	2.0	3.0
1061_1029118	2.0	2.0
1061_1202917	2.0	2.0
1071_0024686	2.0	2.0
1071_0024693	2.0	2.0
1071_0024716	3.0	2.0
1071_0024757	2.0	2.0
1071_0024763	1.0	2.0
1071_0024768	1.0	2.0
1071_0024769	1.0	2.0
1071_0024770	1.0	2.0
1071_0024781	1.0	2.0
1071_0024782	0.0	1.0
1071_0024784	1.0	1.0
1071_0024802	2.0	2.0
1071_0024809	1.0	1.0
1071_0024821	1.0	1.0
1071_0024822	1.0	2.0
1071_0024827	2.0	1.0
1071_0024836	2.0	2.0
1071_0024847	2.0	3.0
1071_0024850	1.0	1.0
1071_0024856	1.0	1.0
1071_0024859	2.0	2.0
1071_0024861	1.0	1.0
1071_0024865	2.0	3.0
1071_0024878	2.0	3.0
1071_0024881	3.0	3.0
1071_0242011	2.0	2.0
1071_0242022	1.0	2.0
1071_0242023	1.0	2.0
1071_0242043	1.0	2.0
1071_0242093	0.0	1.0
1071_0243502	2.0	1.0
1071_0248308	2.0	2.0
1071_0248311	2.0	2.0
1071_0248323	2.0	2.0
1071_0248327	0.0	1.0
1071_0248333	2.0	2.0
1071_0248344	1.0	1.0
1071_0248348	2.0	2.0
1071_0248349	1.0	1.0
1091_0000002	3.0	2.0
1091_0000004	1.0	1.0
1091_0000006	1.0	1.0
1091_0000007	3.0	3.0
1091_0000010	3.0	2.0
1091_0000016	1.0	2.0
1091_0000019	2.0	2.0
1091_0000020	3.0	2.0
1091_0000031	2.0	2.0
1091_0000041	1.0	2.0
1091_0000044	1.0	3.0
1091_0000053	1.0	2.0
1091_0000055	2.0	3.0
1091_0000057	3.0	2.0
1091_0000067	3.0	2.0
1091_0000069	3.0	2.0
1091_0000074	2.0	3.0
1091_0000095	2.0	1.0
1091_0000102	2.0	2.0
1091_0000123	3.0	3.0
1091_0000148	2.0	2.0
1091_0000153	2.0	3.0
1091_0000160	3.0	3.0
1091_0000164	2.0	2.0
1091_0000165	1.0	2.0
1091_0000172	2.0	2.0
1091_0000192	2.0	2.0
1091_0000201	3.0	3.0
1091_0000211	2.0	2.0
1091_0000216	2.0	3.0
1091_0000225	3.0	1.0
1091_0000226	3.0	2.0
1091_0000231	3.0	3.0
1091_0000234	2.0	3.0
1091_0000235	2.0	2.0
1091_0000241	2.0	2.0
1091_0000247	2.0	2.0
1091_0000253	2.0	1.0
1091_0000266	2.0	3.0
1091_0000270	2.0	2.0
1091_0000272	1.0	2.0
LANGUAGE: DE, 1th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.42
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.14
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         3
         1.0       0.00      0.00      0.00        30
         2.0       0.51      0.94      0.66        64
         3.0       0.61      0.49      0.54        68
         4.0       0.42      0.44      0.43        32
         5.0       0.00      0.00      0.00         8

    accuracy                           0.52       205
   macro avg       0.26      0.31      0.27       205
weighted avg       0.43      0.52      0.45       205

[[ 0  0  3  0  0  0]
 [ 0  0 30  0  0  0]
 [ 0  0 60  4  0  0]
 [ 0  0 21 33 14  0]
 [ 0  0  3 15 14  0]
 [ 0  0  1  2  5  0]]
0.4525331845283864
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.13
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.03
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         3
         1.0       0.54      0.47      0.50        30
         2.0       0.63      0.73      0.68        64
         3.0       0.59      0.38      0.46        68
         4.0       0.42      0.78      0.54        32
         5.0       0.00      0.00      0.00         8

    accuracy                           0.55       205
   macro avg       0.36      0.39      0.36       205
weighted avg       0.54      0.55      0.52       205

[[ 0  3  0  0  0  0]
 [ 0 14 15  1  0  0]
 [ 0  6 47 11  0  0]
 [ 0  2 12 26 28  0]
 [ 0  1  1  5 25  0]
 [ 0  0  0  1  7  0]]
0.523138090043039
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.96
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.01
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         3
         1.0       0.56      0.50      0.53        30
         2.0       0.63      0.77      0.69        64
         3.0       0.64      0.37      0.47        68
         4.0       0.43      0.81      0.56        32
         5.0       0.00      0.00      0.00         8

    accuracy                           0.56       205
   macro avg       0.38      0.41      0.37       205
weighted avg       0.56      0.56      0.53       205

[[ 0  3  0  0  0  0]
 [ 0 15 14  1  0  0]
 [ 0  7 49  8  0  0]
 [ 0  1 14 25 28  0]
 [ 0  1  1  4 26  0]
 [ 0  0  0  1  7  0]]
0.5347642040076906
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.87
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.01
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         3
         1.0       0.58      0.37      0.45        30
         2.0       0.63      0.72      0.67        64
         3.0       0.58      0.47      0.52        68
         4.0       0.43      0.78      0.56        32
         5.0       0.00      0.00      0.00         8

    accuracy                           0.56       205
   macro avg       0.37      0.39      0.37       205
weighted avg       0.54      0.56      0.53       205

[[ 0  3  0  0  0  0]
 [ 0 11 18  1  0  0]
 [ 0  3 46 15  0  0]
 [ 0  1  9 32 26  0]
 [ 0  1  0  6 25  0]
 [ 0  0  0  1  7  0]]
0.5346701538721905
205 205 205
Filename	True Label	Prediction
1023_0001423	4.0	3.0
1023_0101675	4.0	3.0
1023_0101694	3.0	4.0
1023_0101700	4.0	4.0
1023_0101841	3.0	3.0
1023_0101844	3.0	4.0
1023_0101853	3.0	4.0
1023_0101854	2.0	3.0
1023_0101855	2.0	2.0
1023_0103821	4.0	4.0
1023_0103829	4.0	4.0
1023_0103831	4.0	4.0
1023_0103838	4.0	4.0
1023_0103841	5.0	4.0
1023_0103844	4.0	4.0
1023_0104209	3.0	4.0
1023_0106816	5.0	4.0
1023_0107672	3.0	4.0
1023_0107682	2.0	1.0
1023_0107780	4.0	4.0
1023_0107784	3.0	3.0
1023_0107787	3.0	4.0
1023_0107788	3.0	4.0
1023_0108307	5.0	4.0
1023_0108423	3.0	2.0
1023_0108648	4.0	4.0
1023_0108752	5.0	4.0
1023_0108753	4.0	4.0
1023_0108932	3.0	4.0
1023_0108933	4.0	4.0
1023_0108992	3.0	3.0
1023_0109027	2.0	3.0
1023_0109151	2.0	3.0
1023_0109396	3.0	4.0
1023_0109422	5.0	4.0
1023_0109518	3.0	4.0
1023_0109528	3.0	4.0
1023_0109588	3.0	4.0
1023_0109614	3.0	3.0
1023_0109649	3.0	3.0
1023_0109651	4.0	4.0
1023_0109721	3.0	3.0
1023_0109878	3.0	4.0
1023_0109880	4.0	4.0
1023_0109914	3.0	4.0
1023_0109951	3.0	3.0
1031_0002032	4.0	4.0
1031_0002042	4.0	4.0
1031_0002084	4.0	4.0
1031_0002086	3.0	4.0
1031_0002091	3.0	4.0
1031_0002092	4.0	4.0
1031_0002195	3.0	3.0
1031_0002196	4.0	4.0
1031_0003054	3.0	4.0
1031_0003071	3.0	3.0
1031_0003129	3.0	3.0
1031_0003130	5.0	4.0
1031_0003136	4.0	4.0
1031_0003146	4.0	4.0
1031_0003161	4.0	4.0
1031_0003165	3.0	4.0
1031_0003166	3.0	3.0
1031_0003174	3.0	3.0
1031_0003180	4.0	4.0
1031_0003183	4.0	4.0
1031_0003185	3.0	3.0
1031_0003187	3.0	3.0
1031_0003189	4.0	4.0
1031_0003211	3.0	4.0
1031_0003240	3.0	4.0
1031_0003262	3.0	4.0
1031_0003313	3.0	4.0
1031_0003315	3.0	3.0
1031_0003330	3.0	4.0
1031_0003338	4.0	4.0
1031_0003353	3.0	3.0
1031_0003355	3.0	3.0
1031_0003390	5.0	4.0
1061_0120272	2.0	2.0
1061_0120273	2.0	2.0
1061_0120286	1.0	2.0
1061_0120289	2.0	2.0
1061_0120299	3.0	3.0
1061_0120306	4.0	3.0
1061_0120310	2.0	2.0
1061_0120315	2.0	2.0
1061_0120319	2.0	2.0
1061_0120320	3.0	3.0
1061_0120324	2.0	2.0
1061_0120327	3.0	4.0
1061_0120330	4.0	3.0
1061_0120347	1.0	2.0
1061_0120349	2.0	2.0
1061_0120359	2.0	2.0
1061_0120360	3.0	4.0
1061_0120366	3.0	3.0
1061_0120367	3.0	3.0
1061_0120370	2.0	2.0
1061_0120376	2.0	2.0
1061_0120383	4.0	4.0
1061_0120387	2.0	3.0
1061_0120389	3.0	3.0
1061_0120406	3.0	3.0
1061_0120409	3.0	3.0
1061_0120460	2.0	2.0
1061_0120480	3.0	3.0
1061_0120489	3.0	4.0
1061_0120858	2.0	3.0
1061_0120883	3.0	2.0
1061_0120890	2.0	2.0
1061_1202910	3.0	3.0
1061_1202911	1.0	2.0
1061_1202913	2.0	2.0
1071_0020001	2.0	1.0
1071_0024683	1.0	1.0
1071_0024685	2.0	2.0
1071_0024688	1.0	2.0
1071_0024692	4.0	3.0
1071_0024704	2.0	2.0
1071_0024708	2.0	2.0
1071_0024711	1.0	1.0
1071_0024712	2.0	2.0
1071_0024714	2.0	2.0
1071_0024773	1.0	2.0
1071_0024775	0.0	1.0
1071_0024776	1.0	1.0
1071_0024778	0.0	1.0
1071_0024799	3.0	2.0
1071_0024823	2.0	2.0
1071_0024831	1.0	2.0
1071_0024835	2.0	2.0
1071_0024844	1.0	2.0
1071_0024848	1.0	2.0
1071_0024852	1.0	1.0
1071_0024862	2.0	2.0
1071_0024863	1.0	2.0
1071_0024866	5.0	3.0
1071_0024876	2.0	2.0
1071_0024877	2.0	2.0
1071_0242041	1.0	2.0
1071_0242072	0.0	1.0
1071_0243581	1.0	1.0
1071_0243582	1.0	2.0
1071_0243593	2.0	2.0
1071_0243623	2.0	2.0
1071_0248309	2.0	2.0
1071_0248313	2.0	2.0
1071_0248314	1.0	1.0
1071_0248316	1.0	1.0
1071_0248319	1.0	1.0
1071_0248321	3.0	2.0
1071_0248325	1.0	2.0
1071_0248329	2.0	2.0
1071_0248342	1.0	2.0
1071_0248345	2.0	3.0
1091_0000005	3.0	3.0
1091_0000008	3.0	3.0
1091_0000015	2.0	3.0
1091_0000017	3.0	3.0
1091_0000018	3.0	3.0
1091_0000022	2.0	3.0
1091_0000023	4.0	1.0
1091_0000028	1.0	1.0
1091_0000030	1.0	2.0
1091_0000033	2.0	2.0
1091_0000036	3.0	3.0
1091_0000038	2.0	2.0
1091_0000043	2.0	2.0
1091_0000045	2.0	3.0
1091_0000046	2.0	2.0
1091_0000052	1.0	1.0
1091_0000061	3.0	1.0
1091_0000062	3.0	2.0
1091_0000065	1.0	2.0
1091_0000066	1.0	2.0
1091_0000071	2.0	2.0
1091_0000072	1.0	3.0
1091_0000078	3.0	2.0
1091_0000092	2.0	2.0
1091_0000114	2.0	3.0
1091_0000126	3.0	2.0
1091_0000140	2.0	1.0
1091_0000145	1.0	1.0
1091_0000152	2.0	2.0
1091_0000155	3.0	3.0
1091_0000156	2.0	3.0
1091_0000158	2.0	3.0
1091_0000163	2.0	2.0
1091_0000166	2.0	2.0
1091_0000167	2.0	3.0
1091_0000190	1.0	2.0
1091_0000191	2.0	2.0
1091_0000206	1.0	2.0
1091_0000209	3.0	2.0
1091_0000210	2.0	2.0
1091_0000212	2.0	2.0
1091_0000214	3.0	2.0
1091_0000228	2.0	2.0
1091_0000238	2.0	2.0
1091_0000240	2.0	2.0
1091_0000252	2.0	3.0
1091_0000257	2.0	3.0
1091_0000261	2.0	2.0
1091_0000263	4.0	3.0
LANGUAGE: DE, 2th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.46
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.19
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         3
         1.0       0.44      0.87      0.58        30
         2.0       0.50      0.42      0.46        64
         3.0       0.45      0.60      0.51        68
         4.0       0.00      0.00      0.00        32
         5.0       0.00      0.00      0.00         8

    accuracy                           0.46       205
   macro avg       0.23      0.32      0.26       205
weighted avg       0.37      0.46      0.40       205

[[ 0  3  0  0  0  0]
 [ 0 26  4  0  0  0]
 [ 0 24 27 13  0  0]
 [ 0  6 21 41  0  0]
 [ 0  0  2 30  0  0]
 [ 0  0  0  8  0  0]]
0.39837183161395506
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.14
  Training epoch took: 31
Running Validation...
  Average evaluation loss: 1.20
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         3
         1.0       0.40      0.07      0.11        30
         2.0       0.44      0.45      0.45        64
         3.0       0.48      0.53      0.50        68
         4.0       0.47      0.88      0.62        32
         5.0       0.00      0.00      0.00         8

    accuracy                           0.46       205
   macro avg       0.30      0.32      0.28       205
weighted avg       0.43      0.46      0.42       205

[[ 0  2  1  0  0  0]
 [ 0  2 27  1  0  0]
 [ 0  1 29 33  1  0]
 [ 0  0  9 36 23  0]
 [ 0  0  0  4 28  0]
 [ 0  0  0  1  7  0]]
0.4190853049389635
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.02
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.92
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         3
         1.0       0.64      0.60      0.62        30
         2.0       0.63      0.73      0.68        64
         3.0       0.63      0.66      0.65        68
         4.0       0.65      0.62      0.63        32
         5.0       0.00      0.00      0.00         8

    accuracy                           0.63       205
   macro avg       0.42      0.44      0.43       205
weighted avg       0.60      0.63      0.62       205

[[ 0  3  0  0  0  0]
 [ 0 18 12  0  0  0]
 [ 0  6 47 11  0  0]
 [ 0  1 15 45  7  0]
 [ 0  0  1 11 20  0]
 [ 0  0  0  4  4  0]]
0.6158414755639919
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.91
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.98
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         3
         1.0       0.63      0.40      0.49        30
         2.0       0.60      0.69      0.64        64
         3.0       0.62      0.50      0.55        68
         4.0       0.48      0.88      0.62        32
         5.0       0.00      0.00      0.00         8

    accuracy                           0.58       205
   macro avg       0.39      0.41      0.38       205
weighted avg       0.56      0.58      0.55       205

[[ 0  3  0  0  0  0]
 [ 0 12 17  1  0  0]
 [ 0  4 44 15  1  0]
 [ 0  0 12 34 22  0]
 [ 0  0  0  4 28  0]
 [ 0  0  0  1  7  0]]
0.5527218226740769
205 205 205
Filename	True Label	Prediction
1023_0001575	5.0	3.0
1023_0101690	2.0	2.0
1023_0101749	4.0	4.0
1023_0101751	3.0	4.0
1023_0101856	2.0	3.0
1023_0101898	3.0	3.0
1023_0101906	3.0	3.0
1023_0102118	3.0	4.0
1023_0103826	4.0	4.0
1023_0103827	3.0	4.0
1023_0103828	2.0	2.0
1023_0103833	5.0	4.0
1023_0103837	5.0	4.0
1023_0104207	3.0	4.0
1023_0107074	3.0	4.0
1023_0107783	3.0	3.0
1023_0108422	4.0	4.0
1023_0108510	4.0	4.0
1023_0108650	3.0	4.0
1023_0108751	3.0	3.0
1023_0108766	3.0	4.0
1023_0108813	4.0	4.0
1023_0108814	4.0	3.0
1023_0108890	4.0	4.0
1023_0108931	4.0	4.0
1023_0108958	3.0	4.0
1023_0108993	4.0	4.0
1023_0109022	3.0	4.0
1023_0109026	3.0	3.0
1023_0109030	3.0	4.0
1023_0109096	4.0	4.0
1023_0109248	3.0	4.0
1023_0109267	3.0	4.0
1023_0109401	3.0	3.0
1023_0109402	3.0	3.0
1023_0109496	3.0	4.0
1023_0109516	5.0	4.0
1023_0109609	3.0	3.0
1023_0109946	2.0	3.0
1031_0001997	3.0	4.0
1031_0002004	3.0	3.0
1031_0002011	5.0	4.0
1031_0002043	4.0	4.0
1031_0002061	4.0	4.0
1031_0002187	4.0	4.0
1031_0002198	4.0	4.0
1031_0003013	5.0	4.0
1031_0003043	5.0	4.0
1031_0003078	3.0	4.0
1031_0003085	4.0	4.0
1031_0003091	2.0	3.0
1031_0003098	4.0	3.0
1031_0003131	3.0	4.0
1031_0003132	3.0	4.0
1031_0003140	4.0	4.0
1031_0003150	4.0	4.0
1031_0003156	3.0	3.0
1031_0003157	4.0	4.0
1031_0003162	4.0	4.0
1031_0003167	3.0	4.0
1031_0003170	4.0	4.0
1031_0003173	3.0	4.0
1031_0003181	4.0	4.0
1031_0003190	3.0	4.0
1031_0003203	2.0	3.0
1031_0003205	4.0	4.0
1031_0003207	4.0	4.0
1031_0003218	4.0	4.0
1031_0003233	3.0	3.0
1031_0003236	4.0	4.0
1031_0003242	3.0	4.0
1031_0003244	4.0	4.0
1031_0003274	4.0	4.0
1031_0003327	2.0	3.0
1031_0003354	3.0	4.0
1031_0003358	5.0	4.0
1031_0003365	3.0	3.0
1031_0003384	2.0	3.0
1031_0003407	3.0	3.0
1031_0003408	2.0	4.0
1031_0003415	4.0	4.0
1061_0120277	2.0	2.0
1061_0120281	2.0	2.0
1061_0120282	1.0	1.0
1061_0120288	3.0	3.0
1061_0120296	2.0	2.0
1061_0120301	2.0	3.0
1061_0120304	2.0	2.0
1061_0120307	3.0	3.0
1061_0120312	2.0	2.0
1061_0120318	3.0	3.0
1061_0120333	2.0	3.0
1061_0120336	2.0	2.0
1061_0120343	3.0	3.0
1061_0120350	4.0	3.0
1061_0120357	3.0	3.0
1061_0120372	3.0	3.0
1061_0120388	2.0	2.0
1061_0120405	2.0	2.0
1061_0120407	3.0	3.0
1061_0120411	4.0	4.0
1061_0120414	4.0	3.0
1061_0120421	3.0	3.0
1061_0120424	3.0	3.0
1061_0120426	2.0	3.0
1061_0120427	3.0	3.0
1061_0120429	2.0	2.0
1061_0120431	3.0	3.0
1061_0120455	3.0	3.0
1061_0120457	3.0	3.0
1061_0120459	3.0	3.0
1061_0120478	3.0	3.0
1061_0120482	2.0	3.0
1061_0120486	2.0	3.0
1061_0120487	2.0	3.0
1061_0120490	3.0	3.0
1061_0120494	2.0	2.0
1061_0120856	1.0	2.0
1061_0120859	3.0	3.0
1061_0120885	3.0	3.0
1061_1029111	3.0	3.0
1061_1029117	2.0	3.0
1061_1029119	3.0	3.0
1061_1029120	1.0	2.0
1061_1202912	3.0	2.0
1061_1202914	2.0	2.0
1061_1202916	2.0	2.0
1071_0024680	2.0	2.0
1071_0024681	2.0	3.0
1071_0024682	3.0	2.0
1071_0024694	2.0	3.0
1071_0024702	2.0	2.0
1071_0024703	2.0	2.0
1071_0024705	2.0	2.0
1071_0024759	1.0	1.0
1071_0024761	2.0	2.0
1071_0024766	1.0	2.0
1071_0024777	1.0	2.0
1071_0024797	1.0	2.0
1071_0024798	1.0	1.0
1071_0024801	1.0	2.0
1071_0024803	1.0	2.0
1071_0024804	1.0	2.0
1071_0024810	2.0	2.0
1071_0024812	1.0	1.0
1071_0024817	1.0	2.0
1071_0024820	1.0	1.0
1071_0024824	2.0	1.0
1071_0024825	1.0	1.0
1071_0024838	1.0	1.0
1071_0024846	0.0	1.0
1071_0024854	1.0	1.0
1071_0024855	2.0	1.0
1071_0024857	1.0	2.0
1071_0024867	3.0	2.0
1071_0241831	1.0	2.0
1071_0241833	2.0	2.0
1071_0242013	2.0	2.0
1071_0242042	1.0	2.0
1071_0243501	2.0	2.0
1071_0243592	2.0	2.0
1071_0248301	2.0	2.0
1071_0248303	1.0	1.0
1071_0248304	1.0	2.0
1071_0248307	3.0	2.0
1071_0248315	0.0	1.0
1071_0248318	0.0	1.0
1071_0248326	2.0	2.0
1071_0248328	1.0	1.0
1071_0248337	2.0	2.0
1071_0248340	1.0	1.0
1091_0000014	1.0	2.0
1091_0000021	1.0	3.0
1091_0000025	1.0	1.0
1091_0000034	3.0	2.0
1091_0000047	3.0	2.0
1091_0000048	1.0	2.0
1091_0000049	2.0	2.0
1091_0000058	3.0	2.0
1091_0000064	2.0	2.0
1091_0000068	2.0	2.0
1091_0000070	3.0	2.0
1091_0000075	2.0	2.0
1091_0000076	2.0	2.0
1091_0000144	2.0	1.0
1091_0000169	3.0	2.0
1091_0000174	2.0	1.0
1091_0000194	2.0	2.0
1091_0000199	3.0	2.0
1091_0000200	3.0	3.0
1091_0000205	2.0	2.0
1091_0000208	1.0	2.0
1091_0000213	2.0	2.0
1091_0000217	3.0	2.0
1091_0000219	2.0	2.0
1091_0000236	3.0	2.0
1091_0000237	2.0	2.0
1091_0000242	1.0	2.0
1091_0000244	2.0	2.0
1091_0000245	2.0	2.0
1091_0000248	2.0	2.0
1091_0000264	2.0	2.0
1091_0000268	2.0	2.0
1091_0000269	2.0	2.0
1091_0000274	2.0	2.0
LANGUAGE: DE, 3th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.40
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.21
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.56      0.17      0.26        30
         2.0       0.46      0.38      0.41        64
         3.0       0.41      0.68      0.51        68
         4.0       0.28      0.28      0.28        32
         5.0       0.00      0.00      0.00         7

    accuracy                           0.41       205
   macro avg       0.28      0.25      0.24       205
weighted avg       0.41      0.41      0.38       205

[[ 0  3  1  0  0  0]
 [ 0  5 21  4  0  0]
 [ 0  1 24 37  2  0]
 [ 0  0  6 46 16  0]
 [ 0  0  0 23  9  0]
 [ 0  0  0  2  5  0]]
0.38014937496855067
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.10
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.48      0.77      0.59        30
         2.0       0.66      0.52      0.58        64
         3.0       0.54      0.79      0.64        68
         4.0       0.43      0.09      0.15        32
         5.0       0.00      0.00      0.00         7

    accuracy                           0.55       205
   macro avg       0.35      0.36      0.33       205
weighted avg       0.52      0.55      0.50       205

[[ 0  4  0  0  0  0]
 [ 0 23  7  0  0  0]
 [ 0 18 33 12  1  0]
 [ 0  3 10 54  1  0]
 [ 0  0  0 29  3  0]
 [ 0  0  0  5  2  0]]
0.5043039117493546
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.95
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.06
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.53      0.60      0.56        30
         2.0       0.65      0.55      0.59        64
         3.0       0.58      0.49      0.53        68
         4.0       0.43      0.81      0.57        32
         5.0       0.00      0.00      0.00         7

    accuracy                           0.55       205
   macro avg       0.36      0.41      0.37       205
weighted avg       0.54      0.55      0.53       205

[[ 0  4  0  0  0  0]
 [ 0 18 10  2  0  0]
 [ 0 10 35 16  3  0]
 [ 0  2  9 33 24  0]
 [ 0  0  0  6 26  0]
 [ 0  0  0  0  7  0]]
0.5308880888617288
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.85
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.03
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.55      0.60      0.57        30
         2.0       0.65      0.56      0.61        64
         3.0       0.63      0.53      0.58        68
         4.0       0.47      0.88      0.61        32
         5.0       0.00      0.00      0.00         7

    accuracy                           0.58       205
   macro avg       0.38      0.43      0.39       205
weighted avg       0.57      0.58      0.56       205

[[ 0  4  0  0  0  0]
 [ 0 18 10  2  0  0]
 [ 0  9 36 15  4  0]
 [ 0  2  9 36 21  0]
 [ 0  0  0  4 28  0]
 [ 0  0  0  0  7  0]]
0.5585941809173298
205 205 205
Filename	True Label	Prediction
1023_0001418	3.0	3.0
1023_0001422	2.0	2.0
1023_0101688	4.0	4.0
1023_0101701	3.0	3.0
1023_0101752	3.0	4.0
1023_0101753	4.0	4.0
1023_0101847	2.0	4.0
1023_0101848	3.0	3.0
1023_0101849	2.0	3.0
1023_0101852	4.0	4.0
1023_0101893	3.0	4.0
1023_0101895	3.0	4.0
1023_0101896	2.0	2.0
1023_0101900	3.0	3.0
1023_0103822	3.0	3.0
1023_0103832	3.0	3.0
1023_0103834	3.0	4.0
1023_0107042	3.0	3.0
1023_0107075	2.0	3.0
1023_0107244	4.0	4.0
1023_0107725	3.0	3.0
1023_0107726	3.0	4.0
1023_0107781	4.0	4.0
1023_0108426	3.0	3.0
1023_0108518	4.0	3.0
1023_0108641	5.0	4.0
1023_0108649	4.0	4.0
1023_0108812	3.0	4.0
1023_0108885	2.0	3.0
1023_0108886	4.0	4.0
1023_0108934	4.0	4.0
1023_0109033	4.0	4.0
1023_0109038	5.0	4.0
1023_0109247	4.0	4.0
1023_0109391	4.0	3.0
1023_0109515	4.0	4.0
1023_0109520	3.0	3.0
1023_0109522	3.0	3.0
1023_0109524	4.0	3.0
1023_0109590	3.0	3.0
1023_0109674	3.0	4.0
1023_0109954	3.0	4.0
1023_0111896	3.0	3.0
1031_0001703	4.0	4.0
1031_0002002	3.0	4.0
1031_0002005	3.0	3.0
1031_0002010	4.0	4.0
1031_0002040	5.0	4.0
1031_0002083	3.0	4.0
1031_0002089	4.0	4.0
1031_0002131	3.0	4.0
1031_0002197	3.0	4.0
1031_0003035	3.0	3.0
1031_0003065	3.0	3.0
1031_0003073	5.0	4.0
1031_0003088	4.0	4.0
1031_0003092	2.0	4.0
1031_0003095	3.0	3.0
1031_0003099	3.0	3.0
1031_0003121	5.0	4.0
1031_0003126	3.0	3.0
1031_0003135	4.0	4.0
1031_0003141	3.0	4.0
1031_0003163	3.0	3.0
1031_0003164	4.0	4.0
1031_0003179	4.0	4.0
1031_0003186	5.0	4.0
1031_0003206	3.0	3.0
1031_0003217	4.0	4.0
1031_0003220	4.0	4.0
1031_0003221	2.0	4.0
1031_0003226	5.0	4.0
1031_0003261	3.0	3.0
1031_0003272	3.0	3.0
1031_0003309	3.0	4.0
1031_0003331	3.0	4.0
1031_0003337	3.0	4.0
1031_0003366	3.0	3.0
1031_0003367	4.0	4.0
1031_0003369	4.0	4.0
1031_0003386	3.0	4.0
1031_0003392	3.0	3.0
1031_0003419	4.0	4.0
1061_0120275	2.0	2.0
1061_0120285	2.0	2.0
1061_0120287	2.0	2.0
1061_0120291	2.0	2.0
1061_0120298	2.0	3.0
1061_0120303	1.0	2.0
1061_0120308	3.0	3.0
1061_0120309	2.0	2.0
1061_0120314	3.0	2.0
1061_0120331	1.0	2.0
1061_0120334	3.0	3.0
1061_0120337	3.0	3.0
1061_0120351	2.0	3.0
1061_0120353	1.0	1.0
1061_0120355	2.0	2.0
1061_0120361	3.0	4.0
1061_0120368	3.0	3.0
1061_0120369	2.0	2.0
1061_0120371	3.0	3.0
1061_0120375	2.0	1.0
1061_0120403	4.0	4.0
1061_0120415	2.0	3.0
1061_0120438	4.0	4.0
1061_0120442	2.0	4.0
1061_0120448	4.0	4.0
1061_0120449	4.0	3.0
1061_0120450	2.0	3.0
1061_0120453	3.0	3.0
1061_0120479	3.0	3.0
1061_0120481	4.0	4.0
1061_0120484	3.0	4.0
1061_0120492	3.0	4.0
1061_0120498	3.0	4.0
1061_0120857	3.0	3.0
1061_0120876	2.0	3.0
1061_0120882	4.0	4.0
1061_0120887	2.0	2.0
1061_1029112	3.0	3.0
1061_1202915	2.0	2.0
1071_0024678	2.0	1.0
1071_0024762	1.0	1.0
1071_0024765	1.0	1.0
1071_0024774	0.0	1.0
1071_0024779	2.0	2.0
1071_0024783	0.0	1.0
1071_0024806	1.0	1.0
1071_0024808	1.0	1.0
1071_0024811	1.0	2.0
1071_0024813	1.0	1.0
1071_0024814	1.0	2.0
1071_0024815	1.0	2.0
1071_0024826	2.0	2.0
1071_0024833	2.0	2.0
1071_0024834	3.0	3.0
1071_0024843	1.0	1.0
1071_0024851	1.0	1.0
1071_0024853	2.0	1.0
1071_0024860	2.0	1.0
1071_0024873	2.0	1.0
1071_0024874	2.0	2.0
1071_0024875	3.0	2.0
1071_0024879	3.0	1.0
1071_0241832	1.0	2.0
1071_0242012	2.0	2.0
1071_0242021	2.0	2.0
1071_0242071	0.0	1.0
1071_0242073	1.0	2.0
1071_0242091	1.0	1.0
1071_0243591	2.0	2.0
1071_0243621	2.0	2.0
1071_0243622	1.0	1.0
1071_0248302	1.0	1.0
1071_0248305	0.0	1.0
1071_0248310	1.0	1.0
1071_0248332	3.0	2.0
1071_0248335	2.0	2.0
1071_0248336	2.0	1.0
1071_0248338	2.0	2.0
1071_0248341	1.0	1.0
1071_0248343	1.0	1.0
1071_0248346	1.0	1.0
1071_0248350	1.0	1.0
1091_0000003	2.0	2.0
1091_0000011	2.0	2.0
1091_0000013	1.0	1.0
1091_0000024	3.0	2.0
1091_0000027	1.0	2.0
1091_0000032	2.0	2.0
1091_0000037	2.0	1.0
1091_0000051	1.0	1.0
1091_0000056	2.0	2.0
1091_0000059	2.0	3.0
1091_0000060	3.0	3.0
1091_0000063	2.0	1.0
1091_0000073	3.0	2.0
1091_0000077	3.0	1.0
1091_0000086	2.0	2.0
1091_0000113	2.0	3.0
1091_0000116	3.0	2.0
1091_0000125	3.0	2.0
1091_0000161	2.0	3.0
1091_0000168	2.0	3.0
1091_0000185	2.0	1.0
1091_0000193	2.0	2.0
1091_0000195	2.0	2.0
1091_0000197	1.0	3.0
1091_0000198	2.0	3.0
1091_0000218	3.0	2.0
1091_0000222	2.0	2.0
1091_0000223	2.0	2.0
1091_0000232	2.0	2.0
1091_0000243	1.0	2.0
1091_0000246	3.0	2.0
1091_0000249	2.0	2.0
1091_0000250	2.0	2.0
1091_0000251	2.0	2.0
1091_0000255	1.0	2.0
1091_0000256	2.0	2.0
1091_0000259	2.0	3.0
1091_0000260	2.0	3.0
1091_0000262	2.0	2.0
1091_0000273	1.0	3.0
LANGUAGE: DE, 4th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.50
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.26
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.57      0.57      0.57        30
         2.0       0.56      0.31      0.40        65
         3.0       0.42      0.88      0.57        67
         4.0       0.00      0.00      0.00        32
         5.0       0.00      0.00      0.00         7

    accuracy                           0.47       205
   macro avg       0.26      0.29      0.26       205
weighted avg       0.40      0.47      0.40       205

[[ 0  4  0  0  0  0]
 [ 0 17 10  3  0  0]
 [ 0  7 20 38  0  0]
 [ 0  2  6 59  0  0]
 [ 0  0  0 32  0  0]
 [ 0  0  0  7  0  0]]
0.39571324406890135
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.19
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.10
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.60      0.60      0.60        30
         2.0       0.62      0.54      0.58        65
         3.0       0.48      0.81      0.60        67
         4.0       0.17      0.03      0.05        32
         5.0       0.00      0.00      0.00         7

    accuracy                           0.53       205
   macro avg       0.31      0.33      0.31       205
weighted avg       0.47      0.53      0.48       205

[[ 0  4  0  0  0  0]
 [ 0 18 12  0  0  0]
 [ 0  6 35 24  0  0]
 [ 0  2  9 54  2  0]
 [ 0  0  0 31  1  0]
 [ 0  0  0  4  3  0]]
0.4755488600558037
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.04
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.09
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.58      0.50      0.54        30
         2.0       0.63      0.55      0.59        65
         3.0       0.48      0.87      0.62        67
         4.0       0.00      0.00      0.00        32
         5.0       0.00      0.00      0.00         7

    accuracy                           0.53       205
   macro avg       0.28      0.32      0.29       205
weighted avg       0.44      0.53      0.47       205

[[ 0  4  0  0  0  0]
 [ 0 15 14  1  0  0]
 [ 0  5 36 24  0  0]
 [ 0  2  7 58  0  0]
 [ 0  0  0 32  0  0]
 [ 0  0  0  6  1  0]]
0.46718297483438237
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.96
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.62      0.53      0.57        30
         2.0       0.62      0.68      0.65        65
         3.0       0.58      0.55      0.56        67
         4.0       0.48      0.66      0.55        32
         5.0       0.00      0.00      0.00         7

    accuracy                           0.58       205
   macro avg       0.38      0.40      0.39       205
weighted avg       0.55      0.58      0.56       205

[[ 0  4  0  0  0  0]
 [ 0 16 14  0  0  0]
 [ 0  4 44 16  1  0]
 [ 0  2 13 37 15  0]
 [ 0  0  0 11 21  0]
 [ 0  0  0  0  7  0]]
0.5596742411846806
205 205 205
Filename	True Label	Prediction
1023_0001416	4.0	4.0
1023_0001419	4.0	3.0
1023_0101684	3.0	3.0
1023_0101689	2.0	2.0
1023_0101691	3.0	3.0
1023_0101695	4.0	3.0
1023_0101846	4.0	4.0
1023_0101851	3.0	4.0
1023_0101901	4.0	3.0
1023_0101904	2.0	3.0
1023_0102117	4.0	4.0
1023_0103825	4.0	4.0
1023_0103830	3.0	3.0
1023_0103836	5.0	4.0
1023_0103839	3.0	4.0
1023_0103843	3.0	3.0
1023_0103880	4.0	3.0
1023_0103955	3.0	3.0
1023_0104206	3.0	3.0
1023_0107727	5.0	4.0
1023_0107729	3.0	3.0
1023_0107740	3.0	3.0
1023_0108306	4.0	4.0
1023_0108815	4.0	4.0
1023_0108887	3.0	3.0
1023_0108908	4.0	3.0
1023_0108935	4.0	4.0
1023_0109029	2.0	2.0
1023_0109039	4.0	4.0
1023_0109392	3.0	3.0
1023_0109399	3.0	3.0
1023_0109400	3.0	3.0
1023_0109591	3.0	4.0
1023_0109606	5.0	4.0
1023_0109671	3.0	3.0
1023_0109890	4.0	4.0
1023_0109917	4.0	3.0
1023_0109945	4.0	4.0
1023_0109947	3.0	4.0
1031_0001949	4.0	4.0
1031_0001950	4.0	4.0
1031_0001951	3.0	4.0
1031_0001998	4.0	4.0
1031_0002003	3.0	3.0
1031_0002087	3.0	3.0
1031_0002088	3.0	4.0
1031_0002184	4.0	4.0
1031_0002185	4.0	4.0
1031_0003042	3.0	3.0
1031_0003053	3.0	3.0
1031_0003063	4.0	3.0
1031_0003097	3.0	3.0
1031_0003106	3.0	3.0
1031_0003128	4.0	4.0
1031_0003133	4.0	4.0
1031_0003149	4.0	4.0
1031_0003154	3.0	4.0
1031_0003155	3.0	3.0
1031_0003182	5.0	4.0
1031_0003214	3.0	3.0
1031_0003225	5.0	4.0
1031_0003230	4.0	4.0
1031_0003231	3.0	4.0
1031_0003234	3.0	4.0
1031_0003237	4.0	4.0
1031_0003239	5.0	4.0
1031_0003243	3.0	4.0
1031_0003245	3.0	4.0
1031_0003246	3.0	3.0
1031_0003249	3.0	4.0
1031_0003260	3.0	3.0
1031_0003273	3.0	3.0
1031_0003310	4.0	3.0
1031_0003314	3.0	4.0
1031_0003339	5.0	4.0
1031_0003352	3.0	3.0
1031_0003359	2.0	4.0
1031_0003383	3.0	3.0
1031_0003388	3.0	4.0
1031_0003410	4.0	4.0
1031_0003414	3.0	3.0
1061_0012029	4.0	3.0
1061_0120271	3.0	3.0
1061_0120276	3.0	3.0
1061_0120280	2.0	2.0
1061_0120283	1.0	2.0
1061_0120290	2.0	2.0
1061_0120300	3.0	2.0
1061_0120317	3.0	3.0
1061_0120323	2.0	2.0
1061_0120325	2.0	3.0
1061_0120329	2.0	3.0
1061_0120335	3.0	4.0
1061_0120338	2.0	3.0
1061_0120341	2.0	2.0
1061_0120348	1.0	1.0
1061_0120352	1.0	1.0
1061_0120358	2.0	2.0
1061_0120373	3.0	3.0
1061_0120384	2.0	2.0
1061_0120390	2.0	3.0
1061_0120404	1.0	2.0
1061_0120408	3.0	2.0
1061_0120432	3.0	3.0
1061_0120433	1.0	2.0
1061_0120440	2.0	2.0
1061_0120456	2.0	3.0
1061_0120483	2.0	2.0
1061_0120485	3.0	3.0
1061_0120488	2.0	3.0
1061_0120493	3.0	3.0
1061_0120496	2.0	2.0
1061_0120499	4.0	3.0
1061_0120500	2.0	2.0
1061_0120875	3.0	3.0
1061_0120880	4.0	3.0
1061_0120894	2.0	3.0
1061_1029114	2.0	2.0
1061_1029115	3.0	2.0
1061_1029116	2.0	3.0
1061_1202918	2.0	2.0
1061_1202919	2.0	2.0
1071_0024687	1.0	1.0
1071_0024689	1.0	2.0
1071_0024690	2.0	3.0
1071_0024691	2.0	3.0
1071_0024699	1.0	2.0
1071_0024701	3.0	2.0
1071_0024706	2.0	2.0
1071_0024709	3.0	3.0
1071_0024710	1.0	2.0
1071_0024713	2.0	2.0
1071_0024715	1.0	2.0
1071_0024756	2.0	2.0
1071_0024758	3.0	2.0
1071_0024767	2.0	2.0
1071_0024772	1.0	1.0
1071_0024800	1.0	1.0
1071_0024807	1.0	2.0
1071_0024816	1.0	1.0
1071_0024818	3.0	1.0
1071_0024819	1.0	2.0
1071_0024837	1.0	1.0
1071_0024840	2.0	2.0
1071_0024841	1.0	1.0
1071_0024845	1.0	1.0
1071_0024849	0.0	1.0
1071_0024864	1.0	1.0
1071_0024871	3.0	2.0
1071_0024872	2.0	2.0
1071_0242092	0.0	1.0
1071_0248312	1.0	2.0
1071_0248317	1.0	1.0
1071_0248320	0.0	1.0
1071_0248322	2.0	2.0
1071_0248324	0.0	1.0
1071_0248330	2.0	2.0
1071_0248331	2.0	2.0
1071_0248334	2.0	2.0
1071_0248339	2.0	2.0
1071_0248347	1.0	1.0
1091_0000001	2.0	2.0
1091_0000009	1.0	1.0
1091_0000012	2.0	1.0
1091_0000026	2.0	1.0
1091_0000029	3.0	2.0
1091_0000035	2.0	2.0
1091_0000039	2.0	1.0
1091_0000042	1.0	1.0
1091_0000050	1.0	2.0
1091_0000054	1.0	2.0
1091_0000079	1.0	2.0
1091_0000087	3.0	2.0
1091_0000101	2.0	2.0
1091_0000127	2.0	2.0
1091_0000146	1.0	1.0
1091_0000151	1.0	1.0
1091_0000154	2.0	2.0
1091_0000157	3.0	2.0
1091_0000159	2.0	2.0
1091_0000162	2.0	3.0
1091_0000170	3.0	2.0
1091_0000171	2.0	2.0
1091_0000173	2.0	2.0
1091_0000196	3.0	3.0
1091_0000202	2.0	3.0
1091_0000203	2.0	2.0
1091_0000204	2.0	3.0
1091_0000207	2.0	3.0
1091_0000215	2.0	3.0
1091_0000220	2.0	2.0
1091_0000221	3.0	2.0
1091_0000224	2.0	1.0
1091_0000227	1.0	2.0
1091_0000229	2.0	2.0
1091_0000230	2.0	2.0
1091_0000233	2.0	2.0
1091_0000239	3.0	2.0
1091_0000254	3.0	1.0
1091_0000258	2.0	2.0
1091_0000265	3.0	2.0
1091_0000267	2.0	2.0
1091_0000271	2.0	2.0
1091_0000275	2.0	2.0
1091_0000276	2.0	2.0
Averaged weighted F1-scores 0.5524956774323293
130.68810916179336 82.83744971317162
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
LANGUAGE: DE, 0th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.30
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.85
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.57      0.09      0.16        43
         2.0       0.57      0.99      0.72        69
         3.0       0.63      0.89      0.74        55
         4.0       0.50      0.04      0.07        28
         5.0       0.00      0.00      0.00         1

    accuracy                           0.59       206
   macro avg       0.38      0.33      0.28       206
weighted avg       0.55      0.59      0.48       206

[[ 0  3  7  0  0  0]
 [ 0  4 39  0  0  0]
 [ 0  0 68  1  0  0]
 [ 0  0  5 49  1  0]
 [ 0  0  0 27  1  0]
 [ 0  0  0  1  0  0]]
0.4814945440444739
206 206 206



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.90
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.76
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.55      0.72      0.63        43
         2.0       0.74      0.78      0.76        69
         3.0       0.87      0.60      0.71        55
         4.0       0.62      0.86      0.72        28
         5.0       0.00      0.00      0.00         1

    accuracy                           0.69       206
   macro avg       0.46      0.49      0.47       206
weighted avg       0.68      0.69      0.67       206

[[ 0  9  1  0  0  0]
 [ 0 31 12  0  0  0]
 [ 0 14 54  1  0  0]
 [ 0  2  6 33 14  0]
 [ 0  0  0  4 24  0]
 [ 0  0  0  0  1  0]]
0.6723307074067135
206 206 206



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.79
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.73
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.55      0.72      0.63        43
         2.0       0.77      0.77      0.77        69
         3.0       0.82      0.67      0.74        55
         4.0       0.61      0.79      0.69        28
         5.0       0.00      0.00      0.00         1

    accuracy                           0.69       206
   macro avg       0.46      0.49      0.47       206
weighted avg       0.68      0.69      0.68       206

[[ 0  9  1  0  0  0]
 [ 0 31 12  0  0  0]
 [ 0 14 53  2  0  0]
 [ 0  2  3 37 13  0]
 [ 0  0  0  6 22  0]
 [ 0  0  0  0  1  0]]
0.6790256938315191
206 206 206



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.71
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.73
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.56      0.67      0.61        43
         2.0       0.74      0.81      0.77        69
         3.0       0.83      0.62      0.71        55
         4.0       0.59      0.79      0.68        28
         5.0       0.00      0.00      0.00         1

    accuracy                           0.68       206
   macro avg       0.45      0.48      0.46       206
weighted avg       0.67      0.68      0.67       206

[[ 0  9  1  0  0  0]
 [ 0 29 14  0  0  0]
 [ 0 12 56  1  0  0]
 [ 0  2  5 34 14  0]
 [ 0  0  0  6 22  0]
 [ 0  0  0  0  1  0]]
0.6672881688847804
206 206 206
Filename	True Label	Prediction
1023_0001420	3.0	3.0
1023_0101683	3.0	3.0
1023_0101693	4.0	3.0
1023_0101845	3.0	3.0
1023_0101856	3.0	3.0
1023_0101896	3.0	2.0
1023_0101897	3.0	3.0
1023_0103822	2.0	3.0
1023_0103824	3.0	3.0
1023_0103825	3.0	3.0
1023_0103828	2.0	2.0
1023_0103833	4.0	3.0
1023_0103836	3.0	3.0
1023_0103838	3.0	3.0
1023_0103883	3.0	3.0
1023_0104209	3.0	3.0
1023_0107074	4.0	3.0
1023_0107740	3.0	3.0
1023_0107781	3.0	3.0
1023_0107788	3.0	3.0
1023_0108304	3.0	3.0
1023_0108423	3.0	3.0
1023_0108751	3.0	3.0
1023_0108812	3.0	3.0
1023_0108890	3.0	3.0
1023_0108908	3.0	3.0
1023_0108932	3.0	3.0
1023_0108958	3.0	3.0
1023_0108993	3.0	3.0
1023_0109249	3.0	3.0
1023_0109395	3.0	3.0
1023_0109496	3.0	3.0
1023_0109516	3.0	3.0
1023_0109528	3.0	3.0
1023_0109609	3.0	3.0
1023_0109651	3.0	3.0
1023_0109880	4.0	3.0
1023_0109891	3.0	3.0
1031_0001949	4.0	4.0
1031_0001951	3.0	4.0
1031_0002084	4.0	4.0
1031_0002086	4.0	4.0
1031_0002131	3.0	4.0
1031_0002197	4.0	4.0
1031_0002199	4.0	4.0
1031_0003048	4.0	4.0
1031_0003063	5.0	4.0
1031_0003077	4.0	4.0
1031_0003085	3.0	4.0
1031_0003097	4.0	4.0
1031_0003126	4.0	4.0
1031_0003135	4.0	4.0
1031_0003146	4.0	4.0
1031_0003155	4.0	4.0
1031_0003160	3.0	4.0
1031_0003161	4.0	4.0
1031_0003174	4.0	4.0
1031_0003179	4.0	4.0
1031_0003182	4.0	4.0
1031_0003184	4.0	3.0
1031_0003212	3.0	4.0
1031_0003214	3.0	4.0
1031_0003216	3.0	4.0
1031_0003231	4.0	4.0
1031_0003236	3.0	4.0
1031_0003245	4.0	4.0
1031_0003246	3.0	4.0
1031_0003261	3.0	4.0
1031_0003327	3.0	4.0
1031_0003331	3.0	4.0
1031_0003336	3.0	4.0
1031_0003337	4.0	4.0
1031_0003338	4.0	4.0
1031_0003357	4.0	4.0
1031_0003365	4.0	3.0
1031_0003386	3.0	4.0
1031_0003409	4.0	4.0
1061_0120273	1.0	2.0
1061_0120290	2.0	2.0
1061_0120300	2.0	2.0
1061_0120307	2.0	2.0
1061_0120310	3.0	2.0
1061_0120314	2.0	2.0
1061_0120323	2.0	2.0
1061_0120329	2.0	2.0
1061_0120347	2.0	2.0
1061_0120360	3.0	3.0
1061_0120361	3.0	2.0
1061_0120366	3.0	2.0
1061_0120372	2.0	2.0
1061_0120384	2.0	2.0
1061_0120389	2.0	2.0
1061_0120421	2.0	2.0
1061_0120423	3.0	3.0
1061_0120429	3.0	2.0
1061_0120450	2.0	2.0
1061_0120453	2.0	2.0
1061_0120459	2.0	2.0
1061_0120485	2.0	2.0
1061_0120491	2.0	2.0
1061_0120497	3.0	3.0
1061_0120877	2.0	2.0
1061_0120887	2.0	2.0
1061_1029111	2.0	2.0
1061_1202911	1.0	2.0
1061_1202912	2.0	2.0
1061_1202914	2.0	2.0
1061_1202916	2.0	2.0
1071_0024680	2.0	2.0
1071_0024681	2.0	2.0
1071_0024683	0.0	1.0
1071_0024685	2.0	2.0
1071_0024689	1.0	2.0
1071_0024693	1.0	2.0
1071_0024699	2.0	2.0
1071_0024702	2.0	2.0
1071_0024704	1.0	1.0
1071_0024708	1.0	1.0
1071_0024709	2.0	2.0
1071_0024714	2.0	2.0
1071_0024761	2.0	1.0
1071_0024763	1.0	1.0
1071_0024772	0.0	1.0
1071_0024781	1.0	1.0
1071_0024797	0.0	1.0
1071_0024799	2.0	2.0
1071_0024806	1.0	1.0
1071_0024808	1.0	1.0
1071_0024812	1.0	1.0
1071_0024816	1.0	1.0
1071_0024820	1.0	1.0
1071_0024831	1.0	1.0
1071_0024841	1.0	1.0
1071_0024847	2.0	2.0
1071_0024849	0.0	1.0
1071_0024851	2.0	1.0
1071_0024854	0.0	1.0
1071_0024863	2.0	1.0
1071_0024872	1.0	2.0
1071_0024874	1.0	1.0
1071_0242011	1.0	1.0
1071_0242012	2.0	2.0
1071_0242022	0.0	1.0
1071_0242043	0.0	1.0
1071_0242071	0.0	1.0
1071_0242093	0.0	1.0
1071_0243501	1.0	1.0
1071_0243591	1.0	1.0
1071_0243593	1.0	2.0
1071_0243622	1.0	1.0
1071_0248302	1.0	1.0
1071_0248314	1.0	1.0
1071_0248326	1.0	1.0
1071_0248329	1.0	1.0
1071_0248334	2.0	2.0
1071_0248335	1.0	1.0
1071_0248337	1.0	2.0
1071_0248338	1.0	1.0
1071_0248339	1.0	1.0
1071_0248342	1.0	1.0
1071_0248345	2.0	2.0
1091_0000003	2.0	1.0
1091_0000008	2.0	2.0
1091_0000022	2.0	2.0
1091_0000031	1.0	1.0
1091_0000037	1.0	1.0
1091_0000052	1.0	1.0
1091_0000053	1.0	1.0
1091_0000067	2.0	1.0
1091_0000070	2.0	1.0
1091_0000073	2.0	1.0
1091_0000076	2.0	2.0
1091_0000077	2.0	1.0
1091_0000078	3.0	1.0
1091_0000102	2.0	2.0
1091_0000123	2.0	2.0
1091_0000140	2.0	1.0
1091_0000144	2.0	1.0
1091_0000162	2.0	2.0
1091_0000163	2.0	1.0
1091_0000170	3.0	1.0
1091_0000172	2.0	2.0
1091_0000185	2.0	1.0
1091_0000190	1.0	1.0
1091_0000192	2.0	2.0
1091_0000206	1.0	2.0
1091_0000207	2.0	2.0
1091_0000212	1.0	2.0
1091_0000215	2.0	2.0
1091_0000221	2.0	2.0
1091_0000223	2.0	2.0
1091_0000229	2.0	2.0
1091_0000231	2.0	2.0
1091_0000237	1.0	2.0
1091_0000240	2.0	2.0
1091_0000241	2.0	2.0
1091_0000242	1.0	2.0
1091_0000243	1.0	2.0
1091_0000247	2.0	2.0
1091_0000249	2.0	2.0
1091_0000255	0.0	2.0
1091_0000256	1.0	2.0
1091_0000258	2.0	2.0
1091_0000271	2.0	2.0
1091_0000272	1.0	2.0
1091_0000275	2.0	2.0
LANGUAGE: DE, 1th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.31
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.94
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.67      0.10      0.17        42
         2.0       0.55      0.87      0.67        70
         3.0       0.73      0.67      0.70        54
         4.0       0.59      0.82      0.69        28
         5.0       0.00      0.00      0.00         1

    accuracy                           0.60       205
   macro avg       0.42      0.41      0.37       205
weighted avg       0.60      0.60      0.54       205

[[ 0  2  8  0  0  0]
 [ 0  4 38  0  0  0]
 [ 0  0 61  8  1  0]
 [ 0  0  4 36 14  0]
 [ 0  0  0  5 23  0]
 [ 0  0  0  0  1  0]]
0.5422135309977456
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.89
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.81
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.56      0.83      0.67        42
         2.0       0.76      0.67      0.71        70
         3.0       0.64      0.81      0.72        54
         4.0       0.64      0.25      0.36        28
         5.0       0.00      0.00      0.00         1

    accuracy                           0.65       205
   macro avg       0.43      0.43      0.41       205
weighted avg       0.63      0.65      0.62       205

[[ 0  9  1  0  0  0]
 [ 0 35  7  0  0  0]
 [ 0 19 47  4  0  0]
 [ 0  0  7 44  3  0]
 [ 0  0  0 21  7  0]
 [ 0  0  0  0  1  0]]
0.6172386011767332
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.77
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.77
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.67      0.74      0.70        42
         2.0       0.76      0.80      0.78        70
         3.0       0.74      0.78      0.76        54
         4.0       0.71      0.71      0.71        28
         5.0       0.00      0.00      0.00         1

    accuracy                           0.73       205
   macro avg       0.48      0.51      0.49       205
weighted avg       0.69      0.73      0.71       205

[[ 0  8  2  0  0  0]
 [ 0 31 11  0  0  0]
 [ 0  7 56  7  0  0]
 [ 0  0  5 42  7  0]
 [ 0  0  0  8 20  0]
 [ 0  0  0  0  1  0]]
0.7068303336596019
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.70
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.77
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.65      0.76      0.70        42
         2.0       0.77      0.76      0.76        70
         3.0       0.75      0.72      0.74        54
         4.0       0.66      0.82      0.73        28
         5.0       0.00      0.00      0.00         1

    accuracy                           0.72       205
   macro avg       0.47      0.51      0.49       205
weighted avg       0.68      0.72      0.70       205

[[ 0  8  2  0  0  0]
 [ 0 32 10  0  0  0]
 [ 0  9 53  8  0  0]
 [ 0  0  4 39 11  0]
 [ 0  0  0  5 23  0]
 [ 0  0  0  0  1  0]]
0.6980490244075049
205 205 205
Filename	True Label	Prediction
1023_0001416	3.0	3.0
1023_0001419	3.0	3.0
1023_0101684	3.0	3.0
1023_0101690	2.0	3.0
1023_0101694	3.0	3.0
1023_0101701	3.0	3.0
1023_0101749	4.0	3.0
1023_0101753	3.0	3.0
1023_0101841	3.0	3.0
1023_0101852	3.0	3.0
1023_0101853	3.0	3.0
1023_0101895	3.0	3.0
1023_0103830	3.0	3.0
1023_0103837	3.0	3.0
1023_0103839	3.0	3.0
1023_0103844	4.0	3.0
1023_0103880	3.0	3.0
1023_0106816	3.0	3.0
1023_0107244	3.0	3.0
1023_0107682	2.0	2.0
1023_0107727	3.0	3.0
1023_0108305	3.0	3.0
1023_0108510	3.0	3.0
1023_0108648	3.0	3.0
1023_0108649	3.0	3.0
1023_0108650	3.0	3.0
1023_0108753	3.0	3.0
1023_0108814	3.0	3.0
1023_0108887	2.0	3.0
1023_0108931	3.0	3.0
1023_0109033	4.0	3.0
1023_0109399	2.0	3.0
1023_0109400	3.0	3.0
1023_0109422	3.0	3.0
1023_0109606	3.0	3.0
1023_0109649	3.0	3.0
1023_0109671	3.0	3.0
1023_0109890	4.0	3.0
1023_0109915	2.0	2.0
1023_0109951	3.0	3.0
1023_0111896	3.0	3.0
1031_0001703	4.0	4.0
1031_0001998	4.0	4.0
1031_0002005	4.0	4.0
1031_0002040	4.0	4.0
1031_0002043	4.0	4.0
1031_0002198	4.0	4.0
1031_0003073	4.0	4.0
1031_0003074	4.0	4.0
1031_0003088	4.0	4.0
1031_0003090	4.0	4.0
1031_0003099	3.0	4.0
1031_0003129	4.0	4.0
1031_0003130	5.0	4.0
1031_0003132	4.0	4.0
1031_0003133	4.0	4.0
1031_0003141	3.0	4.0
1031_0003149	4.0	4.0
1031_0003163	3.0	4.0
1031_0003166	3.0	3.0
1031_0003170	3.0	4.0
1031_0003172	3.0	4.0
1031_0003180	4.0	4.0
1031_0003187	4.0	4.0
1031_0003189	4.0	4.0
1031_0003190	4.0	4.0
1031_0003203	2.0	3.0
1031_0003205	4.0	4.0
1031_0003206	3.0	4.0
1031_0003211	3.0	4.0
1031_0003243	3.0	4.0
1031_0003272	3.0	3.0
1031_0003274	3.0	4.0
1031_0003315	3.0	3.0
1031_0003330	4.0	4.0
1031_0003339	4.0	4.0
1031_0003352	3.0	3.0
1031_0003353	3.0	3.0
1031_0003355	4.0	3.0
1031_0003368	3.0	4.0
1031_0003389	3.0	4.0
1031_0003391	3.0	3.0
1031_0003392	4.0	4.0
1031_0003410	4.0	4.0
1061_0012029	3.0	2.0
1061_0120276	2.0	2.0
1061_0120282	0.0	2.0
1061_0120285	2.0	2.0
1061_0120295	0.0	2.0
1061_0120301	2.0	2.0
1061_0120302	1.0	2.0
1061_0120313	2.0	2.0
1061_0120317	3.0	2.0
1061_0120324	2.0	2.0
1061_0120332	2.0	2.0
1061_0120338	2.0	2.0
1061_0120343	2.0	2.0
1061_0120345	2.0	2.0
1061_0120348	1.0	1.0
1061_0120358	1.0	2.0
1061_0120367	3.0	2.0
1061_0120371	3.0	3.0
1061_0120373	2.0	2.0
1061_0120375	2.0	2.0
1061_0120394	2.0	2.0
1061_0120409	2.0	2.0
1061_0120424	2.0	3.0
1061_0120426	2.0	3.0
1061_0120428	2.0	2.0
1061_0120448	3.0	2.0
1061_0120455	2.0	2.0
1061_0120457	2.0	2.0
1061_0120479	2.0	2.0
1061_0120492	2.0	3.0
1061_0120494	2.0	2.0
1061_0120495	2.0	2.0
1061_0120499	2.0	2.0
1061_0120858	2.0	2.0
1061_0120878	1.0	2.0
1061_0120890	1.0	2.0
1061_0120894	2.0	2.0
1061_1029113	2.0	2.0
1061_1029115	2.0	2.0
1061_1029116	1.0	2.0
1061_1029119	2.0	2.0
1061_1029120	2.0	2.0
1071_0020001	1.0	1.0
1071_0024682	2.0	2.0
1071_0024686	2.0	2.0
1071_0024688	2.0	2.0
1071_0024692	2.0	2.0
1071_0024705	2.0	2.0
1071_0024711	2.0	2.0
1071_0024712	1.0	2.0
1071_0024713	2.0	1.0
1071_0024715	2.0	2.0
1071_0024716	1.0	1.0
1071_0024765	0.0	1.0
1071_0024767	2.0	1.0
1071_0024773	1.0	1.0
1071_0024774	0.0	1.0
1071_0024775	0.0	1.0
1071_0024778	1.0	1.0
1071_0024801	1.0	1.0
1071_0024811	1.0	1.0
1071_0024817	1.0	1.0
1071_0024827	1.0	1.0
1071_0024845	0.0	1.0
1071_0024859	2.0	2.0
1071_0242041	1.0	1.0
1071_0243581	1.0	1.0
1071_0243582	1.0	1.0
1071_0248301	2.0	1.0
1071_0248303	1.0	1.0
1071_0248304	1.0	1.0
1071_0248309	2.0	2.0
1071_0248318	0.0	1.0
1071_0248327	1.0	1.0
1071_0248330	2.0	1.0
1071_0248332	2.0	2.0
1071_0248333	2.0	1.0
1071_0248336	1.0	1.0
1071_0248341	1.0	1.0
1071_0248343	1.0	1.0
1071_0248348	1.0	1.0
1091_0000006	1.0	1.0
1091_0000009	0.0	1.0
1091_0000011	2.0	2.0
1091_0000012	1.0	1.0
1091_0000014	0.0	1.0
1091_0000017	2.0	2.0
1091_0000019	1.0	1.0
1091_0000025	1.0	1.0
1091_0000027	0.0	1.0
1091_0000028	1.0	1.0
1091_0000029	2.0	1.0
1091_0000032	1.0	1.0
1091_0000039	1.0	1.0
1091_0000041	1.0	1.0
1091_0000055	1.0	2.0
1091_0000057	2.0	1.0
1091_0000071	1.0	2.0
1091_0000095	2.0	2.0
1091_0000113	1.0	2.0
1091_0000125	2.0	2.0
1091_0000127	2.0	2.0
1091_0000145	1.0	1.0
1091_0000146	1.0	1.0
1091_0000151	1.0	1.0
1091_0000152	1.0	1.0
1091_0000154	2.0	3.0
1091_0000193	2.0	1.0
1091_0000195	1.0	1.0
1091_0000200	2.0	2.0
1091_0000201	2.0	2.0
1091_0000203	2.0	2.0
1091_0000204	2.0	2.0
1091_0000205	2.0	2.0
1091_0000224	2.0	1.0
1091_0000236	2.0	2.0
1091_0000246	2.0	2.0
1091_0000251	2.0	2.0
1091_0000260	2.0	2.0
1091_0000264	2.0	2.0
1091_0000274	1.0	2.0
LANGUAGE: DE, 2th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.34
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.96
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.44      0.17      0.24        42
         2.0       0.55      0.74      0.63        70
         3.0       0.53      0.93      0.68        54
         4.0       0.00      0.00      0.00        28
         5.0       0.00      0.00      0.00         1

    accuracy                           0.53       205
   macro avg       0.25      0.31      0.26       205
weighted avg       0.42      0.53      0.44       205

[[ 0  6  4  0  0  0]
 [ 0  7 35  0  0  0]
 [ 0  3 52 15  0  0]
 [ 0  0  4 50  0  0]
 [ 0  0  0 28  0  0]
 [ 0  0  0  1  0  0]]
0.4426616080106408
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.93
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.83
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.57      0.71      0.63        42
         2.0       0.71      0.67      0.69        70
         3.0       0.69      0.67      0.68        54
         4.0       0.65      0.79      0.71        28
         5.0       0.00      0.00      0.00         1

    accuracy                           0.66       205
   macro avg       0.44      0.47      0.45       205
weighted avg       0.63      0.66      0.64       205

[[ 0 10  0  0  0  0]
 [ 0 30 12  0  0  0]
 [ 0 13 47 10  0  0]
 [ 0  0  7 36 11  0]
 [ 0  0  0  6 22  0]
 [ 0  0  0  0  1  0]]
0.641262837832217
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.79
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.76
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.60      0.60      0.60        42
         2.0       0.67      0.79      0.72        70
         3.0       0.74      0.57      0.65        54
         4.0       0.62      0.86      0.72        28
         5.0       0.00      0.00      0.00         1

    accuracy                           0.66       205
   macro avg       0.44      0.47      0.45       205
weighted avg       0.63      0.66      0.64       205

[[ 0  9  1  0  0  0]
 [ 0 25 17  0  0  0]
 [ 0  8 55  7  0  0]
 [ 0  0  9 31 14  0]
 [ 0  0  0  4 24  0]
 [ 0  0  0  0  1  0]]
0.6370370547774605
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.73
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.75
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.61      0.64      0.63        42
         2.0       0.73      0.79      0.76        70
         3.0       0.77      0.69      0.73        54
         4.0       0.63      0.86      0.73        28
         5.0       0.00      0.00      0.00         1

    accuracy                           0.70       205
   macro avg       0.46      0.50      0.47       205
weighted avg       0.67      0.70      0.68       205

[[ 0  9  1  0  0  0]
 [ 0 27 15  0  0  0]
 [ 0  8 55  7  0  0]
 [ 0  0  4 37 13  0]
 [ 0  0  0  4 24  0]
 [ 0  0  0  0  1  0]]
0.6781251134194612
205 205 205
Filename	True Label	Prediction
1023_0101675	3.0	3.0
1023_0101695	3.0	3.0
1023_0101700	3.0	3.0
1023_0101751	3.0	3.0
1023_0101752	3.0	3.0
1023_0101844	3.0	3.0
1023_0101846	4.0	3.0
1023_0101848	2.0	2.0
1023_0101854	2.0	2.0
1023_0101855	3.0	3.0
1023_0101893	3.0	3.0
1023_0101894	3.0	3.0
1023_0101900	4.0	3.0
1023_0101906	3.0	3.0
1023_0101907	4.0	3.0
1023_0102118	3.0	3.0
1023_0103834	3.0	3.0
1023_0103840	3.0	3.0
1023_0103843	3.0	3.0
1023_0107672	2.0	3.0
1023_0107729	3.0	3.0
1023_0107780	3.0	3.0
1023_0107784	2.0	2.0
1023_0108810	3.0	3.0
1023_0108888	3.0	3.0
1023_0108935	2.0	3.0
1023_0108992	3.0	3.0
1023_0109029	2.0	2.0
1023_0109096	3.0	3.0
1023_0109250	3.0	3.0
1023_0109267	3.0	3.0
1023_0109391	2.0	3.0
1023_0109396	3.0	3.0
1023_0109505	3.0	3.0
1023_0109519	2.0	3.0
1023_0109522	3.0	3.0
1023_0109614	2.0	3.0
1023_0109674	3.0	3.0
1023_0109717	3.0	3.0
1023_0109721	3.0	3.0
1023_0109914	2.0	3.0
1023_0109946	3.0	3.0
1023_0109947	3.0	3.0
1023_0109954	3.0	3.0
1031_0001950	4.0	4.0
1031_0002004	4.0	4.0
1031_0002006	4.0	4.0
1031_0002079	5.0	4.0
1031_0002083	3.0	4.0
1031_0002087	4.0	4.0
1031_0002092	4.0	4.0
1031_0002185	4.0	4.0
1031_0002187	4.0	4.0
1031_0003013	4.0	4.0
1031_0003071	4.0	4.0
1031_0003072	3.0	4.0
1031_0003076	4.0	4.0
1031_0003078	4.0	4.0
1031_0003092	3.0	4.0
1031_0003095	3.0	4.0
1031_0003106	4.0	4.0
1031_0003131	4.0	4.0
1031_0003140	4.0	4.0
1031_0003144	3.0	4.0
1031_0003145	4.0	3.0
1031_0003156	4.0	4.0
1031_0003169	3.0	4.0
1031_0003186	4.0	4.0
1031_0003217	4.0	4.0
1031_0003224	3.0	4.0
1031_0003232	3.0	4.0
1031_0003234	3.0	4.0
1031_0003237	3.0	4.0
1031_0003239	4.0	4.0
1031_0003314	4.0	4.0
1031_0003354	3.0	4.0
1031_0003356	3.0	4.0
1031_0003383	4.0	4.0
1031_0003384	3.0	3.0
1031_0003387	4.0	4.0
1031_0003388	4.0	4.0
1031_0003393	4.0	4.0
1031_0003407	3.0	4.0
1031_0003415	4.0	4.0
1061_0120272	2.0	2.0
1061_0120275	2.0	2.0
1061_0120277	1.0	2.0
1061_0120286	1.0	1.0
1061_0120287	1.0	2.0
1061_0120288	2.0	2.0
1061_0120291	1.0	1.0
1061_0120306	2.0	2.0
1061_0120318	2.0	2.0
1061_0120319	3.0	2.0
1061_0120321	2.0	2.0
1061_0120325	2.0	2.0
1061_0120335	2.0	3.0
1061_0120337	2.0	2.0
1061_0120349	1.0	1.0
1061_0120351	2.0	2.0
1061_0120357	3.0	3.0
1061_0120383	3.0	3.0
1061_0120386	1.0	2.0
1061_0120387	2.0	2.0
1061_0120391	1.0	2.0
1061_0120404	2.0	2.0
1061_0120405	3.0	2.0
1061_0120406	2.0	2.0
1061_0120413	1.0	2.0
1061_0120414	2.0	2.0
1061_0120425	2.0	2.0
1061_0120438	2.0	2.0
1061_0120439	2.0	2.0
1061_0120440	1.0	2.0
1061_0120460	2.0	2.0
1061_0120487	2.0	2.0
1061_0120493	2.0	2.0
1061_0120498	2.0	2.0
1061_0120853	2.0	2.0
1061_0120856	2.0	2.0
1061_0120874	2.0	2.0
1061_0120880	3.0	3.0
1061_0120882	3.0	3.0
1061_0120884	2.0	2.0
1061_0120886	2.0	2.0
1061_1029118	2.0	2.0
1061_1202918	2.0	2.0
1061_1202919	2.0	2.0
1071_0024687	1.0	1.0
1071_0024701	2.0	2.0
1071_0024756	2.0	1.0
1071_0024757	2.0	2.0
1071_0024803	1.0	1.0
1071_0024804	1.0	1.0
1071_0024807	1.0	1.0
1071_0024810	1.0	2.0
1071_0024818	2.0	1.0
1071_0024824	1.0	1.0
1071_0024825	1.0	1.0
1071_0024826	2.0	2.0
1071_0024834	2.0	2.0
1071_0024837	0.0	1.0
1071_0024843	1.0	1.0
1071_0024844	1.0	1.0
1071_0024846	1.0	1.0
1071_0024853	1.0	1.0
1071_0024855	1.0	1.0
1071_0024867	2.0	2.0
1071_0024871	1.0	1.0
1071_0024873	0.0	1.0
1071_0242072	0.0	1.0
1071_0242092	0.0	1.0
1071_0243592	1.0	1.0
1071_0248313	2.0	2.0
1071_0248315	0.0	1.0
1071_0248316	1.0	1.0
1071_0248319	1.0	1.0
1071_0248320	0.0	1.0
1071_0248321	1.0	1.0
1071_0248322	1.0	1.0
1071_0248340	0.0	1.0
1071_0248347	1.0	1.0
1071_0248350	2.0	1.0
1091_0000001	1.0	2.0
1091_0000002	2.0	2.0
1091_0000004	1.0	1.0
1091_0000005	2.0	2.0
1091_0000007	2.0	2.0
1091_0000013	1.0	1.0
1091_0000015	1.0	2.0
1091_0000016	0.0	1.0
1091_0000030	0.0	1.0
1091_0000035	2.0	1.0
1091_0000036	1.0	2.0
1091_0000042	1.0	1.0
1091_0000046	1.0	2.0
1091_0000050	1.0	1.0
1091_0000051	1.0	1.0
1091_0000054	0.0	2.0
1091_0000072	1.0	2.0
1091_0000086	1.0	2.0
1091_0000116	3.0	2.0
1091_0000148	1.0	1.0
1091_0000157	3.0	2.0
1091_0000158	2.0	2.0
1091_0000160	3.0	3.0
1091_0000164	2.0	1.0
1091_0000165	2.0	1.0
1091_0000173	2.0	2.0
1091_0000202	2.0	2.0
1091_0000208	2.0	2.0
1091_0000216	1.0	2.0
1091_0000222	2.0	2.0
1091_0000225	2.0	1.0
1091_0000228	2.0	2.0
1091_0000232	2.0	2.0
1091_0000233	2.0	2.0
1091_0000239	2.0	2.0
1091_0000244	2.0	2.0
1091_0000248	2.0	2.0
1091_0000253	2.0	1.0
1091_0000259	2.0	2.0
1091_0000261	2.0	2.0
1091_0000269	1.0	2.0
1091_0000270	2.0	2.0
LANGUAGE: DE, 3th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.28
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.87
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.60      0.70      0.65        43
         2.0       0.72      0.74      0.73        70
         3.0       0.73      0.65      0.69        54
         4.0       0.63      0.81      0.71        27
         5.0       0.00      0.00      0.00         1

    accuracy                           0.68       205
   macro avg       0.45      0.48      0.46       205
weighted avg       0.65      0.68      0.66       205

[[ 0 10  0  0  0  0]
 [ 0 30 13  0  0  0]
 [ 0 10 52  8  0  0]
 [ 0  0  7 35 12  0]
 [ 0  0  0  5 22  0]
 [ 0  0  0  0  1  0]]
0.6596568535105667
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.87
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.83
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.53      0.21      0.30        43
         2.0       0.58      0.89      0.70        70
         3.0       0.76      0.65      0.70        54
         4.0       0.63      0.81      0.71        27
         5.0       0.00      0.00      0.00         1

    accuracy                           0.62       205
   macro avg       0.42      0.43      0.40       205
weighted avg       0.59      0.62      0.58       205

[[ 0  6  4  0  0  0]
 [ 0  9 34  0  0  0]
 [ 0  2 62  6  0  0]
 [ 0  0  7 35 12  0]
 [ 0  0  0  5 22  0]
 [ 0  0  0  0  1  0]]
0.5800040894886807
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.76
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.76
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.57      0.60      0.58        43
         2.0       0.70      0.79      0.74        70
         3.0       0.76      0.65      0.70        54
         4.0       0.62      0.78      0.69        27
         5.0       0.00      0.00      0.00         1

    accuracy                           0.67       205
   macro avg       0.44      0.47      0.45       205
weighted avg       0.64      0.67      0.65       205

[[ 0 10  0  0  0  0]
 [ 0 26 17  0  0  0]
 [ 0 10 55  5  0  0]
 [ 0  0  7 35 12  0]
 [ 0  0  0  6 21  0]
 [ 0  0  0  0  1  0]]
0.6497151794587196
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.66
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.78
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.60      0.58      0.59        43
         2.0       0.70      0.80      0.75        70
         3.0       0.77      0.67      0.71        54
         4.0       0.64      0.85      0.73        27
         5.0       0.00      0.00      0.00         1

    accuracy                           0.68       205
   macro avg       0.45      0.48      0.46       205
weighted avg       0.65      0.68      0.66       205

[[ 0 10  0  0  0  0]
 [ 0 25 18  0  0  0]
 [ 0  7 56  7  0  0]
 [ 0  0  6 36 12  0]
 [ 0  0  0  4 23  0]
 [ 0  0  0  0  1  0]]
0.6622932660144473
205 205 205
Filename	True Label	Prediction
1023_0001422	3.0	3.0
1023_0001423	2.0	2.0
1023_0001575	3.0	3.0
1023_0101688	3.0	3.0
1023_0101691	3.0	3.0
1023_0101843	3.0	3.0
1023_0101901	4.0	3.0
1023_0103829	2.0	3.0
1023_0103832	3.0	3.0
1023_0103841	3.0	3.0
1023_0104203	3.0	3.0
1023_0104207	3.0	3.0
1023_0107042	3.0	3.0
1023_0107075	3.0	3.0
1023_0107726	3.0	3.0
1023_0107783	3.0	3.0
1023_0108306	3.0	3.0
1023_0108307	3.0	3.0
1023_0108518	3.0	3.0
1023_0108520	3.0	3.0
1023_0108641	4.0	3.0
1023_0108766	3.0	3.0
1023_0108811	3.0	3.0
1023_0108815	3.0	3.0
1023_0108885	3.0	3.0
1023_0108933	3.0	3.0
1023_0108934	3.0	3.0
1023_0108955	3.0	3.0
1023_0109027	3.0	3.0
1023_0109038	3.0	3.0
1023_0109151	4.0	4.0
1023_0109192	3.0	3.0
1023_0109248	2.0	3.0
1023_0109392	3.0	3.0
1023_0109495	3.0	3.0
1023_0109500	2.0	3.0
1023_0109518	2.0	3.0
1023_0109527	3.0	3.0
1023_0109591	3.0	3.0
1023_0109878	3.0	3.0
1023_0109945	4.0	3.0
1031_0001997	4.0	4.0
1031_0002003	3.0	4.0
1031_0002011	4.0	4.0
1031_0002042	4.0	4.0
1031_0002088	4.0	4.0
1031_0002089	4.0	4.0
1031_0002196	4.0	4.0
1031_0002200	3.0	4.0
1031_0003042	3.0	4.0
1031_0003043	5.0	4.0
1031_0003091	3.0	4.0
1031_0003121	4.0	4.0
1031_0003127	4.0	4.0
1031_0003128	4.0	4.0
1031_0003150	4.0	4.0
1031_0003154	4.0	4.0
1031_0003157	4.0	4.0
1031_0003162	4.0	4.0
1031_0003167	3.0	4.0
1031_0003181	4.0	4.0
1031_0003218	4.0	4.0
1031_0003221	3.0	4.0
1031_0003225	3.0	4.0
1031_0003226	4.0	4.0
1031_0003235	4.0	4.0
1031_0003238	4.0	4.0
1031_0003244	4.0	4.0
1031_0003260	4.0	3.0
1031_0003262	3.0	4.0
1031_0003273	3.0	4.0
1031_0003309	3.0	4.0
1031_0003310	4.0	4.0
1031_0003358	4.0	4.0
1031_0003359	3.0	4.0
1031_0003367	4.0	4.0
1031_0003408	3.0	4.0
1061_0120271	2.0	2.0
1061_0120279	2.0	2.0
1061_0120284	0.0	1.0
1061_0120289	2.0	2.0
1061_0120299	2.0	2.0
1061_0120304	2.0	2.0
1061_0120308	3.0	2.0
1061_0120309	2.0	1.0
1061_0120311	3.0	2.0
1061_0120316	2.0	2.0
1061_0120320	3.0	3.0
1061_0120327	2.0	2.0
1061_0120330	2.0	2.0
1061_0120333	3.0	3.0
1061_0120334	2.0	3.0
1061_0120336	1.0	2.0
1061_0120350	2.0	2.0
1061_0120353	1.0	1.0
1061_0120355	1.0	1.0
1061_0120359	2.0	2.0
1061_0120368	2.0	2.0
1061_0120369	2.0	2.0
1061_0120370	2.0	3.0
1061_0120390	2.0	2.0
1061_0120408	3.0	2.0
1061_0120410	2.0	2.0
1061_0120411	3.0	3.0
1061_0120427	2.0	2.0
1061_0120432	2.0	2.0
1061_0120443	0.0	1.0
1061_0120456	2.0	2.0
1061_0120480	2.0	2.0
1061_0120483	2.0	2.0
1061_0120486	2.0	2.0
1061_0120490	2.0	2.0
1061_0120496	2.0	2.0
1061_0120500	2.0	2.0
1061_0120855	2.0	2.0
1061_0120859	2.0	3.0
1061_0120876	2.0	2.0
1061_0120883	2.0	2.0
1061_0120885	2.0	2.0
1061_0120889	1.0	1.0
1061_1029112	3.0	3.0
1061_1202910	2.0	2.0
1061_1202915	1.0	2.0
1061_1202917	2.0	2.0
1071_0024691	2.0	2.0
1071_0024694	2.0	2.0
1071_0024703	1.0	1.0
1071_0024710	1.0	1.0
1071_0024768	1.0	1.0
1071_0024769	0.0	1.0
1071_0024770	1.0	1.0
1071_0024776	0.0	1.0
1071_0024777	1.0	2.0
1071_0024779	2.0	2.0
1071_0024782	0.0	1.0
1071_0024783	0.0	1.0
1071_0024798	0.0	1.0
1071_0024800	0.0	1.0
1071_0024814	1.0	1.0
1071_0024819	2.0	2.0
1071_0024821	1.0	1.0
1071_0024822	1.0	1.0
1071_0024833	2.0	1.0
1071_0024838	0.0	1.0
1071_0024848	1.0	1.0
1071_0024850	1.0	1.0
1071_0024856	1.0	1.0
1071_0024862	2.0	1.0
1071_0024875	1.0	1.0
1071_0024876	1.0	1.0
1071_0024877	1.0	1.0
1071_0241832	1.0	1.0
1071_0241833	1.0	1.0
1071_0242023	1.0	1.0
1071_0242091	1.0	1.0
1071_0243621	2.0	2.0
1071_0243623	1.0	1.0
1071_0248307	2.0	1.0
1071_0248308	1.0	1.0
1071_0248310	1.0	1.0
1071_0248323	1.0	2.0
1071_0248325	0.0	1.0
1071_0248344	2.0	1.0
1091_0000010	2.0	2.0
1091_0000021	2.0	2.0
1091_0000024	2.0	2.0
1091_0000033	1.0	2.0
1091_0000038	2.0	1.0
1091_0000043	1.0	2.0
1091_0000048	1.0	1.0
1091_0000056	1.0	2.0
1091_0000059	1.0	2.0
1091_0000064	1.0	1.0
1091_0000069	2.0	1.0
1091_0000074	2.0	2.0
1091_0000075	1.0	2.0
1091_0000114	2.0	2.0
1091_0000156	3.0	2.0
1091_0000159	2.0	2.0
1091_0000166	1.0	2.0
1091_0000169	3.0	2.0
1091_0000171	2.0	2.0
1091_0000194	1.0	2.0
1091_0000197	1.0	2.0
1091_0000198	2.0	2.0
1091_0000199	2.0	2.0
1091_0000209	2.0	2.0
1091_0000211	1.0	2.0
1091_0000213	2.0	2.0
1091_0000214	2.0	2.0
1091_0000219	1.0	2.0
1091_0000220	1.0	2.0
1091_0000226	1.0	2.0
1091_0000227	1.0	2.0
1091_0000230	2.0	2.0
1091_0000234	3.0	2.0
1091_0000252	2.0	2.0
1091_0000254	2.0	2.0
1091_0000257	2.0	2.0
1091_0000262	2.0	2.0
1091_0000265	2.0	2.0
1091_0000266	2.0	2.0
1091_0000267	2.0	2.0
1091_0000268	2.0	2.0
1091_0000273	1.0	2.0
LANGUAGE: DE, 4th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.29
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.90
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.49      0.88      0.63        43
         2.0       0.85      0.47      0.61        70
         3.0       0.60      0.96      0.74        54
         4.0       1.00      0.04      0.07        27
         5.0       0.00      0.00      0.00         1

    accuracy                           0.60       205
   macro avg       0.49      0.39      0.34       205
weighted avg       0.68      0.60      0.54       205

[[ 0 10  0  0  0  0]
 [ 0 38  5  0  0  0]
 [ 0 29 33  8  0  0]
 [ 0  1  1 52  0  0]
 [ 0  0  0 26  1  0]
 [ 0  0  0  1  0  0]]
0.5422046059821528
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.88
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.47      0.44      0.46        43
         2.0       0.67      0.79      0.72        70
         3.0       0.82      0.69      0.75        54
         4.0       0.61      0.85      0.71        27
         5.0       0.00      0.00      0.00         1

    accuracy                           0.65       205
   macro avg       0.43      0.46      0.44       205
weighted avg       0.62      0.65      0.63       205

[[ 0 10  0  0  0  0]
 [ 0 19 24  0  0  0]
 [ 0 11 55  4  0  0]
 [ 0  0  3 37 14  0]
 [ 0  0  0  4 23  0]
 [ 0  0  0  0  1  0]]
0.6332486360786468
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.76
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.73
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.57      0.65      0.61        43
         2.0       0.73      0.80      0.76        70
         3.0       0.80      0.69      0.74        54
         4.0       0.64      0.78      0.70        27
         5.0       0.00      0.00      0.00         1

    accuracy                           0.69       205
   macro avg       0.46      0.49      0.47       205
weighted avg       0.66      0.69      0.67       205

[[ 0 10  0  0  0  0]
 [ 0 28 15  0  0  0]
 [ 0 11 56  3  0  0]
 [ 0  0  6 37 11  0]
 [ 0  0  0  6 21  0]
 [ 0  0  0  0  1  0]]
0.6749621774478615
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.66
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.74
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.56      0.67      0.61        43
         2.0       0.77      0.76      0.76        70
         3.0       0.82      0.74      0.78        54
         4.0       0.66      0.85      0.74        27
         5.0       0.00      0.00      0.00         1

    accuracy                           0.71       205
   macro avg       0.47      0.50      0.48       205
weighted avg       0.68      0.71      0.69       205

[[ 0 10  0  0  0  0]
 [ 0 29 13  1  0  0]
 [ 0 13 53  4  0  0]
 [ 0  0  3 40 11  0]
 [ 0  0  0  4 23  0]
 [ 0  0  0  0  1  0]]
0.6907704008795503
205 205 205
Filename	True Label	Prediction
1023_0001418	3.0	3.0
1023_0101689	2.0	2.0
1023_0101847	3.0	3.0
1023_0101849	3.0	3.0
1023_0101851	3.0	3.0
1023_0101898	4.0	3.0
1023_0101899	3.0	3.0
1023_0101904	2.0	3.0
1023_0101909	4.0	3.0
1023_0102117	3.0	3.0
1023_0103821	3.0	3.0
1023_0103823	4.0	3.0
1023_0103826	3.0	3.0
1023_0103827	3.0	3.0
1023_0103831	3.0	3.0
1023_0103955	4.0	3.0
1023_0104206	3.0	3.0
1023_0107725	3.0	3.0
1023_0107773	3.0	3.0
1023_0107787	2.0	3.0
1023_0108422	3.0	3.0
1023_0108426	3.0	3.0
1023_0108752	3.0	3.0
1023_0108813	3.0	3.0
1023_0108886	3.0	3.0
1023_0108889	3.0	3.0
1023_0109022	3.0	3.0
1023_0109026	2.0	3.0
1023_0109030	3.0	3.0
1023_0109039	3.0	3.0
1023_0109247	3.0	3.0
1023_0109401	3.0	3.0
1023_0109402	3.0	3.0
1023_0109515	3.0	3.0
1023_0109520	3.0	3.0
1023_0109524	3.0	3.0
1023_0109588	3.0	3.0
1023_0109590	3.0	3.0
1023_0109716	3.0	3.0
1023_0109917	3.0	3.0
1031_0002002	3.0	4.0
1031_0002010	3.0	4.0
1031_0002032	3.0	4.0
1031_0002036	4.0	4.0
1031_0002061	3.0	4.0
1031_0002085	4.0	4.0
1031_0002091	4.0	4.0
1031_0002184	4.0	4.0
1031_0002195	4.0	4.0
1031_0003012	4.0	4.0
1031_0003023	4.0	4.0
1031_0003029	4.0	4.0
1031_0003035	4.0	4.0
1031_0003052	4.0	4.0
1031_0003053	4.0	4.0
1031_0003054	4.0	4.0
1031_0003065	3.0	4.0
1031_0003098	5.0	4.0
1031_0003136	4.0	4.0
1031_0003164	4.0	4.0
1031_0003165	3.0	3.0
1031_0003173	4.0	4.0
1031_0003183	4.0	4.0
1031_0003185	3.0	3.0
1031_0003191	4.0	4.0
1031_0003207	4.0	4.0
1031_0003219	3.0	4.0
1031_0003220	3.0	3.0
1031_0003230	4.0	4.0
1031_0003233	3.0	3.0
1031_0003240	3.0	4.0
1031_0003242	3.0	4.0
1031_0003249	3.0	4.0
1031_0003313	4.0	4.0
1031_0003366	3.0	4.0
1031_0003369	4.0	4.0
1031_0003390	4.0	4.0
1031_0003414	3.0	4.0
1031_0003419	4.0	4.0
1061_0120274	1.0	2.0
1061_0120278	2.0	2.0
1061_0120280	1.0	1.0
1061_0120281	2.0	2.0
1061_0120283	1.0	1.0
1061_0120296	2.0	2.0
1061_0120297	2.0	2.0
1061_0120298	2.0	2.0
1061_0120303	1.0	2.0
1061_0120312	1.0	1.0
1061_0120315	2.0	2.0
1061_0120326	2.0	2.0
1061_0120328	2.0	2.0
1061_0120331	1.0	1.0
1061_0120341	2.0	1.0
1061_0120346	2.0	2.0
1061_0120352	1.0	1.0
1061_0120354	2.0	2.0
1061_0120356	2.0	2.0
1061_0120374	3.0	3.0
1061_0120376	2.0	2.0
1061_0120382	2.0	2.0
1061_0120388	2.0	2.0
1061_0120403	2.0	2.0
1061_0120407	3.0	2.0
1061_0120415	2.0	2.0
1061_0120430	2.0	2.0
1061_0120431	2.0	2.0
1061_0120433	1.0	2.0
1061_0120441	2.0	2.0
1061_0120442	2.0	2.0
1061_0120449	2.0	2.0
1061_0120458	3.0	2.0
1061_0120478	2.0	2.0
1061_0120481	3.0	3.0
1061_0120482	2.0	2.0
1061_0120484	2.0	2.0
1061_0120488	2.0	2.0
1061_0120489	2.0	2.0
1061_0120857	2.0	2.0
1061_0120875	3.0	3.0
1061_0120881	3.0	3.0
1061_0120888	2.0	2.0
1061_1029114	1.0	2.0
1061_1029117	2.0	2.0
1061_1202913	2.0	2.0
1071_0024678	2.0	1.0
1071_0024690	2.0	2.0
1071_0024706	1.0	1.0
1071_0024758	2.0	2.0
1071_0024759	0.0	1.0
1071_0024762	1.0	1.0
1071_0024766	1.0	1.0
1071_0024784	1.0	1.0
1071_0024802	2.0	2.0
1071_0024809	1.0	1.0
1071_0024813	0.0	1.0
1071_0024815	1.0	1.0
1071_0024823	1.0	1.0
1071_0024835	1.0	1.0
1071_0024836	2.0	2.0
1071_0024840	1.0	1.0
1071_0024852	0.0	1.0
1071_0024857	1.0	2.0
1071_0024860	1.0	1.0
1071_0024861	0.0	1.0
1071_0024864	0.0	1.0
1071_0024865	2.0	2.0
1071_0024866	2.0	2.0
1071_0024878	2.0	2.0
1071_0024879	1.0	1.0
1071_0024881	2.0	2.0
1071_0241831	1.0	2.0
1071_0242013	1.0	1.0
1071_0242021	1.0	1.0
1071_0242042	1.0	1.0
1071_0242073	1.0	1.0
1071_0243502	1.0	1.0
1071_0248305	0.0	1.0
1071_0248311	2.0	1.0
1071_0248312	1.0	1.0
1071_0248317	0.0	1.0
1071_0248324	0.0	1.0
1071_0248328	0.0	1.0
1071_0248331	1.0	1.0
1071_0248346	1.0	1.0
1071_0248349	0.0	1.0
1091_0000018	2.0	2.0
1091_0000020	1.0	2.0
1091_0000023	2.0	1.0
1091_0000026	1.0	1.0
1091_0000034	2.0	1.0
1091_0000044	1.0	1.0
1091_0000045	2.0	2.0
1091_0000047	2.0	1.0
1091_0000049	1.0	1.0
1091_0000058	2.0	2.0
1091_0000060	2.0	2.0
1091_0000061	2.0	1.0
1091_0000062	2.0	1.0
1091_0000063	2.0	1.0
1091_0000065	1.0	1.0
1091_0000066	2.0	1.0
1091_0000068	1.0	2.0
1091_0000079	1.0	2.0
1091_0000087	2.0	2.0
1091_0000092	1.0	2.0
1091_0000101	2.0	2.0
1091_0000126	3.0	2.0
1091_0000153	1.0	3.0
1091_0000155	2.0	3.0
1091_0000161	2.0	2.0
1091_0000167	1.0	2.0
1091_0000168	2.0	2.0
1091_0000174	2.0	1.0
1091_0000191	1.0	2.0
1091_0000196	2.0	1.0
1091_0000210	2.0	2.0
1091_0000217	2.0	1.0
1091_0000218	2.0	2.0
1091_0000235	1.0	1.0
1091_0000238	2.0	2.0
1091_0000245	1.0	2.0
1091_0000250	2.0	2.0
1091_0000263	2.0	2.0
1091_0000276	2.0	2.0
Averaged weighted F1-scores 0.6793051947211488
130.68810916179336 82.83744971317162
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
LANGUAGE: DE, 0th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.46
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.12
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        16
         1.0       0.44      1.00      0.62        52
         2.0       0.67      0.19      0.30        63
         3.0       0.65      0.82      0.72        56
         4.0       0.00      0.00      0.00        16
         5.0       0.00      0.00      0.00         3

    accuracy                           0.53       206
   macro avg       0.29      0.34      0.27       206
weighted avg       0.49      0.53      0.44       206

[[ 0 16  0  0  0  0]
 [ 0 52  0  0  0  0]
 [ 0 44 12  7  0  0]
 [ 0  5  5 46  0  0]
 [ 0  0  1 15  0  0]
 [ 0  0  0  3  0  0]]
0.4428815330122569
206 206 206



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.07
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.99
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        16
         1.0       0.50      0.96      0.66        52
         2.0       0.79      0.41      0.54        63
         3.0       0.64      0.84      0.73        56
         4.0       0.00      0.00      0.00        16
         5.0       0.00      0.00      0.00         3

    accuracy                           0.60       206
   macro avg       0.32      0.37      0.32       206
weighted avg       0.54      0.60      0.53       206

[[ 0 16  0  0  0  0]
 [ 0 50  2  0  0  0]
 [ 0 30 26  7  0  0]
 [ 0  4  5 47  0  0]
 [ 0  0  0 16  0  0]
 [ 0  0  0  3  0  0]]
0.5298142129426072
206 206 206



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.98
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.95
              precision    recall  f1-score   support

         0.0       0.56      0.31      0.40        16
         1.0       0.51      0.77      0.61        52
         2.0       0.67      0.48      0.56        63
         3.0       0.66      0.66      0.66        56
         4.0       0.41      0.44      0.42        16
         5.0       0.00      0.00      0.00         3

    accuracy                           0.58       206
   macro avg       0.47      0.44      0.44       206
weighted avg       0.59      0.58      0.57       206

[[ 5 11  0  0  0  0]
 [ 4 40  8  0  0  0]
 [ 0 26 30  7  0  0]
 [ 0  2  7 37 10  0]
 [ 0  0  0  9  7  0]
 [ 0  0  0  3  0  0]]
0.567687397954944
206 206 206



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.90
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.93
              precision    recall  f1-score   support

         0.0       0.43      0.19      0.26        16
         1.0       0.49      0.67      0.57        52
         2.0       0.62      0.56      0.59        63
         3.0       0.66      0.77      0.71        56
         4.0       0.57      0.25      0.35        16
         5.0       0.00      0.00      0.00         3

    accuracy                           0.58       206
   macro avg       0.46      0.41      0.41       206
weighted avg       0.57      0.58      0.56       206

[[ 3 13  0  0  0  0]
 [ 4 35 13  0  0  0]
 [ 0 21 35  7  0  0]
 [ 0  2  8 43  3  0]
 [ 0  0  0 12  4  0]
 [ 0  0  0  3  0  0]]
0.5640441883092995
206 206 206
Filename	True Label	Prediction
1023_0001422	3.0	2.0
1023_0001575	3.0	3.0
1023_0101683	3.0	3.0
1023_0101695	2.0	2.0
1023_0101700	2.0	3.0
1023_0101843	3.0	3.0
1023_0101851	3.0	3.0
1023_0101852	5.0	3.0
1023_0101901	3.0	3.0
1023_0103844	5.0	3.0
1023_0103955	4.0	3.0
1023_0104206	3.0	3.0
1023_0107725	3.0	3.0
1023_0107727	5.0	3.0
1023_0107784	2.0	2.0
1023_0108650	3.0	3.0
1023_0108751	3.0	3.0
1023_0108766	2.0	3.0
1023_0108813	3.0	3.0
1023_0108814	3.0	3.0
1023_0108958	2.0	3.0
1023_0109151	3.0	3.0
1023_0109392	3.0	3.0
1023_0109401	3.0	3.0
1023_0109402	3.0	3.0
1023_0109505	3.0	3.0
1023_0109515	4.0	3.0
1023_0109516	3.0	3.0
1023_0109606	3.0	3.0
1023_0109614	2.0	2.0
1023_0109649	3.0	3.0
1023_0109671	2.0	3.0
1023_0109716	3.0	3.0
1023_0109717	3.0	3.0
1031_0001950	4.0	3.0
1031_0001997	3.0	3.0
1031_0002006	4.0	3.0
1031_0002086	3.0	3.0
1031_0002185	4.0	3.0
1031_0002196	4.0	3.0
1031_0002200	3.0	3.0
1031_0003052	4.0	3.0
1031_0003071	3.0	3.0
1031_0003076	4.0	3.0
1031_0003077	3.0	3.0
1031_0003121	3.0	3.0
1031_0003130	4.0	4.0
1031_0003131	3.0	4.0
1031_0003133	4.0	3.0
1031_0003157	4.0	4.0
1031_0003160	3.0	3.0
1031_0003164	4.0	3.0
1031_0003166	3.0	3.0
1031_0003167	3.0	3.0
1031_0003173	3.0	3.0
1031_0003181	4.0	4.0
1031_0003182	4.0	4.0
1031_0003184	4.0	3.0
1031_0003186	4.0	3.0
1031_0003221	2.0	3.0
1031_0003238	3.0	3.0
1031_0003244	3.0	3.0
1031_0003245	3.0	3.0
1031_0003262	3.0	3.0
1031_0003272	3.0	3.0
1031_0003274	3.0	3.0
1031_0003313	3.0	4.0
1031_0003337	3.0	3.0
1031_0003338	3.0	4.0
1031_0003354	3.0	3.0
1031_0003356	3.0	3.0
1031_0003388	3.0	3.0
1031_0003419	3.0	3.0
1061_0120276	2.0	2.0
1061_0120279	1.0	2.0
1061_0120282	0.0	1.0
1061_0120298	2.0	2.0
1061_0120300	3.0	2.0
1061_0120303	1.0	1.0
1061_0120313	2.0	1.0
1061_0120317	3.0	2.0
1061_0120326	2.0	2.0
1061_0120328	1.0	2.0
1061_0120330	2.0	2.0
1061_0120331	1.0	1.0
1061_0120333	2.0	3.0
1061_0120358	1.0	2.0
1061_0120366	3.0	2.0
1061_0120367	3.0	2.0
1061_0120370	2.0	2.0
1061_0120383	2.0	3.0
1061_0120390	2.0	2.0
1061_0120409	3.0	2.0
1061_0120415	2.0	2.0
1061_0120431	2.0	2.0
1061_0120443	0.0	1.0
1061_0120448	3.0	2.0
1061_0120459	2.0	2.0
1061_0120460	2.0	2.0
1061_0120480	2.0	2.0
1061_0120485	3.0	2.0
1061_0120487	2.0	2.0
1061_0120491	2.0	2.0
1061_0120494	2.0	2.0
1061_0120855	2.0	2.0
1061_0120883	2.0	2.0
1061_0120887	1.0	2.0
1061_1029111	2.0	2.0
1061_1029112	3.0	3.0
1061_1029113	2.0	2.0
1061_1029119	1.0	2.0
1061_1202912	2.0	2.0
1071_0024680	2.0	2.0
1071_0024683	0.0	1.0
1071_0024693	1.0	1.0
1071_0024701	2.0	2.0
1071_0024708	1.0	1.0
1071_0024711	1.0	1.0
1071_0024712	1.0	1.0
1071_0024713	1.0	1.0
1071_0024756	1.0	1.0
1071_0024759	0.0	1.0
1071_0024761	2.0	1.0
1071_0024775	0.0	1.0
1071_0024800	0.0	1.0
1071_0024814	1.0	1.0
1071_0024816	1.0	1.0
1071_0024817	1.0	1.0
1071_0024833	1.0	1.0
1071_0024834	2.0	1.0
1071_0024835	1.0	0.0
1071_0024836	2.0	1.0
1071_0024837	0.0	0.0
1071_0024846	0.0	1.0
1071_0024851	2.0	1.0
1071_0024877	1.0	1.0
1071_0024878	2.0	1.0
1071_0241833	1.0	1.0
1071_0242011	1.0	1.0
1071_0242043	0.0	1.0
1071_0243502	1.0	0.0
1071_0243622	1.0	0.0
1071_0248303	1.0	1.0
1071_0248308	1.0	1.0
1071_0248313	1.0	1.0
1071_0248314	1.0	1.0
1071_0248316	1.0	0.0
1071_0248318	0.0	0.0
1071_0248320	0.0	0.0
1071_0248321	1.0	1.0
1071_0248323	0.0	1.0
1071_0248324	0.0	1.0
1071_0248326	1.0	1.0
1071_0248331	1.0	1.0
1071_0248333	2.0	1.0
1071_0248338	2.0	1.0
1071_0248349	0.0	1.0
1071_0248350	2.0	1.0
1091_0000001	1.0	1.0
1091_0000013	1.0	1.0
1091_0000016	1.0	1.0
1091_0000018	2.0	2.0
1091_0000020	1.0	2.0
1091_0000028	1.0	1.0
1091_0000030	0.0	1.0
1091_0000036	1.0	2.0
1091_0000044	0.0	1.0
1091_0000052	1.0	1.0
1091_0000059	1.0	2.0
1091_0000063	2.0	1.0
1091_0000077	2.0	1.0
1091_0000078	3.0	1.0
1091_0000086	1.0	1.0
1091_0000087	2.0	2.0
1091_0000101	2.0	1.0
1091_0000113	1.0	2.0
1091_0000116	2.0	2.0
1091_0000123	2.0	2.0
1091_0000126	2.0	2.0
1091_0000145	1.0	1.0
1091_0000152	1.0	1.0
1091_0000166	2.0	1.0
1091_0000170	3.0	1.0
1091_0000172	2.0	1.0
1091_0000195	1.0	1.0
1091_0000196	2.0	1.0
1091_0000211	1.0	2.0
1091_0000214	2.0	1.0
1091_0000218	2.0	1.0
1091_0000222	2.0	2.0
1091_0000226	1.0	1.0
1091_0000228	2.0	1.0
1091_0000229	2.0	2.0
1091_0000234	2.0	2.0
1091_0000236	2.0	2.0
1091_0000239	2.0	1.0
1091_0000242	1.0	1.0
1091_0000247	1.0	2.0
1091_0000248	2.0	1.0
1091_0000249	2.0	2.0
1091_0000252	1.0	2.0
1091_0000253	1.0	1.0
1091_0000264	2.0	1.0
1091_0000268	2.0	2.0
1091_0000269	1.0	2.0
1091_0000272	1.0	1.0
LANGUAGE: DE, 1th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.37
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.07
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        15
         1.0       0.51      0.42      0.46        52
         2.0       0.54      0.67      0.60        64
         3.0       0.60      0.91      0.72        55
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         2

    accuracy                           0.56       205
   macro avg       0.28      0.33      0.30       205
weighted avg       0.46      0.56      0.50       205

[[ 0 15  0  0  0  0]
 [ 0 22 30  0  0  0]
 [ 0  6 43 15  0  0]
 [ 0  0  5 50  0  0]
 [ 0  0  1 16  0  0]
 [ 0  0  0  2  0  0]]
0.49965265111997265
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.05
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.94
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        15
         1.0       0.55      0.81      0.65        52
         2.0       0.68      0.50      0.58        64
         3.0       0.62      0.91      0.74        55
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         2

    accuracy                           0.60       205
   macro avg       0.31      0.37      0.33       205
weighted avg       0.52      0.60      0.54       205

[[ 0 15  0  0  0  0]
 [ 0 42 10  0  0  0]
 [ 0 19 32 13  0  0]
 [ 0  1  4 50  0  0]
 [ 0  0  1 16  0  0]
 [ 0  0  0  2  0  0]]
0.542451426769601
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.95
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.02
              precision    recall  f1-score   support

         0.0       1.00      0.20      0.33        15
         1.0       0.60      0.48      0.53        52
         2.0       0.57      0.67      0.62        64
         3.0       0.60      0.93      0.73        55
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         2

    accuracy                           0.60       205
   macro avg       0.46      0.38      0.37       205
weighted avg       0.56      0.60      0.55       205

[[ 3 12  0  0  0  0]
 [ 0 25 27  0  0  0]
 [ 0  5 43 16  0  0]
 [ 0  0  4 51  0  0]
 [ 0  0  1 16  0  0]
 [ 0  0  0  2  0  0]]
0.5479420748374242
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.89
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.91
              precision    recall  f1-score   support

         0.0       1.00      0.20      0.33        15
         1.0       0.57      0.58      0.57        52
         2.0       0.59      0.62      0.61        64
         3.0       0.60      0.89      0.72        55
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         2

    accuracy                           0.60       205
   macro avg       0.46      0.38      0.37       205
weighted avg       0.56      0.60      0.55       205

[[ 3 12  0  0  0  0]
 [ 0 30 22  0  0  0]
 [ 0 10 40 14  0  0]
 [ 0  1  5 49  0  0]
 [ 0  0  1 16  0  0]
 [ 0  0  0  2  0  0]]
0.5518756948455656
205 205 205
Filename	True Label	Prediction
1023_0101690	2.0	3.0
1023_0101693	3.0	3.0
1023_0101845	3.0	3.0
1023_0101847	3.0	3.0
1023_0101849	3.0	3.0
1023_0101855	2.0	3.0
1023_0101856	3.0	3.0
1023_0101909	4.0	3.0
1023_0103829	2.0	3.0
1023_0103830	3.0	3.0
1023_0103834	3.0	3.0
1023_0103838	3.0	3.0
1023_0103839	3.0	3.0
1023_0107244	3.0	3.0
1023_0107682	2.0	2.0
1023_0107729	3.0	3.0
1023_0107740	3.0	3.0
1023_0107783	3.0	2.0
1023_0108306	3.0	3.0
1023_0108307	3.0	3.0
1023_0108520	3.0	3.0
1023_0108648	2.0	3.0
1023_0108649	3.0	3.0
1023_0108885	2.0	3.0
1023_0108889	3.0	3.0
1023_0108992	3.0	3.0
1023_0108993	3.0	3.0
1023_0109030	2.0	3.0
1023_0109033	4.0	3.0
1023_0109039	3.0	3.0
1023_0109248	2.0	3.0
1023_0109400	3.0	3.0
1023_0109524	2.0	3.0
1023_0109527	4.0	3.0
1023_0109528	3.0	3.0
1023_0109588	3.0	3.0
1023_0109651	3.0	3.0
1023_0109945	5.0	3.0
1031_0001949	3.0	3.0
1031_0002003	3.0	3.0
1031_0002087	4.0	3.0
1031_0002088	4.0	3.0
1031_0002092	4.0	3.0
1031_0002197	4.0	3.0
1031_0002198	3.0	3.0
1031_0002199	4.0	3.0
1031_0003013	4.0	3.0
1031_0003042	3.0	3.0
1031_0003048	4.0	3.0
1031_0003063	4.0	3.0
1031_0003073	3.0	3.0
1031_0003085	3.0	3.0
1031_0003088	4.0	3.0
1031_0003098	4.0	3.0
1031_0003141	3.0	3.0
1031_0003145	4.0	3.0
1031_0003150	3.0	3.0
1031_0003155	3.0	3.0
1031_0003156	3.0	3.0
1031_0003169	3.0	3.0
1031_0003170	3.0	3.0
1031_0003174	3.0	3.0
1031_0003180	4.0	3.0
1031_0003189	3.0	3.0
1031_0003211	2.0	3.0
1031_0003327	3.0	3.0
1031_0003336	3.0	3.0
1031_0003359	3.0	3.0
1031_0003368	3.0	3.0
1031_0003387	3.0	3.0
1031_0003407	3.0	3.0
1031_0003409	5.0	3.0
1031_0003410	3.0	3.0
1031_0003415	4.0	3.0
1061_0012029	4.0	2.0
1061_0120275	2.0	2.0
1061_0120283	1.0	1.0
1061_0120289	1.0	2.0
1061_0120296	2.0	2.0
1061_0120307	2.0	2.0
1061_0120310	2.0	2.0
1061_0120311	3.0	2.0
1061_0120314	2.0	2.0
1061_0120315	2.0	1.0
1061_0120318	1.0	2.0
1061_0120324	2.0	2.0
1061_0120325	2.0	2.0
1061_0120332	2.0	2.0
1061_0120334	2.0	3.0
1061_0120335	3.0	3.0
1061_0120341	2.0	1.0
1061_0120343	2.0	2.0
1061_0120346	2.0	2.0
1061_0120348	1.0	1.0
1061_0120349	1.0	1.0
1061_0120353	1.0	1.0
1061_0120354	1.0	2.0
1061_0120361	3.0	2.0
1061_0120369	2.0	2.0
1061_0120373	2.0	2.0
1061_0120376	2.0	2.0
1061_0120388	2.0	2.0
1061_0120389	2.0	2.0
1061_0120403	3.0	2.0
1061_0120423	2.0	3.0
1061_0120432	2.0	2.0
1061_0120441	2.0	2.0
1061_0120442	3.0	2.0
1061_0120458	3.0	3.0
1061_0120479	2.0	2.0
1061_0120492	3.0	3.0
1061_0120493	2.0	2.0
1061_0120495	2.0	2.0
1061_0120497	2.0	3.0
1061_0120498	2.0	3.0
1061_0120853	2.0	2.0
1061_0120857	2.0	2.0
1061_0120859	2.0	3.0
1061_0120874	1.0	2.0
1061_0120876	2.0	2.0
1061_0120882	3.0	3.0
1061_0120888	1.0	2.0
1061_0120889	1.0	1.0
1061_1029115	1.0	2.0
1061_1029116	1.0	2.0
1061_1029120	2.0	2.0
1061_1202913	2.0	1.0
1071_0024685	2.0	2.0
1071_0024690	1.0	2.0
1071_0024691	1.0	2.0
1071_0024699	1.0	1.0
1071_0024706	1.0	1.0
1071_0024758	2.0	2.0
1071_0024762	0.0	1.0
1071_0024769	0.0	1.0
1071_0024782	0.0	0.0
1071_0024797	0.0	1.0
1071_0024806	1.0	1.0
1071_0024808	1.0	1.0
1071_0024809	0.0	1.0
1071_0024811	1.0	1.0
1071_0024815	1.0	1.0
1071_0024821	0.0	1.0
1071_0024824	1.0	1.0
1071_0024826	1.0	2.0
1071_0024831	0.0	1.0
1071_0024840	1.0	1.0
1071_0024841	0.0	1.0
1071_0024845	0.0	1.0
1071_0024850	0.0	1.0
1071_0024879	1.0	1.0
1071_0241831	1.0	1.0
1071_0241832	1.0	1.0
1071_0242013	1.0	1.0
1071_0242091	1.0	1.0
1071_0243581	1.0	1.0
1071_0243592	1.0	1.0
1071_0243593	1.0	1.0
1071_0243621	2.0	1.0
1071_0248301	2.0	1.0
1071_0248305	0.0	1.0
1071_0248311	2.0	1.0
1071_0248315	0.0	0.0
1071_0248317	0.0	0.0
1071_0248325	0.0	1.0
1071_0248327	0.0	1.0
1071_0248339	2.0	1.0
1071_0248345	1.0	2.0
1091_0000002	2.0	2.0
1091_0000004	1.0	1.0
1091_0000012	1.0	1.0
1091_0000017	2.0	2.0
1091_0000019	1.0	1.0
1091_0000025	1.0	1.0
1091_0000032	1.0	1.0
1091_0000039	1.0	1.0
1091_0000056	1.0	2.0
1091_0000057	2.0	1.0
1091_0000062	2.0	1.0
1091_0000068	1.0	2.0
1091_0000073	3.0	1.0
1091_0000102	2.0	2.0
1091_0000144	1.0	1.0
1091_0000157	2.0	2.0
1091_0000159	2.0	2.0
1091_0000160	2.0	2.0
1091_0000161	2.0	2.0
1091_0000167	1.0	2.0
1091_0000185	2.0	1.0
1091_0000190	1.0	1.0
1091_0000191	1.0	2.0
1091_0000200	2.0	2.0
1091_0000201	2.0	2.0
1091_0000208	1.0	2.0
1091_0000212	1.0	2.0
1091_0000219	1.0	2.0
1091_0000220	1.0	2.0
1091_0000223	2.0	2.0
1091_0000233	2.0	2.0
1091_0000238	1.0	2.0
1091_0000240	1.0	1.0
1091_0000244	2.0	2.0
1091_0000246	2.0	2.0
1091_0000257	1.0	2.0
1091_0000274	1.0	2.0
LANGUAGE: DE, 2th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.40
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.18
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        15
         1.0       0.46      1.00      0.63        52
         2.0       0.53      0.33      0.40        64
         3.0       0.62      0.60      0.61        55
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         2

    accuracy                           0.52       205
   macro avg       0.27      0.32      0.27       205
weighted avg       0.45      0.52      0.45       205

[[ 0 15  0  0  0  0]
 [ 0 52  0  0  0  0]
 [ 0 39 21  4  0  0]
 [ 0  6 16 33  0  0]
 [ 0  0  3 14  0  0]
 [ 0  0  0  2  0  0]]
0.45089207177250007
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.08
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        15
         1.0       0.60      0.83      0.69        52
         2.0       0.69      0.55      0.61        64
         3.0       0.56      0.84      0.67        55
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         2

    accuracy                           0.60       205
   macro avg       0.31      0.37      0.33       205
weighted avg       0.52      0.60      0.55       205

[[ 0 14  1  0  0  0]
 [ 0 43  9  0  0  0]
 [ 0 12 35 17  0  0]
 [ 0  3  6 46  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  2  0  0]]
0.5461236314024671
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.93
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.04
              precision    recall  f1-score   support

         0.0       0.80      0.27      0.40        15
         1.0       0.65      0.67      0.66        52
         2.0       0.61      0.53      0.57        64
         3.0       0.54      0.89      0.68        55
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         2

    accuracy                           0.60       205
   macro avg       0.43      0.39      0.38       205
weighted avg       0.56      0.60      0.56       205

[[ 4  9  2  0  0  0]
 [ 1 35 16  0  0  0]
 [ 0  8 34 22  0  0]
 [ 0  2  4 49  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  2  0  0]]
0.5550180639086807
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.86
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.80      0.27      0.40        15
         1.0       0.67      0.77      0.71        52
         2.0       0.68      0.56      0.62        64
         3.0       0.56      0.89      0.69        55
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         2

    accuracy                           0.63       205
   macro avg       0.45      0.41      0.40       205
weighted avg       0.59      0.63      0.59       205

[[ 4  9  2  0  0  0]
 [ 1 40 11  0  0  0]
 [ 0  9 36 19  0  0]
 [ 0  2  4 49  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  2  0  0]]
0.587732775640711
205 205 205
Filename	True Label	Prediction
1023_0001419	3.0	3.0
1023_0001420	4.0	3.0
1023_0101675	3.0	3.0
1023_0101688	4.0	3.0
1023_0101689	2.0	2.0
1023_0101701	2.0	3.0
1023_0101751	3.0	3.0
1023_0101846	4.0	3.0
1023_0101854	2.0	2.0
1023_0101895	3.0	3.0
1023_0101899	2.0	3.0
1023_0101906	2.0	3.0
1023_0102118	3.0	3.0
1023_0103822	2.0	3.0
1023_0103825	3.0	3.0
1023_0103832	3.0	3.0
1023_0103883	3.0	3.0
1023_0104203	2.0	3.0
1023_0104207	2.0	3.0
1023_0104209	3.0	3.0
1023_0107042	4.0	3.0
1023_0107074	4.0	3.0
1023_0107672	2.0	3.0
1023_0107773	3.0	3.0
1023_0107781	3.0	3.0
1023_0108518	3.0	3.0
1023_0108811	3.0	3.0
1023_0108812	3.0	3.0
1023_0108887	2.0	2.0
1023_0108888	2.0	3.0
1023_0108890	3.0	3.0
1023_0108932	2.0	3.0
1023_0108934	3.0	3.0
1023_0109022	3.0	3.0
1023_0109026	2.0	2.0
1023_0109096	3.0	3.0
1023_0109247	3.0	3.0
1023_0109249	2.0	3.0
1023_0109250	2.0	3.0
1023_0109267	2.0	3.0
1023_0109422	4.0	3.0
1023_0109495	3.0	3.0
1023_0109591	4.0	3.0
1023_0109946	3.0	3.0
1023_0109947	2.0	3.0
1023_0109951	3.0	3.0
1023_0109954	3.0	3.0
1023_0111896	2.0	3.0
1031_0001703	4.0	3.0
1031_0002002	2.0	3.0
1031_0002036	4.0	3.0
1031_0002042	4.0	3.0
1031_0002079	5.0	3.0
1031_0002083	3.0	3.0
1031_0002131	3.0	3.0
1031_0002187	3.0	3.0
1031_0003043	5.0	3.0
1031_0003072	3.0	3.0
1031_0003074	3.0	3.0
1031_0003099	3.0	3.0
1031_0003126	3.0	3.0
1031_0003127	4.0	3.0
1031_0003129	4.0	3.0
1031_0003136	3.0	3.0
1031_0003140	3.0	3.0
1031_0003162	3.0	3.0
1031_0003183	3.0	3.0
1031_0003191	3.0	3.0
1031_0003207	4.0	3.0
1031_0003214	2.0	3.0
1031_0003217	4.0	3.0
1031_0003218	3.0	3.0
1031_0003231	4.0	3.0
1031_0003234	3.0	3.0
1031_0003235	3.0	3.0
1031_0003246	4.0	3.0
1031_0003315	3.0	3.0
1031_0003331	3.0	3.0
1031_0003339	3.0	3.0
1031_0003365	3.0	3.0
1031_0003366	3.0	3.0
1031_0003369	3.0	3.0
1031_0003384	2.0	3.0
1031_0003391	2.0	3.0
1031_0003392	4.0	3.0
1031_0003393	3.0	3.0
1031_0003408	3.0	3.0
1061_0120272	2.0	2.0
1061_0120278	2.0	2.0
1061_0120290	1.0	2.0
1061_0120291	1.0	1.0
1061_0120308	2.0	2.0
1061_0120309	1.0	1.0
1061_0120329	2.0	2.0
1061_0120336	1.0	2.0
1061_0120337	2.0	2.0
1061_0120345	2.0	2.0
1061_0120351	2.0	2.0
1061_0120352	1.0	1.0
1061_0120355	1.0	1.0
1061_0120357	3.0	3.0
1061_0120375	2.0	1.0
1061_0120382	1.0	2.0
1061_0120410	2.0	2.0
1061_0120413	1.0	1.0
1061_0120414	2.0	2.0
1061_0120424	2.0	3.0
1061_0120429	3.0	2.0
1061_0120430	2.0	2.0
1061_0120433	1.0	2.0
1061_0120438	3.0	3.0
1061_0120440	1.0	1.0
1061_0120449	2.0	2.0
1061_0120450	2.0	2.0
1061_0120455	2.0	2.0
1061_0120478	2.0	2.0
1061_0120483	2.0	2.0
1061_0120484	2.0	2.0
1061_0120489	2.0	2.0
1061_0120490	2.0	2.0
1061_0120880	3.0	3.0
1061_1029118	1.0	1.0
1061_1202914	1.0	1.0
1061_1202917	2.0	2.0
1061_1202918	1.0	2.0
1061_1202919	2.0	2.0
1071_0024678	2.0	1.0
1071_0024682	2.0	2.0
1071_0024688	1.0	1.0
1071_0024694	1.0	2.0
1071_0024704	1.0	1.0
1071_0024715	1.0	1.0
1071_0024765	0.0	1.0
1071_0024766	1.0	1.0
1071_0024773	1.0	1.0
1071_0024777	1.0	1.0
1071_0024778	0.0	1.0
1071_0024784	1.0	0.0
1071_0024799	2.0	2.0
1071_0024807	1.0	1.0
1071_0024812	0.0	1.0
1071_0024820	0.0	1.0
1071_0024825	1.0	1.0
1071_0024827	1.0	1.0
1071_0024847	1.0	1.0
1071_0024848	1.0	1.0
1071_0024849	0.0	0.0
1071_0024859	1.0	1.0
1071_0024862	2.0	1.0
1071_0024864	0.0	0.0
1071_0024865	2.0	2.0
1071_0024871	1.0	1.0
1071_0024874	0.0	1.0
1071_0242012	1.0	1.0
1071_0242021	1.0	1.0
1071_0242041	1.0	1.0
1071_0242042	0.0	1.0
1071_0242072	0.0	0.0
1071_0243501	1.0	1.0
1071_0243591	1.0	1.0
1071_0248304	1.0	1.0
1071_0248312	1.0	1.0
1071_0248319	0.0	1.0
1071_0248328	0.0	1.0
1071_0248329	1.0	1.0
1071_0248340	0.0	0.0
1071_0248341	1.0	1.0
1071_0248347	1.0	1.0
1071_0248348	1.0	1.0
1091_0000010	3.0	2.0
1091_0000024	3.0	1.0
1091_0000026	1.0	1.0
1091_0000029	2.0	1.0
1091_0000034	2.0	1.0
1091_0000035	1.0	1.0
1091_0000038	2.0	1.0
1091_0000041	1.0	1.0
1091_0000049	1.0	1.0
1091_0000050	1.0	1.0
1091_0000051	1.0	1.0
1091_0000053	0.0	1.0
1091_0000058	2.0	2.0
1091_0000070	2.0	1.0
1091_0000072	0.0	2.0
1091_0000154	1.0	2.0
1091_0000158	2.0	2.0
1091_0000164	2.0	1.0
1091_0000165	1.0	1.0
1091_0000168	2.0	2.0
1091_0000169	3.0	2.0
1091_0000171	2.0	2.0
1091_0000199	2.0	2.0
1091_0000204	3.0	2.0
1091_0000205	1.0	2.0
1091_0000213	1.0	2.0
1091_0000215	1.0	2.0
1091_0000217	3.0	1.0
1091_0000224	1.0	1.0
1091_0000232	2.0	2.0
1091_0000255	0.0	2.0
1091_0000256	1.0	2.0
1091_0000258	2.0	2.0
1091_0000260	2.0	2.0
1091_0000262	2.0	1.0
1091_0000276	2.0	2.0
LANGUAGE: DE, 3th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.37
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.04
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        15
         1.0       0.51      0.75      0.61        52
         2.0       0.56      0.52      0.54        64
         3.0       0.60      0.75      0.67        56
         4.0       0.00      0.00      0.00        16
         5.0       0.00      0.00      0.00         2

    accuracy                           0.56       205
   macro avg       0.28      0.34      0.30       205
weighted avg       0.47      0.56      0.50       205

[[ 0 14  1  0  0  0]
 [ 0 39 13  0  0  0]
 [ 0 21 33 10  0  0]
 [ 0  2 12 42  0  0]
 [ 0  0  0 16  0  0]
 [ 0  0  0  2  0  0]]
0.5042063255998414
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.07
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.98
              precision    recall  f1-score   support

         0.0       1.00      0.20      0.33        15
         1.0       0.53      0.73      0.61        52
         2.0       0.55      0.45      0.50        64
         3.0       0.58      0.80      0.68        56
         4.0       0.00      0.00      0.00        16
         5.0       0.00      0.00      0.00         2

    accuracy                           0.56       205
   macro avg       0.44      0.36      0.35       205
weighted avg       0.54      0.56      0.52       205

[[ 3 11  1  0  0  0]
 [ 0 38 14  0  0  0]
 [ 0 21 29 14  0  0]
 [ 0  2  9 45  0  0]
 [ 0  0  0 16  0  0]
 [ 0  0  0  2  0  0]]
0.5194741478562754
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.94
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.94
              precision    recall  f1-score   support

         0.0       1.00      0.20      0.33        15
         1.0       0.56      0.62      0.59        52
         2.0       0.55      0.58      0.56        64
         3.0       0.59      0.82      0.69        56
         4.0       0.00      0.00      0.00        16
         5.0       0.00      0.00      0.00         2

    accuracy                           0.58       205
   macro avg       0.45      0.37      0.36       205
weighted avg       0.55      0.58      0.54       205

[[ 3 11  1  0  0  0]
 [ 0 32 20  0  0  0]
 [ 0 13 37 14  0  0]
 [ 0  1  9 46  0  0]
 [ 0  0  0 16  0  0]
 [ 0  0  0  2  0  0]]
0.5372319172756008
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.87
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.92
              precision    recall  f1-score   support

         0.0       1.00      0.40      0.57        15
         1.0       0.56      0.69      0.62        52
         2.0       0.53      0.48      0.51        64
         3.0       0.58      0.80      0.68        56
         4.0       0.00      0.00      0.00        16
         5.0       0.00      0.00      0.00         2

    accuracy                           0.58       205
   macro avg       0.45      0.40      0.40       205
weighted avg       0.54      0.58      0.54       205

[[ 6  8  1  0  0  0]
 [ 0 36 16  0  0  0]
 [ 0 19 31 14  0  0]
 [ 0  1 10 45  0  0]
 [ 0  0  0 16  0  0]
 [ 0  0  0  2  0  0]]
0.5427639885191892
205 205 205
Filename	True Label	Prediction
1023_0001416	5.0	3.0
1023_0001418	4.0	3.0
1023_0101749	4.0	3.0
1023_0101752	3.0	3.0
1023_0101848	2.0	2.0
1023_0101893	3.0	3.0
1023_0101894	2.0	3.0
1023_0101907	4.0	3.0
1023_0102117	3.0	3.0
1023_0103821	3.0	3.0
1023_0103826	3.0	3.0
1023_0103827	3.0	3.0
1023_0103831	3.0	3.0
1023_0103836	4.0	3.0
1023_0103837	3.0	3.0
1023_0103840	3.0	3.0
1023_0103843	3.0	2.0
1023_0106816	3.0	3.0
1023_0107780	3.0	3.0
1023_0108422	3.0	3.0
1023_0108423	3.0	2.0
1023_0108426	2.0	3.0
1023_0108510	3.0	3.0
1023_0108753	2.0	3.0
1023_0108815	2.0	3.0
1023_0108933	3.0	3.0
1023_0108935	2.0	3.0
1023_0109027	3.0	2.0
1023_0109391	2.0	3.0
1023_0109396	2.0	3.0
1023_0109399	2.0	3.0
1023_0109496	3.0	3.0
1023_0109500	2.0	3.0
1023_0109519	2.0	3.0
1023_0109522	3.0	3.0
1023_0109721	2.0	3.0
1023_0109878	2.0	3.0
1023_0109880	3.0	3.0
1023_0109891	3.0	3.0
1023_0109915	2.0	2.0
1023_0109917	5.0	3.0
1031_0002005	4.0	3.0
1031_0002010	3.0	3.0
1031_0002011	4.0	3.0
1031_0002032	3.0	3.0
1031_0002043	4.0	3.0
1031_0002084	4.0	3.0
1031_0002089	4.0	3.0
1031_0003012	3.0	3.0
1031_0003023	4.0	3.0
1031_0003035	3.0	3.0
1031_0003053	3.0	3.0
1031_0003054	4.0	3.0
1031_0003090	4.0	3.0
1031_0003091	2.0	3.0
1031_0003092	3.0	3.0
1031_0003132	4.0	3.0
1031_0003135	3.0	3.0
1031_0003144	3.0	3.0
1031_0003149	4.0	3.0
1031_0003161	3.0	3.0
1031_0003216	3.0	3.0
1031_0003219	3.0	3.0
1031_0003224	3.0	3.0
1031_0003226	3.0	3.0
1031_0003230	3.0	3.0
1031_0003233	3.0	3.0
1031_0003236	3.0	3.0
1031_0003237	3.0	3.0
1031_0003239	4.0	3.0
1031_0003242	3.0	3.0
1031_0003243	3.0	3.0
1031_0003249	3.0	3.0
1031_0003273	3.0	3.0
1031_0003330	4.0	3.0
1031_0003353	3.0	3.0
1031_0003386	3.0	3.0
1031_0003390	3.0	3.0
1031_0003414	3.0	3.0
1061_0120274	1.0	1.0
1061_0120277	1.0	2.0
1061_0120280	1.0	1.0
1061_0120284	0.0	0.0
1061_0120286	1.0	1.0
1061_0120297	1.0	2.0
1061_0120299	2.0	2.0
1061_0120301	2.0	1.0
1061_0120304	1.0	1.0
1061_0120312	1.0	1.0
1061_0120316	2.0	2.0
1061_0120319	2.0	2.0
1061_0120320	3.0	3.0
1061_0120338	2.0	2.0
1061_0120347	2.0	2.0
1061_0120350	2.0	2.0
1061_0120356	2.0	2.0
1061_0120359	2.0	2.0
1061_0120360	3.0	2.0
1061_0120371	3.0	3.0
1061_0120372	1.0	2.0
1061_0120384	1.0	1.0
1061_0120386	1.0	2.0
1061_0120391	1.0	1.0
1061_0120407	3.0	2.0
1061_0120411	3.0	2.0
1061_0120421	2.0	2.0
1061_0120425	2.0	2.0
1061_0120426	2.0	3.0
1061_0120453	2.0	2.0
1061_0120456	2.0	2.0
1061_0120457	3.0	2.0
1061_0120482	2.0	2.0
1061_0120488	3.0	2.0
1061_0120496	2.0	2.0
1061_0120858	2.0	2.0
1061_0120878	2.0	2.0
1061_0120885	2.0	2.0
1061_0120890	1.0	1.0
1061_0120894	2.0	2.0
1061_1029114	1.0	2.0
1061_1202911	0.0	2.0
1061_1202915	1.0	2.0
1071_0020001	1.0	1.0
1071_0024681	2.0	2.0
1071_0024687	1.0	1.0
1071_0024689	1.0	1.0
1071_0024692	2.0	2.0
1071_0024702	1.0	1.0
1071_0024705	1.0	1.0
1071_0024710	0.0	1.0
1071_0024714	2.0	1.0
1071_0024716	1.0	1.0
1071_0024763	1.0	1.0
1071_0024770	1.0	1.0
1071_0024772	0.0	0.0
1071_0024774	0.0	0.0
1071_0024798	0.0	1.0
1071_0024803	1.0	1.0
1071_0024810	1.0	1.0
1071_0024819	1.0	1.0
1071_0024852	0.0	0.0
1071_0024854	0.0	0.0
1071_0024855	1.0	1.0
1071_0024861	0.0	1.0
1071_0024866	2.0	2.0
1071_0024867	1.0	1.0
1071_0024872	1.0	1.0
1071_0242023	1.0	1.0
1071_0242071	0.0	1.0
1071_0242093	0.0	0.0
1071_0243582	1.0	1.0
1071_0243623	1.0	1.0
1071_0248307	2.0	1.0
1071_0248332	2.0	1.0
1071_0248334	2.0	1.0
1071_0248335	1.0	1.0
1071_0248337	1.0	2.0
1071_0248344	1.0	1.0
1091_0000003	2.0	1.0
1091_0000005	2.0	2.0
1091_0000007	3.0	2.0
1091_0000008	2.0	2.0
1091_0000011	2.0	1.0
1091_0000014	0.0	1.0
1091_0000022	2.0	2.0
1091_0000023	2.0	1.0
1091_0000027	0.0	1.0
1091_0000031	1.0	1.0
1091_0000033	1.0	1.0
1091_0000045	1.0	2.0
1091_0000047	2.0	1.0
1091_0000054	0.0	1.0
1091_0000055	1.0	2.0
1091_0000060	2.0	2.0
1091_0000064	2.0	1.0
1091_0000065	1.0	1.0
1091_0000066	1.0	1.0
1091_0000074	2.0	1.0
1091_0000076	2.0	2.0
1091_0000092	1.0	1.0
1091_0000095	1.0	1.0
1091_0000127	2.0	1.0
1091_0000148	1.0	1.0
1091_0000151	0.0	1.0
1091_0000155	3.0	2.0
1091_0000163	2.0	1.0
1091_0000173	2.0	1.0
1091_0000174	2.0	1.0
1091_0000193	2.0	1.0
1091_0000194	1.0	2.0
1091_0000197	1.0	2.0
1091_0000198	2.0	2.0
1091_0000202	1.0	2.0
1091_0000216	1.0	2.0
1091_0000221	2.0	1.0
1091_0000225	2.0	1.0
1091_0000230	1.0	2.0
1091_0000235	1.0	1.0
1091_0000237	2.0	2.0
1091_0000251	1.0	2.0
1091_0000254	3.0	1.0
1091_0000259	2.0	2.0
1091_0000271	1.0	1.0
1091_0000273	1.0	2.0
1091_0000275	2.0	1.0
LANGUAGE: DE, 4th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.52
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.15
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        16
         1.0       0.51      0.94      0.66        51
         2.0       0.80      0.25      0.38        64
         3.0       0.55      0.89      0.68        56
         4.0       0.00      0.00      0.00        16
         5.0       0.00      0.00      0.00         2

    accuracy                           0.56       205
   macro avg       0.31      0.35      0.29       205
weighted avg       0.53      0.56      0.47       205

[[ 0 16  0  0  0  0]
 [ 0 48  3  0  0  0]
 [ 0 25 16 23  0  0]
 [ 0  5  1 50  0  0]
 [ 0  0  0 16  0  0]
 [ 0  0  0  2  0  0]]
0.46947174496375504
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.10
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.12
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        16
         1.0       0.54      0.61      0.57        51
         2.0       0.48      0.36      0.41        64
         3.0       0.51      0.91      0.65        56
         4.0       0.00      0.00      0.00        16
         5.0       0.00      0.00      0.00         2

    accuracy                           0.51       205
   macro avg       0.26      0.31      0.27       205
weighted avg       0.42      0.51      0.45       205

[[ 0 15  1  0  0  0]
 [ 0 31 19  1  0  0]
 [ 0 11 23 30  0  0]
 [ 0  0  5 51  0  0]
 [ 0  0  0 16  0  0]
 [ 0  0  0  2  0  0]]
0.44965305697013014
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.98
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.08
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        16
         1.0       0.55      0.41      0.47        51
         2.0       0.51      0.66      0.57        64
         3.0       0.57      0.86      0.69        56
         4.0       0.00      0.00      0.00        16
         5.0       0.00      0.00      0.00         2

    accuracy                           0.54       205
   macro avg       0.27      0.32      0.29       205
weighted avg       0.45      0.54      0.48       205

[[ 0 13  3  0  0  0]
 [ 0 21 30  0  0  0]
 [ 0  4 42 18  0  0]
 [ 0  0  8 48  0  0]
 [ 0  0  0 16  0  0]
 [ 0  0  0  2  0  0]]
0.48311631366714947
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.91
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.01
              precision    recall  f1-score   support

         0.0       1.00      0.12      0.22        16
         1.0       0.59      0.67      0.62        51
         2.0       0.57      0.52      0.54        64
         3.0       0.56      0.88      0.69        56
         4.0       0.00      0.00      0.00        16
         5.0       0.00      0.00      0.00         2

    accuracy                           0.58       205
   macro avg       0.45      0.36      0.35       205
weighted avg       0.56      0.58      0.53       205

[[ 2 13  1  0  0  0]
 [ 0 34 17  0  0  0]
 [ 0 11 33 20  0  0]
 [ 0  0  7 49  0  0]
 [ 0  0  0 16  0  0]
 [ 0  0  0  2  0  0]]
0.5286470366552096
205 205 205
Filename	True Label	Prediction
1023_0001423	2.0	2.0
1023_0101684	2.0	3.0
1023_0101691	4.0	3.0
1023_0101694	2.0	3.0
1023_0101753	3.0	3.0
1023_0101841	3.0	3.0
1023_0101844	2.0	3.0
1023_0101853	2.0	3.0
1023_0101896	3.0	2.0
1023_0101897	3.0	3.0
1023_0101898	4.0	3.0
1023_0101900	3.0	3.0
1023_0101904	2.0	3.0
1023_0103823	4.0	3.0
1023_0103824	3.0	3.0
1023_0103828	1.0	2.0
1023_0103833	5.0	3.0
1023_0103841	5.0	3.0
1023_0103880	3.0	3.0
1023_0107075	2.0	3.0
1023_0107726	3.0	3.0
1023_0107787	2.0	3.0
1023_0107788	3.0	3.0
1023_0108304	3.0	3.0
1023_0108305	3.0	3.0
1023_0108641	3.0	3.0
1023_0108752	3.0	3.0
1023_0108810	3.0	3.0
1023_0108886	3.0	3.0
1023_0108908	2.0	3.0
1023_0108931	3.0	3.0
1023_0108955	3.0	3.0
1023_0109029	1.0	2.0
1023_0109038	3.0	3.0
1023_0109192	3.0	3.0
1023_0109395	2.0	2.0
1023_0109518	2.0	3.0
1023_0109520	2.0	3.0
1023_0109590	2.0	3.0
1023_0109609	3.0	3.0
1023_0109674	3.0	3.0
1023_0109890	3.0	3.0
1023_0109914	2.0	3.0
1031_0001951	2.0	3.0
1031_0001998	4.0	3.0
1031_0002004	4.0	3.0
1031_0002040	4.0	3.0
1031_0002061	3.0	3.0
1031_0002085	3.0	3.0
1031_0002091	4.0	3.0
1031_0002184	3.0	3.0
1031_0002195	3.0	3.0
1031_0003029	4.0	3.0
1031_0003065	3.0	3.0
1031_0003078	3.0	3.0
1031_0003095	3.0	3.0
1031_0003097	3.0	3.0
1031_0003106	3.0	3.0
1031_0003128	4.0	3.0
1031_0003146	4.0	3.0
1031_0003154	4.0	3.0
1031_0003163	3.0	3.0
1031_0003165	2.0	3.0
1031_0003172	3.0	3.0
1031_0003179	3.0	3.0
1031_0003185	3.0	3.0
1031_0003187	4.0	3.0
1031_0003190	3.0	3.0
1031_0003203	2.0	3.0
1031_0003205	3.0	3.0
1031_0003206	3.0	3.0
1031_0003212	3.0	3.0
1031_0003220	3.0	3.0
1031_0003225	3.0	3.0
1031_0003232	2.0	3.0
1031_0003240	2.0	3.0
1031_0003260	4.0	3.0
1031_0003261	3.0	3.0
1031_0003309	3.0	3.0
1031_0003310	3.0	3.0
1031_0003314	4.0	3.0
1031_0003352	2.0	3.0
1031_0003355	3.0	3.0
1031_0003357	3.0	3.0
1031_0003358	4.0	3.0
1031_0003367	4.0	3.0
1031_0003383	3.0	3.0
1031_0003389	3.0	3.0
1061_0120271	2.0	2.0
1061_0120273	1.0	1.0
1061_0120281	1.0	2.0
1061_0120285	2.0	2.0
1061_0120287	1.0	2.0
1061_0120288	2.0	2.0
1061_0120295	0.0	2.0
1061_0120302	1.0	2.0
1061_0120306	3.0	2.0
1061_0120321	2.0	2.0
1061_0120323	1.0	1.0
1061_0120327	2.0	2.0
1061_0120368	2.0	2.0
1061_0120374	2.0	3.0
1061_0120387	1.0	2.0
1061_0120394	2.0	2.0
1061_0120404	2.0	1.0
1061_0120405	2.0	2.0
1061_0120406	2.0	2.0
1061_0120408	2.0	2.0
1061_0120427	2.0	2.0
1061_0120428	2.0	2.0
1061_0120439	1.0	1.0
1061_0120481	3.0	3.0
1061_0120486	2.0	2.0
1061_0120499	2.0	2.0
1061_0120500	2.0	2.0
1061_0120856	2.0	2.0
1061_0120875	3.0	3.0
1061_0120877	2.0	2.0
1061_0120881	2.0	3.0
1061_0120884	2.0	2.0
1061_0120886	2.0	2.0
1061_1029117	2.0	2.0
1061_1202910	3.0	2.0
1061_1202916	2.0	2.0
1071_0024686	3.0	2.0
1071_0024703	1.0	1.0
1071_0024709	2.0	2.0
1071_0024757	2.0	2.0
1071_0024767	2.0	1.0
1071_0024768	1.0	1.0
1071_0024776	0.0	0.0
1071_0024779	1.0	1.0
1071_0024781	0.0	1.0
1071_0024783	0.0	1.0
1071_0024801	1.0	1.0
1071_0024802	1.0	1.0
1071_0024804	0.0	1.0
1071_0024813	0.0	1.0
1071_0024818	2.0	1.0
1071_0024822	0.0	1.0
1071_0024823	1.0	1.0
1071_0024838	0.0	0.0
1071_0024843	1.0	1.0
1071_0024844	1.0	1.0
1071_0024853	0.0	1.0
1071_0024856	1.0	1.0
1071_0024857	0.0	1.0
1071_0024860	1.0	1.0
1071_0024863	1.0	1.0
1071_0024873	0.0	1.0
1071_0024875	1.0	1.0
1071_0024876	1.0	1.0
1071_0024881	1.0	1.0
1071_0242022	1.0	1.0
1071_0242073	1.0	1.0
1071_0242092	0.0	1.0
1071_0248302	1.0	1.0
1071_0248309	2.0	1.0
1071_0248310	1.0	1.0
1071_0248322	1.0	1.0
1071_0248330	2.0	1.0
1071_0248336	1.0	1.0
1071_0248342	0.0	1.0
1071_0248343	1.0	1.0
1071_0248346	1.0	1.0
1091_0000006	0.0	1.0
1091_0000009	0.0	1.0
1091_0000015	1.0	1.0
1091_0000021	1.0	2.0
1091_0000037	1.0	1.0
1091_0000042	0.0	1.0
1091_0000043	1.0	1.0
1091_0000046	1.0	1.0
1091_0000048	1.0	1.0
1091_0000061	2.0	1.0
1091_0000067	1.0	1.0
1091_0000069	2.0	1.0
1091_0000071	1.0	2.0
1091_0000075	2.0	2.0
1091_0000079	1.0	2.0
1091_0000114	2.0	2.0
1091_0000125	2.0	2.0
1091_0000140	2.0	1.0
1091_0000146	1.0	1.0
1091_0000153	1.0	2.0
1091_0000156	3.0	2.0
1091_0000162	1.0	2.0
1091_0000192	1.0	2.0
1091_0000203	2.0	1.0
1091_0000206	1.0	1.0
1091_0000207	2.0	2.0
1091_0000209	3.0	2.0
1091_0000210	2.0	1.0
1091_0000227	1.0	2.0
1091_0000231	2.0	2.0
1091_0000241	2.0	1.0
1091_0000243	1.0	1.0
1091_0000245	1.0	2.0
1091_0000250	1.0	2.0
1091_0000261	2.0	2.0
1091_0000263	3.0	2.0
1091_0000265	2.0	2.0
1091_0000266	1.0	2.0
1091_0000267	1.0	2.0
1091_0000270	2.0	2.0
Averaged weighted F1-scores 0.555012736793995
130.68810916179336 82.83744971317162
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
LANGUAGE: DE, 0th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.40
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.09
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.43      0.80      0.56        46
         2.0       0.71      0.35      0.47        71
         3.0       0.62      0.96      0.75        54
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.55       206
   macro avg       0.29      0.35      0.30       206
weighted avg       0.50      0.55      0.48       206

[[ 0 17  0  0  0  0]
 [ 0 37  9  0  0  0]
 [ 0 32 25 14  0  0]
 [ 0  1  1 52  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
0.48436991832065485
206 206 206



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.00
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.06
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.43      0.41      0.42        46
         2.0       0.58      0.62      0.60        71
         3.0       0.60      0.96      0.74        54
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.56       206
   macro avg       0.27      0.33      0.29       206
weighted avg       0.45      0.56      0.50       206

[[ 0 14  3  0  0  0]
 [ 0 19 27  0  0  0]
 [ 0 11 44 16  0  0]
 [ 0  0  2 52  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
0.4953393655196706
206 206 206



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.85
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.96
              precision    recall  f1-score   support

         0.0       1.00      0.24      0.38        17
         1.0       0.47      0.46      0.46        46
         2.0       0.61      0.65      0.63        71
         3.0       0.63      0.94      0.76        54
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.59       206
   macro avg       0.45      0.38      0.37       206
weighted avg       0.56      0.59      0.55       206

[[ 4 11  2  0  0  0]
 [ 0 21 25  0  0  0]
 [ 0 13 46 12  0  0]
 [ 0  0  3 51  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
0.5482637565855318
206 206 206



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.76
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.94
              precision    recall  f1-score   support

         0.0       0.88      0.41      0.56        17
         1.0       0.51      0.61      0.55        46
         2.0       0.64      0.59      0.61        71
         3.0       0.62      0.89      0.73        54
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.61       206
   macro avg       0.44      0.42      0.41       206
weighted avg       0.57      0.61      0.57       206

[[ 7  8  2  0  0  0]
 [ 1 28 17  0  0  0]
 [ 0 18 42 11  0  0]
 [ 0  1  5 48  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
0.5734481373512011
206 206 206
Filename	True Label	Prediction
1023_0001416	3.0	3.0
1023_0101675	3.0	2.0
1023_0101846	4.0	3.0
1023_0101907	3.0	3.0
1023_0101909	4.0	3.0
1023_0103823	3.0	3.0
1023_0103830	3.0	3.0
1023_0103831	3.0	2.0
1023_0103833	4.0	3.0
1023_0103880	3.0	2.0
1023_0107074	3.0	3.0
1023_0107075	2.0	3.0
1023_0107672	3.0	3.0
1023_0107740	3.0	3.0
1023_0107781	3.0	3.0
1023_0107784	1.0	2.0
1023_0108648	3.0	3.0
1023_0108650	3.0	3.0
1023_0108753	3.0	3.0
1023_0108811	3.0	3.0
1023_0108812	3.0	3.0
1023_0108958	3.0	3.0
1023_0109039	3.0	3.0
1023_0109250	2.0	2.0
1023_0109399	2.0	3.0
1023_0109422	3.0	3.0
1023_0109495	3.0	3.0
1023_0109505	3.0	3.0
1023_0109522	3.0	3.0
1023_0109527	3.0	3.0
1023_0109614	2.0	2.0
1023_0109717	3.0	3.0
1023_0109878	2.0	3.0
1023_0109891	2.0	3.0
1023_0109946	2.0	3.0
1023_0109954	3.0	3.0
1023_0111896	2.0	3.0
1031_0001703	4.0	3.0
1031_0001949	4.0	3.0
1031_0002004	4.0	3.0
1031_0002006	5.0	3.0
1031_0002036	4.0	3.0
1031_0002085	3.0	3.0
1031_0002091	3.0	3.0
1031_0002092	4.0	3.0
1031_0002196	3.0	3.0
1031_0003013	4.0	3.0
1031_0003023	3.0	3.0
1031_0003042	3.0	3.0
1031_0003048	4.0	3.0
1031_0003085	3.0	3.0
1031_0003092	2.0	3.0
1031_0003121	3.0	3.0
1031_0003136	4.0	3.0
1031_0003144	3.0	3.0
1031_0003156	3.0	3.0
1031_0003169	3.0	3.0
1031_0003182	4.0	3.0
1031_0003184	4.0	3.0
1031_0003205	3.0	3.0
1031_0003206	3.0	3.0
1031_0003207	4.0	3.0
1031_0003211	3.0	3.0
1031_0003218	4.0	3.0
1031_0003225	3.0	3.0
1031_0003233	3.0	3.0
1031_0003236	3.0	3.0
1031_0003246	3.0	3.0
1031_0003260	3.0	3.0
1031_0003261	3.0	3.0
1031_0003336	3.0	3.0
1031_0003355	4.0	3.0
1031_0003366	3.0	3.0
1031_0003387	4.0	3.0
1031_0003390	3.0	3.0
1031_0003410	3.0	3.0
1031_0003419	3.0	3.0
1061_0120274	2.0	2.0
1061_0120277	1.0	2.0
1061_0120287	1.0	2.0
1061_0120291	1.0	1.0
1061_0120300	2.0	2.0
1061_0120301	2.0	2.0
1061_0120313	2.0	2.0
1061_0120316	2.0	2.0
1061_0120331	1.0	1.0
1061_0120335	2.0	3.0
1061_0120341	1.0	1.0
1061_0120355	1.0	1.0
1061_0120360	3.0	3.0
1061_0120366	3.0	2.0
1061_0120373	2.0	2.0
1061_0120374	3.0	2.0
1061_0120375	2.0	1.0
1061_0120387	2.0	2.0
1061_0120394	2.0	2.0
1061_0120408	2.0	2.0
1061_0120448	2.0	2.0
1061_0120456	2.0	2.0
1061_0120460	2.0	2.0
1061_0120487	2.0	2.0
1061_0120489	2.0	3.0
1061_0120490	2.0	2.0
1061_0120495	2.0	2.0
1061_0120496	2.0	2.0
1061_0120499	2.0	2.0
1061_0120859	2.0	3.0
1061_0120881	2.0	3.0
1061_0120882	3.0	3.0
1061_0120884	2.0	2.0
1061_0120887	2.0	2.0
1061_1029113	1.0	2.0
1061_1029115	2.0	2.0
1061_1029117	1.0	2.0
1061_1202910	2.0	2.0
1071_0024682	2.0	2.0
1071_0024685	2.0	2.0
1071_0024692	2.0	2.0
1071_0024715	2.0	1.0
1071_0024765	0.0	0.0
1071_0024766	0.0	1.0
1071_0024768	0.0	1.0
1071_0024772	0.0	0.0
1071_0024774	0.0	0.0
1071_0024781	1.0	1.0
1071_0024806	1.0	1.0
1071_0024812	0.0	1.0
1071_0024823	1.0	1.0
1071_0024835	1.0	1.0
1071_0024836	1.0	2.0
1071_0024837	0.0	0.0
1071_0024840	1.0	1.0
1071_0024841	0.0	1.0
1071_0024844	1.0	1.0
1071_0024848	1.0	1.0
1071_0024857	1.0	1.0
1071_0024865	1.0	1.0
1071_0024874	1.0	1.0
1071_0024881	2.0	1.0
1071_0241831	1.0	1.0
1071_0242043	0.0	1.0
1071_0242093	0.0	0.0
1071_0243581	0.0	1.0
1071_0243582	1.0	1.0
1071_0243591	1.0	1.0
1071_0243593	0.0	2.0
1071_0243621	1.0	1.0
1071_0248301	1.0	1.0
1071_0248309	2.0	1.0
1071_0248311	1.0	1.0
1071_0248317	0.0	0.0
1071_0248320	0.0	0.0
1071_0248333	2.0	1.0
1071_0248335	1.0	1.0
1071_0248339	1.0	1.0
1071_0248344	2.0	1.0
1091_0000006	0.0	1.0
1091_0000008	2.0	2.0
1091_0000016	1.0	1.0
1091_0000033	1.0	1.0
1091_0000039	1.0	1.0
1091_0000042	1.0	0.0
1091_0000046	2.0	1.0
1091_0000053	0.0	1.0
1091_0000055	2.0	2.0
1091_0000057	2.0	1.0
1091_0000059	1.0	2.0
1091_0000060	2.0	2.0
1091_0000061	2.0	1.0
1091_0000063	1.0	1.0
1091_0000064	1.0	1.0
1091_0000065	1.0	2.0
1091_0000070	2.0	1.0
1091_0000072	1.0	2.0
1091_0000073	2.0	1.0
1091_0000077	2.0	1.0
1091_0000087	2.0	2.0
1091_0000123	2.0	2.0
1091_0000140	2.0	1.0
1091_0000148	1.0	1.0
1091_0000153	1.0	2.0
1091_0000158	2.0	2.0
1091_0000161	2.0	2.0
1091_0000162	2.0	2.0
1091_0000166	1.0	2.0
1091_0000170	3.0	1.0
1091_0000194	1.0	2.0
1091_0000197	1.0	2.0
1091_0000198	2.0	2.0
1091_0000203	1.0	2.0
1091_0000205	1.0	2.0
1091_0000209	2.0	2.0
1091_0000211	1.0	2.0
1091_0000214	2.0	1.0
1091_0000217	2.0	1.0
1091_0000223	2.0	2.0
1091_0000225	2.0	1.0
1091_0000234	2.0	2.0
1091_0000235	2.0	1.0
1091_0000238	2.0	2.0
1091_0000240	2.0	1.0
1091_0000252	2.0	2.0
1091_0000255	0.0	2.0
1091_0000269	1.0	2.0
1091_0000270	2.0	2.0
1091_0000276	2.0	2.0
LANGUAGE: DE, 1th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.42
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.96
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        16
         1.0       0.53      0.67      0.60        46
         2.0       0.68      0.72      0.70        71
         3.0       0.65      0.87      0.75        54
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.63       205
   macro avg       0.31      0.38      0.34       205
weighted avg       0.53      0.63      0.57       205

[[ 0 15  1  0  0  0]
 [ 0 31 15  0  0  0]
 [ 0 12 51  8  0  0]
 [ 0  0  7 47  0  0]
 [ 0  0  1 16  0  0]
 [ 0  0  0  1  0  0]]
0.5722513704137494
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.98
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.90
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        16
         1.0       0.54      0.63      0.58        46
         2.0       0.66      0.66      0.66        71
         3.0       0.60      0.89      0.72        54
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.60       205
   macro avg       0.30      0.36      0.33       205
weighted avg       0.51      0.60      0.55       205

[[ 0 15  1  0  0  0]
 [ 0 29 17  0  0  0]
 [ 0 10 47 14  0  0]
 [ 0  0  6 48  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
0.548129595922825
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.87
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.88
              precision    recall  f1-score   support

         0.0       0.56      0.56      0.56        16
         1.0       0.65      0.52      0.58        46
         2.0       0.69      0.72      0.70        71
         3.0       0.62      0.89      0.73        54
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.64       205
   macro avg       0.42      0.45      0.43       205
weighted avg       0.59      0.64      0.61       205

[[ 9  6  1  0  0  0]
 [ 6 24 16  0  0  0]
 [ 1  7 51 12  0  0]
 [ 0  0  6 48  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
0.6088778755975044
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.80
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.87
              precision    recall  f1-score   support

         0.0       0.89      0.50      0.64        16
         1.0       0.59      0.70      0.64        46
         2.0       0.69      0.63      0.66        71
         3.0       0.62      0.89      0.73        54
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.65       205
   macro avg       0.47      0.45      0.45       205
weighted avg       0.61      0.65      0.62       205

[[ 8  7  1  0  0  0]
 [ 1 32 13  0  0  0]
 [ 0 15 45 11  0  0]
 [ 0  0  6 48  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
0.6157942107395927
205 205 205
Filename	True Label	Prediction
1023_0001418	3.0	3.0
1023_0101749	3.0	3.0
1023_0101844	2.0	2.0
1023_0101845	2.0	3.0
1023_0101848	2.0	2.0
1023_0101855	2.0	2.0
1023_0101896	3.0	2.0
1023_0103826	3.0	3.0
1023_0103829	2.0	3.0
1023_0103836	3.0	3.0
1023_0103840	3.0	3.0
1023_0103883	3.0	3.0
1023_0104206	3.0	3.0
1023_0104207	2.0	2.0
1023_0106816	3.0	3.0
1023_0107042	3.0	3.0
1023_0107244	3.0	3.0
1023_0107727	3.0	3.0
1023_0107729	3.0	3.0
1023_0107783	3.0	2.0
1023_0107787	2.0	3.0
1023_0108518	3.0	3.0
1023_0108520	3.0	3.0
1023_0108641	4.0	3.0
1023_0108813	3.0	3.0
1023_0108886	3.0	3.0
1023_0108888	3.0	3.0
1023_0108889	3.0	3.0
1023_0108908	3.0	3.0
1023_0108992	2.0	3.0
1023_0109022	3.0	3.0
1023_0109151	4.0	3.0
1023_0109192	3.0	3.0
1023_0109267	2.0	3.0
1023_0109496	3.0	3.0
1023_0109721	2.0	3.0
1031_0002002	2.0	3.0
1031_0002003	3.0	3.0
1031_0002040	5.0	3.0
1031_0002042	4.0	3.0
1031_0002061	3.0	3.0
1031_0002079	4.0	3.0
1031_0002198	4.0	3.0
1031_0003012	3.0	3.0
1031_0003029	3.0	3.0
1031_0003052	4.0	3.0
1031_0003054	3.0	3.0
1031_0003088	4.0	3.0
1031_0003091	3.0	3.0
1031_0003106	3.0	3.0
1031_0003126	4.0	3.0
1031_0003127	4.0	3.0
1031_0003164	3.0	3.0
1031_0003165	3.0	3.0
1031_0003170	3.0	3.0
1031_0003173	3.0	3.0
1031_0003181	4.0	3.0
1031_0003183	4.0	3.0
1031_0003186	3.0	3.0
1031_0003187	3.0	3.0
1031_0003189	4.0	3.0
1031_0003191	3.0	3.0
1031_0003203	3.0	3.0
1031_0003232	3.0	3.0
1031_0003238	3.0	3.0
1031_0003239	4.0	3.0
1031_0003242	3.0	3.0
1031_0003245	3.0	3.0
1031_0003262	3.0	3.0
1031_0003272	3.0	3.0
1031_0003330	3.0	3.0
1031_0003337	4.0	3.0
1031_0003339	3.0	3.0
1031_0003352	3.0	3.0
1031_0003359	3.0	3.0
1031_0003368	3.0	3.0
1031_0003391	2.0	3.0
1031_0003392	4.0	3.0
1031_0003407	2.0	3.0
1031_0003414	4.0	3.0
1061_0012029	2.0	2.0
1061_0120279	2.0	2.0
1061_0120284	0.0	1.0
1061_0120286	1.0	1.0
1061_0120307	2.0	2.0
1061_0120310	3.0	2.0
1061_0120312	1.0	1.0
1061_0120314	2.0	2.0
1061_0120315	2.0	1.0
1061_0120337	2.0	2.0
1061_0120345	2.0	2.0
1061_0120348	1.0	1.0
1061_0120350	2.0	2.0
1061_0120352	1.0	1.0
1061_0120357	2.0	3.0
1061_0120367	3.0	2.0
1061_0120376	2.0	2.0
1061_0120384	2.0	2.0
1061_0120386	0.0	2.0
1061_0120409	2.0	2.0
1061_0120411	4.0	3.0
1061_0120413	2.0	1.0
1061_0120415	2.0	2.0
1061_0120425	2.0	2.0
1061_0120429	3.0	2.0
1061_0120431	3.0	2.0
1061_0120438	2.0	2.0
1061_0120459	2.0	2.0
1061_0120478	2.0	2.0
1061_0120479	2.0	2.0
1061_0120483	2.0	2.0
1061_0120484	2.0	2.0
1061_0120498	2.0	3.0
1061_0120500	2.0	2.0
1061_0120853	2.0	2.0
1061_0120857	2.0	2.0
1061_0120883	2.0	2.0
1061_0120885	2.0	2.0
1061_0120886	2.0	2.0
1061_0120890	2.0	2.0
1061_1029116	1.0	2.0
1061_1029119	2.0	2.0
1061_1202911	1.0	2.0
1061_1202919	1.0	2.0
1071_0024686	2.0	2.0
1071_0024691	1.0	2.0
1071_0024693	1.0	1.0
1071_0024702	1.0	1.0
1071_0024712	1.0	1.0
1071_0024767	2.0	1.0
1071_0024769	1.0	1.0
1071_0024775	0.0	0.0
1071_0024777	1.0	1.0
1071_0024782	0.0	0.0
1071_0024784	1.0	1.0
1071_0024798	0.0	1.0
1071_0024800	0.0	1.0
1071_0024808	1.0	1.0
1071_0024820	1.0	1.0
1071_0024822	0.0	1.0
1071_0024826	2.0	2.0
1071_0024833	1.0	1.0
1071_0024838	0.0	0.0
1071_0024852	0.0	0.0
1071_0024853	0.0	0.0
1071_0024866	2.0	2.0
1071_0024867	2.0	1.0
1071_0024878	2.0	2.0
1071_0241833	1.0	1.0
1071_0242011	1.0	1.0
1071_0242071	0.0	1.0
1071_0242073	1.0	1.0
1071_0242091	0.0	1.0
1071_0243501	1.0	1.0
1071_0243502	0.0	0.0
1071_0243592	1.0	1.0
1071_0243622	0.0	0.0
1071_0243623	1.0	1.0
1071_0248302	1.0	0.0
1071_0248312	1.0	1.0
1071_0248315	0.0	0.0
1071_0248337	2.0	1.0
1071_0248341	1.0	1.0
1071_0248349	1.0	1.0
1091_0000002	2.0	2.0
1091_0000003	2.0	2.0
1091_0000012	1.0	1.0
1091_0000017	2.0	2.0
1091_0000023	2.0	1.0
1091_0000025	1.0	1.0
1091_0000028	1.0	1.0
1091_0000031	1.0	1.0
1091_0000037	1.0	1.0
1091_0000050	0.0	1.0
1091_0000067	2.0	1.0
1091_0000068	1.0	1.0
1091_0000074	2.0	1.0
1091_0000076	2.0	2.0
1091_0000079	1.0	2.0
1091_0000095	2.0	1.0
1091_0000113	1.0	2.0
1091_0000152	1.0	1.0
1091_0000163	2.0	1.0
1091_0000171	1.0	2.0
1091_0000173	2.0	1.0
1091_0000174	2.0	1.0
1091_0000192	2.0	2.0
1091_0000193	2.0	1.0
1091_0000195	1.0	1.0
1091_0000196	2.0	1.0
1091_0000199	2.0	2.0
1091_0000202	1.0	2.0
1091_0000206	1.0	1.0
1091_0000212	1.0	2.0
1091_0000216	1.0	2.0
1091_0000218	2.0	1.0
1091_0000226	1.0	2.0
1091_0000227	1.0	2.0
1091_0000231	2.0	2.0
1091_0000237	2.0	2.0
1091_0000241	1.0	1.0
1091_0000247	2.0	2.0
1091_0000254	2.0	2.0
1091_0000256	1.0	2.0
1091_0000262	2.0	2.0
LANGUAGE: DE, 2th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.38
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.10
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.46      0.87      0.60        45
         2.0       0.69      0.46      0.55        71
         3.0       0.58      0.78      0.67        54
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.56       205
   macro avg       0.29      0.35      0.30       205
weighted avg       0.49      0.56      0.50       205

[[ 0 17  0  0  0  0]
 [ 0 39  6  0  0  0]
 [ 0 26 33 12  0  0]
 [ 0  3  9 42  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
0.4994056159048985
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.03
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.95
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.47      0.76      0.58        45
         2.0       0.67      0.56      0.61        71
         3.0       0.60      0.80      0.68        54
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.57       205
   macro avg       0.29      0.35      0.31       205
weighted avg       0.49      0.57      0.52       205

[[ 0 16  1  0  0  0]
 [ 0 34 10  1  0  0]
 [ 0 21 40 10  0  0]
 [ 0  2  9 43  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
0.5177957310874515
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.88
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.01
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.44      0.44      0.44        45
         2.0       0.64      0.73      0.68        71
         3.0       0.61      0.89      0.72        54
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.59       205
   macro avg       0.28      0.34      0.31       205
weighted avg       0.48      0.59      0.52       205

[[ 0 16  1  0  0  0]
 [ 0 20 24  1  0  0]
 [ 0  7 52 12  0  0]
 [ 0  2  4 48  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
0.5246653218411882
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.80
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.02
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.48      0.51      0.49        45
         2.0       0.65      0.69      0.67        71
         3.0       0.59      0.89      0.71        54
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.59       205
   macro avg       0.29      0.35      0.31       205
weighted avg       0.49      0.59      0.53       205

[[ 0 16  1  0  0  0]
 [ 0 23 21  1  0  0]
 [ 0  7 49 15  0  0]
 [ 0  2  4 48  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
0.5269914430318435
205 205 205
Filename	True Label	Prediction
1023_0001575	3.0	3.0
1023_0101689	2.0	2.0
1023_0101690	2.0	2.0
1023_0101701	2.0	2.0
1023_0101753	3.0	3.0
1023_0101841	2.0	3.0
1023_0101851	2.0	3.0
1023_0101853	2.0	3.0
1023_0101895	4.0	3.0
1023_0101897	2.0	3.0
1023_0101898	3.0	3.0
1023_0101901	3.0	3.0
1023_0103825	2.0	3.0
1023_0103827	3.0	3.0
1023_0103828	1.0	2.0
1023_0103834	3.0	3.0
1023_0103841	3.0	3.0
1023_0103843	2.0	2.0
1023_0107682	3.0	3.0
1023_0107725	2.0	3.0
1023_0108306	4.0	3.0
1023_0108649	3.0	3.0
1023_0108751	3.0	3.0
1023_0108752	3.0	3.0
1023_0108810	3.0	3.0
1023_0108931	2.0	3.0
1023_0108993	3.0	3.0
1023_0109033	4.0	3.0
1023_0109249	3.0	3.0
1023_0109395	2.0	3.0
1023_0109400	3.0	3.0
1023_0109401	3.0	3.0
1023_0109402	2.0	3.0
1023_0109518	2.0	2.0
1023_0109520	3.0	3.0
1023_0109524	3.0	3.0
1023_0109528	3.0	3.0
1023_0109671	3.0	3.0
1023_0109890	4.0	3.0
1023_0109917	2.0	3.0
1023_0109951	2.0	3.0
1031_0001950	4.0	3.0
1031_0002043	4.0	3.0
1031_0002083	3.0	3.0
1031_0002086	3.0	3.0
1031_0002089	4.0	3.0
1031_0002184	3.0	3.0
1031_0002197	3.0	3.0
1031_0003035	4.0	3.0
1031_0003043	4.0	3.0
1031_0003065	3.0	3.0
1031_0003072	3.0	3.0
1031_0003073	4.0	3.0
1031_0003074	4.0	3.0
1031_0003077	3.0	3.0
1031_0003090	3.0	3.0
1031_0003095	3.0	3.0
1031_0003141	3.0	3.0
1031_0003145	4.0	3.0
1031_0003155	3.0	3.0
1031_0003162	3.0	3.0
1031_0003163	3.0	3.0
1031_0003166	2.0	3.0
1031_0003167	3.0	3.0
1031_0003185	3.0	3.0
1031_0003219	3.0	3.0
1031_0003221	3.0	3.0
1031_0003224	3.0	3.0
1031_0003226	3.0	3.0
1031_0003249	4.0	3.0
1031_0003273	3.0	3.0
1031_0003309	3.0	3.0
1031_0003313	4.0	3.0
1031_0003314	4.0	3.0
1031_0003331	3.0	3.0
1031_0003354	3.0	3.0
1031_0003383	4.0	3.0
1031_0003384	3.0	3.0
1031_0003389	3.0	3.0
1031_0003393	4.0	3.0
1031_0003415	5.0	3.0
1061_0120278	2.0	2.0
1061_0120281	1.0	2.0
1061_0120282	0.0	1.0
1061_0120285	1.0	2.0
1061_0120288	2.0	2.0
1061_0120290	1.0	2.0
1061_0120295	0.0	2.0
1061_0120296	2.0	2.0
1061_0120302	1.0	2.0
1061_0120306	2.0	2.0
1061_0120317	3.0	2.0
1061_0120319	3.0	2.0
1061_0120325	2.0	2.0
1061_0120326	2.0	2.0
1061_0120333	2.0	3.0
1061_0120343	2.0	2.0
1061_0120347	1.0	2.0
1061_0120351	2.0	3.0
1061_0120359	2.0	2.0
1061_0120361	2.0	2.0
1061_0120382	2.0	2.0
1061_0120388	2.0	2.0
1061_0120403	2.0	2.0
1061_0120404	2.0	2.0
1061_0120410	2.0	2.0
1061_0120414	3.0	2.0
1061_0120424	2.0	2.0
1061_0120430	2.0	2.0
1061_0120433	1.0	2.0
1061_0120440	1.0	2.0
1061_0120441	2.0	2.0
1061_0120450	2.0	2.0
1061_0120455	2.0	2.0
1061_0120481	3.0	3.0
1061_0120485	2.0	2.0
1061_0120492	2.0	2.0
1061_0120497	3.0	3.0
1061_0120876	2.0	2.0
1061_0120877	3.0	2.0
1061_0120878	1.0	2.0
1061_0120894	2.0	2.0
1061_1029111	2.0	2.0
1061_1029112	3.0	3.0
1061_1202913	2.0	2.0
1061_1202914	1.0	2.0
1061_1202917	1.0	2.0
1071_0020001	2.0	1.0
1071_0024680	2.0	2.0
1071_0024687	0.0	1.0
1071_0024688	1.0	2.0
1071_0024703	1.0	1.0
1071_0024704	1.0	1.0
1071_0024705	2.0	2.0
1071_0024708	1.0	1.0
1071_0024710	0.0	1.0
1071_0024758	1.0	2.0
1071_0024759	0.0	1.0
1071_0024762	0.0	1.0
1071_0024763	1.0	1.0
1071_0024778	0.0	1.0
1071_0024779	1.0	1.0
1071_0024783	0.0	1.0
1071_0024802	1.0	1.0
1071_0024807	0.0	1.0
1071_0024814	1.0	1.0
1071_0024818	1.0	1.0
1071_0024849	0.0	1.0
1071_0024855	1.0	1.0
1071_0024862	2.0	1.0
1071_0024875	1.0	1.0
1071_0024876	1.0	1.0
1071_0024879	1.0	1.0
1071_0242012	1.0	1.0
1071_0242021	1.0	1.0
1071_0242042	1.0	1.0
1071_0248319	0.0	1.0
1071_0248323	1.0	1.0
1071_0248326	1.0	1.0
1071_0248327	0.0	1.0
1071_0248328	0.0	1.0
1071_0248330	2.0	1.0
1071_0248332	2.0	2.0
1071_0248334	1.0	2.0
1071_0248338	2.0	1.0
1071_0248340	0.0	1.0
1091_0000004	1.0	1.0
1091_0000014	0.0	1.0
1091_0000019	2.0	2.0
1091_0000020	2.0	2.0
1091_0000027	1.0	1.0
1091_0000030	0.0	1.0
1091_0000041	1.0	1.0
1091_0000043	1.0	2.0
1091_0000062	3.0	1.0
1091_0000078	3.0	1.0
1091_0000092	1.0	2.0
1091_0000102	2.0	2.0
1091_0000127	1.0	1.0
1091_0000144	2.0	1.0
1091_0000145	1.0	1.0
1091_0000146	0.0	1.0
1091_0000154	1.0	3.0
1091_0000155	2.0	3.0
1091_0000159	2.0	2.0
1091_0000165	1.0	1.0
1091_0000185	2.0	1.0
1091_0000190	1.0	2.0
1091_0000200	2.0	2.0
1091_0000215	2.0	2.0
1091_0000220	1.0	2.0
1091_0000221	2.0	2.0
1091_0000222	2.0	2.0
1091_0000229	2.0	2.0
1091_0000230	2.0	2.0
1091_0000233	2.0	2.0
1091_0000236	2.0	2.0
1091_0000239	2.0	2.0
1091_0000242	1.0	2.0
1091_0000249	2.0	2.0
1091_0000253	2.0	1.0
1091_0000257	2.0	2.0
1091_0000268	2.0	2.0
1091_0000273	1.0	2.0
1091_0000274	1.0	2.0
LANGUAGE: DE, 3th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.44
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.06
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.50      0.71      0.59        45
         2.0       0.70      0.63      0.67        71
         3.0       0.60      0.85      0.70        54
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.60       205
   macro avg       0.30      0.37      0.33       205
weighted avg       0.51      0.60      0.54       205

[[ 0 17  0  0  0  0]
 [ 0 32 13  0  0  0]
 [ 0 13 45 13  0  0]
 [ 0  2  6 46  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
0.5447756868492419
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.02
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.86
              precision    recall  f1-score   support

         0.0       1.00      0.06      0.11        17
         1.0       0.55      0.71      0.62        45
         2.0       0.69      0.83      0.76        71
         3.0       0.64      0.72      0.68        54
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.64       205
   macro avg       0.48      0.39      0.36       205
weighted avg       0.61      0.64      0.59       205

[[ 1 16  0  0  0  0]
 [ 0 32 13  0  0  0]
 [ 0  8 59  4  0  0]
 [ 0  2 13 39  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
0.5862500931662562
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.93
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.96
              precision    recall  f1-score   support

         0.0       1.00      0.06      0.11        17
         1.0       0.57      0.67      0.61        45
         2.0       0.71      0.62      0.66        71
         3.0       0.56      0.93      0.70        54
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.61       205
   macro avg       0.47      0.38      0.35       205
weighted avg       0.60      0.61      0.56       205

[[ 1 16  0  0  0  0]
 [ 0 30 15  0  0  0]
 [ 0  6 44 21  0  0]
 [ 0  1  3 50  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
0.5569736130161584
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.84
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.81
              precision    recall  f1-score   support

         0.0       0.80      0.24      0.36        17
         1.0       0.56      0.69      0.62        45
         2.0       0.75      0.76      0.76        71
         3.0       0.64      0.87      0.74        54
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.66       205
   macro avg       0.46      0.43      0.41       205
weighted avg       0.62      0.66      0.62       205

[[ 4 13  0  0  0  0]
 [ 1 31 13  0  0  0]
 [ 0  9 54  8  0  0]
 [ 0  2  5 47  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
0.6227936572741642
205 205 205
Filename	True Label	Prediction
1023_0001419	3.0	3.0
1023_0001420	3.0	3.0
1023_0001422	3.0	3.0
1023_0001423	2.0	2.0
1023_0101683	2.0	2.0
1023_0101688	3.0	3.0
1023_0101694	3.0	2.0
1023_0101847	3.0	3.0
1023_0101849	2.0	2.0
1023_0101856	2.0	2.0
1023_0101893	3.0	3.0
1023_0101894	3.0	3.0
1023_0101899	2.0	2.0
1023_0101900	3.0	3.0
1023_0102118	2.0	3.0
1023_0103824	3.0	3.0
1023_0103839	3.0	3.0
1023_0103844	4.0	3.0
1023_0104209	3.0	3.0
1023_0108422	3.0	3.0
1023_0108510	3.0	3.0
1023_0108814	3.0	3.0
1023_0108885	3.0	3.0
1023_0108887	2.0	3.0
1023_0108890	3.0	3.0
1023_0108933	3.0	3.0
1023_0108934	2.0	3.0
1023_0108935	2.0	3.0
1023_0108955	4.0	3.0
1023_0109027	3.0	3.0
1023_0109029	2.0	2.0
1023_0109038	3.0	3.0
1023_0109096	3.0	3.0
1023_0109247	4.0	3.0
1023_0109248	2.0	3.0
1023_0109392	3.0	3.0
1023_0109396	2.0	3.0
1023_0109500	2.0	3.0
1023_0109515	3.0	3.0
1023_0109516	3.0	3.0
1023_0109588	3.0	3.0
1023_0109590	3.0	3.0
1023_0109591	2.0	3.0
1023_0109606	2.0	2.0
1023_0109609	2.0	2.0
1023_0109674	3.0	3.0
1023_0109945	3.0	3.0
1023_0109947	3.0	3.0
1031_0001997	4.0	3.0
1031_0002010	3.0	3.0
1031_0002032	3.0	3.0
1031_0002084	3.0	3.0
1031_0002185	4.0	3.0
1031_0002195	3.0	3.0
1031_0003053	4.0	3.0
1031_0003078	3.0	3.0
1031_0003098	4.0	3.0
1031_0003099	4.0	3.0
1031_0003130	4.0	3.0
1031_0003131	3.0	3.0
1031_0003132	4.0	3.0
1031_0003149	3.0	3.0
1031_0003150	3.0	3.0
1031_0003161	4.0	3.0
1031_0003179	4.0	3.0
1031_0003180	4.0	3.0
1031_0003190	3.0	3.0
1031_0003217	4.0	3.0
1031_0003237	3.0	3.0
1031_0003243	3.0	3.0
1031_0003244	4.0	3.0
1031_0003274	4.0	3.0
1031_0003310	3.0	3.0
1031_0003315	4.0	3.0
1031_0003356	3.0	3.0
1031_0003365	3.0	3.0
1031_0003367	3.0	3.0
1031_0003386	3.0	3.0
1031_0003388	3.0	3.0
1031_0003409	5.0	3.0
1061_0120272	1.0	1.0
1061_0120283	0.0	1.0
1061_0120289	2.0	2.0
1061_0120298	2.0	2.0
1061_0120299	2.0	2.0
1061_0120304	2.0	2.0
1061_0120318	2.0	2.0
1061_0120324	2.0	2.0
1061_0120327	2.0	2.0
1061_0120329	2.0	2.0
1061_0120330	3.0	2.0
1061_0120332	2.0	2.0
1061_0120334	3.0	2.0
1061_0120336	1.0	2.0
1061_0120338	1.0	2.0
1061_0120346	2.0	2.0
1061_0120353	1.0	1.0
1061_0120356	2.0	2.0
1061_0120358	1.0	2.0
1061_0120368	2.0	2.0
1061_0120369	2.0	2.0
1061_0120383	3.0	3.0
1061_0120391	1.0	2.0
1061_0120406	2.0	2.0
1061_0120421	2.0	2.0
1061_0120423	3.0	2.0
1061_0120426	2.0	2.0
1061_0120428	2.0	2.0
1061_0120439	2.0	1.0
1061_0120443	1.0	1.0
1061_0120449	2.0	2.0
1061_0120453	2.0	2.0
1061_0120482	2.0	2.0
1061_0120488	2.0	2.0
1061_0120491	2.0	2.0
1061_0120855	2.0	2.0
1061_0120856	2.0	2.0
1061_0120858	2.0	2.0
1061_0120874	1.0	2.0
1061_0120880	3.0	3.0
1061_0120889	1.0	1.0
1061_1202916	1.0	2.0
1061_1202918	2.0	2.0
1071_0024678	2.0	1.0
1071_0024683	1.0	1.0
1071_0024690	2.0	2.0
1071_0024713	1.0	1.0
1071_0024716	1.0	1.0
1071_0024757	2.0	2.0
1071_0024761	1.0	1.0
1071_0024770	1.0	1.0
1071_0024773	1.0	1.0
1071_0024799	2.0	2.0
1071_0024803	0.0	1.0
1071_0024816	1.0	1.0
1071_0024821	0.0	0.0
1071_0024824	1.0	1.0
1071_0024825	0.0	1.0
1071_0024843	0.0	1.0
1071_0024845	0.0	1.0
1071_0024846	1.0	1.0
1071_0024850	0.0	1.0
1071_0024856	1.0	1.0
1071_0024860	1.0	0.0
1071_0024863	1.0	1.0
1071_0024864	0.0	0.0
1071_0241832	0.0	1.0
1071_0242013	1.0	1.0
1071_0242023	0.0	1.0
1071_0242072	0.0	0.0
1071_0248304	0.0	1.0
1071_0248305	0.0	1.0
1071_0248307	2.0	1.0
1071_0248316	1.0	1.0
1071_0248318	0.0	0.0
1071_0248321	1.0	1.0
1071_0248324	0.0	1.0
1071_0248329	1.0	1.0
1071_0248342	1.0	1.0
1071_0248343	1.0	1.0
1071_0248345	2.0	1.0
1071_0248346	1.0	1.0
1071_0248348	1.0	1.0
1071_0248350	2.0	1.0
1091_0000007	2.0	2.0
1091_0000009	1.0	1.0
1091_0000021	2.0	2.0
1091_0000024	3.0	1.0
1091_0000032	1.0	1.0
1091_0000038	1.0	1.0
1091_0000047	2.0	1.0
1091_0000048	1.0	1.0
1091_0000049	1.0	1.0
1091_0000051	1.0	1.0
1091_0000052	0.0	1.0
1091_0000054	0.0	1.0
1091_0000056	1.0	2.0
1091_0000058	2.0	2.0
1091_0000071	2.0	2.0
1091_0000075	1.0	2.0
1091_0000114	2.0	2.0
1091_0000125	3.0	1.0
1091_0000126	2.0	1.0
1091_0000157	2.0	2.0
1091_0000164	1.0	1.0
1091_0000167	1.0	2.0
1091_0000169	2.0	2.0
1091_0000172	2.0	1.0
1091_0000204	2.0	2.0
1091_0000207	2.0	2.0
1091_0000208	1.0	2.0
1091_0000219	1.0	2.0
1091_0000224	2.0	1.0
1091_0000248	2.0	2.0
1091_0000250	2.0	2.0
1091_0000251	2.0	2.0
1091_0000258	1.0	2.0
1091_0000259	2.0	2.0
1091_0000260	3.0	2.0
1091_0000263	2.0	2.0
1091_0000264	1.0	1.0
1091_0000265	2.0	2.0
1091_0000266	2.0	2.0
1091_0000267	2.0	2.0
1091_0000272	1.0	2.0
LANGUAGE: DE, 4th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.38
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.08
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.59      0.44      0.51        45
         2.0       0.59      0.79      0.67        72
         3.0       0.58      0.81      0.68        53
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.59       205
   macro avg       0.29      0.34      0.31       205
weighted avg       0.49      0.59      0.52       205

[[ 0 12  5  0  0  0]
 [ 0 20 25  0  0  0]
 [ 0  2 57 13  0  0]
 [ 0  0 10 43  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
0.5231347377726241
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.04
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.94
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.56      0.89      0.68        45
         2.0       0.75      0.69      0.72        72
         3.0       0.62      0.77      0.69        53
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.64       205
   macro avg       0.32      0.39      0.35       205
weighted avg       0.54      0.64      0.58       205

[[ 0 17  0  0  0  0]
 [ 0 40  5  0  0  0]
 [ 0 14 50  8  0  0]
 [ 0  1 11 41  0  0]
 [ 0  0  1 16  0  0]
 [ 0  0  0  1  0  0]]
0.5809209771886402
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.91
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.83
              precision    recall  f1-score   support

         0.0       0.71      0.29      0.42        17
         1.0       0.60      0.80      0.69        45
         2.0       0.74      0.71      0.72        72
         3.0       0.62      0.81      0.70        53
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.66       205
   macro avg       0.45      0.44      0.42       205
weighted avg       0.61      0.66      0.62       205

[[ 5 12  0  0  0  0]
 [ 2 36  7  0  0  0]
 [ 0 12 51  9  0  0]
 [ 0  0 10 43  0  0]
 [ 0  0  1 16  0  0]
 [ 0  0  0  1  0  0]]
0.6213962844446822
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.82
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.82
              precision    recall  f1-score   support

         0.0       1.00      0.18      0.30        17
         1.0       0.62      0.64      0.63        45
         2.0       0.70      0.79      0.74        72
         3.0       0.63      0.87      0.73        53
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.66       205
   macro avg       0.49      0.41      0.40       205
weighted avg       0.63      0.66      0.61       205

[[ 3 12  2  0  0  0]
 [ 0 29 16  0  0  0]
 [ 0  6 57  9  0  0]
 [ 0  0  7 46  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
0.6120325815341722
205 205 205
Filename	True Label	Prediction
1023_0101684	2.0	2.0
1023_0101691	4.0	3.0
1023_0101693	4.0	3.0
1023_0101695	2.0	2.0
1023_0101700	3.0	3.0
1023_0101751	3.0	3.0
1023_0101752	3.0	3.0
1023_0101843	2.0	2.0
1023_0101852	2.0	3.0
1023_0101854	2.0	2.0
1023_0101904	2.0	2.0
1023_0101906	2.0	2.0
1023_0102117	2.0	3.0
1023_0103821	3.0	3.0
1023_0103822	2.0	2.0
1023_0103832	2.0	2.0
1023_0103837	3.0	3.0
1023_0103838	3.0	3.0
1023_0103955	4.0	3.0
1023_0104203	3.0	3.0
1023_0107726	3.0	3.0
1023_0107773	2.0	3.0
1023_0107780	3.0	3.0
1023_0107788	3.0	3.0
1023_0108304	3.0	3.0
1023_0108305	3.0	3.0
1023_0108307	3.0	3.0
1023_0108423	3.0	2.0
1023_0108426	2.0	3.0
1023_0108766	2.0	3.0
1023_0108815	3.0	3.0
1023_0108932	3.0	3.0
1023_0109026	2.0	3.0
1023_0109030	3.0	3.0
1023_0109391	2.0	3.0
1023_0109519	2.0	2.0
1023_0109649	3.0	3.0
1023_0109651	3.0	3.0
1023_0109716	3.0	3.0
1023_0109880	3.0	3.0
1023_0109914	2.0	2.0
1023_0109915	2.0	2.0
1031_0001951	3.0	3.0
1031_0001998	4.0	3.0
1031_0002005	4.0	3.0
1031_0002011	4.0	3.0
1031_0002087	4.0	3.0
1031_0002088	3.0	3.0
1031_0002131	3.0	3.0
1031_0002187	3.0	3.0
1031_0002199	3.0	3.0
1031_0002200	3.0	3.0
1031_0003063	5.0	3.0
1031_0003071	3.0	3.0
1031_0003076	4.0	3.0
1031_0003097	4.0	3.0
1031_0003128	3.0	3.0
1031_0003129	3.0	3.0
1031_0003133	4.0	3.0
1031_0003135	3.0	3.0
1031_0003140	3.0	3.0
1031_0003146	4.0	3.0
1031_0003154	3.0	3.0
1031_0003157	4.0	3.0
1031_0003160	3.0	3.0
1031_0003172	3.0	3.0
1031_0003174	4.0	3.0
1031_0003212	3.0	3.0
1031_0003214	3.0	3.0
1031_0003216	3.0	3.0
1031_0003220	3.0	3.0
1031_0003230	3.0	3.0
1031_0003231	3.0	3.0
1031_0003234	3.0	3.0
1031_0003235	4.0	3.0
1031_0003240	3.0	3.0
1031_0003327	3.0	3.0
1031_0003338	4.0	3.0
1031_0003353	2.0	3.0
1031_0003357	3.0	3.0
1031_0003358	4.0	3.0
1031_0003369	4.0	3.0
1031_0003408	2.0	3.0
1061_0120271	2.0	2.0
1061_0120273	2.0	2.0
1061_0120275	2.0	2.0
1061_0120276	2.0	2.0
1061_0120280	1.0	1.0
1061_0120297	2.0	2.0
1061_0120303	0.0	2.0
1061_0120308	3.0	2.0
1061_0120309	1.0	1.0
1061_0120311	3.0	2.0
1061_0120320	3.0	3.0
1061_0120321	2.0	2.0
1061_0120323	1.0	2.0
1061_0120328	1.0	2.0
1061_0120349	1.0	1.0
1061_0120354	1.0	2.0
1061_0120370	2.0	2.0
1061_0120371	3.0	3.0
1061_0120372	2.0	2.0
1061_0120389	2.0	2.0
1061_0120390	2.0	2.0
1061_0120405	2.0	2.0
1061_0120407	3.0	2.0
1061_0120427	2.0	2.0
1061_0120432	2.0	2.0
1061_0120442	2.0	2.0
1061_0120457	2.0	2.0
1061_0120458	3.0	2.0
1061_0120480	2.0	2.0
1061_0120486	1.0	2.0
1061_0120493	2.0	2.0
1061_0120494	2.0	2.0
1061_0120875	3.0	2.0
1061_0120888	1.0	2.0
1061_1029114	2.0	2.0
1061_1029118	1.0	2.0
1061_1029120	2.0	2.0
1061_1202912	2.0	2.0
1061_1202915	1.0	2.0
1071_0024681	2.0	2.0
1071_0024689	1.0	1.0
1071_0024694	2.0	2.0
1071_0024699	1.0	2.0
1071_0024701	2.0	2.0
1071_0024706	1.0	1.0
1071_0024709	2.0	2.0
1071_0024711	1.0	1.0
1071_0024714	2.0	2.0
1071_0024756	1.0	1.0
1071_0024776	0.0	0.0
1071_0024797	0.0	1.0
1071_0024801	1.0	1.0
1071_0024804	0.0	1.0
1071_0024809	1.0	1.0
1071_0024810	1.0	1.0
1071_0024811	1.0	1.0
1071_0024813	1.0	1.0
1071_0024815	0.0	1.0
1071_0024817	1.0	1.0
1071_0024819	1.0	2.0
1071_0024827	1.0	1.0
1071_0024831	0.0	1.0
1071_0024834	2.0	2.0
1071_0024847	1.0	1.0
1071_0024851	1.0	1.0
1071_0024854	0.0	0.0
1071_0024859	1.0	2.0
1071_0024861	0.0	1.0
1071_0024871	1.0	1.0
1071_0024872	1.0	1.0
1071_0024873	1.0	1.0
1071_0024877	1.0	1.0
1071_0242022	0.0	0.0
1071_0242041	1.0	1.0
1071_0242092	0.0	1.0
1071_0248303	0.0	1.0
1071_0248308	1.0	1.0
1071_0248310	1.0	1.0
1071_0248313	1.0	2.0
1071_0248314	1.0	1.0
1071_0248322	0.0	1.0
1071_0248325	0.0	1.0
1071_0248331	1.0	1.0
1071_0248336	0.0	1.0
1071_0248347	1.0	1.0
1091_0000001	1.0	2.0
1091_0000005	2.0	2.0
1091_0000010	3.0	2.0
1091_0000011	2.0	2.0
1091_0000013	1.0	1.0
1091_0000015	2.0	2.0
1091_0000018	2.0	2.0
1091_0000022	2.0	2.0
1091_0000026	1.0	1.0
1091_0000029	2.0	1.0
1091_0000034	2.0	1.0
1091_0000035	2.0	1.0
1091_0000036	1.0	1.0
1091_0000044	0.0	1.0
1091_0000045	2.0	2.0
1091_0000066	2.0	1.0
1091_0000069	2.0	1.0
1091_0000086	1.0	2.0
1091_0000101	2.0	2.0
1091_0000116	2.0	2.0
1091_0000151	0.0	1.0
1091_0000156	2.0	2.0
1091_0000160	2.0	2.0
1091_0000168	2.0	2.0
1091_0000191	1.0	2.0
1091_0000201	2.0	2.0
1091_0000210	2.0	1.0
1091_0000213	2.0	2.0
1091_0000228	1.0	2.0
1091_0000232	2.0	2.0
1091_0000243	0.0	2.0
1091_0000244	2.0	2.0
1091_0000245	1.0	2.0
1091_0000246	2.0	2.0
1091_0000261	2.0	2.0
1091_0000271	2.0	2.0
1091_0000275	2.0	2.0
Averaged weighted F1-scores 0.5902120059861947
130.68810916179336 82.83744971317162
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']
LANGUAGE: DE, 0th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.34
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.11
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.42      1.00      0.59        52
         2.0       0.40      0.03      0.06        59
         3.0       0.70      0.76      0.73        70
         4.0       0.00      0.00      0.00        14

    accuracy                           0.52       206
   macro avg       0.30      0.36      0.28       206
weighted avg       0.46      0.52      0.41       206

[[ 0 11  0  0  0]
 [ 0 52  0  0  0]
 [ 0 48  2  9  0]
 [ 0 14  3 53  0]
 [ 0  0  0 14  0]]
0.41292762196409394
206 206 206



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.02
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.92
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.60      0.81      0.69        52
         2.0       0.57      0.58      0.57        59
         3.0       0.70      0.76      0.73        70
         4.0       0.00      0.00      0.00        14

    accuracy                           0.63       206
   macro avg       0.37      0.43      0.40       206
weighted avg       0.55      0.63      0.58       206

[[ 0 11  0  0  0]
 [ 0 42 10  0  0]
 [ 0 16 34  9  0]
 [ 0  1 16 53  0]
 [ 0  0  0 14  0]]
0.5841722437428616
206 206 206



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.95
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.93
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.61      0.75      0.67        52
         2.0       0.56      0.53      0.54        59
         3.0       0.67      0.83      0.74        70
         4.0       0.00      0.00      0.00        14

    accuracy                           0.62       206
   macro avg       0.37      0.42      0.39       206
weighted avg       0.54      0.62      0.58       206

[[ 0 11  0  0  0]
 [ 0 39 13  0  0]
 [ 0 13 31 15  0]
 [ 0  1 11 58  0]
 [ 0  0  0 14  0]]
0.5765678726337614
206 206 206



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.87
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.91
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.61      0.73      0.67        52
         2.0       0.56      0.54      0.55        59
         3.0       0.68      0.84      0.75        70
         4.0       0.00      0.00      0.00        14

    accuracy                           0.63       206
   macro avg       0.37      0.42      0.39       206
weighted avg       0.55      0.63      0.58       206

[[ 0 10  1  0  0]
 [ 0 38 14  0  0]
 [ 0 13 32 14  0]
 [ 0  1 10 59  0]
 [ 0  0  0 14  0]]
0.5816983289939348
206 206 206
Filename	True Label	Prediction
1023_0001420	3.0	3.0
1023_0001575	3.0	3.0
1023_0101694	3.0	3.0
1023_0101700	3.0	3.0
1023_0101851	3.0	3.0
1023_0102117	3.0	3.0
1023_0103837	3.0	3.0
1023_0103838	3.0	3.0
1023_0103840	4.0	3.0
1023_0103843	3.0	2.0
1023_0103844	4.0	3.0
1023_0103883	3.0	3.0
1023_0107729	3.0	3.0
1023_0107740	3.0	3.0
1023_0107781	3.0	3.0
1023_0108304	4.0	3.0
1023_0108306	4.0	3.0
1023_0108520	3.0	3.0
1023_0108752	4.0	3.0
1023_0108931	3.0	3.0
1023_0108934	3.0	3.0
1023_0108958	3.0	3.0
1023_0108992	3.0	3.0
1023_0109033	4.0	3.0
1023_0109038	4.0	3.0
1023_0109192	3.0	3.0
1023_0109392	3.0	3.0
1023_0109400	3.0	3.0
1023_0109505	3.0	3.0
1023_0109527	3.0	3.0
1023_0109591	2.0	3.0
1023_0109651	3.0	3.0
1023_0109671	3.0	3.0
1023_0111896	3.0	3.0
1031_0001949	3.0	3.0
1031_0002006	4.0	3.0
1031_0002061	3.0	3.0
1031_0002091	3.0	3.0
1031_0002131	3.0	3.0
1031_0002195	3.0	3.0
1031_0002198	3.0	3.0
1031_0003029	3.0	3.0
1031_0003035	3.0	3.0
1031_0003126	4.0	3.0
1031_0003131	3.0	3.0
1031_0003133	4.0	3.0
1031_0003140	3.0	3.0
1031_0003145	3.0	3.0
1031_0003149	3.0	3.0
1031_0003160	3.0	3.0
1031_0003163	3.0	3.0
1031_0003170	3.0	3.0
1031_0003185	3.0	3.0
1031_0003190	3.0	3.0
1031_0003207	4.0	3.0
1031_0003211	2.0	3.0
1031_0003224	2.0	3.0
1031_0003226	3.0	3.0
1031_0003237	3.0	3.0
1031_0003239	4.0	3.0
1031_0003242	2.0	3.0
1031_0003243	3.0	3.0
1031_0003260	3.0	3.0
1031_0003310	3.0	3.0
1031_0003315	4.0	3.0
1031_0003336	3.0	3.0
1031_0003354	3.0	3.0
1031_0003356	3.0	3.0
1031_0003365	3.0	3.0
1031_0003386	2.0	3.0
1031_0003387	4.0	3.0
1031_0003407	3.0	3.0
1061_0120272	1.0	1.0
1061_0120285	2.0	2.0
1061_0120291	1.0	1.0
1061_0120307	3.0	2.0
1061_0120315	2.0	1.0
1061_0120317	2.0	3.0
1061_0120319	3.0	2.0
1061_0120326	3.0	3.0
1061_0120329	2.0	3.0
1061_0120331	1.0	1.0
1061_0120332	1.0	2.0
1061_0120351	2.0	3.0
1061_0120357	3.0	3.0
1061_0120383	2.0	3.0
1061_0120386	1.0	2.0
1061_0120390	3.0	3.0
1061_0120403	3.0	3.0
1061_0120405	3.0	2.0
1061_0120409	3.0	2.0
1061_0120413	1.0	1.0
1061_0120430	2.0	2.0
1061_0120431	3.0	2.0
1061_0120432	2.0	3.0
1061_0120443	0.0	1.0
1061_0120457	3.0	2.0
1061_0120458	3.0	3.0
1061_0120459	3.0	3.0
1061_0120479	2.0	3.0
1061_0120483	2.0	2.0
1061_0120486	2.0	2.0
1061_0120876	3.0	2.0
1061_0120884	2.0	2.0
1061_0120890	1.0	1.0
1061_1029111	3.0	3.0
1061_1029114	1.0	2.0
1061_1029115	2.0	2.0
1061_1029117	2.0	3.0
1061_1029120	2.0	2.0
1061_1202918	2.0	3.0
1071_0024682	2.0	2.0
1071_0024683	0.0	1.0
1071_0024688	2.0	2.0
1071_0024694	2.0	2.0
1071_0024701	2.0	2.0
1071_0024702	2.0	2.0
1071_0024711	2.0	1.0
1071_0024774	0.0	1.0
1071_0024775	0.0	1.0
1071_0024778	0.0	1.0
1071_0024798	1.0	1.0
1071_0024803	1.0	1.0
1071_0024818	2.0	1.0
1071_0024821	1.0	1.0
1071_0024822	0.0	1.0
1071_0024825	0.0	1.0
1071_0024827	1.0	1.0
1071_0024834	2.0	2.0
1071_0024835	1.0	1.0
1071_0024837	1.0	1.0
1071_0024838	1.0	1.0
1071_0024843	1.0	1.0
1071_0024847	2.0	2.0
1071_0024856	1.0	1.0
1071_0024861	0.0	1.0
1071_0024866	2.0	2.0
1071_0024871	1.0	1.0
1071_0024876	1.0	2.0
1071_0024877	1.0	1.0
1071_0242022	1.0	1.0
1071_0242041	1.0	1.0
1071_0242042	1.0	1.0
1071_0243501	1.0	1.0
1071_0243591	1.0	1.0
1071_0248312	1.0	1.0
1071_0248313	1.0	1.0
1071_0248329	1.0	1.0
1071_0248339	1.0	1.0
1071_0248349	1.0	1.0
1091_0000002	2.0	2.0
1091_0000003	2.0	1.0
1091_0000007	2.0	2.0
1091_0000009	0.0	1.0
1091_0000011	1.0	1.0
1091_0000013	1.0	1.0
1091_0000018	3.0	2.0
1091_0000025	1.0	1.0
1091_0000026	1.0	1.0
1091_0000027	1.0	1.0
1091_0000028	1.0	1.0
1091_0000031	1.0	1.0
1091_0000033	1.0	2.0
1091_0000034	2.0	1.0
1091_0000035	2.0	1.0
1091_0000044	0.0	2.0
1091_0000046	2.0	2.0
1091_0000054	0.0	1.0
1091_0000056	1.0	2.0
1091_0000068	1.0	1.0
1091_0000072	1.0	2.0
1091_0000077	2.0	1.0
1091_0000079	1.0	2.0
1091_0000086	1.0	1.0
1091_0000095	2.0	1.0
1091_0000113	1.0	2.0
1091_0000123	2.0	2.0
1091_0000125	3.0	2.0
1091_0000148	1.0	1.0
1091_0000151	1.0	1.0
1091_0000155	2.0	3.0
1091_0000170	2.0	1.0
1091_0000172	2.0	1.0
1091_0000192	1.0	2.0
1091_0000202	2.0	2.0
1091_0000203	1.0	2.0
1091_0000204	2.0	2.0
1091_0000209	2.0	2.0
1091_0000213	2.0	2.0
1091_0000214	2.0	1.0
1091_0000221	2.0	1.0
1091_0000222	2.0	2.0
1091_0000223	1.0	2.0
1091_0000230	2.0	2.0
1091_0000233	2.0	2.0
1091_0000236	2.0	2.0
1091_0000237	1.0	2.0
1091_0000240	1.0	1.0
1091_0000250	1.0	2.0
1091_0000251	2.0	2.0
1091_0000253	2.0	1.0
1091_0000254	3.0	1.0
1091_0000262	2.0	2.0
1091_0000268	2.0	2.0
1091_0000271	2.0	2.0
1091_0000274	2.0	2.0
LANGUAGE: DE, 1th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.29
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.02
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.49      0.87      0.62        52
         2.0       0.36      0.23      0.28        60
         3.0       0.66      0.71      0.69        69
         4.0       0.00      0.00      0.00        14

    accuracy                           0.53       205
   macro avg       0.30      0.36      0.32       205
weighted avg       0.45      0.53      0.47       205

[[ 0 10  0  0  0]
 [ 0 45  7  0  0]
 [ 0 35 14 11  0]
 [ 0  2 18 49  0]
 [ 0  0  0 14  0]]
0.47198248905565976
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.04
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.93
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.60      0.73      0.66        52
         2.0       0.64      0.48      0.55        60
         3.0       0.66      0.93      0.77        69
         4.0       0.00      0.00      0.00        14

    accuracy                           0.64       205
   macro avg       0.38      0.43      0.40       205
weighted avg       0.56      0.64      0.59       205

[[ 0 10  0  0  0]
 [ 0 38 12  2  0]
 [ 0 14 29 17  0]
 [ 0  1  4 64  0]
 [ 0  0  0 14  0]]
0.5888433844452191
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.93
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.91
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.52      0.83      0.64        52
         2.0       0.51      0.32      0.39        60
         3.0       0.66      0.83      0.74        69
         4.0       0.00      0.00      0.00        14

    accuracy                           0.58       205
   macro avg       0.34      0.39      0.35       205
weighted avg       0.51      0.58      0.53       205

[[ 0 10  0  0  0]
 [ 0 43  8  1  0]
 [ 0 27 19 14  0]
 [ 0  2 10 57  0]
 [ 0  0  0 14  0]]
0.525008175923452
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.87
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.93
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.61      0.60      0.60        52
         2.0       0.61      0.60      0.61        60
         3.0       0.67      0.93      0.78        69
         4.0       0.00      0.00      0.00        14

    accuracy                           0.64       205
   macro avg       0.38      0.42      0.40       205
weighted avg       0.56      0.64      0.59       205

[[ 0 10  0  0  0]
 [ 0 31 19  2  0]
 [ 0  9 36 15  0]
 [ 0  1  4 64  0]
 [ 0  0  0 14  0]]
0.59247390448183
205 205 205
Filename	True Label	Prediction
1023_0001419	3.0	3.0
1023_0101689	2.0	2.0
1023_0101845	2.0	3.0
1023_0101846	4.0	3.0
1023_0101849	3.0	3.0
1023_0101854	2.0	2.0
1023_0101893	3.0	3.0
1023_0101897	2.0	3.0
1023_0101900	3.0	3.0
1023_0101907	3.0	3.0
1023_0103822	2.0	2.0
1023_0103828	1.0	2.0
1023_0103829	2.0	3.0
1023_0103830	3.0	3.0
1023_0103834	3.0	3.0
1023_0103839	3.0	3.0
1023_0103841	3.0	3.0
1023_0104209	3.0	3.0
1023_0106816	3.0	3.0
1023_0107244	3.0	3.0
1023_0107672	3.0	3.0
1023_0107727	3.0	3.0
1023_0108510	3.0	3.0
1023_0108650	3.0	3.0
1023_0108766	2.0	3.0
1023_0108885	3.0	3.0
1023_0108886	3.0	3.0
1023_0108888	3.0	3.0
1023_0108908	3.0	3.0
1023_0109026	2.0	3.0
1023_0109096	3.0	3.0
1023_0109248	3.0	3.0
1023_0109267	3.0	3.0
1023_0109396	2.0	3.0
1023_0109496	3.0	3.0
1023_0109515	3.0	3.0
1023_0109522	3.0	3.0
1023_0109606	3.0	3.0
1023_0109649	3.0	3.0
1023_0109717	3.0	3.0
1023_0109880	3.0	3.0
1023_0109915	2.0	2.0
1023_0109945	3.0	3.0
1023_0109954	3.0	3.0
1031_0001998	4.0	3.0
1031_0002010	3.0	3.0
1031_0002036	4.0	3.0
1031_0002084	3.0	3.0
1031_0002085	3.0	3.0
1031_0002086	3.0	3.0
1031_0002087	4.0	3.0
1031_0002089	3.0	3.0
1031_0002197	3.0	3.0
1031_0002200	2.0	3.0
1031_0003043	4.0	3.0
1031_0003088	4.0	3.0
1031_0003098	4.0	3.0
1031_0003121	3.0	3.0
1031_0003130	4.0	3.0
1031_0003165	2.0	3.0
1031_0003169	3.0	3.0
1031_0003181	4.0	3.0
1031_0003183	4.0	3.0
1031_0003184	4.0	3.0
1031_0003218	3.0	3.0
1031_0003221	2.0	3.0
1031_0003233	3.0	3.0
1031_0003246	3.0	3.0
1031_0003249	4.0	3.0
1031_0003272	3.0	3.0
1031_0003330	3.0	3.0
1031_0003352	3.0	3.0
1031_0003353	2.0	3.0
1031_0003359	2.0	3.0
1031_0003367	3.0	3.0
1031_0003388	4.0	3.0
1031_0003390	3.0	3.0
1031_0003391	2.0	3.0
1031_0003393	3.0	3.0
1031_0003415	4.0	3.0
1061_0120271	2.0	2.0
1061_0120273	2.0	2.0
1061_0120276	3.0	2.0
1061_0120282	0.0	1.0
1061_0120289	2.0	2.0
1061_0120296	2.0	2.0
1061_0120299	2.0	2.0
1061_0120303	1.0	1.0
1061_0120316	2.0	2.0
1061_0120327	3.0	3.0
1061_0120333	3.0	3.0
1061_0120336	1.0	2.0
1061_0120337	3.0	2.0
1061_0120345	3.0	3.0
1061_0120349	1.0	1.0
1061_0120353	1.0	1.0
1061_0120358	1.0	2.0
1061_0120371	3.0	3.0
1061_0120374	3.0	3.0
1061_0120376	2.0	2.0
1061_0120384	2.0	1.0
1061_0120388	2.0	2.0
1061_0120394	2.0	3.0
1061_0120415	1.0	2.0
1061_0120423	3.0	3.0
1061_0120429	3.0	3.0
1061_0120438	2.0	3.0
1061_0120441	2.0	2.0
1061_0120448	3.0	3.0
1061_0120456	1.0	3.0
1061_0120478	3.0	3.0
1061_0120481	3.0	3.0
1061_0120488	3.0	2.0
1061_0120490	3.0	3.0
1061_0120491	2.0	2.0
1061_0120492	3.0	3.0
1061_0120494	2.0	2.0
1061_0120495	2.0	3.0
1061_0120498	3.0	3.0
1061_0120500	2.0	2.0
1061_0120874	2.0	2.0
1061_0120877	2.0	2.0
1061_0120880	3.0	3.0
1061_1029116	1.0	2.0
1061_1029118	2.0	2.0
1061_1202910	3.0	3.0
1061_1202914	1.0	1.0
1061_1202916	2.0	2.0
1071_0024678	2.0	1.0
1071_0024681	1.0	2.0
1071_0024686	2.0	2.0
1071_0024703	1.0	1.0
1071_0024704	1.0	1.0
1071_0024705	1.0	2.0
1071_0024712	1.0	2.0
1071_0024713	2.0	1.0
1071_0024714	2.0	2.0
1071_0024758	2.0	2.0
1071_0024763	1.0	1.0
1071_0024772	0.0	1.0
1071_0024781	1.0	1.0
1071_0024800	1.0	1.0
1071_0024802	1.0	2.0
1071_0024817	0.0	1.0
1071_0024826	1.0	2.0
1071_0024831	0.0	1.0
1071_0024836	2.0	2.0
1071_0024849	0.0	1.0
1071_0024851	2.0	1.0
1071_0024853	0.0	1.0
1071_0024865	2.0	2.0
1071_0024872	1.0	2.0
1071_0024873	1.0	1.0
1071_0024875	1.0	1.0
1071_0024879	1.0	1.0
1071_0241833	1.0	1.0
1071_0242021	1.0	1.0
1071_0242023	1.0	1.0
1071_0242091	1.0	1.0
1071_0242093	0.0	1.0
1071_0243502	1.0	1.0
1071_0243592	1.0	1.0
1071_0243621	2.0	2.0
1071_0248311	2.0	1.0
1071_0248314	1.0	1.0
1071_0248324	0.0	1.0
1071_0248336	1.0	1.0
1071_0248341	1.0	1.0
1071_0248344	1.0	1.0
1071_0248347	1.0	1.0
1091_0000008	3.0	2.0
1091_0000015	2.0	2.0
1091_0000029	3.0	1.0
1091_0000030	1.0	1.0
1091_0000037	1.0	1.0
1091_0000043	1.0	1.0
1091_0000049	1.0	1.0
1091_0000055	2.0	2.0
1091_0000063	1.0	1.0
1091_0000064	1.0	1.0
1091_0000067	2.0	1.0
1091_0000069	1.0	1.0
1091_0000071	2.0	2.0
1091_0000074	2.0	1.0
1091_0000075	1.0	2.0
1091_0000076	2.0	2.0
1091_0000092	1.0	2.0
1091_0000140	1.0	1.0
1091_0000146	0.0	1.0
1091_0000153	1.0	2.0
1091_0000160	1.0	3.0
1091_0000162	2.0	2.0
1091_0000185	2.0	1.0
1091_0000190	0.0	1.0
1091_0000193	2.0	1.0
1091_0000196	2.0	2.0
1091_0000197	1.0	2.0
1091_0000210	2.0	2.0
1091_0000215	1.0	2.0
1091_0000219	2.0	2.0
1091_0000220	1.0	2.0
1091_0000231	2.0	2.0
1091_0000238	1.0	2.0
1091_0000243	1.0	2.0
1091_0000275	2.0	2.0
LANGUAGE: DE, 2th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.24
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.12
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.51      0.88      0.64        51
         2.0       0.53      0.45      0.49        60
         3.0       0.69      0.65      0.67        69
         4.0       0.00      0.00      0.00        14

    accuracy                           0.57       205
   macro avg       0.35      0.40      0.36       205
weighted avg       0.51      0.57      0.53       205

[[ 0 11  0  0  0]
 [ 0 45  6  0  0]
 [ 0 27 27  6  0]
 [ 0  6 18 45  0]
 [ 0  0  0 14  0]]
0.5283814002780147
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.01
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.99
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.53      0.78      0.63        51
         2.0       0.57      0.40      0.47        60
         3.0       0.62      0.80      0.70        69
         4.0       0.00      0.00      0.00        14

    accuracy                           0.58       205
   macro avg       0.35      0.40      0.36       205
weighted avg       0.51      0.58      0.53       205

[[ 0 11  0  0  0]
 [ 0 40 10  1  0]
 [ 0 18 24 18  0]
 [ 0  6  8 55  0]
 [ 0  0  0 14  0]]
0.5315131489911484
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.91
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.98
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.60      0.71      0.65        51
         2.0       0.57      0.50      0.53        60
         3.0       0.63      0.84      0.72        69
         4.0       0.00      0.00      0.00        14

    accuracy                           0.60       205
   macro avg       0.36      0.41      0.38       205
weighted avg       0.53      0.60      0.56       205

[[ 0 10  1  0  0]
 [ 0 36 14  1  0]
 [ 0 11 30 19  0]
 [ 0  3  8 58  0]
 [ 0  0  0 14  0]]
0.5592867018293309
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.85
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.95
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.59      0.75      0.66        51
         2.0       0.58      0.55      0.56        60
         3.0       0.64      0.78      0.71        69
         4.0       0.00      0.00      0.00        14

    accuracy                           0.61       205
   macro avg       0.36      0.42      0.39       205
weighted avg       0.53      0.61      0.57       205

[[ 0 10  1  0  0]
 [ 0 38 12  1  0]
 [ 0 12 33 15  0]
 [ 0  4 11 54  0]
 [ 0  0  0 14  0]]
0.5671043123179609
205 205 205
Filename	True Label	Prediction
1023_0001416	3.0	3.0
1023_0001423	2.0	3.0
1023_0101684	2.0	2.0
1023_0101843	2.0	3.0
1023_0101895	3.0	3.0
1023_0101896	2.0	2.0
1023_0101901	3.0	3.0
1023_0101904	2.0	3.0
1023_0102118	3.0	3.0
1023_0103823	3.0	3.0
1023_0103824	3.0	3.0
1023_0103826	3.0	3.0
1023_0103955	3.0	3.0
1023_0107074	4.0	3.0
1023_0107682	2.0	1.0
1023_0107726	2.0	3.0
1023_0107780	3.0	3.0
1023_0107784	2.0	2.0
1023_0108648	3.0	3.0
1023_0108649	4.0	3.0
1023_0108810	3.0	3.0
1023_0108814	3.0	3.0
1023_0108889	3.0	3.0
1023_0108933	3.0	3.0
1023_0108935	3.0	3.0
1023_0108955	4.0	3.0
1023_0109029	2.0	2.0
1023_0109039	3.0	3.0
1023_0109247	4.0	3.0
1023_0109391	3.0	3.0
1023_0109399	2.0	3.0
1023_0109422	3.0	3.0
1023_0109495	3.0	3.0
1023_0109500	2.0	3.0
1023_0109518	2.0	2.0
1023_0109590	2.0	3.0
1023_0109609	2.0	3.0
1023_0109674	3.0	3.0
1023_0109716	3.0	3.0
1023_0109878	3.0	3.0
1023_0109917	3.0	3.0
1023_0109946	2.0	3.0
1031_0001950	3.0	3.0
1031_0002011	3.0	3.0
1031_0002040	4.0	3.0
1031_0002043	4.0	3.0
1031_0002079	4.0	3.0
1031_0002184	3.0	3.0
1031_0002185	3.0	3.0
1031_0002199	4.0	3.0
1031_0003012	4.0	3.0
1031_0003013	4.0	3.0
1031_0003071	3.0	3.0
1031_0003074	3.0	3.0
1031_0003076	3.0	3.0
1031_0003077	3.0	3.0
1031_0003097	3.0	3.0
1031_0003106	3.0	3.0
1031_0003128	3.0	3.0
1031_0003129	3.0	3.0
1031_0003132	3.0	3.0
1031_0003141	3.0	3.0
1031_0003150	3.0	3.0
1031_0003162	4.0	3.0
1031_0003167	3.0	3.0
1031_0003173	3.0	3.0
1031_0003179	3.0	3.0
1031_0003182	4.0	3.0
1031_0003206	3.0	3.0
1031_0003212	2.0	3.0
1031_0003214	3.0	3.0
1031_0003217	4.0	3.0
1031_0003219	3.0	3.0
1031_0003220	3.0	3.0
1031_0003234	2.0	3.0
1031_0003238	3.0	3.0
1031_0003337	3.0	3.0
1031_0003339	3.0	3.0
1031_0003389	3.0	3.0
1031_0003409	4.0	3.0
1061_0012029	2.0	2.0
1061_0120275	3.0	2.0
1061_0120281	1.0	2.0
1061_0120283	1.0	1.0
1061_0120284	0.0	1.0
1061_0120286	1.0	1.0
1061_0120288	2.0	2.0
1061_0120290	1.0	2.0
1061_0120297	2.0	2.0
1061_0120300	2.0	2.0
1061_0120306	3.0	2.0
1061_0120308	2.0	2.0
1061_0120312	1.0	1.0
1061_0120314	2.0	2.0
1061_0120320	3.0	3.0
1061_0120325	3.0	2.0
1061_0120334	3.0	3.0
1061_0120346	2.0	2.0
1061_0120347	2.0	2.0
1061_0120350	3.0	3.0
1061_0120352	1.0	1.0
1061_0120356	2.0	2.0
1061_0120359	1.0	2.0
1061_0120361	2.0	3.0
1061_0120366	3.0	2.0
1061_0120369	2.0	2.0
1061_0120382	2.0	2.0
1061_0120387	2.0	2.0
1061_0120404	2.0	2.0
1061_0120407	3.0	2.0
1061_0120425	3.0	2.0
1061_0120426	2.0	3.0
1061_0120427	2.0	2.0
1061_0120428	2.0	3.0
1061_0120439	1.0	2.0
1061_0120440	1.0	1.0
1061_0120450	1.0	2.0
1061_0120480	2.0	2.0
1061_0120487	3.0	3.0
1061_0120489	3.0	3.0
1061_0120499	2.0	3.0
1061_0120887	2.0	2.0
1061_1029119	1.0	2.0
1061_1202912	3.0	2.0
1061_1202917	2.0	2.0
1061_1202919	2.0	1.0
1071_0020001	2.0	1.0
1071_0024687	0.0	1.0
1071_0024690	2.0	2.0
1071_0024708	2.0	1.0
1071_0024709	3.0	2.0
1071_0024716	1.0	1.0
1071_0024759	0.0	1.0
1071_0024761	1.0	1.0
1071_0024762	1.0	1.0
1071_0024766	1.0	1.0
1071_0024768	1.0	1.0
1071_0024769	1.0	1.0
1071_0024779	1.0	1.0
1071_0024784	1.0	1.0
1071_0024804	1.0	1.0
1071_0024813	0.0	1.0
1071_0024815	1.0	1.0
1071_0024820	0.0	1.0
1071_0024840	1.0	1.0
1071_0024845	1.0	1.0
1071_0024846	1.0	1.0
1071_0024855	1.0	1.0
1071_0024860	0.0	1.0
1071_0024864	0.0	1.0
1071_0024878	2.0	2.0
1071_0024881	2.0	1.0
1071_0241832	1.0	1.0
1071_0242011	2.0	1.0
1071_0243581	1.0	1.0
1071_0243623	1.0	1.0
1071_0248302	1.0	1.0
1071_0248303	1.0	1.0
1071_0248307	2.0	1.0
1071_0248310	1.0	1.0
1071_0248322	1.0	1.0
1071_0248323	1.0	1.0
1071_0248331	1.0	1.0
1071_0248334	2.0	2.0
1071_0248340	0.0	1.0
1071_0248343	1.0	1.0
1071_0248346	0.0	1.0
1091_0000006	0.0	1.0
1091_0000012	1.0	1.0
1091_0000019	1.0	1.0
1091_0000021	1.0	2.0
1091_0000032	1.0	1.0
1091_0000042	1.0	1.0
1091_0000045	2.0	2.0
1091_0000051	1.0	1.0
1091_0000053	1.0	1.0
1091_0000058	2.0	2.0
1091_0000066	2.0	1.0
1091_0000070	2.0	1.0
1091_0000073	3.0	1.0
1091_0000078	3.0	1.0
1091_0000101	1.0	1.0
1091_0000126	2.0	2.0
1091_0000127	2.0	1.0
1091_0000145	1.0	1.0
1091_0000154	1.0	3.0
1091_0000156	3.0	2.0
1091_0000163	2.0	1.0
1091_0000168	1.0	2.0
1091_0000171	1.0	2.0
1091_0000174	1.0	1.0
1091_0000194	2.0	2.0
1091_0000198	2.0	2.0
1091_0000216	1.0	2.0
1091_0000225	3.0	1.0
1091_0000227	0.0	2.0
1091_0000232	2.0	2.0
1091_0000234	3.0	2.0
1091_0000239	3.0	1.0
1091_0000241	2.0	1.0
1091_0000248	3.0	2.0
1091_0000259	2.0	2.0
1091_0000266	1.0	2.0
1091_0000267	2.0	2.0
1091_0000272	1.0	2.0
LANGUAGE: DE, 3th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.29
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.04
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.58      0.82      0.68        51
         2.0       0.58      0.25      0.35        60
         3.0       0.58      0.90      0.70        69
         4.0       0.00      0.00      0.00        14

    accuracy                           0.58       205
   macro avg       0.35      0.39      0.35       205
weighted avg       0.51      0.58      0.51       205

[[ 0 10  1  0  0]
 [ 0 42  7  2  0]
 [ 0 16 15 29  0]
 [ 0  4  3 62  0]
 [ 0  0  0 14  0]]
0.5091372547046773
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.02
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.96
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.61      0.73      0.66        51
         2.0       0.56      0.48      0.52        60
         3.0       0.63      0.84      0.72        69
         4.0       0.00      0.00      0.00        14

    accuracy                           0.60       205
   macro avg       0.36      0.41      0.38       205
weighted avg       0.53      0.60      0.56       205

[[ 0  9  2  0  0]
 [ 0 37 14  0  0]
 [ 0 11 29 20  0]
 [ 0  4  7 58  0]
 [ 0  0  0 14  0]]
0.5584494773519164
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.94
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.93
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.60      0.82      0.69        51
         2.0       0.57      0.55      0.56        60
         3.0       0.65      0.72      0.68        69
         4.0       0.00      0.00      0.00        14

    accuracy                           0.61       205
   macro avg       0.36      0.42      0.39       205
weighted avg       0.53      0.61      0.57       205

[[ 0 10  1  0  0]
 [ 0 42  9  0  0]
 [ 0 14 33 13  0]
 [ 0  4 15 50  0]
 [ 0  0  0 14  0]]
0.5669490472399367
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.86
  Training epoch took: 31
Running Validation...
  Average evaluation loss: 0.93
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.65      0.80      0.72        51
         2.0       0.61      0.55      0.58        60
         3.0       0.65      0.83      0.73        69
         4.0       0.00      0.00      0.00        14

    accuracy                           0.64       205
   macro avg       0.38      0.44      0.40       205
weighted avg       0.56      0.64      0.59       205

[[ 0  9  2  0  0]
 [ 0 41 10  0  0]
 [ 0 10 33 17  0]
 [ 0  3  9 57  0]
 [ 0  0  0 14  0]]
0.5927949437053874
205 205 205
Filename	True Label	Prediction
1023_0001418	3.0	3.0
1023_0101675	3.0	3.0
1023_0101683	3.0	3.0
1023_0101691	3.0	3.0
1023_0101752	3.0	3.0
1023_0101753	3.0	3.0
1023_0101847	3.0	3.0
1023_0101855	2.0	3.0
1023_0101898	3.0	3.0
1023_0101899	3.0	3.0
1023_0101909	4.0	3.0
1023_0103825	2.0	3.0
1023_0103827	3.0	3.0
1023_0103832	2.0	3.0
1023_0103833	3.0	3.0
1023_0103836	2.0	3.0
1023_0104203	3.0	3.0
1023_0104206	2.0	3.0
1023_0107725	3.0	3.0
1023_0107788	3.0	3.0
1023_0108305	3.0	3.0
1023_0108307	2.0	3.0
1023_0108426	2.0	3.0
1023_0108641	4.0	3.0
1023_0108812	2.0	3.0
1023_0108815	3.0	3.0
1023_0108932	3.0	3.0
1023_0108993	4.0	3.0
1023_0109022	3.0	3.0
1023_0109151	4.0	3.0
1023_0109402	3.0	3.0
1023_0109520	3.0	3.0
1023_0109524	3.0	3.0
1023_0109528	3.0	3.0
1023_0109614	2.0	3.0
1023_0109890	4.0	3.0
1023_0109891	3.0	3.0
1031_0002004	4.0	3.0
1031_0002083	3.0	3.0
1031_0002088	3.0	3.0
1031_0002092	3.0	3.0
1031_0002187	3.0	3.0
1031_0003023	3.0	3.0
1031_0003042	3.0	3.0
1031_0003053	3.0	3.0
1031_0003065	3.0	3.0
1031_0003072	3.0	3.0
1031_0003073	4.0	3.0
1031_0003091	2.0	3.0
1031_0003144	3.0	3.0
1031_0003146	4.0	3.0
1031_0003155	3.0	3.0
1031_0003156	3.0	3.0
1031_0003157	4.0	3.0
1031_0003161	3.0	3.0
1031_0003166	2.0	3.0
1031_0003174	4.0	3.0
1031_0003186	4.0	3.0
1031_0003187	3.0	3.0
1031_0003189	4.0	3.0
1031_0003191	3.0	3.0
1031_0003230	3.0	3.0
1031_0003245	3.0	3.0
1031_0003261	3.0	3.0
1031_0003313	3.0	3.0
1031_0003327	2.0	3.0
1031_0003331	3.0	3.0
1031_0003338	3.0	3.0
1031_0003355	3.0	3.0
1031_0003357	3.0	3.0
1031_0003358	4.0	3.0
1031_0003368	3.0	3.0
1031_0003383	3.0	3.0
1031_0003392	4.0	3.0
1031_0003410	3.0	3.0
1031_0003419	3.0	3.0
1061_0120277	1.0	2.0
1061_0120278	2.0	2.0
1061_0120280	1.0	1.0
1061_0120295	0.0	2.0
1061_0120301	2.0	2.0
1061_0120309	1.0	1.0
1061_0120310	2.0	2.0
1061_0120313	2.0	1.0
1061_0120318	2.0	3.0
1061_0120321	2.0	2.0
1061_0120323	1.0	2.0
1061_0120324	3.0	2.0
1061_0120328	1.0	1.0
1061_0120330	3.0	3.0
1061_0120335	3.0	3.0
1061_0120341	1.0	1.0
1061_0120343	3.0	2.0
1061_0120348	1.0	1.0
1061_0120367	3.0	2.0
1061_0120370	2.0	3.0
1061_0120372	2.0	2.0
1061_0120373	2.0	2.0
1061_0120375	2.0	1.0
1061_0120389	2.0	2.0
1061_0120391	1.0	1.0
1061_0120408	3.0	2.0
1061_0120410	2.0	2.0
1061_0120411	3.0	3.0
1061_0120424	2.0	3.0
1061_0120453	2.0	2.0
1061_0120482	2.0	2.0
1061_0120484	2.0	3.0
1061_0120485	3.0	3.0
1061_0120493	2.0	2.0
1061_0120497	3.0	3.0
1061_0120855	1.0	2.0
1061_0120856	2.0	2.0
1061_0120857	2.0	2.0
1061_0120858	2.0	2.0
1061_0120875	3.0	3.0
1061_0120881	2.0	3.0
1061_0120882	3.0	3.0
1061_0120883	2.0	2.0
1061_0120885	3.0	2.0
1061_1202911	1.0	2.0
1061_1202913	1.0	1.0
1061_1202915	1.0	1.0
1071_0024680	2.0	2.0
1071_0024685	2.0	2.0
1071_0024691	2.0	2.0
1071_0024692	3.0	2.0
1071_0024706	1.0	1.0
1071_0024710	1.0	1.0
1071_0024715	2.0	2.0
1071_0024756	1.0	1.0
1071_0024773	1.0	1.0
1071_0024806	1.0	1.0
1071_0024808	1.0	1.0
1071_0024809	1.0	1.0
1071_0024810	1.0	1.0
1071_0024811	1.0	1.0
1071_0024816	1.0	1.0
1071_0024833	2.0	1.0
1071_0024841	1.0	1.0
1071_0024844	1.0	1.0
1071_0024854	1.0	1.0
1071_0024857	1.0	1.0
1071_0024863	2.0	1.0
1071_0024874	1.0	1.0
1071_0242012	2.0	2.0
1071_0242043	0.0	1.0
1071_0242071	0.0	1.0
1071_0242072	0.0	1.0
1071_0243593	1.0	1.0
1071_0243622	1.0	1.0
1071_0248301	1.0	1.0
1071_0248304	1.0	1.0
1071_0248305	0.0	1.0
1071_0248308	1.0	1.0
1071_0248318	0.0	1.0
1071_0248320	0.0	1.0
1071_0248325	1.0	1.0
1071_0248326	1.0	1.0
1071_0248327	0.0	1.0
1071_0248328	0.0	1.0
1071_0248332	2.0	2.0
1071_0248333	2.0	1.0
1071_0248335	1.0	1.0
1071_0248337	1.0	1.0
1071_0248345	2.0	2.0
1091_0000004	1.0	1.0
1091_0000010	3.0	1.0
1091_0000016	1.0	1.0
1091_0000017	3.0	2.0
1091_0000023	3.0	1.0
1091_0000041	0.0	1.0
1091_0000047	2.0	1.0
1091_0000059	1.0	2.0
1091_0000062	3.0	1.0
1091_0000065	1.0	1.0
1091_0000102	2.0	1.0
1091_0000116	2.0	2.0
1091_0000144	1.0	1.0
1091_0000152	1.0	1.0
1091_0000157	3.0	2.0
1091_0000158	1.0	2.0
1091_0000159	2.0	2.0
1091_0000161	2.0	2.0
1091_0000164	1.0	1.0
1091_0000166	1.0	1.0
1091_0000169	2.0	2.0
1091_0000173	2.0	1.0
1091_0000195	1.0	1.0
1091_0000205	1.0	2.0
1091_0000207	2.0	2.0
1091_0000212	2.0	2.0
1091_0000217	2.0	1.0
1091_0000224	2.0	1.0
1091_0000229	1.0	2.0
1091_0000235	1.0	1.0
1091_0000245	1.0	2.0
1091_0000246	2.0	2.0
1091_0000249	2.0	2.0
1091_0000256	0.0	2.0
1091_0000258	2.0	2.0
1091_0000260	2.0	2.0
1091_0000261	2.0	2.0
1091_0000265	3.0	2.0
1091_0000269	1.0	2.0
LANGUAGE: DE, 4th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.35
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.08
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.50      0.96      0.65        52
         2.0       0.54      0.12      0.19        60
         3.0       0.63      0.83      0.71        69
         4.0       0.00      0.00      0.00        13

    accuracy                           0.56       205
   macro avg       0.33      0.38      0.31       205
weighted avg       0.49      0.56      0.46       205

[[ 0 11  0  0  0]
 [ 0 50  2  0  0]
 [ 0 32  7 21  0]
 [ 0  8  4 57  0]
 [ 0  0  0 13  0]]
0.4617379386324081
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.08
  Training epoch took: 31
Running Validation...
  Average evaluation loss: 1.07
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.57      0.87      0.69        52
         2.0       0.52      0.23      0.32        60
         3.0       0.60      0.86      0.70        69
         4.0       0.00      0.00      0.00        13

    accuracy                           0.58       205
   macro avg       0.34      0.39      0.34       205
weighted avg       0.50      0.58      0.50       205

[[ 0 11  0  0  0]
 [ 0 45  7  0  0]
 [ 0 19 14 27  0]
 [ 0  4  6 59  0]
 [ 0  0  0 13  0]]
0.5048771774710565
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.97
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.01
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.59      0.69      0.64        52
         2.0       0.47      0.37      0.41        60
         3.0       0.60      0.84      0.70        69
         4.0       0.00      0.00      0.00        13

    accuracy                           0.57       205
   macro avg       0.33      0.38      0.35       205
weighted avg       0.49      0.57      0.52       205

[[ 0 11  0  0  0]
 [ 0 36 16  0  0]
 [ 0 12 22 26  0]
 [ 0  2  9 58  0]
 [ 0  0  0 13  0]]
0.5171829659949906
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.88
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.58      0.63      0.61        52
         2.0       0.44      0.37      0.40        60
         3.0       0.60      0.86      0.71        69
         4.0       0.00      0.00      0.00        13

    accuracy                           0.56       205
   macro avg       0.32      0.37      0.34       205
weighted avg       0.48      0.56      0.51       205

[[ 0 10  1  0  0]
 [ 0 33 19  0  0]
 [ 0 12 22 26  0]
 [ 0  2  8 59  0]
 [ 0  0  0 13  0]]
0.5084913636588982
205 205 205
Filename	True Label	Prediction
1023_0001422	2.0	3.0
1023_0101688	3.0	3.0
1023_0101690	2.0	3.0
1023_0101693	4.0	3.0
1023_0101695	2.0	2.0
1023_0101701	2.0	3.0
1023_0101749	3.0	3.0
1023_0101751	3.0	3.0
1023_0101841	3.0	3.0
1023_0101844	2.0	3.0
1023_0101848	2.0	2.0
1023_0101852	3.0	3.0
1023_0101853	2.0	3.0
1023_0101856	2.0	3.0
1023_0101894	2.0	3.0
1023_0101906	2.0	3.0
1023_0103821	3.0	3.0
1023_0103831	3.0	3.0
1023_0103880	3.0	3.0
1023_0104207	3.0	3.0
1023_0107042	3.0	3.0
1023_0107075	3.0	3.0
1023_0107773	3.0	3.0
1023_0107783	3.0	3.0
1023_0107787	3.0	3.0
1023_0108422	4.0	3.0
1023_0108423	3.0	2.0
1023_0108518	3.0	3.0
1023_0108751	3.0	3.0
1023_0108753	2.0	3.0
1023_0108811	4.0	3.0
1023_0108813	2.0	3.0
1023_0108887	2.0	3.0
1023_0108890	3.0	3.0
1023_0109027	3.0	3.0
1023_0109030	3.0	3.0
1023_0109249	3.0	3.0
1023_0109250	2.0	3.0
1023_0109395	2.0	3.0
1023_0109401	3.0	3.0
1023_0109516	3.0	3.0
1023_0109519	2.0	3.0
1023_0109588	2.0	3.0
1023_0109721	2.0	3.0
1023_0109914	3.0	3.0
1023_0109947	3.0	3.0
1023_0109951	3.0	3.0
1031_0001703	3.0	3.0
1031_0001951	2.0	3.0
1031_0001997	3.0	3.0
1031_0002002	2.0	3.0
1031_0002003	3.0	3.0
1031_0002005	4.0	3.0
1031_0002032	3.0	3.0
1031_0002042	3.0	3.0
1031_0002196	3.0	3.0
1031_0003048	4.0	3.0
1031_0003052	3.0	3.0
1031_0003054	3.0	3.0
1031_0003063	4.0	3.0
1031_0003078	3.0	3.0
1031_0003085	3.0	3.0
1031_0003090	4.0	3.0
1031_0003092	2.0	3.0
1031_0003095	3.0	3.0
1031_0003099	3.0	3.0
1031_0003127	4.0	3.0
1031_0003135	3.0	3.0
1031_0003136	3.0	3.0
1031_0003154	3.0	3.0
1031_0003164	4.0	3.0
1031_0003172	3.0	3.0
1031_0003180	4.0	3.0
1031_0003203	2.0	3.0
1031_0003205	3.0	3.0
1031_0003216	2.0	3.0
1031_0003225	3.0	3.0
1031_0003231	3.0	3.0
1031_0003232	2.0	3.0
1031_0003235	3.0	3.0
1031_0003236	3.0	3.0
1031_0003240	3.0	3.0
1031_0003244	4.0	3.0
1031_0003262	3.0	3.0
1031_0003273	3.0	3.0
1031_0003274	3.0	3.0
1031_0003309	3.0	3.0
1031_0003314	4.0	3.0
1031_0003366	3.0	3.0
1031_0003369	4.0	3.0
1031_0003384	3.0	3.0
1031_0003408	3.0	3.0
1031_0003414	3.0	3.0
1061_0120274	2.0	1.0
1061_0120279	1.0	2.0
1061_0120287	1.0	2.0
1061_0120298	2.0	2.0
1061_0120302	1.0	2.0
1061_0120304	2.0	2.0
1061_0120311	2.0	2.0
1061_0120338	1.0	2.0
1061_0120354	2.0	2.0
1061_0120355	1.0	1.0
1061_0120360	3.0	3.0
1061_0120368	2.0	2.0
1061_0120406	2.0	3.0
1061_0120414	3.0	2.0
1061_0120421	2.0	3.0
1061_0120433	1.0	2.0
1061_0120442	3.0	3.0
1061_0120449	3.0	2.0
1061_0120455	2.0	2.0
1061_0120460	2.0	2.0
1061_0120496	2.0	2.0
1061_0120853	2.0	3.0
1061_0120859	3.0	3.0
1061_0120878	2.0	1.0
1061_0120886	2.0	2.0
1061_0120888	2.0	2.0
1061_0120889	1.0	1.0
1061_0120894	2.0	3.0
1061_1029112	3.0	3.0
1061_1029113	1.0	2.0
1071_0024689	2.0	1.0
1071_0024693	1.0	1.0
1071_0024699	1.0	1.0
1071_0024757	2.0	2.0
1071_0024765	1.0	1.0
1071_0024767	2.0	1.0
1071_0024770	1.0	1.0
1071_0024776	0.0	1.0
1071_0024777	1.0	2.0
1071_0024782	0.0	1.0
1071_0024783	0.0	1.0
1071_0024797	0.0	1.0
1071_0024799	3.0	2.0
1071_0024801	1.0	1.0
1071_0024807	1.0	1.0
1071_0024812	1.0	1.0
1071_0024814	0.0	1.0
1071_0024819	1.0	2.0
1071_0024823	1.0	1.0
1071_0024824	1.0	1.0
1071_0024848	1.0	1.0
1071_0024850	1.0	1.0
1071_0024852	0.0	1.0
1071_0024859	1.0	2.0
1071_0024862	1.0	1.0
1071_0024867	2.0	1.0
1071_0241831	1.0	1.0
1071_0242013	1.0	1.0
1071_0242073	1.0	1.0
1071_0242092	0.0	1.0
1071_0243582	1.0	1.0
1071_0248309	2.0	1.0
1071_0248315	0.0	1.0
1071_0248316	1.0	1.0
1071_0248317	0.0	1.0
1071_0248319	0.0	1.0
1071_0248321	2.0	1.0
1071_0248330	1.0	1.0
1071_0248338	2.0	1.0
1071_0248342	1.0	1.0
1071_0248348	1.0	1.0
1071_0248350	1.0	1.0
1091_0000001	1.0	1.0
1091_0000005	3.0	2.0
1091_0000014	1.0	1.0
1091_0000020	1.0	2.0
1091_0000022	2.0	2.0
1091_0000024	3.0	1.0
1091_0000036	1.0	2.0
1091_0000038	2.0	1.0
1091_0000039	1.0	1.0
1091_0000048	1.0	1.0
1091_0000050	1.0	1.0
1091_0000052	1.0	1.0
1091_0000057	2.0	1.0
1091_0000060	3.0	2.0
1091_0000061	2.0	1.0
1091_0000087	2.0	2.0
1091_0000114	1.0	2.0
1091_0000165	2.0	1.0
1091_0000167	1.0	2.0
1091_0000191	1.0	2.0
1091_0000199	2.0	2.0
1091_0000200	2.0	2.0
1091_0000201	2.0	2.0
1091_0000206	1.0	1.0
1091_0000208	1.0	2.0
1091_0000211	1.0	2.0
1091_0000218	3.0	1.0
1091_0000226	1.0	1.0
1091_0000228	1.0	2.0
1091_0000242	1.0	1.0
1091_0000244	2.0	2.0
1091_0000247	2.0	2.0
1091_0000252	2.0	2.0
1091_0000255	0.0	2.0
1091_0000257	1.0	2.0
1091_0000263	3.0	2.0
1091_0000264	1.0	1.0
1091_0000270	3.0	2.0
1091_0000273	1.0	2.0
1091_0000276	2.0	2.0
Averaged weighted F1-scores 0.5685125706316023
MONOLINGUAL Experiments with:  CZ
144.90552995391704 65.35717405024758
LABEL SET ['A2', 'B1', 'B2']
LANGUAGE: CZ, 0th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.97
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.78
              precision    recall  f1-score   support

         0.0       0.74      0.97      0.84        38
         1.0       0.54      0.61      0.57        33
         2.0       0.00      0.00      0.00        16

    accuracy                           0.66        87
   macro avg       0.43      0.53      0.47        87
weighted avg       0.53      0.66      0.58        87

[[37  1  0]
 [13 20  0]
 [ 0 16  0]]
0.584042394387222
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.80
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.63
              precision    recall  f1-score   support

         0.0       0.84      0.82      0.83        38
         1.0       0.54      0.82      0.65        33
         2.0       0.00      0.00      0.00        16

    accuracy                           0.67        87
   macro avg       0.46      0.54      0.49        87
weighted avg       0.57      0.67      0.61        87

[[31  7  0]
 [ 6 27  0]
 [ 0 16  0]]
0.6078530212805243
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.61
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.53
              precision    recall  f1-score   support

         0.0       0.90      0.68      0.78        38
         1.0       0.60      0.73      0.66        33
         2.0       0.67      0.75      0.71        16

    accuracy                           0.71        87
   macro avg       0.72      0.72      0.71        87
weighted avg       0.74      0.71      0.72        87

[[26 12  0]
 [ 3 24  6]
 [ 0  4 12]]
0.7182216677871031
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.49
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.52
              precision    recall  f1-score   support

         0.0       0.79      0.89      0.84        38
         1.0       0.78      0.42      0.55        33
         2.0       0.62      1.00      0.76        16

    accuracy                           0.74        87
   macro avg       0.73      0.77      0.72        87
weighted avg       0.75      0.74      0.72        87

[[34  4  0]
 [ 9 14 10]
 [ 0  0 16]]
0.7150500898528845
87 87 87
Filename	True Label	Prediction
0607	1.0	1.0
0614	1.0	1.0
0615	0.0	0.0
0616	0.0	0.0
0619	1.0	0.0
0633	1.0	1.0
0634	1.0	1.0
0635	0.0	0.0
0641	0.0	1.0
0720	0.0	0.0
0722	1.0	0.0
0801	0.0	1.0
0811	1.0	0.0
0822	0.0	0.0
0827	0.0	0.0
0829	0.0	1.0
0901	1.0	1.0
0902	1.0	0.0
0915	1.0	1.0
0918	0.0	0.0
0929	0.0	0.0
1005	0.0	0.0
1006	1.0	0.0
1007	1.0	0.0
1016	0.0	0.0
1021	0.0	1.0
1022	1.0	0.0
KYJ0611004A	0.0	0.0
KYJ0611005B	0.0	0.0
KYJ0611009A	0.0	0.0
LIB0611003A	0.0	0.0
LON0610002A	0.0	0.0
LON0611003	2.0	2.0
MOS0509004	1.0	1.0
MOS0611013	1.0	2.0
PAR1011009A	0.0	0.0
PAR1011009B	0.0	0.0
PAR1011017	2.0	2.0
PHA0111003A	0.0	0.0
PHA0112006B	1.0	0.0
PHA0112012A	0.0	0.0
PHA0112012B	0.0	0.0
PHA0209001	0.0	0.0
PHA0209013	0.0	0.0
PHA0411009B	0.0	0.0
PHA0411012B	0.0	0.0
PHA0411028	1.0	1.0
PHA0411029	1.0	1.0
PHA0411030	2.0	2.0
PHA0411033	1.0	1.0
PHA0411036	2.0	2.0
PHA0411041	2.0	2.0
PHA0411042	1.0	2.0
PHA0411055	2.0	2.0
PHA0411062	1.0	2.0
PHA0509021	1.0	1.0
PHA0509034	1.0	2.0
PHA0509035	1.0	2.0
PHA0509043	2.0	2.0
PHA0510003A	0.0	0.0
PHA0510010B	0.0	0.0
PHA0510032	2.0	2.0
PHA0510036	2.0	2.0
PHA0510047	1.0	1.0
PHA0510049	1.0	2.0
PHA0510050	1.0	2.0
PHA0610005B	0.0	0.0
PHA0610006A	0.0	0.0
PHA0610006B	0.0	0.0
PHA0610016	2.0	2.0
PHA0710012	2.0	2.0
PHA0710017	2.0	2.0
PHA0810003	1.0	2.0
PHA0810010	1.0	2.0
PHA0811014	1.0	1.0
PHA0811019	2.0	2.0
PHA1109001	0.0	0.0
PHA1109004	2.0	2.0
PHA1109008	0.0	0.0
PHA1109026	2.0	2.0
PHA1110002A	1.0	0.0
PHA1110015	2.0	2.0
PHA1111001B	0.0	0.0
PHA1111003B	0.0	0.0
PHA1111004B	0.0	0.0
VAR0910005	1.0	1.0
VAR0910006	1.0	2.0
LANGUAGE: CZ, 1th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.99
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.82
              precision    recall  f1-score   support

         0.0       0.77      0.95      0.85        38
         1.0       0.55      0.67      0.60        33
         2.0       0.00      0.00      0.00        16

    accuracy                           0.67        87
   macro avg       0.44      0.54      0.48        87
weighted avg       0.54      0.67      0.60        87

[[36  2  0]
 [11 22  0]
 [ 0 16  0]]
0.5986051293450775
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.81
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.75
              precision    recall  f1-score   support

         0.0       0.64      1.00      0.78        38
         1.0       0.43      0.36      0.39        33
         2.0       0.00      0.00      0.00        16

    accuracy                           0.57        87
   macro avg       0.36      0.45      0.39        87
weighted avg       0.44      0.57      0.49        87

[[38  0  0]
 [21 12  0]
 [ 0 16  0]]
0.49145749923753684
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.66
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.54
              precision    recall  f1-score   support

         0.0       0.77      0.95      0.85        38
         1.0       0.74      0.42      0.54        33
         2.0       0.62      0.81      0.70        16

    accuracy                           0.72        87
   macro avg       0.71      0.73      0.70        87
weighted avg       0.73      0.72      0.70        87

[[36  2  0]
 [11 14  8]
 [ 0  3 13]]
0.7034564288113984
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.53
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.52
              precision    recall  f1-score   support

         0.0       0.76      0.92      0.83        38
         1.0       0.74      0.42      0.54        33
         2.0       0.64      0.88      0.74        16

    accuracy                           0.72        87
   macro avg       0.71      0.74      0.70        87
weighted avg       0.73      0.72      0.70        87

[[35  3  0]
 [11 14  8]
 [ 0  2 14]]
0.7037398979322754
87 87 87
Filename	True Label	Prediction
0602	0.0	0.0
0608	0.0	0.0
0611	1.0	0.0
0621	1.0	0.0
0623	0.0	0.0
0627	1.0	0.0
0631	1.0	0.0
0632	0.0	0.0
0644	0.0	0.0
0715	1.0	0.0
0721	1.0	0.0
0725	0.0	0.0
0812	0.0	0.0
0813	0.0	0.0
0814	0.0	0.0
0815	1.0	0.0
0820	0.0	0.0
0906	1.0	0.0
0913	1.0	0.0
0919	0.0	0.0
0922	0.0	0.0
0923	1.0	0.0
0930	0.0	0.0
1001	0.0	0.0
1018	0.0	0.0
1111	0.0	0.0
1112	0.0	1.0
1113	0.0	0.0
1114	1.0	0.0
BER0609003	1.0	1.0
BER0611006	1.0	2.0
KYJ0611005A	0.0	0.0
KYJ0611006B	0.0	0.0
LIB0611002B	0.0	0.0
LON0611002B	0.0	0.0
LON0611004A	0.0	0.0
MOS0509001	1.0	1.0
MOS0611014	0.0	1.0
PAR1011008A	0.0	0.0
PAR1011014	1.0	2.0
PAR1011015	1.0	2.0
PAR1011018	2.0	1.0
PHA0111005A	0.0	0.0
PHA0111018	1.0	1.0
PHA0112003A	0.0	0.0
PHA0209034	1.0	1.0
PHA0411011A	0.0	0.0
PHA0411011B	0.0	0.0
PHA0411012A	0.0	0.0
PHA0411034	0.0	1.0
PHA0411035	2.0	2.0
PHA0411039	1.0	1.0
PHA0411047	1.0	2.0
PHA0411053	2.0	2.0
PHA0411060	1.0	2.0
PHA0509022	2.0	2.0
PHA0509027	1.0	1.0
PHA0509030	2.0	2.0
PHA0509036	2.0	2.0
PHA0509038	1.0	1.0
PHA0509039	2.0	2.0
PHA0509041	1.0	2.0
PHA0510010A	0.0	0.0
PHA0510034	2.0	2.0
PHA0510035	2.0	2.0
PHA0510037	1.0	1.0
PHA0610007B	0.0	0.0
PHA0610026	2.0	2.0
PHA0710013	2.0	2.0
PHA0710014	2.0	2.0
PHA0810001	2.0	2.0
PHA0810002	1.0	1.0
PHA0811017	2.0	2.0
PHA0811020	1.0	2.0
PHA1109005	1.0	1.0
PHA1109025	0.0	0.0
PHA1109027	2.0	1.0
PHA1110003A	0.0	0.0
PHA1110013	1.0	2.0
PHA1110022	2.0	2.0
PHA1111009A	0.0	0.0
ST071122B	0.0	0.0
TI071122B	0.0	0.0
VAR0909003	1.0	1.0
VAR0909005	1.0	1.0
VAR0909007	1.0	1.0
VAR0910011	1.0	1.0
LANGUAGE: CZ, 2th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.97
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.83
              precision    recall  f1-score   support

         0.0       0.44      1.00      0.61        38
         1.0       0.00      0.00      0.00        33
         2.0       0.00      0.00      0.00        16

    accuracy                           0.44        87
   macro avg       0.15      0.33      0.20        87
weighted avg       0.19      0.44      0.27        87

[[38  0  0]
 [33  0  0]
 [16  0  0]]
0.2655632183908046
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.86
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.67
              precision    recall  f1-score   support

         0.0       0.90      0.71      0.79        38
         1.0       0.56      0.91      0.69        33
         2.0       1.00      0.19      0.32        16

    accuracy                           0.69        87
   macro avg       0.82      0.60      0.60        87
weighted avg       0.79      0.69      0.67        87

[[27 11  0]
 [ 3 30  0]
 [ 0 13  3]]
0.6665255500785958
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.61
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.58
              precision    recall  f1-score   support

         0.0       0.73      0.97      0.83        38
         1.0       0.67      0.42      0.52        33
         2.0       0.60      0.56      0.58        16

    accuracy                           0.69        87
   macro avg       0.66      0.65      0.64        87
weighted avg       0.68      0.69      0.67        87

[[37  1  0]
 [13 14  6]
 [ 1  6  9]]
0.6666314863187793
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.52
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.55
              precision    recall  f1-score   support

         0.0       0.83      0.92      0.88        38
         1.0       0.72      0.55      0.62        33
         2.0       0.60      0.75      0.67        16

    accuracy                           0.75        87
   macro avg       0.72      0.74      0.72        87
weighted avg       0.75      0.75      0.74        87

[[35  3  0]
 [ 7 18  8]
 [ 0  4 12]]
0.7402232791650151
87 87 87
Filename	True Label	Prediction
0601	0.0	0.0
0604	1.0	1.0
0622	0.0	0.0
0624	1.0	1.0
0626	1.0	0.0
0628	1.0	1.0
0629	1.0	1.0
0718	0.0	0.0
0719	1.0	0.0
0802	0.0	0.0
0806	0.0	1.0
0807	1.0	0.0
0808	0.0	0.0
0816	1.0	1.0
0817	0.0	0.0
0826	0.0	0.0
0903	0.0	0.0
0905	1.0	1.0
0910	0.0	1.0
0912	1.0	1.0
0916	0.0	0.0
0920	1.0	1.0
0924	0.0	0.0
0927	1.0	0.0
1002	1.0	0.0
1008	0.0	0.0
1010	0.0	0.0
1017	0.0	0.0
1019	0.0	0.0
1115	0.0	1.0
BER0611003	1.0	2.0
BER0611005	1.0	1.0
KYJ0611006A	0.0	0.0
KYJ0611009B	0.0	0.0
PHA0111001A	0.0	0.0
PHA0111001B	0.0	0.0
PHA0111002B	1.0	0.0
PHA0111003B	0.0	0.0
PHA0111015	2.0	1.0
PHA0111016	2.0	2.0
PHA0112002A	0.0	0.0
PHA0112007A	0.0	0.0
PHA0112007B	0.0	0.0
PHA0112009A	1.0	0.0
PHA0209026	2.0	2.0
PHA0209039	1.0	2.0
PHA0411008A	0.0	0.0
PHA0411009A	0.0	0.0
PHA0411010B	0.0	0.0
PHA0411038	2.0	2.0
PHA0411044	2.0	2.0
PHA0411045	1.0	1.0
PHA0509002	0.0	0.0
PHA0509007	0.0	0.0
PHA0509013	0.0	0.0
PHA0509017	1.0	2.0
PHA0509019	1.0	1.0
PHA0509025	2.0	2.0
PHA0509031	1.0	1.0
PHA0509032	1.0	2.0
PHA0509040	1.0	1.0
PHA0510002A	0.0	0.0
PHA0510003B	0.0	0.0
PHA0510004A	0.0	0.0
PHA0510013A	0.0	0.0
PHA0510030	1.0	2.0
PHA0510039	1.0	2.0
PHA0709008	2.0	1.0
PHA0710011	2.0	2.0
PHA0710016	2.0	1.0
PHA0710018	2.0	2.0
PHA0710019	2.0	2.0
PHA0809010	1.0	1.0
PHA0810008	1.0	2.0
PHA0810012	1.0	1.0
PHA0810015	2.0	2.0
PHA0811012	2.0	2.0
PHA0811016	1.0	1.0
PHA1109006	1.0	1.0
PHA1109023	0.0	0.0
PHA1110001B	0.0	0.0
PHA1110003B	0.0	0.0
PHA1110019	1.0	1.0
VAR0209036	1.0	2.0
VAR0909006	2.0	2.0
VAR0910004	2.0	2.0
VAR0910010	2.0	1.0
LANGUAGE: CZ, 3th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.02
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.89
              precision    recall  f1-score   support

         0.0       0.64      1.00      0.78        37
         1.0       0.52      0.45      0.48        33
         2.0       0.00      0.00      0.00        17

    accuracy                           0.60        87
   macro avg       0.39      0.48      0.42        87
weighted avg       0.47      0.60      0.51        87

[[37  0  0]
 [18 15  0]
 [ 3 14  0]]
0.5148137306558945
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.82
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.68
              precision    recall  f1-score   support

         0.0       0.70      1.00      0.82        37
         1.0       0.59      0.48      0.53        33
         2.0       0.86      0.35      0.50        17

    accuracy                           0.68        87
   macro avg       0.72      0.61      0.62        87
weighted avg       0.69      0.68      0.65        87

[[37  0  0]
 [16 16  1]
 [ 0 11  6]]
0.6496807151979567
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.66
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.59
              precision    recall  f1-score   support

         0.0       0.75      0.97      0.85        37
         1.0       0.86      0.18      0.30        33
         2.0       0.53      1.00      0.69        17

    accuracy                           0.68        87
   macro avg       0.71      0.72      0.61        87
weighted avg       0.75      0.68      0.61        87

[[36  1  0]
 [12  6 15]
 [ 0  0 17]]
0.6096217797463814
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.53
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.54
              precision    recall  f1-score   support

         0.0       0.90      0.73      0.81        37
         1.0       0.68      0.85      0.76        33
         2.0       0.88      0.82      0.85        17

    accuracy                           0.79        87
   macro avg       0.82      0.80      0.80        87
weighted avg       0.81      0.79      0.80        87

[[27 10  0]
 [ 3 28  2]
 [ 0  3 14]]
0.7956104703402695
87 87 87
Filename	True Label	Prediction
0603	1.0	1.0
0605	1.0	1.0
0606	1.0	1.0
0610	0.0	1.0
0612	0.0	0.0
0613	0.0	0.0
0617	0.0	0.0
0618	0.0	1.0
0630	0.0	0.0
0636	1.0	1.0
0638	0.0	1.0
0645	1.0	1.0
0714	1.0	1.0
0717	0.0	0.0
0723	0.0	1.0
0724	1.0	1.0
0803	0.0	1.0
0804	0.0	1.0
0805	1.0	1.0
0809	1.0	0.0
0818	0.0	0.0
0821	1.0	1.0
0824	0.0	0.0
0825	0.0	1.0
0904	0.0	0.0
0914	0.0	1.0
0925	1.0	1.0
0928	1.0	1.0
1003	0.0	0.0
1004	0.0	0.0
1009	1.0	1.0
1014	1.0	0.0
1015	0.0	1.0
1020	1.0	1.0
1116	0.0	1.0
9999	0.0	0.0
BER0611007	1.0	1.0
KYJ0611003A	0.0	0.0
LON0611002A	0.0	0.0
LON0611004B	0.0	0.0
MOS0611012	1.0	1.0
PAR1011013	1.0	1.0
PAR1011016	2.0	2.0
PHA0111004A	0.0	0.0
PHA0111010	2.0	1.0
PHA0112009B	1.0	0.0
PHA0210001	0.0	0.0
PHA0210007	0.0	0.0
PHA0411031	2.0	2.0
PHA0411051	2.0	2.0
PHA0411056	2.0	2.0
PHA0411058	2.0	2.0
PHA0411061	2.0	2.0
PHA0509015	2.0	1.0
PHA0509024	1.0	1.0
PHA0509028	2.0	1.0
PHA0509045	1.0	1.0
PHA0510013B	0.0	0.0
PHA0510031	1.0	1.0
PHA0510040	1.0	2.0
PHA0510048	1.0	1.0
PHA0610005A	0.0	0.0
PHA0610015	1.0	2.0
PHA0610017	2.0	2.0
PHA0610018	2.0	2.0
PHA0610019A	0.0	0.0
PHA0610019B	0.0	0.0
PHA0610025	2.0	2.0
PHA0710009	1.0	1.0
PHA0710015	2.0	2.0
PHA0810004	1.0	1.0
PHA0810009	2.0	2.0
PHA0811010	1.0	1.0
PHA0811013	2.0	2.0
PHA1109002	2.0	2.0
PHA1109007	1.0	1.0
PHA1109024	2.0	2.0
PHA1110001A	0.0	0.0
PHA1110004A	0.0	0.0
PHA1110017	1.0	1.0
PHA1111001A	0.0	0.0
PHA1111002A	0.0	0.0
PHA1111006B	0.0	0.0
PHA1111008B	0.0	0.0
VAR0909004	1.0	1.0
VAR0909010	1.0	1.0
VAR0910007	1.0	1.0
LANGUAGE: CZ, 4th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.97
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.89
              precision    recall  f1-score   support

         0.0       0.83      0.81      0.82        37
         1.0       0.56      0.85      0.67        33
         2.0       0.00      0.00      0.00        16

    accuracy                           0.67        86
   macro avg       0.46      0.55      0.50        86
weighted avg       0.57      0.67      0.61        86

[[30  7  0]
 [ 5 28  0]
 [ 1 15  0]]
0.6125118505241098
86 86 86



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.81
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.78
              precision    recall  f1-score   support

         0.0       0.72      0.92      0.81        37
         1.0       0.54      0.64      0.58        33
         2.0       0.00      0.00      0.00        16

    accuracy                           0.64        86
   macro avg       0.42      0.52      0.46        86
weighted avg       0.52      0.64      0.57        86

[[34  3  0]
 [12 21  0]
 [ 1 15  0]]
0.5721207087486156
86 86 86



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.65
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.74
              precision    recall  f1-score   support

         0.0       0.81      0.81      0.81        37
         1.0       0.71      0.73      0.72        33
         2.0       0.80      0.75      0.77        16

    accuracy                           0.77        86
   macro avg       0.77      0.76      0.77        86
weighted avg       0.77      0.77      0.77        86

[[30  7  0]
 [ 6 24  3]
 [ 1  3 12]]
0.7677777653368566
86 86 86



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.52
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.66
              precision    recall  f1-score   support

         0.0       0.81      0.81      0.81        37
         1.0       0.68      0.64      0.66        33
         2.0       0.67      0.75      0.71        16

    accuracy                           0.73        86
   macro avg       0.72      0.73      0.72        86
weighted avg       0.73      0.73      0.73        86

[[30  7  0]
 [ 6 21  6]
 [ 1  3 12]]
0.7319810191518468
86 86 86
Filename	True Label	Prediction
0609	0.0	1.0
0620	0.0	0.0
0625	0.0	0.0
0637	1.0	1.0
0639	0.0	1.0
0640	1.0	1.0
0642	0.0	1.0
0643	1.0	1.0
0716	1.0	0.0
0810	0.0	1.0
0819	2.0	0.0
0823	1.0	0.0
0828	1.0	0.0
0907	1.0	1.0
0911	0.0	0.0
0917	0.0	0.0
0921	0.0	0.0
0926	1.0	1.0
1023	0.0	0.0
1117	0.0	0.0
LIB0611001A	0.0	0.0
LIB0611001B	0.0	0.0
LIB0611002A	0.0	0.0
LIB0611004A	0.0	0.0
LIB0611004B	0.0	0.0
LIB0611011	0.0	1.0
LON0610002B	0.0	0.0
MOS0611015	1.0	2.0
PHA0111002A	1.0	0.0
PHA0111004B	0.0	0.0
PHA0111005B	0.0	0.0
PHA0111011	1.0	1.0
PHA0111012	1.0	1.0
PHA0111014	0.0	1.0
PHA0112002B	0.0	0.0
PHA0112003B	0.0	0.0
PHA0112006A	1.0	0.0
PHA0209008	0.0	0.0
PHA0209024	1.0	1.0
PHA0209028	1.0	1.0
PHA0209031	2.0	2.0
PHA0209038	2.0	2.0
PHA0210004	0.0	0.0
PHA0210008	0.0	0.0
PHA0411008B	0.0	0.0
PHA0411010A	0.0	0.0
PHA0411027	1.0	2.0
PHA0411032	1.0	2.0
PHA0411037	1.0	1.0
PHA0411043	1.0	1.0
PHA0411054	2.0	1.0
PHA0411059	2.0	2.0
PHA0509018	2.0	2.0
PHA0509020	2.0	2.0
PHA0509026	2.0	2.0
PHA0509033	0.0	1.0
PHA0509037	2.0	1.0
PHA0509042	2.0	2.0
PHA0509044	1.0	1.0
PHA0510002B	0.0	0.0
PHA0510004B	0.0	0.0
PHA0510023	2.0	2.0
PHA0510027	1.0	1.0
PHA0510029	2.0	2.0
PHA0510038	2.0	2.0
PHA0510046	1.0	1.0
PHA0610007A	0.0	0.0
PHA0710010	1.0	2.0
PHA0710021	2.0	2.0
PHA0809009	1.0	1.0
PHA0810006	1.0	1.0
PHA0810011	1.0	1.0
PHA1109003	1.0	1.0
PHA1109028	1.0	2.0
PHA1110002B	1.0	0.0
PHA1110014	1.0	2.0
PHA1110016	1.0	1.0
PHA1110021	1.0	1.0
PHA1111002B	0.0	0.0
PHA1111003A	0.0	0.0
PHA1111004A	0.0	0.0
PHA1111006A	0.0	0.0
PHA1111008A	0.0	0.0
VAR0909008	1.0	1.0
VAR0909009	2.0	1.0
VAR0910009	2.0	2.0
Averaged weighted F1-scores 0.7373209512884582
144.90552995391704 65.35717405024758
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']
LANGUAGE: CZ, 0th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.29
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.05
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.56      0.95      0.71        37
         2.0       0.28      0.22      0.25        32
         3.0       0.00      0.00      0.00        16
         4.0       0.00      0.00      0.00         1

    accuracy                           0.48        87
   macro avg       0.17      0.23      0.19        87
weighted avg       0.34      0.48      0.39        87

[[ 0  1  0  0  0]
 [ 0 35  2  0  0]
 [ 0 25  7  0  0]
 [ 0  1 15  0  0]
 [ 0  0  1  0  0]]
0.3910490262577377
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.04
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.92
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.55      0.84      0.67        37
         2.0       0.33      0.12      0.18        32
         3.0       0.68      0.81      0.74        16
         4.0       0.00      0.00      0.00         1

    accuracy                           0.55        87
   macro avg       0.31      0.36      0.32        87
weighted avg       0.48      0.55      0.49        87

[[ 0  1  0  0  0]
 [ 0 31  6  0  0]
 [ 0 23  4  5  0]
 [ 0  1  2 13  0]
 [ 0  0  0  1  0]]
0.48701796288003185
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.88
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.90
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.62      0.70      0.66        37
         2.0       0.47      0.25      0.33        32
         3.0       0.54      0.94      0.68        16
         4.0       0.00      0.00      0.00         1

    accuracy                           0.56        87
   macro avg       0.33      0.38      0.33        87
weighted avg       0.53      0.56      0.53        87

[[ 0  1  0  0  0]
 [ 0 26  8  3  0]
 [ 0 15  8  9  0]
 [ 0  0  1 15  0]
 [ 0  0  0  1  0]]
0.5254310446054538
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.79
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.85
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.65      0.65      0.65        37
         2.0       0.56      0.47      0.51        32
         3.0       0.65      0.94      0.77        16
         4.0       0.00      0.00      0.00         1

    accuracy                           0.62        87
   macro avg       0.37      0.41      0.39        87
weighted avg       0.60      0.62      0.60        87

[[ 0  1  0  0  0]
 [ 0 24 12  1  0]
 [ 0 11 15  6  0]
 [ 0  1  0 15  0]
 [ 0  0  0  1  0]]
0.604354928142187
87 87 87
Filename	True Label	Prediction
0603	2.0	2.0
0621	2.0	2.0
0622	1.0	1.0
0626	2.0	2.0
0639	1.0	2.0
0715	2.0	1.0
0721	2.0	2.0
0723	2.0	2.0
0806	1.0	2.0
0807	2.0	2.0
0813	1.0	1.0
0816	2.0	2.0
0817	1.0	2.0
0821	2.0	1.0
0904	1.0	2.0
0914	1.0	2.0
0915	2.0	2.0
0920	2.0	2.0
0921	1.0	1.0
0923	2.0	2.0
0928	1.0	2.0
0930	2.0	1.0
1005	1.0	1.0
1010	1.0	1.0
1111	1.0	1.0
BER0609003	2.0	2.0
KYJ0611006A	1.0	1.0
KYJ0611009A	1.0	1.0
LIB0611001A	1.0	1.0
LIB0611002B	2.0	1.0
LIB0611011	1.0	1.0
LON0610002A	2.0	1.0
LON0611004A	1.0	1.0
MOS0611012	2.0	2.0
PAR1011009A	2.0	2.0
PAR1011015	2.0	3.0
PHA0111001B	1.0	1.0
PHA0111003A	1.0	1.0
PHA0111003B	2.0	1.0
PHA0111005A	1.0	1.0
PHA0111010	3.0	3.0
PHA0111015	3.0	3.0
PHA0112006B	2.0	2.0
PHA0112009A	2.0	1.0
PHA0209008	1.0	1.0
PHA0209013	1.0	1.0
PHA0209024	1.0	2.0
PHA0209031	4.0	3.0
PHA0210004	1.0	1.0
PHA0411010A	0.0	1.0
PHA0411011B	1.0	1.0
PHA0411029	2.0	1.0
PHA0411030	3.0	3.0
PHA0411031	3.0	3.0
PHA0411032	1.0	3.0
PHA0411036	3.0	3.0
PHA0411037	2.0	3.0
PHA0411039	2.0	2.0
PHA0411045	3.0	1.0
PHA0509013	1.0	1.0
PHA0509028	2.0	3.0
PHA0509036	3.0	3.0
PHA0509038	1.0	2.0
PHA0509041	2.0	3.0
PHA0509042	3.0	3.0
PHA0510029	2.0	3.0
PHA0510034	3.0	3.0
PHA0510037	1.0	2.0
PHA0610006B	1.0	1.0
PHA0610019A	2.0	1.0
PHA0610026	3.0	3.0
PHA0710012	3.0	3.0
PHA0810004	1.0	1.0
PHA0810008	3.0	3.0
PHA0811012	3.0	3.0
PHA0811014	1.0	2.0
PHA0811016	1.0	2.0
PHA0811017	3.0	3.0
PHA1109002	3.0	3.0
PHA1109007	1.0	1.0
PHA1110013	2.0	3.0
PHA1111001A	1.0	1.0
PHA1111002A	2.0	1.0
PHA1111004A	1.0	2.0
PHA1111006A	1.0	1.0
VAR0909008	2.0	1.0
VAR0910009	3.0	3.0
LANGUAGE: CZ, 1th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.28
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.09
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.43      1.00      0.60        37
         2.0       0.00      0.00      0.00        31
         3.0       0.00      0.00      0.00        17
         4.0       0.00      0.00      0.00         1

    accuracy                           0.43        87
   macro avg       0.09      0.20      0.12        87
weighted avg       0.18      0.43      0.25        87

[[ 0  1  0  0  0]
 [ 0 37  0  0  0]
 [ 0 31  0  0  0]
 [ 0 17  0  0  0]
 [ 0  1  0  0  0]]
0.25380051909529106
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.03
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.93
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.60      0.92      0.72        37
         2.0       0.38      0.19      0.26        31
         3.0       0.64      0.53      0.58        17
         4.0       0.00      0.00      0.00         1

    accuracy                           0.56        87
   macro avg       0.32      0.33      0.31        87
weighted avg       0.51      0.56      0.51        87

[[ 0  1  0  0  0]
 [ 0 34  3  0  0]
 [ 0 21  6  4  0]
 [ 0  1  7  9  0]
 [ 0  0  0  1  0]]
0.5120898713306352
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.94
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.88
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.62      0.81      0.71        37
         2.0       0.38      0.32      0.35        31
         3.0       0.62      0.47      0.53        17
         4.0       0.00      0.00      0.00         1

    accuracy                           0.55        87
   macro avg       0.33      0.32      0.32        87
weighted avg       0.52      0.55      0.53        87

[[ 0  1  0  0  0]
 [ 0 30  7  0  0]
 [ 0 17 10  4  0]
 [ 0  0  9  8  0]
 [ 0  0  0  1  0]]
0.5294426058384636
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.87
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.85
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.66      0.68      0.67        37
         2.0       0.41      0.35      0.38        31
         3.0       0.59      0.76      0.67        17
         4.0       0.00      0.00      0.00         1

    accuracy                           0.56        87
   macro avg       0.33      0.36      0.34        87
weighted avg       0.54      0.56      0.55        87

[[ 0  1  0  0  0]
 [ 0 25 12  0  0]
 [ 0 12 11  8  0]
 [ 0  0  4 13  0]
 [ 0  0  0  1  0]]
0.5489496630994848
87 87 87
Filename	True Label	Prediction
0604	2.0	2.0
0607	2.0	2.0
0617	1.0	1.0
0627	2.0	2.0
0628	2.0	1.0
0631	2.0	1.0
0635	1.0	1.0
0641	1.0	1.0
0717	1.0	1.0
0718	1.0	1.0
0719	2.0	1.0
0803	2.0	1.0
0804	1.0	1.0
0808	1.0	1.0
0810	2.0	1.0
0812	2.0	1.0
0814	1.0	1.0
0815	2.0	2.0
0826	1.0	1.0
0829	1.0	2.0
0903	1.0	2.0
0907	2.0	2.0
0913	2.0	1.0
0922	1.0	1.0
1007	2.0	1.0
1009	2.0	1.0
1018	1.0	2.0
1019	1.0	1.0
1113	1.0	2.0
1114	2.0	1.0
1115	1.0	1.0
BER0611007	2.0	2.0
KYJ0611006B	1.0	1.0
KYJ0611009B	1.0	1.0
LIB0611004B	1.0	1.0
MOS0509004	1.0	1.0
MOS0611014	1.0	2.0
MOS0611015	2.0	3.0
PAR1011009B	1.0	1.0
PAR1011013	2.0	2.0
PAR1011014	2.0	3.0
PAR1011016	3.0	3.0
PAR1011018	3.0	2.0
PHA0111001A	1.0	1.0
PHA0111011	2.0	2.0
PHA0111018	1.0	2.0
PHA0112007B	1.0	1.0
PHA0411009A	2.0	1.0
PHA0411012A	1.0	1.0
PHA0411035	3.0	2.0
PHA0411061	3.0	3.0
PHA0411062	3.0	3.0
PHA0509017	2.0	3.0
PHA0509018	3.0	3.0
PHA0509024	2.0	2.0
PHA0509025	3.0	3.0
PHA0509031	1.0	2.0
PHA0509033	1.0	2.0
PHA0509034	1.0	2.0
PHA0509045	1.0	2.0
PHA0510010B	0.0	1.0
PHA0510023	3.0	3.0
PHA0510032	3.0	3.0
PHA0510039	3.0	3.0
PHA0510040	2.0	3.0
PHA0510050	2.0	3.0
PHA0610006A	1.0	1.0
PHA0610025	2.0	3.0
PHA0709008	3.0	2.0
PHA0710009	3.0	2.0
PHA0710013	4.0	3.0
PHA0710018	3.0	3.0
PHA0810003	3.0	3.0
PHA0810010	2.0	3.0
PHA0810015	3.0	3.0
PHA0811010	2.0	2.0
PHA0811019	3.0	3.0
PHA1109004	3.0	3.0
PHA1109006	2.0	2.0
PHA1109008	1.0	1.0
PHA1109025	1.0	1.0
PHA1110002B	2.0	1.0
PHA1110003B	1.0	1.0
PHA1110019	2.0	3.0
TI071122B	1.0	1.0
VAR0909005	1.0	2.0
VAR0909010	1.0	2.0
LANGUAGE: CZ, 2th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.31
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.10
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.56      0.97      0.71        37
         2.0       0.39      0.29      0.33        31
         3.0       0.00      0.00      0.00        17
         4.0       0.00      0.00      0.00         1

    accuracy                           0.52        87
   macro avg       0.19      0.25      0.21        87
weighted avg       0.38      0.52      0.42        87

[[ 0  1  0  0  0]
 [ 0 36  1  0  0]
 [ 0 22  9  0  0]
 [ 0  5 12  0  0]
 [ 0  0  1  0  0]]
0.4219490914608703
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.99
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.92
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.59      0.86      0.70        37
         2.0       0.38      0.16      0.23        31
         3.0       0.55      0.65      0.59        17
         4.0       0.00      0.00      0.00         1

    accuracy                           0.55        87
   macro avg       0.31      0.33      0.31        87
weighted avg       0.50      0.55      0.50        87

[[ 0  1  0  0  0]
 [ 0 32  4  1  0]
 [ 0 19  5  7  0]
 [ 0  2  4 11  0]
 [ 0  0  0  1  0]]
0.496270582477479
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.94
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.93
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.74      0.68      0.70        37
         2.0       0.50      0.39      0.44        31
         3.0       0.48      0.82      0.61        17
         4.0       0.00      0.00      0.00         1

    accuracy                           0.59        87
   macro avg       0.34      0.38      0.35        87
weighted avg       0.59      0.59      0.57        87

[[ 0  1  0  0  0]
 [ 0 25  9  3  0]
 [ 0  8 12 11  0]
 [ 0  0  3 14  0]
 [ 0  0  0  1  0]]
0.5739245614068765
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.83
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.88
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.74      0.76      0.75        37
         2.0       0.55      0.52      0.53        31
         3.0       0.55      0.65      0.59        17
         4.0       0.00      0.00      0.00         1

    accuracy                           0.63        87
   macro avg       0.37      0.38      0.37        87
weighted avg       0.62      0.63      0.62        87

[[ 0  1  0  0  0]
 [ 0 28  8  1  0]
 [ 0  8 16  7  0]
 [ 0  1  5 11  0]
 [ 0  0  0  1  0]]
0.623771357564461
87 87 87
Filename	True Label	Prediction
0616	1.0	2.0
0618	1.0	1.0
0619	2.0	1.0
0624	2.0	2.0
0630	1.0	1.0
0633	2.0	1.0
0634	2.0	2.0
0636	2.0	2.0
0643	2.0	2.0
0645	2.0	2.0
0724	2.0	2.0
0818	1.0	2.0
0819	3.0	2.0
0822	1.0	2.0
0823	2.0	2.0
0824	2.0	1.0
0827	1.0	1.0
0901	2.0	2.0
0906	2.0	2.0
0912	2.0	2.0
0917	1.0	1.0
0918	1.0	2.0
0919	1.0	1.0
0925	2.0	2.0
0929	0.0	1.0
1003	1.0	1.0
1016	1.0	1.0
1021	1.0	2.0
1022	2.0	1.0
1023	1.0	2.0
1112	1.0	1.0
9999	1.0	1.0
LIB0611001B	1.0	1.0
LIB0611002A	1.0	1.0
MOS0611013	2.0	3.0
PHA0111002A	2.0	1.0
PHA0111002B	3.0	1.0
PHA0111004A	1.0	1.0
PHA0111004B	1.0	1.0
PHA0111014	1.0	1.0
PHA0112002A	2.0	1.0
PHA0112003B	1.0	1.0
PHA0112012B	1.0	1.0
PHA0209001	1.0	1.0
PHA0209026	2.0	3.0
PHA0209039	2.0	3.0
PHA0411011A	1.0	1.0
PHA0411012B	1.0	1.0
PHA0411027	2.0	2.0
PHA0411043	1.0	1.0
PHA0411044	3.0	3.0
PHA0411053	3.0	3.0
PHA0411060	2.0	3.0
PHA0509007	1.0	1.0
PHA0509015	3.0	2.0
PHA0509027	1.0	2.0
PHA0509040	2.0	2.0
PHA0510002B	2.0	1.0
PHA0510003A	1.0	1.0
PHA0510003B	1.0	1.0
PHA0510013B	1.0	1.0
PHA0510030	2.0	3.0
PHA0510031	2.0	3.0
PHA0510047	1.0	2.0
PHA0510048	1.0	3.0
PHA0610007A	1.0	1.0
PHA0610015	2.0	3.0
PHA0710011	3.0	3.0
PHA0710014	3.0	3.0
PHA0710019	3.0	3.0
PHA0710021	3.0	3.0
PHA0809009	2.0	2.0
PHA1109001	1.0	1.0
PHA1109023	1.0	1.0
PHA1109024	4.0	3.0
PHA1109026	3.0	3.0
PHA1109027	3.0	3.0
PHA1110003A	1.0	1.0
PHA1110015	3.0	3.0
PHA1110021	2.0	2.0
PHA1111003A	2.0	1.0
VAR0909007	2.0	2.0
VAR0909009	3.0	2.0
VAR0910004	3.0	3.0
VAR0910005	3.0	2.0
VAR0910006	3.0	3.0
VAR0910011	3.0	2.0
LANGUAGE: CZ, 3th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.21
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.06
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.46      1.00      0.63        37
         2.0       0.43      0.10      0.16        31
         3.0       0.00      0.00      0.00        16
         4.0       0.00      0.00      0.00         1

    accuracy                           0.46        87
   macro avg       0.18      0.22      0.16        87
weighted avg       0.35      0.46      0.33        87

[[ 0  2  0  0  0]
 [ 0 37  0  0  0]
 [ 0 28  3  0  0]
 [ 0 12  4  0  0]
 [ 0  1  0  0  0]]
0.32524650854959386
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.04
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.99
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.76      0.35      0.48        37
         2.0       0.43      0.65      0.52        31
         3.0       0.50      0.75      0.60        16
         4.0       0.00      0.00      0.00         1

    accuracy                           0.52        87
   macro avg       0.34      0.35      0.32        87
weighted avg       0.57      0.52      0.50        87

[[ 0  2  0  0  0]
 [ 0 13 22  2  0]
 [ 0  2 20  9  0]
 [ 0  0  4 12  0]
 [ 0  0  0  1  0]]
0.50021506803116
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.92
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.91
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.85      0.59      0.70        37
         2.0       0.52      0.55      0.53        31
         3.0       0.50      0.88      0.64        16
         4.0       0.00      0.00      0.00         1

    accuracy                           0.61        87
   macro avg       0.37      0.40      0.37        87
weighted avg       0.64      0.61      0.60        87

[[ 0  2  0  0  0]
 [ 0 22 14  1  0]
 [ 0  2 17 12  0]
 [ 0  0  2 14  0]
 [ 0  0  0  1  0]]
0.6033544600354945
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.78
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.87
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.81      0.59      0.69        37
         2.0       0.50      0.55      0.52        31
         3.0       0.50      0.81      0.62        16
         4.0       0.00      0.00      0.00         1

    accuracy                           0.60        87
   macro avg       0.36      0.39      0.37        87
weighted avg       0.62      0.60      0.59        87

[[ 0  2  0  0  0]
 [ 0 22 14  1  0]
 [ 0  3 17 11  0]
 [ 0  0  3 13  0]
 [ 0  0  0  1  0]]
0.5926166266683508
87 87 87
Filename	True Label	Prediction
0602	2.0	2.0
0606	1.0	2.0
0608	1.0	1.0
0609	1.0	2.0
0610	2.0	2.0
0614	1.0	2.0
0620	1.0	2.0
0625	2.0	1.0
0629	2.0	2.0
0637	2.0	2.0
0638	2.0	2.0
0640	2.0	2.0
0642	1.0	2.0
0714	2.0	2.0
0801	1.0	2.0
0805	2.0	2.0
0809	2.0	2.0
0811	2.0	2.0
0820	1.0	1.0
0825	1.0	2.0
0911	1.0	2.0
0916	1.0	1.0
0927	1.0	2.0
1001	1.0	1.0
1002	2.0	2.0
1004	1.0	1.0
1006	2.0	2.0
1014	2.0	2.0
1015	1.0	2.0
1020	2.0	2.0
BER0611003	2.0	3.0
KYJ0611003A	1.0	1.0
KYJ0611005A	1.0	1.0
KYJ0611005B	1.0	1.0
LON0611004B	1.0	1.0
MOS0509001	1.0	2.0
PAR1011008A	1.0	1.0
PHA0111005B	2.0	1.0
PHA0111016	3.0	3.0
PHA0112006A	3.0	2.0
PHA0112012A	1.0	2.0
PHA0209028	2.0	3.0
PHA0209034	2.0	3.0
PHA0209038	4.0	3.0
PHA0210007	1.0	1.0
PHA0411008A	1.0	2.0
PHA0411009B	1.0	1.0
PHA0411010B	0.0	1.0
PHA0411033	2.0	2.0
PHA0411038	3.0	3.0
PHA0411041	3.0	3.0
PHA0411042	2.0	3.0
PHA0411051	3.0	3.0
PHA0411054	3.0	2.0
PHA0411055	3.0	3.0
PHA0411056	3.0	3.0
PHA0509019	3.0	2.0
PHA0509030	2.0	3.0
PHA0509035	2.0	3.0
PHA0509044	2.0	3.0
PHA0510002A	2.0	1.0
PHA0510004A	1.0	1.0
PHA0510004B	0.0	1.0
PHA0510038	3.0	3.0
PHA0610007B	1.0	1.0
PHA0710015	2.0	3.0
PHA0710017	3.0	3.0
PHA0810001	3.0	3.0
PHA0810002	1.0	3.0
PHA0810006	2.0	3.0
PHA0810009	3.0	3.0
PHA0810011	2.0	3.0
PHA0810012	3.0	3.0
PHA0811013	3.0	3.0
PHA1109003	1.0	1.0
PHA1109005	1.0	2.0
PHA1110001B	1.0	1.0
PHA1110002A	2.0	2.0
PHA1110004A	1.0	1.0
PHA1110014	2.0	3.0
PHA1110016	1.0	1.0
PHA1110017	1.0	1.0
PHA1110022	3.0	3.0
PHA1111001B	1.0	1.0
PHA1111008B	1.0	1.0
ST071122B	1.0	1.0
VAR0909004	2.0	2.0
LANGUAGE: CZ, 4th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.29
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.13
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.55      1.00      0.71        37
         2.0       0.00      0.00      0.00        31
         3.0       0.63      0.75      0.69        16
         4.0       0.00      0.00      0.00         1

    accuracy                           0.57        86
   macro avg       0.24      0.35      0.28        86
weighted avg       0.36      0.57      0.43        86

[[ 0  1  0  0  0]
 [ 0 37  0  0  0]
 [ 0 25  0  6  0]
 [ 0  4  0 12  0]
 [ 0  0  0  1  0]]
0.43370176335292615
86 86 86



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.07
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.99
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.76      0.51      0.61        37
         2.0       0.48      0.52      0.50        31
         3.0       0.50      0.88      0.64        16
         4.0       0.00      0.00      0.00         1

    accuracy                           0.57        86
   macro avg       0.35      0.38      0.35        86
weighted avg       0.59      0.57      0.56        86

[[ 0  1  0  0  0]
 [ 0 19 15  3  0]
 [ 0  5 16 10  0]
 [ 0  0  2 14  0]
 [ 0  0  0  1  0]]
0.562316715542522
86 86 86



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.95
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.85
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.61      0.92      0.73        37
         2.0       0.40      0.06      0.11        31
         3.0       0.56      0.88      0.68        16
         4.0       0.00      0.00      0.00         1

    accuracy                           0.58        86
   macro avg       0.31      0.37      0.31        86
weighted avg       0.51      0.58      0.48        86

[[ 0  1  0  0  0]
 [ 0 34  2  1  0]
 [ 0 20  2  9  0]
 [ 0  1  1 14  0]
 [ 0  0  0  1  0]]
0.4816864785302017
86 86 86



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.83
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.81
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.67      0.89      0.77        37
         2.0       0.67      0.26      0.37        31
         3.0       0.56      0.88      0.68        16
         4.0       0.00      0.00      0.00         1

    accuracy                           0.64        86
   macro avg       0.38      0.40      0.36        86
weighted avg       0.63      0.64      0.59        86

[[ 0  1  0  0  0]
 [ 0 33  3  1  0]
 [ 0 14  8  9  0]
 [ 0  1  1 14  0]
 [ 0  0  0  1  0]]
0.5913611840282816
86 86 86
Filename	True Label	Prediction
0601	1.0	1.0
0605	2.0	2.0
0611	2.0	1.0
0612	1.0	1.0
0613	1.0	1.0
0615	1.0	1.0
0623	2.0	1.0
0632	1.0	1.0
0644	1.0	1.0
0716	2.0	1.0
0720	1.0	1.0
0722	2.0	1.0
0725	1.0	1.0
0802	2.0	1.0
0828	2.0	1.0
0902	2.0	2.0
0905	2.0	2.0
0910	1.0	1.0
0924	1.0	1.0
0926	2.0	2.0
1008	2.0	1.0
1017	1.0	1.0
1116	1.0	2.0
1117	2.0	1.0
BER0611005	2.0	2.0
BER0611006	2.0	3.0
KYJ0611004A	1.0	1.0
LIB0611003A	1.0	1.0
LIB0611004A	1.0	1.0
LON0610002B	1.0	1.0
LON0611002A	1.0	1.0
LON0611002B	1.0	1.0
LON0611003	2.0	3.0
PAR1011017	3.0	3.0
PHA0111012	1.0	2.0
PHA0112002B	1.0	1.0
PHA0112003A	1.0	1.0
PHA0112007A	1.0	1.0
PHA0112009B	2.0	1.0
PHA0210001	1.0	1.0
PHA0210008	1.0	1.0
PHA0411008B	1.0	1.0
PHA0411028	2.0	1.0
PHA0411034	1.0	1.0
PHA0411047	2.0	3.0
PHA0411058	3.0	3.0
PHA0411059	3.0	3.0
PHA0509002	1.0	1.0
PHA0509020	3.0	3.0
PHA0509021	1.0	2.0
PHA0509022	4.0	3.0
PHA0509026	3.0	3.0
PHA0509032	2.0	3.0
PHA0509037	3.0	1.0
PHA0509039	3.0	3.0
PHA0509043	2.0	3.0
PHA0510010A	1.0	1.0
PHA0510013A	2.0	1.0
PHA0510027	1.0	1.0
PHA0510035	3.0	3.0
PHA0510036	3.0	3.0
PHA0510046	2.0	2.0
PHA0510049	3.0	3.0
PHA0610005A	1.0	1.0
PHA0610005B	0.0	1.0
PHA0610016	2.0	3.0
PHA0610017	3.0	3.0
PHA0610018	2.0	3.0
PHA0610019B	2.0	1.0
PHA0710010	2.0	3.0
PHA0710016	3.0	3.0
PHA0809010	2.0	2.0
PHA0811020	1.0	3.0
PHA1109028	3.0	3.0
PHA1110001A	2.0	1.0
PHA1111002B	1.0	1.0
PHA1111003B	1.0	1.0
PHA1111004B	1.0	1.0
PHA1111006B	1.0	1.0
PHA1111008A	2.0	1.0
PHA1111009A	1.0	1.0
VAR0209036	3.0	2.0
VAR0909003	2.0	2.0
VAR0909006	3.0	3.0
VAR0910007	2.0	3.0
VAR0910010	3.0	3.0
Averaged weighted F1-scores 0.5922107519005531
144.90552995391704 65.35717405024758
LABEL SET ['A2', 'B1', 'B2', 'C1']
LANGUAGE: CZ, 0th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.08
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.94
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         9
         1.0       0.62      1.00      0.77        54
         2.0       0.00      0.00      0.00        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.62        87
   macro avg       0.16      0.25      0.19        87
weighted avg       0.39      0.62      0.48        87

[[ 0  9  0  0]
 [ 0 54  0  0]
 [ 0 23  0  0]
 [ 0  1  0  0]]
0.47542186353631705
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.96
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.96
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         9
         1.0       0.62      1.00      0.77        54
         2.0       0.00      0.00      0.00        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.62        87
   macro avg       0.16      0.25      0.19        87
weighted avg       0.39      0.62      0.48        87

[[ 0  9  0  0]
 [ 0 54  0  0]
 [ 0 23  0  0]
 [ 0  1  0  0]]
0.47542186353631705
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.93
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.88
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         9
         1.0       0.62      1.00      0.77        54
         2.0       0.00      0.00      0.00        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.62        87
   macro avg       0.16      0.25      0.19        87
weighted avg       0.39      0.62      0.48        87

[[ 0  9  0  0]
 [ 0 54  0  0]
 [ 0 23  0  0]
 [ 0  1  0  0]]
0.47542186353631705
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.85
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.80
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         9
         1.0       0.72      0.93      0.81        54
         2.0       0.72      0.57      0.63        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.72        87
   macro avg       0.36      0.37      0.36        87
weighted avg       0.64      0.72      0.67        87

[[ 0  9  0  0]
 [ 0 50  4  0]
 [ 0 10 13  0]
 [ 0  0  1  0]]
0.672273619287917
87 87 87
Filename	True Label	Prediction
0601	1.0	1.0
0609	1.0	1.0
0625	1.0	1.0
0626	2.0	1.0
0628	1.0	1.0
0632	1.0	1.0
0635	1.0	1.0
0636	2.0	1.0
0714	1.0	1.0
0721	2.0	1.0
0722	1.0	1.0
0725	1.0	1.0
0805	1.0	1.0
0808	1.0	1.0
0825	1.0	1.0
0827	1.0	1.0
0903	1.0	1.0
0905	1.0	1.0
0906	2.0	1.0
0907	1.0	1.0
0912	2.0	1.0
0923	1.0	1.0
1004	1.0	1.0
1008	0.0	1.0
1016	1.0	1.0
1017	1.0	1.0
1019	1.0	1.0
1023	1.0	1.0
1112	1.0	1.0
9999	0.0	1.0
KYJ0611005B	0.0	1.0
KYJ0611006B	0.0	1.0
KYJ0611009A	1.0	1.0
LIB0611002B	1.0	1.0
LIB0611004A	2.0	1.0
LON0611003	2.0	2.0
PAR1011008A	1.0	1.0
PAR1011014	1.0	2.0
PAR1011017	2.0	1.0
PHA0111003B	1.0	1.0
PHA0111005A	2.0	1.0
PHA0111010	2.0	1.0
PHA0111014	0.0	1.0
PHA0111018	1.0	1.0
PHA0112002B	1.0	1.0
PHA0209001	1.0	1.0
PHA0210001	1.0	1.0
PHA0411010B	1.0	1.0
PHA0411028	1.0	1.0
PHA0411041	1.0	2.0
PHA0411044	3.0	2.0
PHA0411053	2.0	2.0
PHA0411058	2.0	2.0
PHA0509013	0.0	1.0
PHA0509020	2.0	2.0
PHA0509026	2.0	2.0
PHA0509027	1.0	1.0
PHA0509031	1.0	2.0
PHA0509033	1.0	1.0
PHA0509039	2.0	2.0
PHA0509042	2.0	2.0
PHA0510002A	1.0	1.0
PHA0510010B	0.0	1.0
PHA0510013B	1.0	1.0
PHA0510031	1.0	1.0
PHA0510032	2.0	2.0
PHA0510035	2.0	2.0
PHA0610006B	0.0	1.0
PHA0610007A	1.0	1.0
PHA0610007B	1.0	1.0
PHA0610018	2.0	2.0
PHA0610019A	0.0	1.0
PHA0610019B	1.0	1.0
PHA0710013	2.0	2.0
PHA0710018	2.0	1.0
PHA0710021	2.0	2.0
PHA0810003	1.0	2.0
PHA0811010	1.0	1.0
PHA1109005	1.0	1.0
PHA1109007	1.0	1.0
PHA1109023	1.0	1.0
PHA1110003B	1.0	1.0
PHA1110004A	1.0	1.0
PHA1110021	1.0	1.0
VAR0209036	2.0	2.0
VAR0909008	1.0	1.0
VAR0910010	1.0	1.0
LANGUAGE: CZ, 1th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.99
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.99
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         9
         1.0       0.62      1.00      0.77        54
         2.0       0.00      0.00      0.00        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.62        87
   macro avg       0.16      0.25      0.19        87
weighted avg       0.39      0.62      0.48        87

[[ 0  9  0  0]
 [ 0 54  0  0]
 [ 0 23  0  0]
 [ 0  1  0  0]]
0.47542186353631705
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.92
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.84
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         9
         1.0       0.65      0.81      0.72        54
         2.0       0.47      0.39      0.43        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.61        87
   macro avg       0.28      0.30      0.29        87
weighted avg       0.53      0.61      0.56        87

[[ 0  9  0  0]
 [ 0 44 10  0]
 [ 0 14  9  0]
 [ 0  1  0  0]]
0.5610110635548736
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.71
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.78
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         9
         1.0       0.70      0.81      0.75        54
         2.0       0.54      0.57      0.55        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.66        87
   macro avg       0.31      0.35      0.33        87
weighted avg       0.58      0.66      0.61        87

[[ 0  9  0  0]
 [ 0 44 10  0]
 [ 0 10 13  0]
 [ 0  0  1  0]]
0.6130895272494685
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.64
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.75
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         9
         1.0       0.70      0.89      0.78        54
         2.0       0.61      0.48      0.54        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.68        87
   macro avg       0.33      0.34      0.33        87
weighted avg       0.59      0.68      0.63        87

[[ 0  9  0  0]
 [ 0 48  6  0]
 [ 0 12 11  0]
 [ 0  0  1  0]]
0.6262966077936641
87 87 87
Filename	True Label	Prediction
0602	1.0	1.0
0613	1.0	1.0
0620	1.0	1.0
0630	1.0	1.0
0638	1.0	1.0
0643	1.0	1.0
0715	1.0	1.0
0717	1.0	1.0
0719	1.0	1.0
0720	1.0	1.0
0723	1.0	1.0
0724	2.0	1.0
0803	1.0	1.0
0804	1.0	1.0
0809	1.0	1.0
0815	2.0	1.0
0822	1.0	1.0
0824	1.0	1.0
0910	1.0	1.0
0915	1.0	1.0
0916	1.0	1.0
0917	1.0	1.0
0919	1.0	1.0
0920	2.0	1.0
0925	1.0	1.0
0927	2.0	1.0
0928	2.0	1.0
1006	1.0	1.0
1014	2.0	1.0
1018	1.0	1.0
BER0611003	2.0	2.0
BER0611005	2.0	1.0
KYJ0611005A	0.0	1.0
LIB0611003A	0.0	1.0
LON0610002A	1.0	1.0
LON0611004A	1.0	1.0
MOS0611014	0.0	1.0
PHA0111002A	2.0	1.0
PHA0111004A	2.0	1.0
PHA0111012	1.0	1.0
PHA0112002A	1.0	1.0
PHA0112003A	1.0	1.0
PHA0112003B	0.0	1.0
PHA0112006B	2.0	1.0
PHA0209013	1.0	1.0
PHA0209024	0.0	1.0
PHA0209026	2.0	2.0
PHA0209039	1.0	2.0
PHA0411035	2.0	2.0
PHA0411036	1.0	2.0
PHA0411037	1.0	1.0
PHA0411038	2.0	2.0
PHA0411043	1.0	1.0
PHA0411047	1.0	2.0
PHA0411051	2.0	2.0
PHA0509007	1.0	1.0
PHA0509015	1.0	1.0
PHA0509019	1.0	1.0
PHA0509022	2.0	2.0
PHA0509024	1.0	1.0
PHA0509041	2.0	2.0
PHA0510004A	0.0	1.0
PHA0510013A	1.0	1.0
PHA0510036	2.0	1.0
PHA0510038	2.0	2.0
PHA0510039	1.0	2.0
PHA0510047	1.0	1.0
PHA0610005A	1.0	1.0
PHA0610016	2.0	2.0
PHA0610026	1.0	2.0
PHA0709008	1.0	1.0
PHA0809010	1.0	1.0
PHA0810004	1.0	1.0
PHA0810006	1.0	1.0
PHA0810011	1.0	1.0
PHA0811012	3.0	2.0
PHA0811016	1.0	1.0
PHA0811017	2.0	2.0
PHA1109027	1.0	1.0
PHA1110013	2.0	2.0
PHA1110014	1.0	2.0
PHA1110016	0.0	1.0
PHA1111003A	2.0	1.0
PHA1111004A	0.0	1.0
PHA1111008A	1.0	1.0
PHA1111008B	1.0	1.0
VAR0910005	0.0	1.0
LANGUAGE: CZ, 2th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.02
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.84
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         9
         1.0       0.63      1.00      0.77        54
         2.0       0.00      0.00      0.00        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.62        87
   macro avg       0.16      0.25      0.19        87
weighted avg       0.39      0.62      0.48        87

[[ 0  9  0  0]
 [ 0 54  0  0]
 [ 0 23  0  0]
 [ 0  0  1  0]]
0.4788177339901478
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.85
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.82
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         9
         1.0       0.70      0.91      0.79        54
         2.0       0.65      0.48      0.55        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.69        87
   macro avg       0.34      0.35      0.34        87
weighted avg       0.61      0.69      0.64        87

[[ 0  9  0  0]
 [ 0 49  5  0]
 [ 0 12 11  0]
 [ 0  0  1  0]]
0.635947348906192
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.77
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         9
         1.0       0.72      0.78      0.75        54
         2.0       0.55      0.70      0.62        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.67        87
   macro avg       0.32      0.37      0.34        87
weighted avg       0.60      0.67      0.63        87

[[ 0  9  0  0]
 [ 0 42 12  0]
 [ 0  7 16  0]
 [ 0  0  1  0]]
0.6282051282051282
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.70
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.76
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         9
         1.0       0.74      0.78      0.76        54
         2.0       0.57      0.74      0.64        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.68        87
   macro avg       0.33      0.38      0.35        87
weighted avg       0.61      0.68      0.64        87

[[ 0  9  0  0]
 [ 0 42 12  0]
 [ 0  6 17  0]
 [ 0  0  1  0]]
0.6393055384597349
87 87 87
Filename	True Label	Prediction
0606	1.0	1.0
0616	1.0	1.0
0621	1.0	1.0
0623	1.0	1.0
0624	2.0	1.0
0631	1.0	1.0
0637	1.0	1.0
0641	0.0	1.0
0644	1.0	1.0
0801	1.0	1.0
0807	2.0	1.0
0819	2.0	1.0
0826	1.0	1.0
0828	1.0	1.0
0829	1.0	1.0
0904	1.0	1.0
0924	1.0	1.0
0926	2.0	1.0
0929	0.0	1.0
1007	1.0	1.0
1010	1.0	1.0
1015	1.0	1.0
1022	1.0	1.0
1111	1.0	1.0
1113	1.0	1.0
BER0609003	1.0	2.0
LIB0611011	0.0	1.0
LON0611002B	0.0	1.0
MOS0509001	1.0	1.0
PAR1011009A	1.0	1.0
PHA0111001A	1.0	1.0
PHA0111004B	1.0	1.0
PHA0111015	2.0	2.0
PHA0111016	2.0	2.0
PHA0112007A	1.0	1.0
PHA0112007B	0.0	1.0
PHA0112009B	1.0	1.0
PHA0112012A	1.0	1.0
PHA0209008	1.0	1.0
PHA0209028	2.0	1.0
PHA0209034	1.0	1.0
PHA0411009B	0.0	1.0
PHA0411010A	1.0	1.0
PHA0411011A	2.0	1.0
PHA0411027	1.0	2.0
PHA0411030	3.0	2.0
PHA0411031	2.0	2.0
PHA0411032	1.0	2.0
PHA0411033	1.0	1.0
PHA0411039	1.0	1.0
PHA0411056	2.0	2.0
PHA0411060	2.0	2.0
PHA0411062	1.0	2.0
PHA0509018	2.0	2.0
PHA0509021	2.0	2.0
PHA0509030	2.0	2.0
PHA0509037	2.0	2.0
PHA0509040	1.0	1.0
PHA0509044	1.0	2.0
PHA0509045	1.0	1.0
PHA0510002B	1.0	1.0
PHA0510004B	0.0	1.0
PHA0510034	2.0	2.0
PHA0510040	2.0	2.0
PHA0610005B	0.0	1.0
PHA0610017	2.0	2.0
PHA0710009	1.0	1.0
PHA0710012	2.0	2.0
PHA0710014	2.0	2.0
PHA0710017	1.0	2.0
PHA0810002	1.0	1.0
PHA0810008	1.0	2.0
PHA0811013	2.0	2.0
PHA0811020	1.0	2.0
PHA1109002	2.0	2.0
PHA1109004	1.0	2.0
PHA1110001A	1.0	1.0
PHA1110001B	1.0	1.0
PHA1110022	2.0	2.0
PHA1111003B	1.0	1.0
PHA1111006B	0.0	1.0
TI071122B	1.0	1.0
VAR0909003	1.0	1.0
VAR0909005	1.0	1.0
VAR0909009	1.0	2.0
VAR0910006	1.0	2.0
VAR0910007	1.0	2.0
LANGUAGE: CZ, 3th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.05
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.93
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.62      1.00      0.77        54
         2.0       0.00      0.00      0.00        22
         3.0       0.00      0.00      0.00         1

    accuracy                           0.62        87
   macro avg       0.16      0.25      0.19        87
weighted avg       0.39      0.62      0.48        87

[[ 0 10  0  0]
 [ 0 54  0  0]
 [ 0 22  0  0]
 [ 0  1  0  0]]
0.47542186353631705
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.93
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.84
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.62      1.00      0.77        54
         2.0       0.00      0.00      0.00        22
         3.0       0.00      0.00      0.00         1

    accuracy                           0.62        87
   macro avg       0.16      0.25      0.19        87
weighted avg       0.39      0.62      0.48        87

[[ 0 10  0  0]
 [ 0 54  0  0]
 [ 0 22  0  0]
 [ 0  1  0  0]]
0.47542186353631705
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.92
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.93
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.62      1.00      0.77        54
         2.0       0.00      0.00      0.00        22
         3.0       0.00      0.00      0.00         1

    accuracy                           0.62        87
   macro avg       0.16      0.25      0.19        87
weighted avg       0.39      0.62      0.48        87

[[ 0 10  0  0]
 [ 0 54  0  0]
 [ 0 22  0  0]
 [ 0  1  0  0]]
0.47542186353631705
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.88
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.81
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.63      1.00      0.77        54
         2.0       1.00      0.05      0.09        22
         3.0       0.00      0.00      0.00         1

    accuracy                           0.63        87
   macro avg       0.41      0.26      0.21        87
weighted avg       0.64      0.63      0.50        87

[[ 0 10  0  0]
 [ 0 54  0  0]
 [ 0 21  1  0]
 [ 0  1  0  0]]
0.5008067394873992
87 87 87
Filename	True Label	Prediction
0603	1.0	1.0
0604	1.0	1.0
0605	1.0	1.0
0608	0.0	1.0
0610	1.0	1.0
0611	1.0	1.0
0617	1.0	1.0
0618	1.0	1.0
0619	1.0	1.0
0622	1.0	1.0
0629	1.0	1.0
0642	1.0	1.0
0718	1.0	1.0
0817	1.0	1.0
0820	0.0	1.0
0821	1.0	1.0
0823	2.0	1.0
0902	2.0	1.0
0911	1.0	1.0
0914	1.0	1.0
0918	1.0	1.0
0922	1.0	1.0
0930	1.0	1.0
1002	1.0	1.0
1003	0.0	1.0
1005	1.0	1.0
1009	1.0	1.0
1020	1.0	1.0
1021	1.0	1.0
1114	1.0	1.0
1115	1.0	1.0
KYJ0611003A	1.0	1.0
KYJ0611009B	0.0	1.0
LIB0611001B	1.0	1.0
LIB0611002A	1.0	1.0
LON0610002B	1.0	1.0
LON0611002A	1.0	1.0
MOS0509004	1.0	1.0
MOS0611012	2.0	1.0
MOS0611013	2.0	1.0
MOS0611015	2.0	1.0
PAR1011013	2.0	1.0
PAR1011015	2.0	1.0
PHA0111002B	1.0	1.0
PHA0112006A	2.0	1.0
PHA0112012B	1.0	1.0
PHA0411008A	1.0	1.0
PHA0411011B	1.0	1.0
PHA0411012A	1.0	1.0
PHA0411034	0.0	1.0
PHA0411054	2.0	1.0
PHA0411055	2.0	1.0
PHA0411061	2.0	1.0
PHA0509002	0.0	1.0
PHA0509017	1.0	1.0
PHA0509025	3.0	1.0
PHA0509028	2.0	1.0
PHA0509036	2.0	1.0
PHA0510029	2.0	1.0
PHA0510030	2.0	1.0
PHA0510037	1.0	1.0
PHA0510048	1.0	1.0
PHA0510050	2.0	1.0
PHA0610006A	0.0	1.0
PHA0610025	2.0	2.0
PHA0710011	2.0	1.0
PHA0710015	2.0	1.0
PHA0710016	2.0	1.0
PHA0710019	1.0	1.0
PHA0810009	1.0	1.0
PHA0810012	1.0	1.0
PHA0811014	1.0	1.0
PHA1109001	1.0	1.0
PHA1109003	1.0	1.0
PHA1110002B	1.0	1.0
PHA1110003A	1.0	1.0
PHA1110015	2.0	1.0
PHA1110019	2.0	1.0
PHA1111001A	1.0	1.0
PHA1111001B	0.0	1.0
PHA1111002B	0.0	1.0
PHA1111004B	0.0	1.0
PHA1111006A	1.0	1.0
VAR0909007	1.0	1.0
VAR0910004	1.0	1.0
VAR0910009	1.0	1.0
VAR0910011	1.0	1.0
LANGUAGE: CZ, 4th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.07
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.02
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         9
         1.0       0.62      1.00      0.76        53
         2.0       0.00      0.00      0.00        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.62        86
   macro avg       0.15      0.25      0.19        86
weighted avg       0.38      0.62      0.47        86

[[ 0  9  0  0]
 [ 0 53  0  0]
 [ 0 23  0  0]
 [ 0  1  0  0]]
0.46996821147732976
86 86 86



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.99
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.93
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         9
         1.0       0.62      1.00      0.76        53
         2.0       0.00      0.00      0.00        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.62        86
   macro avg       0.15      0.25      0.19        86
weighted avg       0.38      0.62      0.47        86

[[ 0  9  0  0]
 [ 0 53  0  0]
 [ 0 23  0  0]
 [ 0  1  0  0]]
0.46996821147732976
86 86 86



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.94
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.92
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         9
         1.0       0.62      1.00      0.76        53
         2.0       0.00      0.00      0.00        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.62        86
   macro avg       0.15      0.25      0.19        86
weighted avg       0.38      0.62      0.47        86

[[ 0  9  0  0]
 [ 0 53  0  0]
 [ 0 23  0  0]
 [ 0  1  0  0]]
0.46996821147732976
86 86 86



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.91
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.90
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         9
         1.0       0.62      1.00      0.76        53
         2.0       0.00      0.00      0.00        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.62        86
   macro avg       0.15      0.25      0.19        86
weighted avg       0.38      0.62      0.47        86

[[ 0  9  0  0]
 [ 0 53  0  0]
 [ 0 23  0  0]
 [ 0  1  0  0]]
0.46996821147732976
86 86 86
Filename	True Label	Prediction
0607	2.0	1.0
0612	1.0	1.0
0614	1.0	1.0
0615	1.0	1.0
0627	1.0	1.0
0633	2.0	1.0
0634	2.0	1.0
0639	1.0	1.0
0640	1.0	1.0
0645	2.0	1.0
0716	1.0	1.0
0802	1.0	1.0
0806	1.0	1.0
0810	1.0	1.0
0811	2.0	1.0
0812	1.0	1.0
0813	1.0	1.0
0814	1.0	1.0
0816	2.0	1.0
0818	1.0	1.0
0901	2.0	1.0
0913	1.0	1.0
0921	1.0	1.0
1001	1.0	1.0
1116	1.0	1.0
1117	1.0	1.0
BER0611006	2.0	1.0
BER0611007	0.0	1.0
KYJ0611004A	1.0	1.0
KYJ0611006A	0.0	1.0
LIB0611001A	1.0	1.0
LIB0611004B	1.0	1.0
LON0611004B	0.0	1.0
PAR1011009B	1.0	1.0
PAR1011016	2.0	1.0
PAR1011018	1.0	1.0
PHA0111001B	1.0	1.0
PHA0111003A	0.0	1.0
PHA0111005B	1.0	1.0
PHA0111011	1.0	1.0
PHA0112009A	1.0	1.0
PHA0209031	2.0	1.0
PHA0209038	2.0	1.0
PHA0210004	1.0	1.0
PHA0210007	1.0	1.0
PHA0210008	0.0	1.0
PHA0411008B	1.0	1.0
PHA0411009A	1.0	1.0
PHA0411012B	1.0	1.0
PHA0411029	1.0	1.0
PHA0411042	2.0	1.0
PHA0411045	1.0	1.0
PHA0411059	2.0	1.0
PHA0509032	2.0	1.0
PHA0509034	1.0	1.0
PHA0509035	1.0	1.0
PHA0509038	1.0	1.0
PHA0509043	1.0	1.0
PHA0510003A	1.0	1.0
PHA0510003B	1.0	1.0
PHA0510010A	0.0	1.0
PHA0510023	2.0	1.0
PHA0510027	1.0	1.0
PHA0510046	1.0	1.0
PHA0510049	2.0	1.0
PHA0610015	2.0	1.0
PHA0710010	2.0	1.0
PHA0809009	1.0	1.0
PHA0810001	2.0	1.0
PHA0810010	1.0	1.0
PHA0810015	3.0	1.0
PHA0811019	2.0	1.0
PHA1109006	2.0	1.0
PHA1109008	0.0	1.0
PHA1109024	2.0	1.0
PHA1109025	0.0	1.0
PHA1109026	1.0	1.0
PHA1109028	1.0	1.0
PHA1110002A	1.0	1.0
PHA1110017	1.0	1.0
PHA1111002A	1.0	1.0
PHA1111009A	0.0	1.0
ST071122B	1.0	1.0
VAR0909004	1.0	1.0
VAR0909006	2.0	1.0
VAR0909010	1.0	1.0
Averaged weighted F1-scores 0.581730143301209
144.90552995391704 65.35717405024758
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']
LANGUAGE: CZ, 0th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.21
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.80
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.80      0.77      0.78        26
         2.0       0.80      0.69      0.74        29
         3.0       0.76      1.00      0.86        28
         4.0       0.00      0.00      0.00         2

    accuracy                           0.78        87
   macro avg       0.47      0.49      0.48        87
weighted avg       0.75      0.78      0.76        87

[[ 0  2  0  0  0]
 [ 0 20  5  1  0]
 [ 0  3 20  6  0]
 [ 0  0  0 28  0]
 [ 0  0  0  2  0]]
0.7585829341069368
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.80
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.74
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.59      1.00      0.74        26
         2.0       0.75      0.31      0.44        29
         3.0       0.81      0.89      0.85        28
         4.0       0.00      0.00      0.00         2

    accuracy                           0.69        87
   macro avg       0.43      0.44      0.41        87
weighted avg       0.69      0.69      0.64        87

[[ 0  2  0  0  0]
 [ 0 26  0  0  0]
 [ 0 16  9  4  0]
 [ 0  0  3 25  0]
 [ 0  0  0  2  0]]
0.641089730927367
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.70
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.62
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.78      0.81      0.79        26
         2.0       0.80      0.69      0.74        29
         3.0       0.80      1.00      0.89        28
         4.0       0.00      0.00      0.00         2

    accuracy                           0.79        87
   macro avg       0.48      0.50      0.48        87
weighted avg       0.76      0.79      0.77        87

[[ 0  2  0  0  0]
 [ 0 21  5  0  0]
 [ 0  4 20  5  0]
 [ 0  0  0 28  0]
 [ 0  0  0  2  0]]
0.7698177466123682
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.60
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.61
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.78      0.81      0.79        26
         2.0       0.80      0.69      0.74        29
         3.0       0.80      1.00      0.89        28
         4.0       0.00      0.00      0.00         2

    accuracy                           0.79        87
   macro avg       0.48      0.50      0.48        87
weighted avg       0.76      0.79      0.77        87

[[ 0  2  0  0  0]
 [ 0 21  5  0  0]
 [ 0  4 20  5  0]
 [ 0  0  0 28  0]
 [ 0  0  0  2  0]]
0.7698177466123682
87 87 87
Filename	True Label	Prediction
0601	2.0	2.0
0610	2.0	2.0
0616	1.0	2.0
0620	2.0	2.0
0629	1.0	2.0
0641	1.0	2.0
0644	2.0	2.0
0714	2.0	2.0
0715	2.0	2.0
0722	2.0	2.0
0809	2.0	2.0
0815	2.0	2.0
0819	2.0	2.0
0827	1.0	2.0
0905	2.0	2.0
0915	2.0	2.0
0920	2.0	2.0
0922	2.0	2.0
0927	1.0	2.0
0928	2.0	2.0
1002	2.0	2.0
1006	2.0	2.0
1021	2.0	2.0
1116	2.0	2.0
BER0611006	2.0	3.0
BER0611007	3.0	3.0
KYJ0611003A	1.0	1.0
KYJ0611005A	1.0	1.0
LIB0611002A	1.0	1.0
LIB0611004B	1.0	1.0
LON0611002A	1.0	1.0
LON0611002B	1.0	1.0
MOS0611013	3.0	3.0
PAR1011008A	2.0	1.0
PAR1011016	3.0	3.0
PAR1011017	3.0	3.0
PAR1011018	4.0	3.0
PHA0111001A	1.0	1.0
PHA0111001B	1.0	1.0
PHA0111012	2.0	3.0
PHA0111014	2.0	3.0
PHA0111015	3.0	3.0
PHA0111018	3.0	3.0
PHA0112006A	2.0	1.0
PHA0112006B	2.0	1.0
PHA0210008	1.0	1.0
PHA0411009A	1.0	1.0
PHA0411009B	1.0	1.0
PHA0411010A	0.0	1.0
PHA0411010B	0.0	1.0
PHA0411027	2.0	3.0
PHA0411028	2.0	3.0
PHA0411061	3.0	3.0
PHA0509007	1.0	1.0
PHA0509013	1.0	1.0
PHA0509015	3.0	3.0
PHA0509018	3.0	3.0
PHA0509026	3.0	3.0
PHA0509030	3.0	3.0
PHA0509034	3.0	3.0
PHA0509035	3.0	3.0
PHA0509039	3.0	3.0
PHA0510002B	1.0	1.0
PHA0510004B	1.0	1.0
PHA0510013B	1.0	1.0
PHA0510048	3.0	3.0
PHA0610005A	1.0	1.0
PHA0610025	3.0	3.0
PHA0710011	3.0	3.0
PHA0710016	3.0	3.0
PHA0810001	3.0	3.0
PHA0811013	4.0	3.0
PHA0811020	3.0	3.0
PHA1109002	3.0	3.0
PHA1109005	2.0	2.0
PHA1109023	2.0	1.0
PHA1110019	3.0	3.0
PHA1110022	3.0	3.0
PHA1111006A	1.0	1.0
PHA1111006B	1.0	1.0
PHA1111008A	1.0	1.0
PHA1111009A	1.0	1.0
VAR0909006	3.0	3.0
VAR0909007	3.0	3.0
VAR0910006	3.0	3.0
VAR0910007	3.0	3.0
VAR0910010	3.0	3.0
LANGUAGE: CZ, 1th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.23
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.85
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.86      0.70      0.78        27
         2.0       0.68      0.75      0.71        28
         3.0       0.79      0.93      0.86        29
         4.0       0.00      0.00      0.00         2

    accuracy                           0.77        87
   macro avg       0.47      0.48      0.47        87
weighted avg       0.75      0.77      0.76        87

[[ 0  1  0  0  0]
 [ 0 19  8  0  0]
 [ 0  2 21  5  0]
 [ 0  0  2 27  0]
 [ 0  0  0  2  0]]
0.75549565238135
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.80
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.66
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.67      0.96      0.79        27
         2.0       0.83      0.36      0.50        28
         3.0       0.78      0.97      0.86        29
         4.0       0.00      0.00      0.00         2

    accuracy                           0.74        87
   macro avg       0.46      0.46      0.43        87
weighted avg       0.73      0.74      0.69        87

[[ 0  1  0  0  0]
 [ 0 26  1  0  0]
 [ 0 12 10  6  0]
 [ 0  0  1 28  0]
 [ 0  0  0  2  0]]
0.6926131339924443
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.68
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.63
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.79      0.81      0.80        27
         2.0       0.72      0.82      0.77        28
         3.0       0.93      0.86      0.89        29
         4.0       0.00      0.00      0.00         2

    accuracy                           0.80        87
   macro avg       0.49      0.50      0.49        87
weighted avg       0.78      0.80      0.79        87

[[ 0  1  0  0  0]
 [ 0 22  5  0  0]
 [ 0  5 23  0  0]
 [ 0  0  4 25  0]
 [ 0  0  0  2  0]]
0.7926382047071702
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.61
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.57
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.86      0.70      0.78        27
         2.0       0.69      0.71      0.70        28
         3.0       0.78      0.97      0.86        29
         4.0       0.00      0.00      0.00         2

    accuracy                           0.77        87
   macro avg       0.47      0.48      0.47        87
weighted avg       0.75      0.77      0.75        87

[[ 0  1  0  0  0]
 [ 0 19  8  0  0]
 [ 0  2 20  6  0]
 [ 0  0  1 28  0]
 [ 0  0  0  2  0]]
0.7537070540441035
87 87 87
Filename	True Label	Prediction
0605	2.0	2.0
0606	2.0	2.0
0608	1.0	2.0
0611	2.0	2.0
0614	2.0	2.0
0618	2.0	2.0
0622	1.0	2.0
0623	2.0	2.0
0624	2.0	2.0
0640	2.0	2.0
0717	2.0	2.0
0719	2.0	2.0
0806	1.0	2.0
0812	2.0	2.0
0814	1.0	2.0
0816	2.0	2.0
0824	2.0	2.0
0826	1.0	2.0
0914	2.0	2.0
0919	1.0	2.0
0925	1.0	2.0
0930	1.0	2.0
1007	2.0	2.0
1009	2.0	2.0
1015	2.0	2.0
1020	2.0	2.0
1022	2.0	2.0
1112	2.0	2.0
LIB0611001B	1.0	1.0
LIB0611002B	1.0	1.0
LIB0611011	2.0	3.0
LON0611003	3.0	3.0
PAR1011009B	1.0	1.0
PHA0111002A	1.0	1.0
PHA0111003B	1.0	1.0
PHA0111004B	1.0	1.0
PHA0111005A	1.0	1.0
PHA0111011	3.0	3.0
PHA0112009B	1.0	1.0
PHA0209013	1.0	1.0
PHA0209024	3.0	3.0
PHA0209028	3.0	3.0
PHA0209031	4.0	3.0
PHA0209039	3.0	3.0
PHA0210007	1.0	1.0
PHA0411008A	1.0	1.0
PHA0411012A	1.0	1.0
PHA0411043	3.0	3.0
PHA0509017	3.0	3.0
PHA0509019	3.0	2.0
PHA0509021	2.0	3.0
PHA0509022	4.0	3.0
PHA0509025	3.0	3.0
PHA0509028	3.0	3.0
PHA0509031	2.0	3.0
PHA0509038	2.0	3.0
PHA0509041	3.0	3.0
PHA0509044	3.0	3.0
PHA0510003B	1.0	1.0
PHA0510004A	1.0	1.0
PHA0510010B	0.0	1.0
PHA0510031	3.0	3.0
PHA0510035	3.0	3.0
PHA0510050	3.0	3.0
PHA0610006B	1.0	1.0
PHA0610018	3.0	3.0
PHA0610019B	1.0	1.0
PHA0710010	3.0	3.0
PHA0710014	3.0	3.0
PHA0710018	3.0	3.0
PHA0710021	3.0	3.0
PHA0810002	3.0	3.0
PHA0810004	3.0	3.0
PHA0811010	3.0	3.0
PHA1109001	1.0	1.0
PHA1109007	2.0	3.0
PHA1110001B	1.0	1.0
PHA1110002A	2.0	1.0
PHA1110015	3.0	3.0
PHA1110017	3.0	3.0
PHA1110021	3.0	3.0
PHA1111001A	2.0	1.0
PHA1111001B	1.0	1.0
VAR0209036	2.0	3.0
VAR0909008	3.0	3.0
VAR0910004	3.0	3.0
VAR0910011	3.0	3.0
LANGUAGE: CZ, 2th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.33
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.90
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.84      0.59      0.70        27
         2.0       0.62      0.64      0.63        28
         3.0       0.74      1.00      0.85        29
         4.0       0.00      0.00      0.00         2

    accuracy                           0.72        87
   macro avg       0.44      0.45      0.44        87
weighted avg       0.71      0.72      0.70        87

[[ 0  1  0  0  0]
 [ 0 16 11  0  0]
 [ 0  2 18  8  0]
 [ 0  0  0 29  0]
 [ 0  0  0  2  0]]
0.7034725671220118
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.87
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.81
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.80      0.59      0.68        27
         2.0       0.55      0.82      0.66        28
         3.0       0.84      0.72      0.78        29
         4.0       0.00      0.00      0.00         2

    accuracy                           0.69        87
   macro avg       0.44      0.43      0.42        87
weighted avg       0.70      0.69      0.68        87

[[ 0  1  0  0  0]
 [ 0 16 11  0  0]
 [ 0  3 23  2  0]
 [ 0  0  8 21  0]
 [ 0  0  0  2  0]]
0.6820521181489634
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.73
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.77
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.85      0.63      0.72        27
         2.0       0.64      0.64      0.64        28
         3.0       0.74      1.00      0.85        29
         4.0       0.00      0.00      0.00         2

    accuracy                           0.74        87
   macro avg       0.45      0.45      0.44        87
weighted avg       0.72      0.74      0.72        87

[[ 0  1  0  0  0]
 [ 0 17 10  0  0]
 [ 0  2 18  8  0]
 [ 0  0  0 29  0]
 [ 0  0  0  2  0]]
0.7157150461064836
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.61
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.77
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.84      0.59      0.70        27
         2.0       0.58      0.68      0.62        28
         3.0       0.74      0.90      0.81        29
         4.0       0.00      0.00      0.00         2

    accuracy                           0.70        87
   macro avg       0.43      0.43      0.43        87
weighted avg       0.69      0.70      0.69        87

[[ 0  1  0  0  0]
 [ 0 16 11  0  0]
 [ 0  2 19  7  0]
 [ 0  0  3 26  0]
 [ 0  0  0  2  0]]
0.6872153062812856
87 87 87
Filename	True Label	Prediction
0607	2.0	2.0
0609	1.0	2.0
0613	1.0	2.0
0632	1.0	2.0
0634	2.0	2.0
0635	1.0	1.0
0637	2.0	2.0
0643	2.0	2.0
0716	2.0	2.0
0720	2.0	2.0
0802	1.0	2.0
0808	1.0	2.0
0810	2.0	2.0
0811	2.0	2.0
0813	2.0	2.0
0823	2.0	2.0
0901	2.0	2.0
0906	2.0	2.0
0907	2.0	2.0
0910	1.0	2.0
0911	2.0	2.0
0916	1.0	2.0
0923	1.0	2.0
0926	2.0	2.0
0929	1.0	2.0
1014	2.0	2.0
1017	1.0	2.0
1018	1.0	2.0
1019	2.0	2.0
1023	2.0	2.0
KYJ0611005B	1.0	1.0
KYJ0611009A	1.0	1.0
LIB0611001A	1.0	1.0
LIB0611003A	1.0	1.0
LON0610002A	1.0	1.0
LON0610002B	1.0	1.0
MOS0509001	2.0	2.0
MOS0509004	3.0	2.0
MOS0611012	3.0	3.0
MOS0611014	2.0	3.0
PAR1011014	2.0	3.0
PHA0111010	2.0	3.0
PHA0112003B	1.0	1.0
PHA0112012A	2.0	1.0
PHA0112012B	1.0	1.0
PHA0209038	4.0	3.0
PHA0411030	3.0	3.0
PHA0411033	3.0	3.0
PHA0411035	2.0	3.0
PHA0411036	3.0	3.0
PHA0411038	3.0	3.0
PHA0411047	3.0	3.0
PHA0411054	3.0	2.0
PHA0411055	3.0	3.0
PHA0411058	3.0	3.0
PHA0411059	3.0	3.0
PHA0509002	1.0	1.0
PHA0509027	3.0	3.0
PHA0509033	2.0	3.0
PHA0509042	3.0	3.0
PHA0509045	3.0	2.0
PHA0510002A	1.0	1.0
PHA0510034	3.0	3.0
PHA0510038	3.0	3.0
PHA0510047	2.0	3.0
PHA0510049	2.0	3.0
PHA0610005B	0.0	1.0
PHA0610006A	2.0	1.0
PHA0709008	3.0	3.0
PHA0809009	3.0	3.0
PHA0810003	3.0	3.0
PHA0810006	3.0	3.0
PHA0810011	3.0	3.0
PHA0811012	3.0	3.0
PHA0811017	3.0	3.0
PHA0811019	4.0	3.0
PHA1109024	3.0	3.0
PHA1110004A	1.0	1.0
PHA1110013	3.0	3.0
PHA1110014	3.0	3.0
PHA1111002A	1.0	1.0
PHA1111004A	1.0	1.0
PHA1111004B	1.0	1.0
PHA1111008B	1.0	1.0
VAR0909003	3.0	3.0
VAR0909004	3.0	3.0
VAR0909009	3.0	3.0
LANGUAGE: CZ, 3th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.21
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.82
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.62      0.31      0.41        26
         2.0       0.55      0.79      0.65        28
         3.0       0.85      1.00      0.92        29
         4.0       0.00      0.00      0.00         2

    accuracy                           0.68        87
   macro avg       0.40      0.42      0.40        87
weighted avg       0.65      0.68      0.64        87

[[ 0  1  1  0  0]
 [ 0  8 17  1  0]
 [ 0  4 22  2  0]
 [ 0  0  0 29  0]
 [ 0  0  0  2  0]]
0.6377324876310677
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.85
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.63
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.86      0.73      0.79        26
         2.0       0.75      0.86      0.80        28
         3.0       0.88      1.00      0.94        29
         4.0       0.00      0.00      0.00         2

    accuracy                           0.83        87
   macro avg       0.50      0.52      0.51        87
weighted avg       0.79      0.83      0.81        87

[[ 0  1  1  0  0]
 [ 0 19  7  0  0]
 [ 0  2 24  2  0]
 [ 0  0  0 29  0]
 [ 0  0  0  2  0]]
0.8058892596712396
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.72
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.60
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.86      0.73      0.79        26
         2.0       0.71      0.89      0.79        28
         3.0       0.90      0.93      0.92        29
         4.0       0.00      0.00      0.00         2

    accuracy                           0.82        87
   macro avg       0.50      0.51      0.50        87
weighted avg       0.79      0.82      0.80        87

[[ 0  1  1  0  0]
 [ 0 19  7  0  0]
 [ 0  2 25  1  0]
 [ 0  0  2 27  0]
 [ 0  0  0  2  0]]
0.7971026257116263
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.61
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.53
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.87      0.77      0.82        26
         2.0       0.78      0.89      0.83        28
         3.0       0.91      1.00      0.95        29
         4.0       0.00      0.00      0.00         2

    accuracy                           0.85        87
   macro avg       0.51      0.53      0.52        87
weighted avg       0.81      0.85      0.83        87

[[ 0  1  1  0  0]
 [ 0 20  6  0  0]
 [ 0  2 25  1  0]
 [ 0  0  0 29  0]
 [ 0  0  0  2  0]]
0.8290987772535054
87 87 87
Filename	True Label	Prediction
0602	2.0	2.0
0603	2.0	2.0
0604	2.0	2.0
0615	2.0	2.0
0625	2.0	2.0
0626	2.0	2.0
0631	2.0	2.0
0633	2.0	2.0
0642	2.0	2.0
0645	2.0	2.0
0721	2.0	2.0
0723	1.0	2.0
0724	2.0	2.0
0801	2.0	2.0
0804	1.0	2.0
0807	2.0	2.0
0817	2.0	2.0
0821	2.0	2.0
0822	1.0	2.0
0825	1.0	2.0
0829	2.0	2.0
0904	2.0	2.0
0917	1.0	2.0
0918	2.0	2.0
1005	2.0	2.0
1010	2.0	2.0
1016	1.0	2.0
1111	2.0	2.0
1114	2.0	2.0
1115	2.0	2.0
1117	1.0	1.0
9999	0.0	2.0
BER0611003	2.0	3.0
KYJ0611004A	1.0	1.0
KYJ0611006A	0.0	1.0
KYJ0611009B	1.0	1.0
LON0611004B	1.0	1.0
MOS0611015	3.0	3.0
PAR1011013	3.0	3.0
PHA0111005B	1.0	1.0
PHA0111016	3.0	3.0
PHA0112002A	1.0	1.0
PHA0112002B	1.0	1.0
PHA0112003A	1.0	1.0
PHA0112007A	1.0	1.0
PHA0112007B	1.0	1.0
PHA0209008	1.0	1.0
PHA0209034	3.0	3.0
PHA0210004	1.0	1.0
PHA0411008B	2.0	1.0
PHA0411011A	1.0	1.0
PHA0411032	3.0	3.0
PHA0411034	2.0	2.0
PHA0411037	3.0	3.0
PHA0411039	3.0	3.0
PHA0411041	3.0	3.0
PHA0411045	3.0	3.0
PHA0411051	4.0	3.0
PHA0411053	4.0	3.0
PHA0411062	3.0	3.0
PHA0509020	3.0	3.0
PHA0509024	3.0	3.0
PHA0509032	3.0	3.0
PHA0509040	3.0	3.0
PHA0509043	3.0	3.0
PHA0510003A	1.0	1.0
PHA0510027	3.0	3.0
PHA0510030	3.0	3.0
PHA0610007B	1.0	1.0
PHA0610015	3.0	3.0
PHA0610017	3.0	3.0
PHA0610019A	2.0	1.0
PHA0710015	3.0	3.0
PHA0710017	3.0	3.0
PHA0710019	3.0	3.0
PHA0810012	3.0	3.0
PHA0810015	3.0	3.0
PHA0811014	3.0	3.0
PHA0811016	3.0	3.0
PHA1109008	1.0	1.0
PHA1109026	3.0	3.0
PHA1109027	3.0	3.0
PHA1110001A	1.0	1.0
PHA1111002B	1.0	1.0
PHA1111003A	1.0	1.0
ST071122B	1.0	1.0
VAR0909005	3.0	3.0
LANGUAGE: CZ, 4th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.35
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.02
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.61      0.65      0.63        26
         2.0       0.58      0.50      0.54        28
         3.0       0.79      0.96      0.87        28
         4.0       0.00      0.00      0.00         2

    accuracy                           0.67        86
   macro avg       0.40      0.42      0.41        86
weighted avg       0.63      0.67      0.65        86

[[ 0  2  0  0  0]
 [ 0 17  9  0  0]
 [ 0  9 14  5  0]
 [ 0  0  1 27  0]
 [ 0  0  0  2  0]]
0.649237095598686
86 86 86



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.84
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.70
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.65      0.65      0.65        26
         2.0       0.62      0.54      0.58        28
         3.0       0.78      1.00      0.88        28
         4.0       0.00      0.00      0.00         2

    accuracy                           0.70        86
   macro avg       0.41      0.44      0.42        86
weighted avg       0.65      0.70      0.67        86

[[ 0  2  0  0  0]
 [ 0 17  9  0  0]
 [ 0  7 15  6  0]
 [ 0  0  0 28  0]
 [ 0  0  0  2  0]]
0.6703935599284437
86 86 86



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.67
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.64
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.74      0.65      0.69        26
         2.0       0.71      0.79      0.75        28
         3.0       0.88      1.00      0.93        28
         4.0       0.00      0.00      0.00         2

    accuracy                           0.78        86
   macro avg       0.46      0.49      0.47        86
weighted avg       0.74      0.78      0.76        86

[[ 0  2  0  0  0]
 [ 0 17  9  0  0]
 [ 0  4 22  2  0]
 [ 0  0  0 28  0]
 [ 0  0  0  2  0]]
0.7564593673496202
86 86 86



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.62
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.63
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.71      0.65      0.68        26
         2.0       0.71      0.79      0.75        28
         3.0       0.90      1.00      0.95        28
         4.0       0.00      0.00      0.00         2

    accuracy                           0.78        86
   macro avg       0.46      0.49      0.47        86
weighted avg       0.74      0.78      0.76        86

[[ 0  2  0  0  0]
 [ 0 17  9  0  0]
 [ 0  5 22  1  0]
 [ 0  0  0 28  0]
 [ 0  0  0  2  0]]
0.7574142688214426
86 86 86
Filename	True Label	Prediction
0612	1.0	2.0
0617	1.0	2.0
0619	2.0	2.0
0621	2.0	2.0
0627	2.0	2.0
0628	1.0	2.0
0630	1.0	1.0
0636	2.0	2.0
0638	1.0	2.0
0639	1.0	2.0
0718	1.0	2.0
0725	2.0	2.0
0803	1.0	2.0
0805	1.0	2.0
0818	2.0	2.0
0820	1.0	1.0
0828	2.0	2.0
0902	1.0	2.0
0903	2.0	2.0
0912	2.0	2.0
0913	2.0	2.0
0921	2.0	2.0
0924	2.0	2.0
1001	2.0	2.0
1003	2.0	2.0
1004	2.0	1.0
1008	2.0	2.0
1113	2.0	2.0
BER0609003	3.0	3.0
BER0611005	2.0	2.0
KYJ0611006B	0.0	1.0
LIB0611004A	1.0	1.0
LON0611004A	1.0	1.0
PAR1011009A	1.0	1.0
PAR1011015	3.0	3.0
PHA0111002B	2.0	1.0
PHA0111003A	1.0	1.0
PHA0111004A	1.0	1.0
PHA0112009A	2.0	1.0
PHA0209001	1.0	1.0
PHA0209026	3.0	3.0
PHA0210001	1.0	1.0
PHA0411011B	1.0	1.0
PHA0411012B	1.0	1.0
PHA0411029	3.0	3.0
PHA0411031	3.0	3.0
PHA0411042	3.0	3.0
PHA0411044	4.0	3.0
PHA0411056	3.0	3.0
PHA0411060	2.0	3.0
PHA0509036	3.0	3.0
PHA0509037	2.0	2.0
PHA0510010A	2.0	1.0
PHA0510013A	1.0	1.0
PHA0510023	3.0	3.0
PHA0510029	3.0	3.0
PHA0510032	3.0	3.0
PHA0510036	3.0	3.0
PHA0510037	2.0	2.0
PHA0510039	3.0	3.0
PHA0510040	3.0	3.0
PHA0510046	3.0	3.0
PHA0610007A	1.0	1.0
PHA0610016	3.0	3.0
PHA0610026	3.0	3.0
PHA0710009	3.0	3.0
PHA0710012	3.0	3.0
PHA0710013	4.0	3.0
PHA0809010	2.0	2.0
PHA0810008	3.0	3.0
PHA0810009	3.0	3.0
PHA0810010	3.0	3.0
PHA1109003	2.0	2.0
PHA1109004	3.0	3.0
PHA1109006	2.0	2.0
PHA1109025	2.0	1.0
PHA1109028	3.0	3.0
PHA1110002B	1.0	1.0
PHA1110003A	1.0	1.0
PHA1110003B	0.0	1.0
PHA1110016	3.0	3.0
PHA1111003B	1.0	1.0
TI071122B	1.0	1.0
VAR0909010	3.0	3.0
VAR0910005	3.0	3.0
VAR0910009	3.0	3.0
Averaged weighted F1-scores 0.7594506306025411
144.90552995391704 65.35717405024758
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']
LANGUAGE: CZ, 0th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.30
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.09
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.59      1.00      0.74        27
         2.0       0.49      0.56      0.52        36
         3.0       0.00      0.00      0.00        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.54        87
   macro avg       0.21      0.31      0.25        87
weighted avg       0.38      0.54      0.44        87

[[ 0  1  0  0  0]
 [ 0 27  0  0  0]
 [ 0 16 20  0  0]
 [ 0  2 18  0  0]
 [ 0  0  3  0  0]]
0.4445276027703992
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.10
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.93
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.68      0.48      0.57        27
         2.0       0.47      0.47      0.47        36
         3.0       0.47      0.75      0.58        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.52        87
   macro avg       0.33      0.34      0.32        87
weighted avg       0.52      0.52      0.50        87

[[ 0  1  0  0  0]
 [ 0 13 14  0  0]
 [ 0  4 17 15  0]
 [ 0  1  4 15  0]
 [ 0  0  1  2  0]]
0.5034405873986083
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.98
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.94
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.68      0.48      0.57        27
         2.0       0.50      0.42      0.45        36
         3.0       0.47      0.90      0.62        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.53        87
   macro avg       0.33      0.36      0.33        87
weighted avg       0.53      0.53      0.51        87

[[ 0  1  0  0  0]
 [ 0 13 14  0  0]
 [ 0  4 15 17  0]
 [ 0  1  1 18  0]
 [ 0  0  0  3  0]]
0.5061873451988739
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.91
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.88
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.71      0.74      0.73        27
         2.0       0.62      0.42      0.50        36
         3.0       0.49      0.85      0.62        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.60        87
   macro avg       0.37      0.40      0.37        87
weighted avg       0.59      0.60      0.57        87

[[ 0  1  0  0  0]
 [ 0 20  7  0  0]
 [ 0  6 15 15  0]
 [ 0  1  2 17  0]
 [ 0  0  0  3  0]]
0.574712643678161
87 87 87
Filename	True Label	Prediction
0603	2.0	2.0
0610	1.0	2.0
0622	1.0	1.0
0636	2.0	2.0
0642	1.0	2.0
0723	2.0	2.0
0813	1.0	1.0
0824	1.0	2.0
0904	1.0	1.0
0905	2.0	2.0
0912	1.0	2.0
0913	2.0	1.0
0915	3.0	2.0
0923	2.0	2.0
0924	1.0	1.0
0927	2.0	1.0
1003	1.0	2.0
1006	2.0	2.0
1008	1.0	2.0
1014	2.0	2.0
1019	2.0	2.0
1111	1.0	2.0
1115	2.0	2.0
BER0611006	3.0	3.0
BER0611007	3.0	3.0
KYJ0611006B	0.0	1.0
LIB0611001A	1.0	1.0
LIB0611001B	1.0	1.0
LIB0611004A	1.0	1.0
LIB0611011	2.0	2.0
MOS0509004	2.0	2.0
PAR1011009A	2.0	1.0
PAR1011014	3.0	3.0
PHA0111003A	1.0	1.0
PHA0111005B	1.0	1.0
PHA0111011	3.0	2.0
PHA0111018	2.0	3.0
PHA0112012B	1.0	1.0
PHA0209028	2.0	3.0
PHA0411009A	1.0	1.0
PHA0411009B	1.0	1.0
PHA0411010A	1.0	1.0
PHA0411010B	1.0	1.0
PHA0411012A	1.0	1.0
PHA0411012B	1.0	1.0
PHA0411028	2.0	2.0
PHA0411035	3.0	3.0
PHA0411047	3.0	3.0
PHA0411060	3.0	3.0
PHA0509019	2.0	2.0
PHA0509020	3.0	3.0
PHA0509027	2.0	3.0
PHA0509028	3.0	3.0
PHA0509036	2.0	3.0
PHA0509041	3.0	3.0
PHA0510002A	2.0	2.0
PHA0510023	4.0	3.0
PHA0510030	2.0	3.0
PHA0510048	3.0	3.0
PHA0610007A	3.0	1.0
PHA0610019A	2.0	2.0
PHA0610019B	2.0	1.0
PHA0610025	3.0	3.0
PHA0710009	2.0	3.0
PHA0710012	3.0	3.0
PHA0710013	4.0	3.0
PHA0810003	2.0	3.0
PHA0810012	2.0	3.0
PHA0811010	2.0	3.0
PHA0811014	2.0	3.0
PHA0811017	3.0	3.0
PHA0811020	2.0	3.0
PHA1109007	3.0	3.0
PHA1109024	4.0	3.0
PHA1110003B	1.0	1.0
PHA1110017	2.0	3.0
PHA1110019	2.0	3.0
PHA1110021	3.0	3.0
PHA1111001A	2.0	1.0
PHA1111002A	1.0	1.0
PHA1111003A	1.0	1.0
PHA1111004B	1.0	1.0
PHA1111008A	2.0	1.0
VAR0909003	3.0	3.0
VAR0910004	3.0	3.0
VAR0910006	2.0	3.0
VAR0910007	2.0	3.0
LANGUAGE: CZ, 1th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.32
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.21
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.00      0.00      0.00        26
         2.0       0.43      0.95      0.59        37
         3.0       0.40      0.10      0.16        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.43        87
   macro avg       0.17      0.21      0.15        87
weighted avg       0.27      0.43      0.29        87

[[ 0  0  1  0  0]
 [ 0  0 26  0  0]
 [ 0  0 35  2  0]
 [ 0  0 18  2  0]
 [ 0  0  2  1  0]]
0.28695064232589584
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.09
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.01
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.48      0.81      0.60        26
         2.0       0.50      0.14      0.21        37
         3.0       0.52      0.85      0.64        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.49        87
   macro avg       0.30      0.36      0.29        87
weighted avg       0.47      0.49      0.42        87

[[ 0  1  0  0  0]
 [ 0 21  5  0  0]
 [ 0 19  5 13  0]
 [ 0  3  0 17  0]
 [ 0  0  0  3  0]]
0.4172704494801977
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.98
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.97
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.52      0.96      0.68        26
         2.0       0.67      0.16      0.26        37
         3.0       0.53      0.80      0.64        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.54        87
   macro avg       0.34      0.38      0.32        87
weighted avg       0.56      0.54      0.46        87

[[ 0  1  0  0  0]
 [ 0 25  1  0  0]
 [ 0 20  6 11  0]
 [ 0  2  2 16  0]
 [ 0  0  0  3  0]]
0.45999702851277063
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.93
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.92
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.50      0.77      0.61        26
         2.0       0.47      0.24      0.32        37
         3.0       0.54      0.75      0.63        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.51        87
   macro avg       0.30      0.35      0.31        87
weighted avg       0.47      0.51      0.46        87

[[ 0  1  0  0  0]
 [ 0 20  6  0  0]
 [ 0 18  9 10  0]
 [ 0  1  4 15  0]
 [ 0  0  0  3  0]]
0.4614992287406081
87 87 87
Filename	True Label	Prediction
0604	2.0	1.0
0606	2.0	1.0
0609	2.0	1.0
0611	2.0	1.0
0612	1.0	1.0
0613	1.0	1.0
0618	1.0	2.0
0619	1.0	1.0
0624	2.0	1.0
0625	1.0	1.0
0629	2.0	1.0
0631	2.0	1.0
0634	3.0	2.0
0637	2.0	1.0
0714	2.0	1.0
0721	2.0	2.0
0804	1.0	1.0
0816	3.0	2.0
0817	1.0	1.0
0823	1.0	2.0
0825	2.0	1.0
0826	2.0	1.0
0829	1.0	2.0
0914	1.0	2.0
0920	2.0	2.0
0926	2.0	1.0
1004	1.0	1.0
1009	2.0	1.0
1023	2.0	2.0
KYJ0611004A	1.0	1.0
LON0611002A	1.0	1.0
LON0611003	3.0	3.0
LON0611004A	0.0	1.0
MOS0509001	2.0	2.0
PHA0111001B	1.0	1.0
PHA0111002A	2.0	1.0
PHA0111002B	3.0	1.0
PHA0111003B	1.0	1.0
PHA0111004A	1.0	1.0
PHA0112002B	1.0	1.0
PHA0112003B	1.0	2.0
PHA0112012A	2.0	2.0
PHA0209031	4.0	3.0
PHA0210004	2.0	1.0
PHA0210008	2.0	1.0
PHA0411008A	2.0	1.0
PHA0411031	3.0	3.0
PHA0411038	3.0	3.0
PHA0411043	3.0	3.0
PHA0411045	2.0	3.0
PHA0411058	3.0	3.0
PHA0411061	2.0	3.0
PHA0509013	1.0	1.0
PHA0509026	4.0	3.0
PHA0509030	3.0	3.0
PHA0509031	2.0	3.0
PHA0509035	3.0	3.0
PHA0509039	2.0	3.0
PHA0509044	2.0	3.0
PHA0510002B	1.0	1.0
PHA0510003A	2.0	2.0
PHA0510004A	1.0	1.0
PHA0510031	3.0	3.0
PHA0610006A	2.0	1.0
PHA0610015	2.0	3.0
PHA0610018	3.0	3.0
PHA0710011	3.0	3.0
PHA0710014	3.0	3.0
PHA0710016	3.0	3.0
PHA0710019	4.0	3.0
PHA0809009	2.0	3.0
PHA0810006	2.0	2.0
PHA0810009	3.0	3.0
PHA0810011	2.0	3.0
PHA0811016	3.0	2.0
PHA1109028	2.0	3.0
PHA1110001A	2.0	2.0
PHA1110001B	1.0	1.0
PHA1110002A	2.0	2.0
PHA1110003A	1.0	2.0
PHA1110016	2.0	3.0
PHA1111002B	1.0	1.0
PHA1111009A	1.0	1.0
ST071122B	1.0	1.0
VAR0209036	3.0	3.0
VAR0910009	3.0	3.0
VAR0910010	3.0	2.0
LANGUAGE: CZ, 2th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.32
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.10
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.67      0.08      0.14        26
         2.0       0.43      0.97      0.60        37
         3.0       0.00      0.00      0.00        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.44        87
   macro avg       0.22      0.21      0.15        87
weighted avg       0.38      0.44      0.29        87

[[ 0  0  1  0  0]
 [ 0  2 24  0  0]
 [ 0  1 36  0  0]
 [ 0  0 20  0  0]
 [ 0  0  3  0  0]]
0.2942843197950754
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.05
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.91
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.73      0.85      0.79        26
         2.0       0.54      0.70      0.61        37
         3.0       0.44      0.20      0.28        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.60        87
   macro avg       0.34      0.35      0.33        87
weighted avg       0.55      0.60      0.56        87

[[ 0  1  0  0  0]
 [ 0 22  4  0  0]
 [ 0  7 26  4  0]
 [ 0  0 16  4  0]
 [ 0  0  2  1  0]]
0.5584035278796418
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.98
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.88
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.69      0.69      0.69        26
         2.0       0.63      0.51      0.57        37
         3.0       0.52      0.80      0.63        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.61        87
   macro avg       0.37      0.40      0.38        87
weighted avg       0.59      0.61      0.59        87

[[ 0  1  0  0  0]
 [ 0 18  8  0  0]
 [ 0  6 19 12  0]
 [ 0  1  3 16  0]
 [ 0  0  0  3  0]]
0.5923459107437793
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.91
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.87
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.53      0.92      0.68        26
         2.0       0.65      0.30      0.41        37
         3.0       0.56      0.70      0.62        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.56        87
   macro avg       0.35      0.38      0.34        87
weighted avg       0.56      0.56      0.52        87

[[ 0  1  0  0  0]
 [ 0 24  2  0  0]
 [ 0 18 11  8  0]
 [ 0  2  4 14  0]
 [ 0  0  0  3  0]]
0.5183446357155277
87 87 87
Filename	True Label	Prediction
0602	1.0	2.0
0608	1.0	1.0
0616	1.0	1.0
0617	1.0	1.0
0630	1.0	1.0
0632	1.0	1.0
0643	2.0	1.0
0644	1.0	1.0
0645	2.0	2.0
0715	2.0	1.0
0716	2.0	1.0
0722	2.0	1.0
0725	1.0	2.0
0806	2.0	2.0
0808	2.0	1.0
0809	2.0	1.0
0819	3.0	1.0
0828	2.0	1.0
0901	2.0	2.0
0902	2.0	1.0
0922	1.0	1.0
1010	2.0	1.0
1016	2.0	1.0
1017	2.0	1.0
1021	2.0	1.0
1022	2.0	1.0
1113	2.0	2.0
9999	1.0	1.0
BER0609003	3.0	3.0
BER0611005	3.0	2.0
KYJ0611003A	1.0	1.0
KYJ0611005A	0.0	1.0
KYJ0611005B	1.0	1.0
KYJ0611009A	1.0	1.0
MOS0611015	2.0	3.0
PAR1011008A	1.0	1.0
PAR1011009B	1.0	1.0
PAR1011015	2.0	3.0
PAR1011017	3.0	2.0
PHA0111001A	1.0	1.0
PHA0111016	4.0	3.0
PHA0112006A	3.0	1.0
PHA0112007B	1.0	1.0
PHA0112009B	1.0	1.0
PHA0209001	2.0	1.0
PHA0411008B	1.0	1.0
PHA0411011B	1.0	1.0
PHA0411027	2.0	2.0
PHA0411032	2.0	3.0
PHA0411042	2.0	3.0
PHA0411056	4.0	3.0
PHA0411059	2.0	3.0
PHA0509002	1.0	1.0
PHA0509018	3.0	3.0
PHA0509025	4.0	3.0
PHA0509037	3.0	2.0
PHA0509038	2.0	2.0
PHA0509040	2.0	3.0
PHA0509042	3.0	3.0
PHA0509045	2.0	2.0
PHA0510010A	2.0	1.0
PHA0510010B	1.0	1.0
PHA0510013B	1.0	1.0
PHA0510027	2.0	2.0
PHA0510036	3.0	3.0
PHA0510038	3.0	3.0
PHA0610006B	1.0	1.0
PHA0610017	3.0	3.0
PHA0710015	3.0	3.0
PHA0710018	3.0	3.0
PHA0810008	2.0	3.0
PHA1109002	3.0	3.0
PHA1109003	2.0	2.0
PHA1109004	3.0	3.0
PHA1109005	3.0	2.0
PHA1109006	2.0	2.0
PHA1109026	3.0	3.0
PHA1110004A	2.0	1.0
PHA1110014	3.0	3.0
PHA1110015	3.0	3.0
PHA1111004A	2.0	1.0
PHA1111006B	1.0	1.0
PHA1111008B	1.0	1.0
TI071122B	2.0	1.0
VAR0909006	2.0	3.0
VAR0909010	3.0	3.0
VAR0910011	2.0	2.0
LANGUAGE: CZ, 3th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.32
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.23
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.45      0.96      0.62        26
         2.0       0.38      0.33      0.35        36
         3.0       0.00      0.00      0.00        20
         4.0       0.00      0.00      0.00         4

    accuracy                           0.43        87
   macro avg       0.17      0.26      0.19        87
weighted avg       0.29      0.43      0.33        87

[[ 0  1  0  0  0]
 [ 0 25  1  0  0]
 [ 0 24 12  0  0]
 [ 0  4 16  0  0]
 [ 0  1  3  0  0]]
0.33052028814931683
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.03
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.03
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.62      0.38      0.48        26
         2.0       0.44      0.67      0.53        36
         3.0       0.47      0.40      0.43        20
         4.0       0.00      0.00      0.00         4

    accuracy                           0.48        87
   macro avg       0.31      0.29      0.29        87
weighted avg       0.48      0.48      0.46        87

[[ 0  1  0  0  0]
 [ 0 10 16  0  0]
 [ 0  5 24  7  0]
 [ 0  0 12  8  0]
 [ 0  0  2  2  0]]
0.4624092072367934
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.94
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.99
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.59      0.65      0.62        26
         2.0       0.50      0.44      0.47        36
         3.0       0.50      0.65      0.57        20
         4.0       0.00      0.00      0.00         4

    accuracy                           0.53        87
   macro avg       0.32      0.35      0.33        87
weighted avg       0.50      0.53      0.51        87

[[ 0  1  0  0  0]
 [ 0 17  9  0  0]
 [ 0 10 16 10  0]
 [ 0  1  6 13  0]
 [ 0  0  1  3  0]]
0.5094051904529019
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.86
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.10
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.62      0.38      0.48        26
         2.0       0.49      0.56      0.52        36
         3.0       0.53      0.80      0.64        20
         4.0       0.00      0.00      0.00         4

    accuracy                           0.53        87
   macro avg       0.33      0.35      0.33        87
weighted avg       0.51      0.53      0.50        87

[[ 0  1  0  0  0]
 [ 0 10 16  0  0]
 [ 0  5 20 11  0]
 [ 0  0  4 16  0]
 [ 0  0  1  3  0]]
0.5043936906005873
87 87 87
Filename	True Label	Prediction
0601	1.0	2.0
0605	2.0	2.0
0614	2.0	2.0
0620	1.0	2.0
0621	1.0	2.0
0623	1.0	2.0
0626	2.0	2.0
0633	2.0	2.0
0635	2.0	2.0
0639	1.0	2.0
0641	1.0	2.0
0718	2.0	2.0
0724	3.0	2.0
0801	2.0	2.0
0803	2.0	2.0
0805	2.0	2.0
0810	1.0	2.0
0812	1.0	1.0
0814	2.0	1.0
0815	2.0	2.0
0818	1.0	2.0
0822	1.0	2.0
0827	2.0	2.0
0921	1.0	2.0
0929	1.0	2.0
1005	1.0	2.0
1007	2.0	2.0
1015	1.0	2.0
1018	2.0	2.0
1020	2.0	2.0
1112	1.0	2.0
KYJ0611006A	1.0	1.0
KYJ0611009B	0.0	1.0
LIB0611002A	1.0	1.0
LIB0611002B	1.0	1.0
LIB0611003A	1.0	1.0
LON0610002B	2.0	1.0
LON0611004B	1.0	1.0
MOS0611012	2.0	3.0
PAR1011013	2.0	3.0
PAR1011016	2.0	3.0
PAR1011018	3.0	3.0
PHA0111005A	1.0	1.0
PHA0111010	4.0	2.0
PHA0111012	2.0	3.0
PHA0111014	1.0	2.0
PHA0112003A	2.0	1.0
PHA0112007A	2.0	2.0
PHA0112009A	2.0	2.0
PHA0209008	2.0	1.0
PHA0209013	1.0	1.0
PHA0209024	3.0	3.0
PHA0209034	2.0	3.0
PHA0209038	4.0	3.0
PHA0209039	3.0	3.0
PHA0210001	1.0	1.0
PHA0411029	3.0	2.0
PHA0411030	3.0	3.0
PHA0411033	2.0	3.0
PHA0411037	2.0	3.0
PHA0411039	3.0	3.0
PHA0411041	3.0	3.0
PHA0411044	3.0	3.0
PHA0411051	3.0	3.0
PHA0411053	3.0	3.0
PHA0509015	3.0	3.0
PHA0509017	3.0	3.0
PHA0509021	3.0	2.0
PHA0509024	3.0	3.0
PHA0509033	2.0	2.0
PHA0509043	3.0	3.0
PHA0510013A	1.0	2.0
PHA0510037	2.0	2.0
PHA0510039	2.0	3.0
PHA0510049	2.0	2.0
PHA0510050	2.0	3.0
PHA0610005A	1.0	1.0
PHA0610026	3.0	3.0
PHA0810004	3.0	3.0
PHA0810015	2.0	3.0
PHA0811012	4.0	3.0
PHA1109027	4.0	3.0
PHA1110002B	2.0	1.0
PHA1110013	2.0	3.0
VAR0909004	3.0	3.0
VAR0909005	2.0	2.0
VAR0909008	3.0	2.0
LANGUAGE: CZ, 4th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.30
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.13
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.49      0.96      0.65        26
         2.0       0.47      0.22      0.30        36
         3.0       0.50      0.45      0.47        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.49        86
   macro avg       0.29      0.33      0.28        86
weighted avg       0.46      0.49      0.43        86

[[ 0  1  0  0  0]
 [ 0 25  1  0  0]
 [ 0 21  8  7  0]
 [ 0  4  7  9  0]
 [ 0  0  1  2  0]]
0.4328456467668033
86 86 86



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.08
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.95
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.71      0.46      0.56        26
         2.0       0.48      0.56      0.51        36
         3.0       0.44      0.60      0.51        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.51        86
   macro avg       0.33      0.32      0.32        86
weighted avg       0.52      0.51      0.50        86

[[ 0  1  0  0  0]
 [ 0 12 14  0  0]
 [ 0  4 20 12  0]
 [ 0  0  8 12  0]
 [ 0  0  0  3  0]]
0.502162003790256
86 86 86



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.98
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.91
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.81      0.50      0.62        26
         2.0       0.55      0.67      0.60        36
         3.0       0.50      0.65      0.57        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.58        86
   macro avg       0.37      0.36      0.36        86
weighted avg       0.59      0.58      0.57        86

[[ 0  1  0  0  0]
 [ 0 13 13  0  0]
 [ 0  2 24 10  0]
 [ 0  0  7 13  0]
 [ 0  0  0  3  0]]
0.5697626269921517
86 86 86



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.91
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.88
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.67      0.62      0.64        26
         2.0       0.54      0.42      0.47        36
         3.0       0.50      0.85      0.63        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.56        86
   macro avg       0.34      0.38      0.35        86
weighted avg       0.54      0.56      0.54        86

[[ 0  1  0  0  0]
 [ 0 16 10  0  0]
 [ 0  7 15 14  0]
 [ 0  0  3 17  0]
 [ 0  0  0  3  0]]
0.536134797588286
86 86 86
Filename	True Label	Prediction
0607	3.0	2.0
0615	1.0	1.0
0627	2.0	2.0
0628	2.0	2.0
0638	2.0	2.0
0640	2.0	2.0
0717	1.0	1.0
0719	2.0	1.0
0720	1.0	2.0
0802	2.0	1.0
0807	2.0	2.0
0811	2.0	1.0
0820	2.0	1.0
0821	1.0	2.0
0903	1.0	2.0
0906	2.0	2.0
0907	2.0	2.0
0910	1.0	1.0
0911	1.0	1.0
0916	2.0	2.0
0917	1.0	2.0
0918	1.0	2.0
0919	2.0	1.0
0925	2.0	2.0
0928	1.0	2.0
0930	2.0	2.0
1001	1.0	2.0
1002	2.0	2.0
1114	3.0	2.0
1116	2.0	2.0
1117	1.0	2.0
BER0611003	2.0	3.0
LIB0611004B	1.0	1.0
LON0610002A	1.0	2.0
LON0611002B	0.0	1.0
MOS0611013	2.0	3.0
MOS0611014	2.0	3.0
PHA0111004B	1.0	1.0
PHA0111015	4.0	3.0
PHA0112002A	2.0	2.0
PHA0112006B	3.0	2.0
PHA0209026	3.0	3.0
PHA0210007	2.0	2.0
PHA0411011A	1.0	1.0
PHA0411034	2.0	2.0
PHA0411036	3.0	3.0
PHA0411054	3.0	3.0
PHA0411055	3.0	3.0
PHA0411062	2.0	3.0
PHA0509007	1.0	2.0
PHA0509022	4.0	3.0
PHA0509032	2.0	3.0
PHA0509034	2.0	3.0
PHA0510003B	1.0	1.0
PHA0510004B	1.0	1.0
PHA0510029	3.0	3.0
PHA0510032	2.0	3.0
PHA0510034	3.0	3.0
PHA0510035	2.0	3.0
PHA0510040	2.0	3.0
PHA0510046	2.0	3.0
PHA0510047	2.0	3.0
PHA0610005B	1.0	1.0
PHA0610007B	1.0	1.0
PHA0610016	3.0	3.0
PHA0709008	3.0	3.0
PHA0710010	2.0	3.0
PHA0710017	3.0	3.0
PHA0710021	4.0	3.0
PHA0809010	3.0	3.0
PHA0810001	2.0	3.0
PHA0810002	2.0	3.0
PHA0810010	3.0	3.0
PHA0811013	3.0	3.0
PHA0811019	3.0	3.0
PHA1109001	2.0	1.0
PHA1109008	1.0	1.0
PHA1109023	2.0	1.0
PHA1109025	1.0	1.0
PHA1110022	3.0	3.0
PHA1111001B	1.0	1.0
PHA1111003B	1.0	1.0
PHA1111006A	1.0	1.0
VAR0909007	3.0	3.0
VAR0909009	3.0	3.0
VAR0910005	3.0	3.0
Averaged weighted F1-scores 0.519016999264634
144.90552995391704 65.35717405024758
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']
LANGUAGE: CZ, 0th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.18
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.74
              precision    recall  f1-score   support

         1.0       0.90      0.45      0.60        20
         2.0       0.68      0.80      0.74        35
         3.0       0.81      0.94      0.87        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.76        87
   macro avg       0.60      0.55      0.55        87
weighted avg       0.77      0.76      0.74        87

[[ 9 11  0  0]
 [ 1 28  6  0]
 [ 0  2 29  0]
 [ 0  0  1  0]]
0.7428194779279647
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.69
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.73
              precision    recall  f1-score   support

         1.0       0.83      0.75      0.79        20
         2.0       0.65      0.86      0.74        35
         3.0       0.91      0.68      0.78        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.76        87
   macro avg       0.60      0.57      0.58        87
weighted avg       0.78      0.76      0.76        87

[[15  5  0  0]
 [ 3 30  2  0]
 [ 0 10 21  0]
 [ 0  1  0  0]]
0.7566265600143397
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.57
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.67
              precision    recall  f1-score   support

         1.0       0.80      0.40      0.53        20
         2.0       0.64      0.86      0.73        35
         3.0       0.87      0.84      0.85        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.74        87
   macro avg       0.58      0.52      0.53        87
weighted avg       0.75      0.74      0.72        87

[[ 8 12  0  0]
 [ 2 30  3  0]
 [ 0  5 26  0]
 [ 0  0  1  0]]
0.7207201410623489
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.49
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.66
              precision    recall  f1-score   support

         1.0       0.87      0.65      0.74        20
         2.0       0.69      0.89      0.78        35
         3.0       0.89      0.77      0.83        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.78        87
   macro avg       0.61      0.58      0.59        87
weighted avg       0.79      0.78      0.78        87

[[13  7  0  0]
 [ 2 31  2  0]
 [ 0  7 24  0]
 [ 0  0  1  0]]
0.7774404054130571
87 87 87
Filename	True Label	Prediction
0612	2.0	2.0
0618	2.0	2.0
0624	2.0	2.0
0628	2.0	2.0
0631	2.0	2.0
0634	2.0	2.0
0636	2.0	2.0
0641	2.0	2.0
0644	2.0	2.0
0717	2.0	2.0
0802	1.0	2.0
0819	3.0	2.0
0822	2.0	2.0
0824	2.0	2.0
0826	2.0	2.0
0901	3.0	2.0
0904	1.0	2.0
0905	2.0	2.0
0910	1.0	2.0
0912	2.0	2.0
0920	2.0	2.0
0921	2.0	2.0
0923	2.0	2.0
0924	1.0	2.0
0929	1.0	2.0
1005	2.0	2.0
1006	2.0	2.0
1007	2.0	2.0
1009	2.0	2.0
1016	2.0	2.0
1019	2.0	2.0
1112	2.0	2.0
KYJ0611003A	1.0	1.0
KYJ0611004A	1.0	1.0
KYJ0611006B	1.0	1.0
LON0610002B	1.0	1.0
MOS0611015	3.0	3.0
PAR1011008A	2.0	2.0
PAR1011013	3.0	3.0
PAR1011017	3.0	3.0
PHA0111002A	1.0	1.0
PHA0111004B	1.0	2.0
PHA0111010	3.0	3.0
PHA0111015	4.0	3.0
PHA0111018	2.0	3.0
PHA0112012A	1.0	1.0
PHA0209001	2.0	1.0
PHA0209008	1.0	1.0
PHA0209024	3.0	3.0
PHA0209026	3.0	3.0
PHA0209038	3.0	3.0
PHA0210008	1.0	1.0
PHA0411028	2.0	2.0
PHA0411031	3.0	3.0
PHA0411034	3.0	2.0
PHA0411037	2.0	3.0
PHA0509019	3.0	2.0
PHA0509021	2.0	2.0
PHA0509022	3.0	3.0
PHA0509026	3.0	3.0
PHA0509037	3.0	2.0
PHA0509038	2.0	2.0
PHA0509041	3.0	3.0
PHA0509045	3.0	2.0
PHA0510003A	1.0	1.0
PHA0510010B	1.0	1.0
PHA0510023	3.0	3.0
PHA0610006B	1.0	2.0
PHA0610026	3.0	3.0
PHA0710015	3.0	3.0
PHA0810001	3.0	3.0
PHA0810008	3.0	3.0
PHA0811010	3.0	3.0
PHA0811012	3.0	3.0
PHA0811019	3.0	3.0
PHA1109002	3.0	3.0
PHA1109003	2.0	2.0
PHA1110001A	2.0	1.0
PHA1110002A	2.0	2.0
PHA1110003B	1.0	1.0
PHA1110013	3.0	3.0
PHA1110021	3.0	3.0
PHA1111003A	1.0	1.0
ST071122B	1.0	1.0
VAR0909008	3.0	2.0
VAR0909009	3.0	3.0
VAR0910011	3.0	3.0
LANGUAGE: CZ, 1th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.15
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.68
              precision    recall  f1-score   support

         1.0       1.00      0.05      0.09        21
         2.0       0.61      0.91      0.73        34
         3.0       0.89      1.00      0.94        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.72        87
   macro avg       0.62      0.49      0.44        87
weighted avg       0.79      0.72      0.64        87

[[ 1 20  0  0]
 [ 0 31  3  0]
 [ 0  0 31  0]
 [ 0  0  1  0]]
0.6417276210379659
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.75
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.58
              precision    recall  f1-score   support

         1.0       1.00      0.24      0.38        21
         2.0       0.66      0.91      0.77        34
         3.0       0.89      1.00      0.94        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.77        87
   macro avg       0.64      0.54      0.52        87
weighted avg       0.81      0.77      0.73        87

[[ 5 16  0  0]
 [ 0 31  3  0]
 [ 0  0 31  0]
 [ 0  0  1  0]]
0.7266991558179298
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.59
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.53
              precision    recall  f1-score   support

         1.0       0.68      0.62      0.65        21
         2.0       0.74      0.82      0.78        34
         3.0       0.97      0.94      0.95        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.80        87
   macro avg       0.60      0.59      0.59        87
weighted avg       0.80      0.80      0.80        87

[[13  8  0  0]
 [ 6 28  0  0]
 [ 0  2 29  0]
 [ 0  0  1  0]]
0.7996534974771268
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.55
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.50
              precision    recall  f1-score   support

         1.0       0.70      0.67      0.68        21
         2.0       0.76      0.82      0.79        34
         3.0       0.97      0.94      0.95        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.82        87
   macro avg       0.61      0.61      0.61        87
weighted avg       0.81      0.82      0.81        87

[[14  7  0  0]
 [ 6 28  0  0]
 [ 0  2 29  0]
 [ 0  0  1  0]]
0.8118824673465567
87 87 87
Filename	True Label	Prediction
0601	2.0	2.0
0602	2.0	2.0
0606	2.0	2.0
0607	2.0	2.0
0608	1.0	2.0
0629	2.0	2.0
0635	2.0	2.0
0639	2.0	2.0
0640	2.0	2.0
0643	2.0	2.0
0716	2.0	2.0
0720	2.0	2.0
0723	1.0	2.0
0725	2.0	2.0
0801	1.0	2.0
0803	1.0	2.0
0807	2.0	2.0
0809	2.0	2.0
0810	2.0	2.0
0815	2.0	2.0
0818	1.0	2.0
0914	2.0	2.0
0915	2.0	2.0
0916	1.0	2.0
0918	2.0	2.0
0927	2.0	2.0
1002	2.0	2.0
1017	2.0	2.0
1023	2.0	2.0
1113	2.0	2.0
1117	2.0	2.0
9999	1.0	2.0
BER0611006	3.0	3.0
BER0611007	3.0	3.0
LIB0611001A	1.0	1.0
LON0610002A	2.0	1.0
LON0611002A	1.0	1.0
LON0611004B	1.0	1.0
MOS0509004	2.0	2.0
PAR1011009A	2.0	1.0
PHA0111011	3.0	2.0
PHA0111016	3.0	3.0
PHA0112003A	1.0	1.0
PHA0112007A	1.0	1.0
PHA0112009B	2.0	1.0
PHA0209028	3.0	3.0
PHA0411033	3.0	3.0
PHA0411036	3.0	3.0
PHA0411054	3.0	3.0
PHA0411060	3.0	3.0
PHA0509025	3.0	3.0
PHA0509028	3.0	3.0
PHA0509035	3.0	3.0
PHA0510002B	2.0	1.0
PHA0510003B	1.0	1.0
PHA0510029	3.0	3.0
PHA0510031	3.0	3.0
PHA0510037	2.0	2.0
PHA0510039	3.0	3.0
PHA0510049	3.0	3.0
PHA0610005B	1.0	1.0
PHA0610007A	1.0	1.0
PHA0610007B	1.0	1.0
PHA0610017	3.0	3.0
PHA0610019A	1.0	1.0
PHA0709008	3.0	3.0
PHA0710014	3.0	3.0
PHA0809009	3.0	3.0
PHA0810003	3.0	3.0
PHA0810010	3.0	3.0
PHA0810012	3.0	3.0
PHA0810015	3.0	3.0
PHA0811014	3.0	3.0
PHA0811017	4.0	3.0
PHA1109023	1.0	1.0
PHA1109026	3.0	3.0
PHA1110014	3.0	3.0
PHA1111003B	1.0	1.0
PHA1111004B	1.0	1.0
PHA1111006B	2.0	1.0
PHA1111008A	2.0	1.0
TI071122B	1.0	1.0
VAR0209036	2.0	2.0
VAR0909003	3.0	3.0
VAR0909005	3.0	2.0
VAR0909010	3.0	3.0
VAR0910009	3.0	3.0
LANGUAGE: CZ, 2th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.25
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.90
              precision    recall  f1-score   support

         1.0       0.00      0.00      0.00        20
         2.0       0.61      0.88      0.72        34
         3.0       0.76      0.91      0.83        32
         4.0       0.00      0.00      0.00         1

    accuracy                           0.68        87
   macro avg       0.34      0.45      0.39        87
weighted avg       0.52      0.68      0.59        87

[[ 0 16  4  0]
 [ 0 30  4  0]
 [ 0  3 29  0]
 [ 0  0  1  0]]
0.5872712524976754
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.80
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.55
              precision    recall  f1-score   support

         1.0       0.73      0.55      0.63        20
         2.0       0.70      0.76      0.73        34
         3.0       0.86      0.94      0.90        32
         4.0       0.00      0.00      0.00         1

    accuracy                           0.77        87
   macro avg       0.57      0.56      0.56        87
weighted avg       0.76      0.77      0.76        87

[[11  9  0  0]
 [ 4 26  4  0]
 [ 0  2 30  0]
 [ 0  0  1  0]]
0.7601098096556695
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.62
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.50
              precision    recall  f1-score   support

         1.0       0.71      1.00      0.83        20
         2.0       0.89      0.71      0.79        34
         3.0       0.91      0.91      0.91        32
         4.0       0.00      0.00      0.00         1

    accuracy                           0.84        87
   macro avg       0.63      0.65      0.63        87
weighted avg       0.84      0.84      0.83        87

[[20  0  0  0]
 [ 8 24  2  0]
 [ 0  3 29  0]
 [ 0  0  1  0]]
0.8324225865209471
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.53
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.48
              precision    recall  f1-score   support

         1.0       0.74      1.00      0.85        20
         2.0       0.89      0.74      0.81        34
         3.0       0.91      0.91      0.91        32
         4.0       0.00      0.00      0.00         1

    accuracy                           0.85        87
   macro avg       0.63      0.66      0.64        87
weighted avg       0.85      0.85      0.84        87

[[20  0  0  0]
 [ 7 25  2  0]
 [ 0  3 29  0]
 [ 0  0  1  0]]
0.8441451889017743
87 87 87
Filename	True Label	Prediction
0603	2.0	2.0
0604	2.0	2.0
0611	2.0	2.0
0619	2.0	2.0
0620	2.0	2.0
0621	2.0	2.0
0622	2.0	2.0
0714	2.0	2.0
0718	2.0	2.0
0724	3.0	2.0
0808	2.0	2.0
0816	2.0	2.0
0817	2.0	2.0
0903	2.0	2.0
0919	2.0	2.0
0926	2.0	2.0
1001	2.0	2.0
1004	2.0	2.0
1018	2.0	2.0
1022	2.0	2.0
1111	2.0	2.0
1115	2.0	2.0
1116	2.0	2.0
KYJ0611009A	2.0	2.0
LIB0611004B	2.0	1.0
LON0611002B	1.0	1.0
LON0611003	3.0	3.0
LON0611004A	1.0	1.0
MOS0509001	2.0	2.0
MOS0611013	3.0	3.0
PAR1011015	2.0	3.0
PAR1011018	3.0	3.0
PHA0111001B	1.0	1.0
PHA0111002B	2.0	1.0
PHA0111005B	2.0	1.0
PHA0111012	3.0	3.0
PHA0112002B	2.0	1.0
PHA0112003B	1.0	1.0
PHA0112006B	2.0	1.0
PHA0209034	3.0	3.0
PHA0210001	1.0	1.0
PHA0210004	1.0	1.0
PHA0210007	1.0	1.0
PHA0411008B	1.0	1.0
PHA0411012B	1.0	1.0
PHA0411027	3.0	3.0
PHA0411032	3.0	3.0
PHA0411039	3.0	3.0
PHA0411041	3.0	3.0
PHA0411045	3.0	2.0
PHA0411056	3.0	3.0
PHA0411058	3.0	3.0
PHA0411061	3.0	3.0
PHA0411062	3.0	3.0
PHA0509007	1.0	1.0
PHA0509013	1.0	1.0
PHA0509017	3.0	3.0
PHA0509018	3.0	3.0
PHA0509031	2.0	3.0
PHA0509044	3.0	3.0
PHA0510002A	1.0	1.0
PHA0510013A	2.0	1.0
PHA0510013B	1.0	1.0
PHA0510032	3.0	3.0
PHA0510036	3.0	3.0
PHA0510047	2.0	2.0
PHA0610019B	2.0	1.0
PHA0710013	3.0	3.0
PHA0710017	3.0	3.0
PHA0710018	3.0	3.0
PHA0710019	3.0	3.0
PHA0811013	3.0	3.0
PHA0811016	3.0	3.0
PHA1109005	3.0	2.0
PHA1109008	1.0	1.0
PHA1109025	1.0	1.0
PHA1109027	3.0	3.0
PHA1110003A	1.0	1.0
PHA1110004A	1.0	1.0
PHA1110017	3.0	3.0
PHA1110019	3.0	3.0
PHA1110022	4.0	3.0
PHA1111001B	1.0	1.0
PHA1111002B	1.0	1.0
PHA1111008B	1.0	1.0
VAR0909006	3.0	3.0
VAR0910004	3.0	3.0
LANGUAGE: CZ, 3th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.13
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.75
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.71      0.25      0.37        20
         2.0       0.63      0.76      0.69        34
         3.0       0.79      1.00      0.89        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.71        87
   macro avg       0.43      0.40      0.39        87
weighted avg       0.70      0.71      0.67        87

[[ 0  0  1  0  0]
 [ 0  5 14  1  0]
 [ 0  2 26  6  0]
 [ 0  0  0 31  0]
 [ 0  0  0  1  0]]
0.6716998114699264
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.72
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.71
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.75      0.30      0.43        20
         2.0       0.62      0.76      0.68        34
         3.0       0.81      0.97      0.88        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.71        87
   macro avg       0.44      0.41      0.40        87
weighted avg       0.70      0.71      0.68        87

[[ 0  0  1  0  0]
 [ 0  6 14  0  0]
 [ 0  2 26  6  0]
 [ 0  0  1 30  0]
 [ 0  0  0  1  0]]
0.6803164096854712
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.57
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.62
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.72      0.65      0.68        20
         2.0       0.80      0.71      0.75        34
         3.0       0.79      1.00      0.89        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.78        87
   macro avg       0.46      0.47      0.46        87
weighted avg       0.76      0.78      0.77        87

[[ 0  1  0  0  0]
 [ 0 13  6  1  0]
 [ 0  4 24  6  0]
 [ 0  0  0 31  0]
 [ 0  0  0  1  0]]
0.7659925676259615
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.50
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.61
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.72      0.65      0.68        20
         2.0       0.76      0.74      0.75        34
         3.0       0.83      0.97      0.90        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.78        87
   macro avg       0.46      0.47      0.47        87
weighted avg       0.76      0.78      0.77        87

[[ 0  1  0  0  0]
 [ 0 13  7  0  0]
 [ 0  4 25  5  0]
 [ 0  0  1 30  0]
 [ 0  0  0  1  0]]
0.7680291825807442
87 87 87
Filename	True Label	Prediction
0609	2.0	2.0
0610	2.0	2.0
0613	2.0	2.0
0614	2.0	2.0
0615	2.0	2.0
0616	2.0	2.0
0617	2.0	2.0
0625	1.0	2.0
0627	2.0	2.0
0630	1.0	2.0
0637	2.0	2.0
0638	2.0	2.0
0645	2.0	2.0
0715	2.0	2.0
0719	2.0	2.0
0721	2.0	2.0
0804	2.0	2.0
0805	2.0	2.0
0811	2.0	2.0
0828	2.0	2.0
0913	2.0	2.0
0917	2.0	2.0
0922	1.0	2.0
1003	2.0	2.0
1010	1.0	2.0
1014	2.0	2.0
BER0609003	3.0	3.0
BER0611003	3.0	3.0
BER0611005	3.0	2.0
KYJ0611005A	1.0	1.0
KYJ0611005B	1.0	1.0
KYJ0611009B	1.0	2.0
LIB0611001B	1.0	1.0
LIB0611002B	1.0	1.0
LIB0611003A	2.0	2.0
LIB0611004A	1.0	1.0
LIB0611011	2.0	2.0
MOS0611012	3.0	3.0
PHA0111001A	1.0	1.0
PHA0111004A	1.0	1.0
PHA0111005A	2.0	1.0
PHA0112007B	1.0	1.0
PHA0209031	3.0	3.0
PHA0411008A	2.0	1.0
PHA0411009A	1.0	2.0
PHA0411009B	2.0	1.0
PHA0411010A	0.0	1.0
PHA0411010B	1.0	1.0
PHA0411011B	1.0	1.0
PHA0411029	3.0	3.0
PHA0411042	3.0	3.0
PHA0411044	4.0	3.0
PHA0411051	3.0	3.0
PHA0411053	3.0	3.0
PHA0509020	3.0	3.0
PHA0509024	3.0	3.0
PHA0509032	3.0	3.0
PHA0509033	2.0	3.0
PHA0509040	3.0	3.0
PHA0509042	3.0	3.0
PHA0510004A	1.0	1.0
PHA0510004B	1.0	2.0
PHA0510027	3.0	3.0
PHA0510030	3.0	3.0
PHA0510034	3.0	3.0
PHA0510038	3.0	3.0
PHA0510040	3.0	3.0
PHA0510046	2.0	3.0
PHA0510048	3.0	3.0
PHA0610005A	1.0	1.0
PHA0610006A	1.0	1.0
PHA0610015	3.0	3.0
PHA0610016	3.0	3.0
PHA0610025	3.0	3.0
PHA0710011	3.0	3.0
PHA0710012	3.0	3.0
PHA0809010	2.0	3.0
PHA0810004	3.0	3.0
PHA0810006	3.0	3.0
PHA0811020	2.0	3.0
PHA1109001	2.0	1.0
PHA1109006	2.0	3.0
PHA1109024	3.0	3.0
PHA1109028	3.0	3.0
PHA1110015	3.0	3.0
PHA1111001A	2.0	2.0
VAR0909007	3.0	3.0
LANGUAGE: CZ, 4th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.15
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.75
              precision    recall  f1-score   support

         1.0       0.75      0.30      0.43        20
         2.0       0.66      0.79      0.72        34
         3.0       0.81      0.97      0.88        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.73        86
   macro avg       0.55      0.52      0.51        86
weighted avg       0.73      0.73      0.70        86

[[ 6 13  1  0]
 [ 2 27  5  0]
 [ 0  1 30  0]
 [ 0  0  1  0]]
0.7023763924174321
86 86 86



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.70
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.69
              precision    recall  f1-score   support

         1.0       0.71      0.25      0.37        20
         2.0       0.64      0.79      0.71        34
         3.0       0.81      0.97      0.88        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.72        86
   macro avg       0.54      0.50      0.49        86
weighted avg       0.71      0.72      0.69        86

[[ 5 14  1  0]
 [ 2 27  5  0]
 [ 0  1 30  0]
 [ 0  0  1  0]]
0.6850958525665128
86 86 86



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.54
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.65
              precision    recall  f1-score   support

         1.0       0.69      0.55      0.61        20
         2.0       0.76      0.74      0.75        34
         3.0       0.81      0.97      0.88        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.77        86
   macro avg       0.56      0.56      0.56        86
weighted avg       0.75      0.77      0.76        86

[[11  8  1  0]
 [ 4 25  5  0]
 [ 1  0 30  0]
 [ 0  0  1  0]]
0.7552127642680352
86 86 86



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.50
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.60
              precision    recall  f1-score   support

         1.0       0.79      0.55      0.65        20
         2.0       0.75      0.79      0.77        34
         3.0       0.83      0.97      0.90        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.79        86
   macro avg       0.59      0.58      0.58        86
weighted avg       0.78      0.79      0.78        86

[[11  8  1  0]
 [ 3 27  4  0]
 [ 0  1 30  0]
 [ 0  0  1  0]]
0.7782667666164002
86 86 86
Filename	True Label	Prediction
0605	2.0	2.0
0623	2.0	2.0
0626	2.0	2.0
0632	1.0	2.0
0633	2.0	2.0
0642	2.0	2.0
0722	2.0	2.0
0806	2.0	2.0
0812	1.0	2.0
0813	2.0	2.0
0814	1.0	2.0
0820	1.0	2.0
0821	2.0	2.0
0823	2.0	2.0
0825	2.0	2.0
0827	2.0	2.0
0829	2.0	2.0
0902	2.0	2.0
0906	2.0	2.0
0907	2.0	2.0
0911	2.0	2.0
0925	2.0	2.0
0928	2.0	2.0
0930	2.0	2.0
1008	2.0	2.0
1015	2.0	2.0
1020	2.0	2.0
1021	2.0	2.0
1114	2.0	2.0
KYJ0611006A	1.0	2.0
LIB0611002A	1.0	2.0
MOS0611014	1.0	3.0
PAR1011009B	1.0	1.0
PAR1011014	3.0	3.0
PAR1011016	3.0	3.0
PHA0111003A	1.0	1.0
PHA0111003B	1.0	1.0
PHA0111014	2.0	3.0
PHA0112002A	1.0	1.0
PHA0112006A	3.0	2.0
PHA0112009A	2.0	2.0
PHA0112012B	1.0	1.0
PHA0209013	1.0	1.0
PHA0209039	3.0	3.0
PHA0411011A	1.0	1.0
PHA0411012A	2.0	1.0
PHA0411030	3.0	3.0
PHA0411035	3.0	3.0
PHA0411038	3.0	3.0
PHA0411043	3.0	3.0
PHA0411047	3.0	3.0
PHA0411055	3.0	3.0
PHA0411059	3.0	3.0
PHA0509002	1.0	1.0
PHA0509015	3.0	3.0
PHA0509027	2.0	3.0
PHA0509030	3.0	3.0
PHA0509034	2.0	3.0
PHA0509036	3.0	3.0
PHA0509039	3.0	3.0
PHA0509043	3.0	3.0
PHA0510010A	1.0	1.0
PHA0510035	3.0	3.0
PHA0510050	3.0	3.0
PHA0610018	3.0	3.0
PHA0710009	3.0	3.0
PHA0710010	3.0	3.0
PHA0710016	3.0	3.0
PHA0710021	4.0	3.0
PHA0810002	3.0	3.0
PHA0810009	3.0	3.0
PHA0810011	3.0	3.0
PHA1109004	3.0	3.0
PHA1109007	2.0	2.0
PHA1110001B	2.0	1.0
PHA1110002B	2.0	1.0
PHA1110016	2.0	3.0
PHA1111002A	1.0	1.0
PHA1111004A	1.0	2.0
PHA1111006A	1.0	2.0
PHA1111009A	1.0	1.0
VAR0909004	3.0	3.0
VAR0910005	3.0	3.0
VAR0910006	3.0	3.0
VAR0910007	3.0	3.0
VAR0910010	3.0	3.0
Averaged weighted F1-scores 0.7959528021717064
144.90552995391704 65.35717405024758
LABEL SET ['A1', 'A2', 'B1', 'B2']
LANGUAGE: CZ, 0th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.79
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.51
              precision    recall  f1-score   support

         0.0       1.00      0.90      0.95        52
         1.0       0.55      1.00      0.71        22
         2.0       0.00      0.00      0.00        12
         3.0       0.00      0.00      0.00         1

    accuracy                           0.79        87
   macro avg       0.39      0.48      0.41        87
weighted avg       0.74      0.79      0.75        87

[[47  5  0  0]
 [ 0 22  0  0]
 [ 0 12  0  0]
 [ 0  1  0  0]]
0.7469728804545268
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.48
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.48
              precision    recall  f1-score   support

         0.0       1.00      0.92      0.96        52
         1.0       0.56      1.00      0.72        22
         2.0       0.00      0.00      0.00        12
         3.0       0.00      0.00      0.00         1

    accuracy                           0.80        87
   macro avg       0.39      0.48      0.42        87
weighted avg       0.74      0.80      0.76        87

[[48  4  0  0]
 [ 0 22  0  0]
 [ 0 12  0  0]
 [ 0  1  0  0]]
0.7561937064254758
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.46
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.48
              precision    recall  f1-score   support

         0.0       1.00      0.92      0.96        52
         1.0       0.56      1.00      0.72        22
         2.0       0.00      0.00      0.00        12
         3.0       0.00      0.00      0.00         1

    accuracy                           0.80        87
   macro avg       0.39      0.48      0.42        87
weighted avg       0.74      0.80      0.76        87

[[48  4  0  0]
 [ 0 22  0  0]
 [ 0 12  0  0]
 [ 0  1  0  0]]
0.7561937064254758
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.39
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.50
              precision    recall  f1-score   support

         0.0       1.00      0.92      0.96        52
         1.0       0.56      1.00      0.72        22
         2.0       0.00      0.00      0.00        12
         3.0       0.00      0.00      0.00         1

    accuracy                           0.80        87
   macro avg       0.39      0.48      0.42        87
weighted avg       0.74      0.80      0.76        87

[[48  4  0  0]
 [ 0 22  0  0]
 [ 0 12  0  0]
 [ 0  1  0  0]]
0.7561937064254758
87 87 87
Filename	True Label	Prediction
0603	2.0	1.0
0618	2.0	1.0
0621	2.0	1.0
0627	2.0	1.0
0630	0.0	1.0
0633	2.0	1.0
0636	2.0	1.0
0722	2.0	1.0
0802	1.0	1.0
0806	2.0	1.0
0808	1.0	1.0
0812	1.0	1.0
0814	1.0	1.0
0816	3.0	1.0
0828	1.0	1.0
0903	1.0	1.0
0907	2.0	1.0
0914	1.0	1.0
0915	2.0	1.0
0922	1.0	1.0
0927	2.0	1.0
1004	1.0	1.0
1005	1.0	1.0
1017	1.0	1.0
1020	1.0	1.0
1022	1.0	1.0
1113	1.0	1.0
9999	0.0	1.0
KYJ0611003A	1.0	1.0
KYJ0611005A	1.0	1.0
KYJ0611006A	1.0	1.0
KYJ0611006B	0.0	0.0
KYJ0611009A	1.0	1.0
KYJ0611009B	0.0	0.0
LIB0611003A	1.0	1.0
LIB0611004B	0.0	0.0
LON0610002A	1.0	1.0
LON0610002B	0.0	0.0
LON0611002A	1.0	1.0
PAR1011009A	2.0	1.0
PHA0112006B	0.0	0.0
PHA0112007B	0.0	0.0
PHA0112009B	0.0	0.0
PHA0112012B	0.0	0.0
PHA0209028	0.0	0.0
PHA0209034	0.0	0.0
PHA0210008	0.0	0.0
PHA0411008A	0.0	1.0
PHA0411011B	0.0	0.0
PHA0411051	0.0	0.0
PHA0411053	0.0	0.0
PHA0411062	0.0	0.0
PHA0509021	0.0	0.0
PHA0509024	0.0	0.0
PHA0509026	0.0	0.0
PHA0509034	0.0	0.0
PHA0509036	0.0	0.0
PHA0509038	0.0	0.0
PHA0509039	0.0	0.0
PHA0509044	0.0	0.0
PHA0510003B	0.0	0.0
PHA0510004A	0.0	0.0
PHA0510030	0.0	0.0
PHA0510031	0.0	0.0
PHA0510034	0.0	0.0
PHA0510047	0.0	0.0
PHA0610005B	0.0	0.0
PHA0610018	0.0	0.0
PHA0610019A	0.0	1.0
PHA0710011	0.0	0.0
PHA0710015	0.0	0.0
PHA0710021	0.0	0.0
PHA0810003	0.0	0.0
PHA0810009	0.0	0.0
PHA0810012	0.0	0.0
PHA1109003	0.0	0.0
PHA1109007	0.0	0.0
PHA1109026	0.0	0.0
PHA1110003A	1.0	1.0
PHA1110013	0.0	0.0
PHA1110022	0.0	0.0
PHA1111003B	0.0	0.0
PHA1111008B	0.0	0.0
VAR0909003	0.0	0.0
VAR0910006	0.0	0.0
VAR0910007	0.0	0.0
VAR0910009	0.0	0.0
LANGUAGE: CZ, 1th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.91
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.57
              precision    recall  f1-score   support

         0.0       1.00      0.94      0.97        52
         1.0       0.55      1.00      0.71        21
         2.0       0.00      0.00      0.00        13
         3.0       0.00      0.00      0.00         1

    accuracy                           0.80        87
   macro avg       0.39      0.49      0.42        87
weighted avg       0.73      0.80      0.75        87

[[49  3  0  0]
 [ 0 21  0  0]
 [ 0 13  0  0]
 [ 0  1  0  0]]
0.7517769895049119
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.54
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.45
              precision    recall  f1-score   support

         0.0       1.00      0.94      0.97        52
         1.0       0.55      1.00      0.71        21
         2.0       0.00      0.00      0.00        13
         3.0       0.00      0.00      0.00         1

    accuracy                           0.80        87
   macro avg       0.39      0.49      0.42        87
weighted avg       0.73      0.80      0.75        87

[[49  3  0  0]
 [ 0 21  0  0]
 [ 0 13  0  0]
 [ 0  1  0  0]]
0.7517769895049119
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.48
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.48
              precision    recall  f1-score   support

         0.0       1.00      0.94      0.97        52
         1.0       0.55      1.00      0.71        21
         2.0       0.00      0.00      0.00        13
         3.0       0.00      0.00      0.00         1

    accuracy                           0.80        87
   macro avg       0.39      0.49      0.42        87
weighted avg       0.73      0.80      0.75        87

[[49  3  0  0]
 [ 0 21  0  0]
 [ 0 13  0  0]
 [ 0  1  0  0]]
0.7517769895049119
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.46
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.46
              precision    recall  f1-score   support

         0.0       1.00      0.94      0.97        52
         1.0       0.55      1.00      0.71        21
         2.0       0.00      0.00      0.00        13
         3.0       0.00      0.00      0.00         1

    accuracy                           0.80        87
   macro avg       0.39      0.49      0.42        87
weighted avg       0.73      0.80      0.75        87

[[49  3  0  0]
 [ 0 21  0  0]
 [ 0 13  0  0]
 [ 0  1  0  0]]
0.7517769895049119
87 87 87
Filename	True Label	Prediction
0601	1.0	1.0
0608	0.0	1.0
0622	1.0	1.0
0624	2.0	1.0
0632	1.0	1.0
0637	2.0	1.0
0642	2.0	1.0
0644	1.0	1.0
0717	1.0	1.0
0719	1.0	1.0
0723	2.0	1.0
0725	1.0	1.0
0805	2.0	1.0
0807	2.0	1.0
0811	2.0	1.0
0818	1.0	1.0
0819	3.0	1.0
0821	2.0	1.0
0824	1.0	1.0
0913	2.0	1.0
0918	1.0	1.0
0919	2.0	1.0
0923	2.0	1.0
0928	2.0	1.0
1015	1.0	1.0
1021	1.0	1.0
1111	1.0	1.0
1114	1.0	1.0
BER0609003	0.0	0.0
BER0611005	0.0	0.0
KYJ0611005B	0.0	0.0
LIB0611011	0.0	0.0
LON0611002B	0.0	0.0
LON0611003	0.0	0.0
MOS0611012	0.0	0.0
PAR1011008A	2.0	1.0
PAR1011009B	0.0	0.0
PAR1011013	0.0	0.0
PAR1011016	0.0	0.0
PHA0111002B	0.0	0.0
PHA0111003A	1.0	1.0
PHA0111004A	1.0	1.0
PHA0111004B	0.0	0.0
PHA0209001	0.0	0.0
PHA0209008	0.0	0.0
PHA0209024	0.0	0.0
PHA0210007	0.0	0.0
PHA0411012B	0.0	0.0
PHA0411029	0.0	0.0
PHA0411033	0.0	0.0
PHA0411042	0.0	0.0
PHA0411047	0.0	0.0
PHA0411058	0.0	0.0
PHA0411059	0.0	0.0
PHA0509022	0.0	0.0
PHA0509025	0.0	0.0
PHA0510002B	0.0	0.0
PHA0510003A	0.0	1.0
PHA0510004B	0.0	0.0
PHA0510013A	1.0	1.0
PHA0510038	0.0	0.0
PHA0510046	0.0	0.0
PHA0510049	0.0	0.0
PHA0510050	0.0	0.0
PHA0610005A	1.0	1.0
PHA0610007A	0.0	1.0
PHA0610015	0.0	0.0
PHA0709008	0.0	0.0
PHA0710012	0.0	0.0
PHA0710014	0.0	0.0
PHA0710018	0.0	0.0
PHA0809009	0.0	0.0
PHA0809010	0.0	0.0
PHA0810004	0.0	0.0
PHA0811014	0.0	0.0
PHA0811020	0.0	0.0
PHA1109001	0.0	0.0
PHA1109004	0.0	0.0
PHA1109028	0.0	0.0
PHA1110001A	1.0	1.0
PHA1110002B	0.0	0.0
PHA1110004A	1.0	1.0
PHA1110015	0.0	0.0
PHA1110021	0.0	0.0
PHA1111003A	1.0	1.0
VAR0909005	0.0	0.0
VAR0909006	0.0	0.0
LANGUAGE: CZ, 2th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.05
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.64
              precision    recall  f1-score   support

         0.0       0.74      1.00      0.85        52
         1.0       0.41      0.33      0.37        21
         2.0       0.00      0.00      0.00        13
         3.0       0.00      0.00      0.00         1

    accuracy                           0.68        87
   macro avg       0.29      0.33      0.31        87
weighted avg       0.54      0.68      0.60        87

[[52  0  0  0]
 [14  7  0  0]
 [ 3 10  0  0]
 [ 1  0  0  0]]
0.5984449535370365
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.58
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.40
              precision    recall  f1-score   support

         0.0       0.98      0.98      0.98        52
         1.0       0.57      0.95      0.71        21
         2.0       0.00      0.00      0.00        13
         3.0       0.00      0.00      0.00         1

    accuracy                           0.82        87
   macro avg       0.39      0.48      0.42        87
weighted avg       0.72      0.82      0.76        87

[[51  1  0  0]
 [ 1 20  0  0]
 [ 0 13  0  0]
 [ 0  1  0  0]]
0.7586206896551724
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.49
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.51
              precision    recall  f1-score   support

         0.0       0.89      0.98      0.94        52
         1.0       0.53      0.76      0.63        21
         2.0       0.00      0.00      0.00        13
         3.0       0.00      0.00      0.00         1

    accuracy                           0.77        87
   macro avg       0.36      0.44      0.39        87
weighted avg       0.66      0.77      0.71        87

[[51  1  0  0]
 [ 5 16  0  0]
 [ 0 13  0  0]
 [ 1  0  0  0]]
0.7107703568615044
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.43
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.36
              precision    recall  f1-score   support

         0.0       1.00      0.98      0.99        52
         1.0       0.58      1.00      0.74        21
         2.0       0.00      0.00      0.00        13
         3.0       0.00      0.00      0.00         1

    accuracy                           0.83        87
   macro avg       0.40      0.50      0.43        87
weighted avg       0.74      0.83      0.77        87

[[51  1  0  0]
 [ 0 21  0  0]
 [ 0 13  0  0]
 [ 0  1  0  0]]
0.7697566648459112
87 87 87
Filename	True Label	Prediction
0602	1.0	1.0
0606	2.0	1.0
0609	1.0	1.0
0612	2.0	1.0
0615	1.0	1.0
0620	1.0	1.0
0625	1.0	1.0
0626	2.0	1.0
0629	2.0	1.0
0631	2.0	1.0
0641	1.0	1.0
0714	2.0	1.0
0716	2.0	1.0
0718	1.0	1.0
0720	2.0	1.0
0813	2.0	1.0
0820	1.0	1.0
0827	2.0	1.0
0904	1.0	1.0
0906	2.0	1.0
0921	2.0	1.0
1014	1.0	1.0
1018	1.0	1.0
1019	1.0	1.0
1023	1.0	1.0
1115	1.0	1.0
1116	1.0	1.0
BER0611006	0.0	0.0
LON0611004B	0.0	0.0
MOS0509004	0.0	0.0
MOS0611013	0.0	0.0
PAR1011017	0.0	0.0
PHA0111002A	0.0	1.0
PHA0111010	0.0	0.0
PHA0111011	0.0	0.0
PHA0111012	0.0	0.0
PHA0111016	0.0	0.0
PHA0111018	0.0	0.0
PHA0112002A	1.0	1.0
PHA0112006A	3.0	1.0
PHA0112009A	2.0	1.0
PHA0411008B	0.0	0.0
PHA0411010A	1.0	1.0
PHA0411038	0.0	0.0
PHA0411039	0.0	0.0
PHA0411043	0.0	0.0
PHA0411054	0.0	0.0
PHA0411061	0.0	0.0
PHA0509002	0.0	0.0
PHA0509017	0.0	0.0
PHA0509028	0.0	0.0
PHA0509032	0.0	0.0
PHA0509033	0.0	0.0
PHA0509043	0.0	0.0
PHA0509045	0.0	0.0
PHA0510010A	1.0	1.0
PHA0510013B	0.0	0.0
PHA0510023	0.0	0.0
PHA0510029	0.0	0.0
PHA0510035	0.0	0.0
PHA0510036	0.0	0.0
PHA0510039	0.0	0.0
PHA0510040	0.0	0.0
PHA0610006B	0.0	0.0
PHA0610007B	0.0	0.0
PHA0610019B	0.0	0.0
PHA0610025	0.0	0.0
PHA0710019	0.0	0.0
PHA0810006	0.0	0.0
PHA0811012	0.0	0.0
PHA0811013	0.0	0.0
PHA1109002	0.0	0.0
PHA1109005	0.0	0.0
PHA1109006	0.0	0.0
PHA1109027	0.0	0.0
PHA1110002A	1.0	1.0
PHA1110016	0.0	0.0
PHA1110017	0.0	0.0
PHA1110019	0.0	0.0
PHA1111001B	0.0	0.0
PHA1111002A	1.0	1.0
PHA1111006B	0.0	0.0
PHA1111008A	1.0	1.0
VAR0209036	0.0	0.0
VAR0909008	0.0	0.0
VAR0910005	0.0	0.0
VAR0910010	0.0	0.0
LANGUAGE: CZ, 3th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.92
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.61
              precision    recall  f1-score   support

         0.0       1.00      0.87      0.93        52
         1.0       0.50      1.00      0.67        21
         2.0       0.00      0.00      0.00        13
         3.0       0.00      0.00      0.00         1

    accuracy                           0.76        87
   macro avg       0.38      0.47      0.40        87
weighted avg       0.72      0.76      0.72        87

[[45  7  0  0]
 [ 0 21  0  0]
 [ 0 13  0  0]
 [ 0  1  0  0]]
0.7154876170162341
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.49
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.62
              precision    recall  f1-score   support

         0.0       1.00      0.87      0.93        52
         1.0       0.50      1.00      0.67        21
         2.0       0.00      0.00      0.00        13
         3.0       0.00      0.00      0.00         1

    accuracy                           0.76        87
   macro avg       0.38      0.47      0.40        87
weighted avg       0.72      0.76      0.72        87

[[45  7  0  0]
 [ 0 21  0  0]
 [ 0 13  0  0]
 [ 0  1  0  0]]
0.7154876170162341
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.40
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.65
              precision    recall  f1-score   support

         0.0       1.00      0.87      0.93        52
         1.0       0.50      1.00      0.67        21
         2.0       0.00      0.00      0.00        13
         3.0       0.00      0.00      0.00         1

    accuracy                           0.76        87
   macro avg       0.38      0.47      0.40        87
weighted avg       0.72      0.76      0.72        87

[[45  7  0  0]
 [ 0 21  0  0]
 [ 0 13  0  0]
 [ 0  1  0  0]]
0.7154876170162341
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.40
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.61
              precision    recall  f1-score   support

         0.0       1.00      0.88      0.94        52
         1.0       0.51      1.00      0.68        21
         2.0       0.00      0.00      0.00        13
         3.0       0.00      0.00      0.00         1

    accuracy                           0.77        87
   macro avg       0.38      0.47      0.40        87
weighted avg       0.72      0.77      0.72        87

[[46  6  0  0]
 [ 0 21  0  0]
 [ 0 13  0  0]
 [ 0  1  0  0]]
0.724622218186496
87 87 87
Filename	True Label	Prediction
0607	2.0	1.0
0610	1.0	1.0
0613	0.0	1.0
0614	2.0	1.0
0617	0.0	1.0
0619	2.0	1.0
0623	0.0	1.0
0628	2.0	1.0
0634	3.0	1.0
0638	1.0	1.0
0640	2.0	1.0
0721	2.0	1.0
0724	2.0	1.0
0804	2.0	1.0
0810	1.0	1.0
0823	1.0	1.0
0826	2.0	1.0
0829	1.0	1.0
0912	2.0	1.0
0916	1.0	1.0
0920	2.0	1.0
0924	1.0	1.0
0925	2.0	1.0
0926	2.0	1.0
0929	0.0	1.0
0930	1.0	1.0
1001	1.0	1.0
1008	1.0	1.0
1016	1.0	1.0
1112	1.0	1.0
1117	1.0	1.0
BER0611007	0.0	0.0
KYJ0611004A	1.0	1.0
LIB0611002A	0.0	1.0
LIB0611004A	1.0	1.0
LON0611004A	1.0	1.0
PHA0111003B	0.0	0.0
PHA0111005B	0.0	0.0
PHA0112002B	0.0	0.0
PHA0112003B	0.0	0.0
PHA0112007A	1.0	1.0
PHA0112012A	1.0	1.0
PHA0209031	0.0	0.0
PHA0209039	0.0	0.0
PHA0210004	0.0	0.0
PHA0411009A	1.0	1.0
PHA0411009B	0.0	0.0
PHA0411027	0.0	0.0
PHA0411028	0.0	0.0
PHA0411032	0.0	0.0
PHA0411044	0.0	0.0
PHA0411055	0.0	0.0
PHA0411060	0.0	0.0
PHA0509007	0.0	0.0
PHA0509013	0.0	0.0
PHA0509015	0.0	0.0
PHA0509019	0.0	0.0
PHA0509027	0.0	0.0
PHA0509030	0.0	0.0
PHA0509035	0.0	0.0
PHA0509040	0.0	0.0
PHA0509042	0.0	0.0
PHA0510010B	0.0	0.0
PHA0510027	0.0	0.0
PHA0510032	0.0	0.0
PHA0610006A	0.0	0.0
PHA0610026	0.0	0.0
PHA0710009	0.0	0.0
PHA0710013	0.0	0.0
PHA0710016	0.0	0.0
PHA0710017	0.0	0.0
PHA0810011	0.0	0.0
PHA0810015	0.0	0.0
PHA0811017	0.0	0.0
PHA0811019	0.0	0.0
PHA1109008	0.0	0.0
PHA1109023	0.0	0.0
PHA1109024	0.0	0.0
PHA1110003B	0.0	0.0
PHA1111001A	1.0	1.0
PHA1111002B	0.0	0.0
PHA1111004A	1.0	1.0
PHA1111004B	0.0	0.0
PHA1111006A	0.0	1.0
TI071122B	0.0	0.0
VAR0909007	0.0	0.0
VAR0909010	0.0	0.0
LANGUAGE: CZ, 4th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.92
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.51
              precision    recall  f1-score   support

         0.0       0.96      0.98      0.97        53
         1.0       0.62      0.95      0.75        21
         2.0       0.00      0.00      0.00        12

    accuracy                           0.84        86
   macro avg       0.53      0.64      0.58        86
weighted avg       0.75      0.84      0.78        86

[[52  1  0]
 [ 1 20  0]
 [ 1 11  0]]
0.7832915732018881
86 86 86



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.52
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.41
              precision    recall  f1-score   support

         0.0       1.00      0.94      0.97        53
         1.0       0.58      1.00      0.74        21
         2.0       0.00      0.00      0.00        12

    accuracy                           0.83        86
   macro avg       0.53      0.65      0.57        86
weighted avg       0.76      0.83      0.78        86

[[50  3  0]
 [ 0 21  0]
 [ 0 12  0]]
0.7782557545364881
86 86 86



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.49
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.35
              precision    recall  f1-score   support

         0.0       1.00      0.96      0.98        53
         1.0       0.60      1.00      0.75        21
         2.0       0.00      0.00      0.00        12

    accuracy                           0.84        86
   macro avg       0.53      0.65      0.58        86
weighted avg       0.76      0.84      0.79        86

[[51  2  0]
 [ 0 21  0]
 [ 0 12  0]]
0.7875670840787119
86 86 86



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.43
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.39
              precision    recall  f1-score   support

         0.0       0.98      0.96      0.97        53
         1.0       0.59      0.95      0.73        21
         2.0       0.00      0.00      0.00        12

    accuracy                           0.83        86
   macro avg       0.52      0.64      0.57        86
weighted avg       0.75      0.83      0.78        86

[[51  2  0]
 [ 1 20  0]
 [ 0 12  0]]
0.7762609483539716
86 86 86
Filename	True Label	Prediction
0604	1.0	1.0
0605	2.0	1.0
0611	1.0	1.0
0616	2.0	1.0
0635	2.0	1.0
0639	1.0	1.0
0643	1.0	1.0
0645	2.0	1.0
0715	2.0	1.0
0801	1.0	1.0
0803	1.0	1.0
0809	1.0	1.0
0815	2.0	1.0
0817	1.0	1.0
0822	1.0	1.0
0825	1.0	1.0
0901	2.0	1.0
0902	2.0	1.0
0905	1.0	1.0
0910	2.0	1.0
0911	2.0	1.0
0917	2.0	1.0
1002	1.0	1.0
1003	1.0	1.0
1006	1.0	1.0
1007	1.0	1.0
1009	2.0	1.0
1010	1.0	1.0
BER0611003	0.0	0.0
LIB0611001A	0.0	1.0
LIB0611001B	0.0	0.0
LIB0611002B	0.0	0.0
MOS0509001	0.0	0.0
MOS0611014	0.0	0.0
MOS0611015	0.0	0.0
PAR1011014	0.0	0.0
PAR1011015	0.0	0.0
PAR1011018	0.0	0.0
PHA0111001A	1.0	1.0
PHA0111001B	0.0	0.0
PHA0111005A	1.0	1.0
PHA0111014	0.0	0.0
PHA0111015	0.0	0.0
PHA0112003A	1.0	0.0
PHA0209013	0.0	0.0
PHA0209026	0.0	0.0
PHA0209038	0.0	0.0
PHA0210001	0.0	0.0
PHA0411010B	0.0	0.0
PHA0411011A	1.0	1.0
PHA0411012A	1.0	1.0
PHA0411030	0.0	0.0
PHA0411031	0.0	0.0
PHA0411034	0.0	0.0
PHA0411035	0.0	0.0
PHA0411036	0.0	0.0
PHA0411037	0.0	0.0
PHA0411041	0.0	0.0
PHA0411045	0.0	0.0
PHA0411056	0.0	0.0
PHA0509018	0.0	0.0
PHA0509020	0.0	0.0
PHA0509031	0.0	0.0
PHA0509037	0.0	0.0
PHA0509041	0.0	0.0
PHA0510002A	0.0	0.0
PHA0510037	0.0	0.0
PHA0510048	0.0	0.0
PHA0610016	0.0	0.0
PHA0610017	0.0	0.0
PHA0710010	0.0	0.0
PHA0810001	0.0	0.0
PHA0810002	0.0	0.0
PHA0810008	0.0	0.0
PHA0810010	0.0	0.0
PHA0811010	0.0	0.0
PHA0811016	0.0	0.0
PHA1109025	0.0	0.0
PHA1110001B	0.0	0.0
PHA1110014	0.0	0.0
PHA1111009A	0.0	1.0
ST071122B	0.0	0.0
VAR0909004	0.0	0.0
VAR0909009	0.0	0.0
VAR0910004	0.0	0.0
VAR0910011	0.0	0.0
Averaged weighted F1-scores 0.7557221054633534
MONOLINGUAL Experiments with:  IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1']
LANGUAGE: IT, 0th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.67
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.52
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.79      0.86      0.82        76
         2.0       0.86      0.85      0.85        79

    accuracy                           0.82       160
   macro avg       0.55      0.57      0.56       160
weighted avg       0.80      0.82      0.81       160

[[ 0  5  0]
 [ 0 65 11]
 [ 0 12 67]]
0.8122399822623558
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.50
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.49
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.80      0.84      0.82        76
         2.0       0.85      0.86      0.86        79

    accuracy                           0.82       160
   macro avg       0.55      0.57      0.56       160
weighted avg       0.80      0.82      0.81       160

[[ 0  5  0]
 [ 0 64 12]
 [ 0 11 68]]
0.8120706337687469
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.44
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.60
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.84      0.78      0.81        76
         2.0       0.81      0.92      0.86        79

    accuracy                           0.82       160
   macro avg       0.55      0.57      0.56       160
weighted avg       0.80      0.82      0.81       160

[[ 0  5  0]
 [ 0 59 17]
 [ 0  6 73]]
0.8104573640269109
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.39
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.62
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.84      0.82      0.83        76
         2.0       0.84      0.91      0.87        79

    accuracy                           0.84       160
   macro avg       0.56      0.58      0.57       160
weighted avg       0.81      0.84      0.82       160

[[ 0  5  0]
 [ 0 62 14]
 [ 0  7 72]]
0.8235757575757574
160 160 160
Filename	True Label	Prediction
1325_1001008	2.0	2.0
1325_1001009	2.0	2.0
1325_1001011	2.0	2.0
1325_1001016	2.0	2.0
1325_1001020	2.0	2.0
1325_1001023	2.0	2.0
1325_1001036	2.0	2.0
1325_1001041	2.0	2.0
1325_1001044	2.0	2.0
1325_1001046	2.0	2.0
1325_1001054	2.0	2.0
1325_1001059	2.0	2.0
1325_1001075	1.0	2.0
1325_1001077	2.0	2.0
1325_1001087	2.0	2.0
1325_1001092	2.0	2.0
1325_1001093	2.0	2.0
1325_1001107	2.0	2.0
1325_1001111	2.0	2.0
1325_1001121	2.0	2.0
1325_1001124	2.0	2.0
1325_1001136	2.0	2.0
1325_1001138	2.0	2.0
1325_9000059	2.0	2.0
1325_9000087	2.0	2.0
1325_9000090	2.0	2.0
1325_9000102	2.0	2.0
1325_9000106	2.0	2.0
1325_9000137	2.0	2.0
1325_9000140	2.0	2.0
1325_9000187	2.0	2.0
1325_9000213	2.0	2.0
1325_9000239	2.0	2.0
1325_9000279	2.0	2.0
1325_9000304	2.0	2.0
1325_9000322	2.0	2.0
1325_9000503	2.0	2.0
1325_9000505	2.0	2.0
1325_9000675	2.0	2.0
1365_0100004	1.0	2.0
1365_0100006	2.0	2.0
1365_0100012	2.0	1.0
1365_0100017	2.0	2.0
1365_0100031	2.0	1.0
1365_0100057	2.0	2.0
1365_0100058	2.0	2.0
1365_0100067	1.0	2.0
1365_0100073	2.0	2.0
1365_0100074	1.0	2.0
1365_0100119	2.0	2.0
1365_0100125	2.0	2.0
1365_0100135	2.0	2.0
1365_0100137	2.0	2.0
1365_0100148	2.0	2.0
1365_0100151	1.0	1.0
1365_0100164	2.0	2.0
1365_0100167	1.0	2.0
1365_0100169	2.0	2.0
1365_0100171	1.0	2.0
1365_0100176	2.0	2.0
1365_0100178	2.0	2.0
1365_0100187	2.0	2.0
1365_0100192	2.0	2.0
1365_0100198	1.0	2.0
1365_0100199	2.0	2.0
1365_0100200	2.0	2.0
1365_0100211	2.0	2.0
1365_0100217	2.0	2.0
1365_0100223	2.0	2.0
1365_0100228	1.0	2.0
1365_0100230	2.0	2.0
1365_0100231	2.0	2.0
1365_0100257	2.0	2.0
1365_0100268	1.0	2.0
1365_0100270	2.0	2.0
1365_0100279	2.0	2.0
1365_0100282	2.0	2.0
1365_0100285	2.0	2.0
1365_0100286	1.0	2.0
1365_0100289	2.0	2.0
1365_0100448	1.0	2.0
1365_0100458	2.0	2.0
1365_0100474	2.0	2.0
1365_0100475	2.0	2.0
1365_0100476	2.0	2.0
1365_0100477	1.0	2.0
1385_0000036	1.0	1.0
1385_0000038	1.0	1.0
1385_0000051	1.0	1.0
1385_0000097	1.0	1.0
1385_0000123	1.0	1.0
1385_0000130	1.0	1.0
1385_0001103	1.0	1.0
1385_0001107	1.0	1.0
1385_0001124	1.0	1.0
1385_0001125	1.0	1.0
1385_0001129	1.0	1.0
1385_0001132	1.0	1.0
1385_0001133	1.0	1.0
1385_0001151	1.0	1.0
1385_0001152	1.0	1.0
1385_0001161	1.0	1.0
1385_0001165	1.0	1.0
1385_0001167	1.0	1.0
1385_0001170	1.0	1.0
1385_0001178	0.0	1.0
1385_0001197	1.0	1.0
1385_0001503	1.0	1.0
1385_0001526	0.0	1.0
1385_0001528	1.0	1.0
1385_0001725	1.0	1.0
1385_0001737	1.0	1.0
1385_0001738	0.0	1.0
1385_0001748	1.0	1.0
1385_0001751	1.0	1.0
1385_0001759	1.0	1.0
1385_0001767	1.0	1.0
1385_0001789	1.0	1.0
1385_0001790	1.0	1.0
1385_0001795	1.0	1.0
1385_0001796	1.0	1.0
1385_0001798	1.0	1.0
1395_0000355	1.0	2.0
1395_0000360	2.0	1.0
1395_0000364	1.0	1.0
1395_0000376	2.0	1.0
1395_0000415	1.0	1.0
1395_0000432	1.0	1.0
1395_0000451	1.0	1.0
1395_0000462	2.0	1.0
1395_0000471	1.0	1.0
1395_0000515	2.0	1.0
1395_0000531	1.0	1.0
1395_0000537	1.0	1.0
1395_0000555	1.0	1.0
1395_0000585	1.0	1.0
1395_0000587	0.0	1.0
1395_0000593	1.0	1.0
1395_0000595	0.0	1.0
1395_0000597	1.0	1.0
1395_0000599	1.0	1.0
1395_0000608	1.0	1.0
1395_0000610	1.0	1.0
1395_0000627	1.0	1.0
1395_0000630	1.0	1.0
1395_0000646	1.0	1.0
1395_0001016	2.0	1.0
1395_0001021	1.0	1.0
1395_0001045	1.0	1.0
1395_0001068	1.0	1.0
1395_0001071	1.0	1.0
1395_0001078	1.0	1.0
1395_0001101	1.0	1.0
1395_0001109	1.0	1.0
1395_0001133	1.0	1.0
1395_0001145	2.0	2.0
1395_0001150	1.0	1.0
1395_0001161	1.0	1.0
1395_0001170	1.0	2.0
1395_0001171	1.0	1.0
LANGUAGE: IT, 1th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.72
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.51
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.77      0.88      0.82        76
         2.0       0.88      0.81      0.84        79

    accuracy                           0.82       160
   macro avg       0.55      0.56      0.55       160
weighted avg       0.80      0.82      0.81       160

[[ 0  5  0]
 [ 0 67  9]
 [ 0 15 64]]
0.8062802712302227
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.55
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.48
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.79      0.95      0.86        76
         2.0       0.94      0.82      0.88        79

    accuracy                           0.86       160
   macro avg       0.58      0.59      0.58       160
weighted avg       0.84      0.86      0.84       160

[[ 0  5  0]
 [ 0 72  4]
 [ 0 14 65]]
0.8432801626476776
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.45
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.50
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.81      0.95      0.87        76
         2.0       0.94      0.85      0.89        79

    accuracy                           0.87       160
   macro avg       0.58      0.60      0.59       160
weighted avg       0.85      0.87      0.86       160

[[ 0  5  0]
 [ 0 72  4]
 [ 0 12 67]]
0.8556287878787879
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.43
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.54
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.80      0.87      0.84        76
         2.0       0.87      0.86      0.87        79

    accuracy                           0.84       160
   macro avg       0.56      0.58      0.57       160
weighted avg       0.81      0.84      0.82       160

[[ 0  5  0]
 [ 0 66 10]
 [ 0 11 68]]
0.8245424494074015
160 160 160
Filename	True Label	Prediction
1325_1001024	2.0	2.0
1325_1001037	2.0	2.0
1325_1001042	2.0	2.0
1325_1001047	2.0	2.0
1325_1001051	2.0	2.0
1325_1001055	2.0	2.0
1325_1001063	2.0	2.0
1325_1001080	2.0	2.0
1325_1001085	2.0	2.0
1325_1001086	2.0	2.0
1325_1001091	2.0	2.0
1325_1001097	1.0	2.0
1325_1001098	2.0	2.0
1325_1001109	2.0	2.0
1325_1001130	2.0	2.0
1325_1001132	2.0	2.0
1325_1001142	2.0	2.0
1325_1001143	2.0	2.0
1325_1001144	2.0	2.0
1325_1001155	2.0	2.0
1325_1001156	2.0	2.0
1325_1001161	2.0	2.0
1325_1001163	2.0	2.0
1325_1001164	2.0	2.0
1325_1001168	2.0	2.0
1325_1001170	2.0	2.0
1325_9000143	2.0	2.0
1325_9000185	2.0	2.0
1325_9000186	2.0	2.0
1325_9000209	2.0	2.0
1325_9000215	2.0	2.0
1325_9000302	2.0	2.0
1325_9000315	1.0	2.0
1325_9000317	2.0	2.0
1325_9000323	2.0	2.0
1325_9000554	2.0	2.0
1325_9000612	1.0	2.0
1325_9000678	2.0	2.0
1325_9000684	2.0	2.0
1325_9000685	2.0	2.0
1365_0100002	2.0	2.0
1365_0100003	1.0	1.0
1365_0100005	1.0	2.0
1365_0100014	2.0	2.0
1365_0100019	1.0	2.0
1365_0100028	2.0	2.0
1365_0100029	1.0	2.0
1365_0100065	1.0	1.0
1365_0100071	2.0	2.0
1365_0100092	2.0	2.0
1365_0100105	2.0	2.0
1365_0100138	2.0	1.0
1365_0100165	2.0	2.0
1365_0100168	2.0	2.0
1365_0100173	2.0	2.0
1365_0100184	2.0	2.0
1365_0100190	2.0	2.0
1365_0100195	1.0	2.0
1365_0100204	2.0	2.0
1365_0100215	2.0	2.0
1365_0100219	2.0	2.0
1365_0100221	2.0	2.0
1365_0100227	2.0	2.0
1365_0100229	2.0	2.0
1365_0100253	1.0	2.0
1365_0100256	2.0	2.0
1365_0100258	2.0	2.0
1365_0100260	2.0	2.0
1365_0100265	2.0	2.0
1365_0100267	2.0	2.0
1365_0100275	2.0	2.0
1365_0100277	2.0	2.0
1365_0100278	2.0	2.0
1365_0100280	1.0	2.0
1365_0100447	2.0	2.0
1365_0100457	2.0	2.0
1365_0100469	2.0	2.0
1365_0100478	2.0	2.0
1365_0100479	2.0	2.0
1365_0100482	2.0	2.0
1385_0000012	1.0	1.0
1385_0000013	0.0	1.0
1385_0000016	1.0	1.0
1385_0000021	1.0	1.0
1385_0000023	1.0	1.0
1385_0000042	1.0	1.0
1385_0000047	1.0	1.0
1385_0000058	1.0	1.0
1385_0000059	1.0	1.0
1385_0000099	1.0	1.0
1385_0000100	1.0	1.0
1385_0000124	1.0	1.0
1385_0000127	1.0	1.0
1385_0001104	1.0	1.0
1385_0001110	1.0	1.0
1385_0001111	1.0	1.0
1385_0001119	1.0	1.0
1385_0001120	1.0	1.0
1385_0001121	1.0	1.0
1385_0001128	0.0	1.0
1385_0001130	1.0	1.0
1385_0001137	1.0	1.0
1385_0001150	1.0	1.0
1385_0001158	1.0	1.0
1385_0001159	1.0	1.0
1385_0001163	1.0	1.0
1385_0001173	0.0	1.0
1385_0001190	1.0	1.0
1385_0001193	2.0	1.0
1385_0001196	1.0	1.0
1385_0001198	2.0	1.0
1385_0001199	1.0	1.0
1385_0001523	1.0	1.0
1385_0001714	1.0	1.0
1385_0001730	2.0	1.0
1385_0001739	1.0	1.0
1385_0001746	1.0	1.0
1385_0001747	1.0	1.0
1385_0001749	1.0	1.0
1385_0001754	1.0	1.0
1385_0001758	1.0	1.0
1385_0001764	1.0	1.0
1385_0001766	1.0	1.0
1385_0001773	1.0	1.0
1385_0001775	1.0	1.0
1385_0001800	1.0	1.0
1395_0000333	1.0	1.0
1395_0000341	1.0	1.0
1395_0000365	2.0	1.0
1395_0000389	0.0	1.0
1395_0000392	1.0	1.0
1395_0000398	2.0	1.0
1395_0000402	1.0	1.0
1395_0000409	2.0	1.0
1395_0000448	1.0	1.0
1395_0000452	1.0	1.0
1395_0000470	1.0	1.0
1395_0000500	1.0	1.0
1395_0000526	1.0	1.0
1395_0000529	1.0	1.0
1395_0000534	1.0	2.0
1395_0000548	1.0	1.0
1395_0000551	2.0	1.0
1395_0000554	1.0	1.0
1395_0000559	1.0	1.0
1395_0000563	1.0	1.0
1395_0000564	1.0	1.0
1395_0000591	0.0	1.0
1395_0000644	1.0	1.0
1395_0001024	1.0	1.0
1395_0001060	2.0	1.0
1395_0001064	1.0	1.0
1395_0001074	1.0	1.0
1395_0001090	2.0	1.0
1395_0001103	1.0	1.0
1395_0001108	1.0	1.0
1395_0001115	2.0	1.0
1395_0001117	1.0	1.0
1395_0001122	1.0	1.0
1395_0001167	1.0	1.0
LANGUAGE: IT, 2th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.69
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.43
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         6
         1.0       0.79      0.95      0.86        76
         2.0       0.94      0.83      0.88        78

    accuracy                           0.86       160
   macro avg       0.58      0.59      0.58       160
weighted avg       0.84      0.86      0.84       160

[[ 0  6  0]
 [ 0 72  4]
 [ 0 13 65]]
0.8407032873029451
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.48
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.45
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         6
         1.0       0.89      0.75      0.81        76
         2.0       0.80      0.99      0.89        78

    accuracy                           0.84       160
   macro avg       0.56      0.58      0.57       160
weighted avg       0.81      0.84      0.82       160

[[ 0  6  0]
 [ 0 57 19]
 [ 0  1 77]]
0.8182512315270936
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.45
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.38
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         6
         1.0       0.83      0.92      0.88        76
         2.0       0.92      0.90      0.91        78

    accuracy                           0.88       160
   macro avg       0.58      0.61      0.59       160
weighted avg       0.84      0.88      0.86       160

[[ 0  6  0]
 [ 0 70  6]
 [ 0  8 70]]
0.8588068181818181
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.38
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.39
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         6
         1.0       0.84      0.92      0.88        76
         2.0       0.92      0.91      0.92        78

    accuracy                           0.88       160
   macro avg       0.59      0.61      0.60       160
weighted avg       0.85      0.88      0.86       160

[[ 0  6  0]
 [ 0 70  6]
 [ 0  7 71]]
0.8648518969364982
160 160 160
Filename	True Label	Prediction
1325_1001013	2.0	2.0
1325_1001018	2.0	2.0
1325_1001021	2.0	2.0
1325_1001027	2.0	2.0
1325_1001033	2.0	2.0
1325_1001039	2.0	2.0
1325_1001043	2.0	2.0
1325_1001045	2.0	2.0
1325_1001050	2.0	2.0
1325_1001058	2.0	2.0
1325_1001062	2.0	2.0
1325_1001076	2.0	2.0
1325_1001083	2.0	2.0
1325_1001090	2.0	2.0
1325_1001099	2.0	2.0
1325_1001100	2.0	2.0
1325_1001108	2.0	2.0
1325_1001119	2.0	2.0
1325_1001122	2.0	2.0
1325_1001123	2.0	2.0
1325_1001127	2.0	2.0
1325_1001128	2.0	2.0
1325_1001131	2.0	2.0
1325_1001153	2.0	2.0
1325_1001157	2.0	2.0
1325_1001162	2.0	2.0
1325_1001165	2.0	2.0
1325_9000088	2.0	2.0
1325_9000095	2.0	2.0
1325_9000107	2.0	2.0
1325_9000136	2.0	2.0
1325_9000139	2.0	2.0
1325_9000144	2.0	2.0
1325_9000303	2.0	2.0
1325_9000314	2.0	2.0
1325_9000318	2.0	2.0
1325_9000321	2.0	2.0
1325_9000533	2.0	2.0
1325_9000601	2.0	2.0
1325_9000611	2.0	2.0
1325_9000676	2.0	2.0
1325_9000677	2.0	2.0
1325_9000686	2.0	2.0
1325_9000700	2.0	2.0
1325_9000750	2.0	2.0
1365_0100008	1.0	2.0
1365_0100009	1.0	1.0
1365_0100015	1.0	1.0
1365_0100018	1.0	2.0
1365_0100020	2.0	2.0
1365_0100021	2.0	2.0
1365_0100023	1.0	2.0
1365_0100094	2.0	2.0
1365_0100095	2.0	2.0
1365_0100096	2.0	2.0
1365_0100099	1.0	2.0
1365_0100103	2.0	2.0
1365_0100107	2.0	2.0
1365_0100116	2.0	2.0
1365_0100117	2.0	2.0
1365_0100133	2.0	2.0
1365_0100175	2.0	2.0
1365_0100180	1.0	1.0
1365_0100182	2.0	2.0
1365_0100201	2.0	2.0
1365_0100202	1.0	2.0
1365_0100205	2.0	2.0
1365_0100212	2.0	2.0
1365_0100213	2.0	2.0
1365_0100226	2.0	2.0
1365_0100233	2.0	2.0
1365_0100261	2.0	2.0
1365_0100269	2.0	2.0
1365_0100276	2.0	2.0
1365_0100287	2.0	2.0
1365_0100456	2.0	2.0
1365_0100481	2.0	2.0
1385_0000033	1.0	1.0
1385_0000037	1.0	1.0
1385_0000044	1.0	1.0
1385_0000045	1.0	1.0
1385_0000049	1.0	1.0
1385_0000050	1.0	1.0
1385_0000057	1.0	1.0
1385_0000095	1.0	1.0
1385_0000101	1.0	1.0
1385_0000102	1.0	1.0
1385_0000103	1.0	1.0
1385_0000120	0.0	1.0
1385_0000125	1.0	1.0
1385_0000128	1.0	1.0
1385_0000129	1.0	1.0
1385_0001105	1.0	1.0
1385_0001108	1.0	1.0
1385_0001123	1.0	1.0
1385_0001127	1.0	1.0
1385_0001136	1.0	1.0
1385_0001148	1.0	1.0
1385_0001153	2.0	1.0
1385_0001154	1.0	1.0
1385_0001155	1.0	1.0
1385_0001160	1.0	1.0
1385_0001162	1.0	1.0
1385_0001171	1.0	1.0
1385_0001175	1.0	1.0
1385_0001189	1.0	1.0
1385_0001194	1.0	1.0
1385_0001522	1.0	1.0
1385_0001525	1.0	1.0
1385_0001527	1.0	1.0
1385_0001715	1.0	1.0
1385_0001718	1.0	1.0
1385_0001723	0.0	1.0
1385_0001732	1.0	1.0
1385_0001741	0.0	1.0
1385_0001742	0.0	1.0
1385_0001756	1.0	1.0
1385_0001760	1.0	1.0
1385_0001768	1.0	1.0
1385_0001791	1.0	1.0
1385_0001792	1.0	1.0
1385_0001799	1.0	1.0
1395_0000337	0.0	1.0
1395_0000354	1.0	1.0
1395_0000359	1.0	1.0
1395_0000378	1.0	1.0
1395_0000379	1.0	1.0
1395_0000383	1.0	1.0
1395_0000438	2.0	2.0
1395_0000443	2.0	1.0
1395_0000447	1.0	1.0
1395_0000454	1.0	1.0
1395_0000499	1.0	1.0
1395_0000512	1.0	1.0
1395_0000516	1.0	1.0
1395_0000535	1.0	1.0
1395_0000552	1.0	1.0
1395_0000565	1.0	1.0
1395_0000575	1.0	1.0
1395_0000579	1.0	1.0
1395_0000581	1.0	2.0
1395_0000602	1.0	1.0
1395_0000628	1.0	1.0
1395_0000635	1.0	1.0
1395_0000636	1.0	1.0
1395_0000649	2.0	1.0
1395_0001010	2.0	1.0
1395_0001019	1.0	1.0
1395_0001020	1.0	1.0
1395_0001061	2.0	2.0
1395_0001065	1.0	1.0
1395_0001066	1.0	1.0
1395_0001067	1.0	1.0
1395_0001073	2.0	1.0
1395_0001118	1.0	1.0
1395_0001121	0.0	1.0
1395_0001131	1.0	1.0
1395_0001132	2.0	1.0
1395_0001160	2.0	1.0
1395_0001169	1.0	1.0
LANGUAGE: IT, 3th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.70
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.55
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         6
         1.0       0.72      0.93      0.82        76
         2.0       0.92      0.73      0.81        78

    accuracy                           0.80       160
   macro avg       0.55      0.55      0.54       160
weighted avg       0.79      0.80      0.78       160

[[ 0  6  0]
 [ 0 71  5]
 [ 0 21 57]]
0.7846079638752054
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.46
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.49
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         6
         1.0       0.80      0.87      0.83        76
         2.0       0.87      0.86      0.86        78

    accuracy                           0.83       160
   macro avg       0.56      0.58      0.56       160
weighted avg       0.80      0.83      0.82       160

[[ 0  6  0]
 [ 0 66 10]
 [ 0 11 67]]
0.8157912355447353
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.37
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.56
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         6
         1.0       0.78      0.88      0.83        76
         2.0       0.88      0.83      0.86        78

    accuracy                           0.82       160
   macro avg       0.55      0.57      0.56       160
weighted avg       0.80      0.82      0.81       160

[[ 0  6  0]
 [ 0 67  9]
 [ 0 13 65]]
0.8098420240415856
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.33
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.54
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         6
         1.0       0.81      0.84      0.83        76
         2.0       0.85      0.88      0.87        78

    accuracy                           0.83       160
   macro avg       0.55      0.58      0.56       160
weighted avg       0.80      0.83      0.82       160

[[ 0  6  0]
 [ 0 64 12]
 [ 0  9 69]]
0.8153712720632988
160 160 160
Filename	True Label	Prediction
1325_1001010	2.0	2.0
1325_1001012	2.0	2.0
1325_1001015	2.0	2.0
1325_1001019	2.0	2.0
1325_1001029	2.0	2.0
1325_1001032	2.0	2.0
1325_1001035	2.0	2.0
1325_1001048	2.0	2.0
1325_1001053	1.0	2.0
1325_1001056	2.0	2.0
1325_1001082	2.0	2.0
1325_1001094	2.0	2.0
1325_1001096	2.0	2.0
1325_1001120	2.0	2.0
1325_1001135	2.0	2.0
1325_1001152	2.0	2.0
1325_1001159	2.0	2.0
1325_1001160	2.0	2.0
1325_1001167	2.0	2.0
1325_1001169	2.0	2.0
1325_9000099	2.0	2.0
1325_9000104	2.0	2.0
1325_9000105	2.0	2.0
1325_9000188	2.0	2.0
1325_9000211	2.0	2.0
1325_9000214	2.0	2.0
1325_9000237	2.0	2.0
1325_9000241	2.0	2.0
1325_9000278	2.0	2.0
1325_9000316	2.0	2.0
1325_9000319	2.0	2.0
1325_9000504	2.0	2.0
1325_9000534	2.0	2.0
1325_9000536	2.0	2.0
1325_9000602	2.0	2.0
1365_0100010	1.0	1.0
1365_0100011	2.0	2.0
1365_0100013	2.0	2.0
1365_0100016	2.0	2.0
1365_0100056	2.0	2.0
1365_0100061	2.0	2.0
1365_0100063	2.0	2.0
1365_0100066	1.0	2.0
1365_0100072	2.0	2.0
1365_0100097	2.0	2.0
1365_0100100	2.0	2.0
1365_0100101	2.0	2.0
1365_0100102	2.0	2.0
1365_0100104	1.0	2.0
1365_0100106	1.0	2.0
1365_0100118	2.0	2.0
1365_0100134	2.0	2.0
1365_0100136	2.0	2.0
1365_0100145	2.0	2.0
1365_0100166	1.0	2.0
1365_0100170	1.0	2.0
1365_0100174	1.0	2.0
1365_0100177	2.0	2.0
1365_0100181	1.0	2.0
1365_0100185	1.0	2.0
1365_0100186	2.0	2.0
1365_0100191	1.0	2.0
1365_0100194	2.0	2.0
1365_0100220	2.0	2.0
1365_0100225	2.0	2.0
1365_0100251	2.0	2.0
1365_0100252	2.0	2.0
1365_0100259	2.0	2.0
1365_0100262	2.0	2.0
1365_0100263	2.0	2.0
1365_0100266	2.0	2.0
1365_0100281	2.0	2.0
1365_0100288	2.0	2.0
1365_0100290	2.0	2.0
1365_0100471	1.0	2.0
1365_0100480	2.0	2.0
1385_0000011	0.0	1.0
1385_0000034	1.0	1.0
1385_0000035	1.0	1.0
1385_0000041	1.0	1.0
1385_0000053	1.0	1.0
1385_0000054	1.0	1.0
1385_0000098	1.0	1.0
1385_0000114	1.0	1.0
1385_0000122	1.0	1.0
1385_0001109	1.0	1.0
1385_0001113	1.0	1.0
1385_0001131	1.0	1.0
1385_0001134	1.0	1.0
1385_0001138	1.0	1.0
1385_0001147	1.0	1.0
1385_0001166	1.0	1.0
1385_0001172	1.0	1.0
1385_0001192	1.0	1.0
1385_0001501	1.0	1.0
1385_0001524	1.0	1.0
1385_0001717	2.0	1.0
1385_0001719	1.0	1.0
1385_0001728	1.0	1.0
1385_0001734	1.0	1.0
1385_0001740	1.0	1.0
1385_0001744	0.0	1.0
1385_0001750	0.0	1.0
1385_0001752	1.0	1.0
1385_0001753	1.0	1.0
1385_0001761	1.0	1.0
1385_0001762	1.0	1.0
1385_0001771	1.0	1.0
1385_0001788	1.0	1.0
1385_0001794	1.0	1.0
1395_0000340	1.0	1.0
1395_0000357	2.0	2.0
1395_0000361	1.0	1.0
1395_0000366	2.0	1.0
1395_0000368	0.0	1.0
1395_0000369	2.0	1.0
1395_0000380	1.0	1.0
1395_0000399	1.0	1.0
1395_0000403	1.0	1.0
1395_0000404	1.0	1.0
1395_0000446	2.0	2.0
1395_0000449	2.0	1.0
1395_0000465	1.0	1.0
1395_0000504	1.0	1.0
1395_0000513	2.0	1.0
1395_0000514	2.0	2.0
1395_0000518	2.0	1.0
1395_0000549	1.0	1.0
1395_0000550	1.0	2.0
1395_0000556	1.0	1.0
1395_0000582	0.0	1.0
1395_0000583	1.0	1.0
1395_0000584	0.0	1.0
1395_0000596	2.0	1.0
1395_0000604	1.0	1.0
1395_0000609	1.0	1.0
1395_0000626	2.0	2.0
1395_0000631	1.0	1.0
1395_0000639	1.0	1.0
1395_0001015	1.0	1.0
1395_0001017	1.0	1.0
1395_0001022	1.0	1.0
1395_0001023	1.0	1.0
1395_0001033	1.0	1.0
1395_0001034	1.0	1.0
1395_0001040	1.0	1.0
1395_0001058	1.0	1.0
1395_0001070	2.0	2.0
1395_0001075	1.0	1.0
1395_0001076	1.0	1.0
1395_0001080	1.0	1.0
1395_0001084	1.0	1.0
1395_0001104	1.0	1.0
1395_0001114	1.0	1.0
1395_0001116	1.0	1.0
1395_0001123	1.0	1.0
1395_0001126	1.0	1.0
1395_0001141	1.0	1.0
1395_0001158	2.0	1.0
1395_0001164	2.0	1.0
LANGUAGE: IT, 4th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.69
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.53
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         6
         1.0       0.80      0.88      0.84        76
         2.0       0.88      0.86      0.87        78

    accuracy                           0.84       160
   macro avg       0.56      0.58      0.57       160
weighted avg       0.81      0.84      0.82       160

[[ 0  6  0]
 [ 0 67  9]
 [ 0 11 67]]
0.8220008116883116
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.49
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.45
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         6
         1.0       0.85      0.87      0.86        76
         2.0       0.88      0.92      0.90        78

    accuracy                           0.86       160
   macro avg       0.57      0.60      0.59       160
weighted avg       0.83      0.86      0.85       160

[[ 0  6  0]
 [ 0 66 10]
 [ 0  6 72]]
0.8458928571428572
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.40
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.47
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         6
         1.0       0.86      0.83      0.85        76
         2.0       0.85      0.95      0.90        78

    accuracy                           0.86       160
   macro avg       0.57      0.59      0.58       160
weighted avg       0.82      0.86      0.84       160

[[ 0  6  0]
 [ 0 63 13]
 [ 0  4 74]]
0.8389505796217206
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.31
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.49
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         6
         1.0       0.86      0.80      0.83        76
         2.0       0.83      0.95      0.89        78

    accuracy                           0.84       160
   macro avg       0.56      0.58      0.57       160
weighted avg       0.81      0.84      0.83       160

[[ 0  6  0]
 [ 0 61 15]
 [ 0  4 74]]
0.8262536152185426
160 160 160
Filename	True Label	Prediction
1325_1001014	2.0	2.0
1325_1001017	2.0	2.0
1325_1001022	2.0	2.0
1325_1001025	2.0	2.0
1325_1001028	2.0	2.0
1325_1001040	2.0	2.0
1325_1001052	2.0	2.0
1325_1001057	2.0	2.0
1325_1001078	2.0	2.0
1325_1001079	2.0	2.0
1325_1001081	2.0	2.0
1325_1001084	2.0	2.0
1325_1001088	2.0	2.0
1325_1001089	2.0	2.0
1325_1001095	2.0	2.0
1325_1001101	2.0	2.0
1325_1001110	2.0	2.0
1325_1001113	2.0	2.0
1325_1001125	2.0	2.0
1325_1001126	2.0	2.0
1325_1001129	1.0	2.0
1325_1001133	2.0	2.0
1325_1001134	2.0	2.0
1325_1001139	2.0	2.0
1325_1001141	1.0	2.0
1325_1001154	2.0	2.0
1325_1001158	2.0	2.0
1325_1001166	2.0	2.0
1325_9000089	2.0	2.0
1325_9000138	2.0	2.0
1325_9000152	2.0	2.0
1325_9000210	1.0	2.0
1325_9000240	2.0	2.0
1325_9000296	2.0	2.0
1325_9000320	2.0	2.0
1325_9000674	2.0	2.0
1365_0100007	1.0	1.0
1365_0100022	2.0	2.0
1365_0100024	1.0	2.0
1365_0100026	1.0	1.0
1365_0100027	2.0	2.0
1365_0100030	1.0	2.0
1365_0100051	1.0	1.0
1365_0100064	2.0	2.0
1365_0100069	2.0	2.0
1365_0100070	2.0	2.0
1365_0100079	2.0	2.0
1365_0100080	2.0	2.0
1365_0100093	2.0	2.0
1365_0100098	1.0	2.0
1365_0100120	2.0	2.0
1365_0100121	2.0	2.0
1365_0100123	2.0	2.0
1365_0100139	2.0	2.0
1365_0100146	2.0	2.0
1365_0100147	2.0	2.0
1365_0100162	2.0	2.0
1365_0100163	2.0	2.0
1365_0100172	2.0	2.0
1365_0100179	2.0	2.0
1365_0100183	2.0	2.0
1365_0100188	2.0	2.0
1365_0100196	1.0	2.0
1365_0100203	2.0	2.0
1365_0100218	2.0	2.0
1365_0100222	2.0	2.0
1365_0100224	2.0	2.0
1365_0100232	2.0	2.0
1365_0100255	1.0	2.0
1365_0100274	2.0	2.0
1365_0100299	2.0	2.0
1365_0100451	2.0	2.0
1365_0100455	2.0	2.0
1365_0100459	2.0	2.0
1365_0100461	2.0	2.0
1365_0100470	2.0	2.0
1365_0100472	2.0	2.0
1365_0100473	2.0	2.0
1385_0000017	1.0	1.0
1385_0000020	1.0	1.0
1385_0000022	1.0	1.0
1385_0000039	1.0	1.0
1385_0000040	1.0	1.0
1385_0000043	1.0	1.0
1385_0000048	1.0	1.0
1385_0000052	1.0	1.0
1385_0000104	1.0	1.0
1385_0000119	1.0	1.0
1385_0000126	1.0	1.0
1385_0001112	1.0	1.0
1385_0001118	1.0	1.0
1385_0001122	1.0	1.0
1385_0001126	0.0	1.0
1385_0001135	1.0	1.0
1385_0001149	1.0	1.0
1385_0001156	1.0	1.0
1385_0001157	1.0	1.0
1385_0001164	1.0	1.0
1385_0001169	1.0	1.0
1385_0001174	1.0	1.0
1385_0001188	1.0	1.0
1385_0001191	1.0	1.0
1385_0001195	1.0	1.0
1385_0001712	1.0	1.0
1385_0001716	1.0	1.0
1385_0001720	0.0	1.0
1385_0001724	2.0	1.0
1385_0001726	1.0	1.0
1385_0001727	0.0	1.0
1385_0001729	1.0	1.0
1385_0001733	1.0	1.0
1385_0001736	2.0	2.0
1385_0001757	2.0	1.0
1385_0001765	0.0	1.0
1385_0001772	1.0	1.0
1385_0001774	0.0	1.0
1385_0001785	1.0	1.0
1385_0001786	1.0	1.0
1385_0001787	1.0	1.0
1385_0001793	1.0	1.0
1395_0000338	1.0	1.0
1395_0000353	1.0	1.0
1395_0000356	1.0	1.0
1395_0000387	2.0	2.0
1395_0000388	2.0	2.0
1395_0000390	1.0	1.0
1395_0000391	2.0	2.0
1395_0000396	1.0	2.0
1395_0000413	1.0	2.0
1395_0000414	1.0	1.0
1395_0000450	1.0	1.0
1395_0000455	1.0	1.0
1395_0000458	1.0	1.0
1395_0000460	1.0	1.0
1395_0000469	1.0	1.0
1395_0000525	2.0	1.0
1395_0000527	1.0	1.0
1395_0000528	2.0	1.0
1395_0000533	2.0	2.0
1395_0000547	1.0	2.0
1395_0000553	1.0	1.0
1395_0000557	2.0	2.0
1395_0000560	1.0	2.0
1395_0000572	1.0	1.0
1395_0000598	1.0	1.0
1395_0000606	1.0	1.0
1395_0000607	1.0	1.0
1395_0000611	1.0	1.0
1395_0000612	1.0	1.0
1395_0000642	1.0	1.0
1395_0001013	1.0	1.0
1395_0001028	1.0	2.0
1395_0001069	1.0	2.0
1395_0001093	1.0	1.0
1395_0001119	2.0	2.0
1395_0001120	1.0	1.0
1395_0001124	1.0	1.0
1395_0001146	0.0	1.0
1395_0001147	1.0	2.0
1395_0001149	1.0	1.0
Averaged weighted F1-scores 0.8309189982402996
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2']
LANGUAGE: IT, 0th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.23
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.08
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.59      0.21      0.31        48
         2.0       0.50      0.95      0.66        76
         3.0       0.00      0.00      0.00        22

    accuracy                           0.51       160
   macro avg       0.27      0.29      0.24       160
weighted avg       0.42      0.51      0.40       160

[[ 0  3 11  0]
 [ 0 10 38  0]
 [ 0  4 72  0]
 [ 0  0 22  0]]
0.40463645943098003
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.99
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.01
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.58      0.40      0.47        48
         2.0       0.56      0.93      0.70        76
         3.0       0.00      0.00      0.00        22

    accuracy                           0.56       160
   macro avg       0.28      0.33      0.29       160
weighted avg       0.44      0.56      0.47       160

[[ 0  9  5  0]
 [ 0 19 29  0]
 [ 0  5 71  0]
 [ 0  0 22  0]]
0.47300675059295755
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.85
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.10
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.48      0.33      0.40        48
         2.0       0.54      0.79      0.64        76
         3.0       0.40      0.27      0.32        22

    accuracy                           0.51       160
   macro avg       0.36      0.35      0.34       160
weighted avg       0.45      0.51      0.47       160

[[ 0 10  4  0]
 [ 0 16 32  0]
 [ 0  7 60  9]
 [ 0  0 16  6]]
0.46630460247481514
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.72
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.03
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.52      0.48      0.50        48
         2.0       0.60      0.82      0.69        76
         3.0       0.54      0.32      0.40        22

    accuracy                           0.57       160
   macro avg       0.42      0.40      0.40       160
weighted avg       0.52      0.57      0.53       160

[[ 0 13  1  0]
 [ 0 23 25  0]
 [ 0  8 62  6]
 [ 0  0 15  7]]
0.5340502793296089
160 160 160
Filename	True Label	Prediction
1325_1001017	2.0	2.0
1325_1001019	2.0	2.0
1325_1001023	2.0	2.0
1325_1001036	2.0	2.0
1325_1001037	2.0	2.0
1325_1001044	3.0	2.0
1325_1001045	2.0	2.0
1325_1001048	1.0	2.0
1325_1001053	1.0	2.0
1325_1001077	2.0	3.0
1325_1001079	3.0	3.0
1325_1001080	2.0	3.0
1325_1001081	3.0	2.0
1325_1001092	2.0	2.0
1325_1001095	2.0	2.0
1325_1001109	2.0	2.0
1325_1001123	3.0	3.0
1325_1001124	2.0	2.0
1325_1001125	2.0	3.0
1325_1001128	2.0	2.0
1325_1001131	3.0	2.0
1325_1001132	2.0	3.0
1325_1001139	2.0	2.0
1325_1001141	2.0	2.0
1325_1001144	3.0	3.0
1325_9000087	2.0	2.0
1325_9000089	2.0	2.0
1325_9000099	2.0	2.0
1325_9000104	2.0	2.0
1325_9000138	3.0	2.0
1325_9000139	2.0	2.0
1325_9000185	3.0	2.0
1325_9000186	3.0	3.0
1325_9000187	3.0	2.0
1325_9000188	2.0	3.0
1325_9000278	3.0	2.0
1325_9000303	3.0	2.0
1325_9000317	3.0	3.0
1325_9000554	2.0	2.0
1325_9000678	3.0	2.0
1325_9000685	3.0	3.0
1325_9000700	2.0	3.0
1365_0100003	1.0	1.0
1365_0100004	2.0	2.0
1365_0100008	1.0	2.0
1365_0100009	2.0	1.0
1365_0100018	1.0	2.0
1365_0100019	1.0	2.0
1365_0100021	2.0	2.0
1365_0100028	2.0	2.0
1365_0100030	1.0	2.0
1365_0100061	3.0	2.0
1365_0100063	3.0	2.0
1365_0100070	2.0	2.0
1365_0100074	2.0	2.0
1365_0100097	2.0	2.0
1365_0100098	1.0	2.0
1365_0100100	2.0	2.0
1365_0100106	2.0	2.0
1365_0100138	2.0	2.0
1365_0100164	2.0	2.0
1365_0100167	1.0	2.0
1365_0100169	2.0	2.0
1365_0100170	1.0	2.0
1365_0100174	2.0	2.0
1365_0100178	2.0	2.0
1365_0100181	1.0	2.0
1365_0100182	2.0	2.0
1365_0100183	1.0	2.0
1365_0100184	2.0	2.0
1365_0100194	3.0	2.0
1365_0100196	1.0	2.0
1365_0100201	2.0	2.0
1365_0100253	2.0	2.0
1365_0100258	2.0	2.0
1365_0100261	2.0	2.0
1365_0100262	2.0	2.0
1365_0100263	3.0	3.0
1365_0100275	2.0	2.0
1365_0100282	2.0	2.0
1365_0100288	2.0	2.0
1365_0100456	2.0	2.0
1365_0100458	2.0	2.0
1365_0100478	2.0	2.0
1365_0100480	2.0	2.0
1385_0000020	2.0	2.0
1385_0000023	2.0	1.0
1385_0000034	2.0	2.0
1385_0000040	1.0	1.0
1385_0000044	2.0	2.0
1385_0000050	2.0	2.0
1385_0000057	1.0	1.0
1385_0000097	2.0	1.0
1385_0000098	1.0	1.0
1385_0000101	1.0	1.0
1385_0000123	1.0	2.0
1385_0000126	2.0	1.0
1385_0000127	2.0	1.0
1385_0001107	2.0	1.0
1385_0001122	1.0	1.0
1385_0001125	1.0	1.0
1385_0001130	1.0	1.0
1385_0001132	1.0	1.0
1385_0001134	1.0	1.0
1385_0001147	1.0	1.0
1385_0001150	2.0	2.0
1385_0001161	2.0	2.0
1385_0001173	0.0	1.0
1385_0001188	1.0	1.0
1385_0001191	1.0	1.0
1385_0001193	2.0	2.0
1385_0001198	2.0	1.0
1385_0001716	1.0	1.0
1385_0001726	1.0	1.0
1385_0001736	2.0	2.0
1385_0001741	0.0	1.0
1385_0001748	1.0	2.0
1385_0001751	1.0	2.0
1385_0001760	1.0	1.0
1385_0001761	0.0	1.0
1385_0001785	0.0	1.0
1385_0001792	0.0	2.0
1385_0001794	0.0	1.0
1395_0000338	1.0	1.0
1395_0000355	2.0	2.0
1395_0000357	3.0	2.0
1395_0000368	0.0	1.0
1395_0000369	2.0	2.0
1395_0000376	2.0	2.0
1395_0000378	1.0	2.0
1395_0000380	2.0	2.0
1395_0000447	1.0	2.0
1395_0000449	2.0	2.0
1395_0000462	2.0	2.0
1395_0000470	1.0	1.0
1395_0000514	3.0	2.0
1395_0000533	3.0	2.0
1395_0000537	1.0	2.0
1395_0000550	1.0	2.0
1395_0000551	2.0	2.0
1395_0000563	1.0	2.0
1395_0000584	0.0	1.0
1395_0000587	0.0	1.0
1395_0000591	0.0	1.0
1395_0000598	1.0	2.0
1395_0000608	1.0	1.0
1395_0000639	1.0	2.0
1395_0001016	2.0	1.0
1395_0001023	1.0	1.0
1395_0001024	1.0	2.0
1395_0001061	1.0	2.0
1395_0001067	1.0	1.0
1395_0001076	0.0	1.0
1395_0001103	1.0	1.0
1395_0001108	0.0	1.0
1395_0001109	0.0	1.0
1395_0001115	1.0	2.0
1395_0001123	1.0	1.0
1395_0001133	0.0	1.0
1395_0001158	2.0	2.0
LANGUAGE: IT, 1th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.25
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.17
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.00      0.00      0.00        48
         2.0       0.47      1.00      0.64        75
         3.0       0.00      0.00      0.00        23

    accuracy                           0.47       160
   macro avg       0.12      0.25      0.16       160
weighted avg       0.22      0.47      0.30       160

[[ 0  0 14  0]
 [ 0  0 48  0]
 [ 0  0 75  0]
 [ 0  0 23  0]]
0.2992021276595745
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.10
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.95
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.52      0.94      0.67        48
         2.0       0.66      0.64      0.65        75
         3.0       0.00      0.00      0.00        23

    accuracy                           0.58       160
   macro avg       0.29      0.39      0.33       160
weighted avg       0.46      0.58      0.50       160

[[ 0 14  0  0]
 [ 0 45  3  0]
 [ 0 27 48  0]
 [ 0  1 22  0]]
0.504054054054054
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.95
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.88
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.53      0.58      0.55        48
         2.0       0.60      0.85      0.70        75
         3.0       0.00      0.00      0.00        23

    accuracy                           0.57       160
   macro avg       0.28      0.36      0.31       160
weighted avg       0.44      0.57      0.50       160

[[ 0 14  0  0]
 [ 0 28 20  0]
 [ 0 11 64  0]
 [ 0  0 23  0]]
0.49600696333369604
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.89
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.83
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.52      0.77      0.62        48
         2.0       0.62      0.73      0.67        75
         3.0       0.00      0.00      0.00        23

    accuracy                           0.57       160
   macro avg       0.28      0.38      0.32       160
weighted avg       0.45      0.57      0.50       160

[[ 0 14  0  0]
 [ 0 37 11  0]
 [ 0 20 55  0]
 [ 0  0 23  0]]
0.5009601096536175
160 160 160
Filename	True Label	Prediction
1325_1001009	2.0	2.0
1325_1001011	2.0	2.0
1325_1001013	2.0	2.0
1325_1001020	2.0	2.0
1325_1001022	2.0	2.0
1325_1001024	2.0	2.0
1325_1001027	3.0	2.0
1325_1001032	2.0	2.0
1325_1001035	3.0	2.0
1325_1001042	2.0	2.0
1325_1001043	2.0	2.0
1325_1001046	2.0	2.0
1325_1001057	2.0	2.0
1325_1001062	2.0	2.0
1325_1001075	1.0	2.0
1325_1001076	3.0	2.0
1325_1001082	2.0	2.0
1325_1001084	2.0	2.0
1325_1001099	3.0	2.0
1325_1001126	2.0	2.0
1325_1001135	2.0	2.0
1325_1001142	2.0	2.0
1325_1001154	3.0	2.0
1325_1001162	2.0	2.0
1325_1001164	2.0	2.0
1325_1001165	2.0	2.0
1325_9000059	2.0	2.0
1325_9000088	3.0	2.0
1325_9000137	3.0	2.0
1325_9000140	3.0	2.0
1325_9000144	3.0	2.0
1325_9000152	3.0	2.0
1325_9000214	2.0	2.0
1325_9000237	3.0	2.0
1325_9000241	3.0	2.0
1325_9000322	3.0	2.0
1325_9000534	2.0	2.0
1325_9000677	3.0	2.0
1325_9000686	3.0	2.0
1325_9000750	3.0	2.0
1365_0100002	2.0	2.0
1365_0100005	2.0	2.0
1365_0100007	1.0	1.0
1365_0100017	2.0	2.0
1365_0100020	2.0	2.0
1365_0100023	2.0	2.0
1365_0100024	1.0	2.0
1365_0100026	1.0	1.0
1365_0100031	2.0	2.0
1365_0100057	2.0	2.0
1365_0100072	2.0	2.0
1365_0100092	2.0	2.0
1365_0100099	2.0	2.0
1365_0100121	3.0	2.0
1365_0100134	2.0	2.0
1365_0100145	2.0	2.0
1365_0100146	2.0	2.0
1365_0100147	2.0	2.0
1365_0100162	2.0	2.0
1365_0100163	3.0	2.0
1365_0100172	2.0	2.0
1365_0100177	2.0	2.0
1365_0100198	2.0	2.0
1365_0100200	3.0	2.0
1365_0100211	2.0	2.0
1365_0100228	1.0	2.0
1365_0100229	2.0	2.0
1365_0100265	3.0	2.0
1365_0100266	3.0	2.0
1365_0100287	2.0	2.0
1365_0100290	2.0	2.0
1365_0100455	3.0	2.0
1365_0100472	2.0	2.0
1365_0100482	2.0	2.0
1385_0000013	1.0	2.0
1385_0000022	1.0	2.0
1385_0000036	1.0	1.0
1385_0000039	1.0	2.0
1385_0000042	2.0	2.0
1385_0000047	1.0	2.0
1385_0000049	2.0	2.0
1385_0000053	2.0	2.0
1385_0000058	2.0	2.0
1385_0000099	0.0	1.0
1385_0000100	1.0	1.0
1385_0000102	1.0	1.0
1385_0000122	2.0	1.0
1385_0000124	2.0	2.0
1385_0001105	2.0	1.0
1385_0001108	2.0	1.0
1385_0001110	2.0	1.0
1385_0001126	0.0	1.0
1385_0001127	2.0	1.0
1385_0001136	1.0	1.0
1385_0001138	1.0	1.0
1385_0001148	2.0	1.0
1385_0001155	2.0	2.0
1385_0001156	2.0	2.0
1385_0001162	1.0	1.0
1385_0001166	0.0	1.0
1385_0001169	1.0	1.0
1385_0001197	1.0	1.0
1385_0001526	0.0	1.0
1385_0001528	1.0	1.0
1385_0001714	1.0	1.0
1385_0001723	0.0	1.0
1385_0001727	0.0	1.0
1385_0001734	1.0	1.0
1385_0001738	0.0	1.0
1385_0001742	0.0	1.0
1385_0001752	0.0	1.0
1385_0001756	1.0	1.0
1385_0001762	1.0	1.0
1385_0001766	2.0	1.0
1385_0001767	0.0	1.0
1385_0001768	1.0	1.0
1385_0001773	0.0	1.0
1395_0000337	0.0	1.0
1395_0000340	2.0	1.0
1395_0000353	1.0	1.0
1395_0000364	1.0	1.0
1395_0000365	2.0	1.0
1395_0000383	2.0	1.0
1395_0000388	2.0	1.0
1395_0000398	2.0	1.0
1395_0000402	1.0	1.0
1395_0000404	2.0	2.0
1395_0000409	2.0	1.0
1395_0000414	2.0	1.0
1395_0000451	2.0	1.0
1395_0000452	1.0	1.0
1395_0000460	1.0	1.0
1395_0000469	1.0	1.0
1395_0000499	1.0	1.0
1395_0000500	1.0	1.0
1395_0000518	2.0	1.0
1395_0000525	2.0	1.0
1395_0000526	1.0	1.0
1395_0000527	1.0	1.0
1395_0000547	2.0	1.0
1395_0000557	3.0	2.0
1395_0000559	2.0	1.0
1395_0000607	1.0	1.0
1395_0000626	1.0	2.0
1395_0000630	1.0	1.0
1395_0000636	0.0	1.0
1395_0000646	1.0	1.0
1395_0001013	1.0	1.0
1395_0001020	1.0	1.0
1395_0001033	1.0	1.0
1395_0001060	1.0	1.0
1395_0001070	1.0	2.0
1395_0001071	1.0	2.0
1395_0001084	1.0	1.0
1395_0001090	2.0	1.0
1395_0001093	1.0	1.0
1395_0001101	1.0	1.0
1395_0001114	1.0	1.0
1395_0001149	0.0	1.0
1395_0001170	1.0	2.0
LANGUAGE: IT, 2th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.28
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.23
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.00      0.00      0.00        49
         2.0       0.47      1.00      0.64        75
         3.0       0.00      0.00      0.00        22

    accuracy                           0.47       160
   macro avg       0.12      0.25      0.16       160
weighted avg       0.22      0.47      0.30       160

[[ 0  0 14  0]
 [ 0  0 49  0]
 [ 0  0 75  0]
 [ 0  0 22  0]]
0.2992021276595745
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.22
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.10
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.47      0.53      0.50        49
         2.0       0.55      0.77      0.64        75
         3.0       0.00      0.00      0.00        22

    accuracy                           0.53       160
   macro avg       0.26      0.33      0.29       160
weighted avg       0.40      0.53      0.46       160

[[ 0 12  2  0]
 [ 0 26 23  0]
 [ 0 17 58  0]
 [ 0  0 22  0]]
0.4552083333333334
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.09
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.51      0.63      0.56        49
         2.0       0.60      0.79      0.68        75
         3.0       0.00      0.00      0.00        22

    accuracy                           0.56       160
   macro avg       0.28      0.35      0.31       160
weighted avg       0.43      0.56      0.49       160

[[ 0 14  0  0]
 [ 0 31 18  0]
 [ 0 16 59  0]
 [ 0  0 22  0]]
0.49050156739811906
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.00
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.97
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.53      0.63      0.57        49
         2.0       0.60      0.81      0.69        75
         3.0       0.00      0.00      0.00        22

    accuracy                           0.57       160
   macro avg       0.28      0.36      0.32       160
weighted avg       0.44      0.57      0.50       160

[[ 0 14  0  0]
 [ 0 31 18  0]
 [ 0 14 61  0]
 [ 0  0 22  0]]
0.5007391624579125
160 160 160
Filename	True Label	Prediction
1325_1001021	2.0	2.0
1325_1001025	2.0	2.0
1325_1001033	3.0	2.0
1325_1001040	3.0	2.0
1325_1001041	3.0	2.0
1325_1001047	1.0	2.0
1325_1001050	2.0	2.0
1325_1001051	2.0	2.0
1325_1001058	2.0	2.0
1325_1001063	2.0	2.0
1325_1001083	2.0	2.0
1325_1001086	2.0	2.0
1325_1001087	2.0	2.0
1325_1001090	2.0	2.0
1325_1001091	3.0	2.0
1325_1001093	2.0	2.0
1325_1001096	2.0	2.0
1325_1001098	2.0	2.0
1325_1001100	2.0	2.0
1325_1001110	3.0	2.0
1325_1001111	3.0	2.0
1325_1001113	3.0	2.0
1325_1001120	3.0	2.0
1325_1001121	2.0	2.0
1325_1001122	2.0	2.0
1325_1001129	1.0	2.0
1325_1001133	2.0	2.0
1325_1001134	2.0	2.0
1325_1001136	2.0	2.0
1325_1001152	2.0	2.0
1325_1001156	2.0	2.0
1325_1001158	2.0	2.0
1325_1001161	2.0	2.0
1325_1001163	2.0	2.0
1325_1001166	2.0	2.0
1325_1001168	2.0	2.0
1325_9000102	2.0	1.0
1325_9000211	2.0	2.0
1325_9000239	3.0	2.0
1325_9000304	2.0	2.0
1325_9000316	1.0	2.0
1325_9000318	3.0	2.0
1325_9000601	3.0	2.0
1325_9000674	2.0	2.0
1325_9000684	3.0	2.0
1365_0100006	2.0	2.0
1365_0100012	2.0	1.0
1365_0100014	2.0	2.0
1365_0100015	1.0	2.0
1365_0100051	1.0	2.0
1365_0100058	2.0	2.0
1365_0100065	1.0	2.0
1365_0100066	1.0	2.0
1365_0100069	1.0	2.0
1365_0100080	2.0	2.0
1365_0100093	2.0	2.0
1365_0100102	3.0	2.0
1365_0100103	3.0	2.0
1365_0100107	3.0	2.0
1365_0100118	3.0	2.0
1365_0100137	2.0	2.0
1365_0100168	3.0	2.0
1365_0100175	2.0	2.0
1365_0100180	1.0	2.0
1365_0100192	3.0	2.0
1365_0100202	1.0	2.0
1365_0100213	2.0	2.0
1365_0100215	2.0	2.0
1365_0100218	3.0	2.0
1365_0100219	2.0	2.0
1365_0100220	2.0	2.0
1365_0100224	2.0	2.0
1365_0100226	2.0	2.0
1365_0100230	2.0	2.0
1365_0100231	2.0	2.0
1365_0100232	3.0	2.0
1365_0100233	3.0	2.0
1365_0100251	2.0	2.0
1365_0100259	2.0	2.0
1365_0100285	2.0	2.0
1365_0100451	2.0	2.0
1365_0100469	2.0	2.0
1365_0100470	2.0	2.0
1365_0100473	2.0	2.0
1365_0100475	2.0	2.0
1365_0100477	1.0	2.0
1385_0000011	0.0	1.0
1385_0000035	2.0	1.0
1385_0000038	1.0	2.0
1385_0000043	2.0	2.0
1385_0000045	2.0	2.0
1385_0000048	2.0	2.0
1385_0000095	1.0	1.0
1385_0000103	1.0	1.0
1385_0000119	2.0	2.0
1385_0000120	0.0	1.0
1385_0000130	2.0	1.0
1385_0001111	2.0	1.0
1385_0001113	1.0	1.0
1385_0001121	1.0	1.0
1385_0001131	1.0	1.0
1385_0001137	1.0	1.0
1385_0001149	2.0	2.0
1385_0001152	2.0	2.0
1385_0001159	1.0	2.0
1385_0001165	1.0	1.0
1385_0001170	1.0	1.0
1385_0001171	1.0	1.0
1385_0001190	0.0	1.0
1385_0001192	1.0	1.0
1385_0001194	1.0	1.0
1385_0001524	0.0	1.0
1385_0001527	1.0	1.0
1385_0001715	1.0	1.0
1385_0001717	2.0	1.0
1385_0001719	0.0	1.0
1385_0001725	1.0	1.0
1385_0001746	0.0	1.0
1385_0001749	1.0	1.0
1385_0001750	0.0	1.0
1385_0001775	0.0	1.0
1385_0001795	1.0	1.0
1385_0001799	1.0	2.0
1385_0001800	1.0	2.0
1395_0000333	2.0	1.0
1395_0000356	1.0	1.0
1395_0000389	0.0	1.0
1395_0000396	2.0	1.0
1395_0000403	2.0	1.0
1395_0000415	1.0	1.0
1395_0000438	3.0	2.0
1395_0000450	1.0	1.0
1395_0000516	1.0	1.0
1395_0000529	2.0	1.0
1395_0000535	1.0	1.0
1395_0000565	1.0	1.0
1395_0000579	1.0	1.0
1395_0000585	1.0	1.0
1395_0000595	0.0	1.0
1395_0000599	1.0	1.0
1395_0000606	1.0	1.0
1395_0000610	2.0	1.0
1395_0000611	1.0	1.0
1395_0000628	0.0	1.0
1395_0000649	2.0	2.0
1395_0001017	1.0	2.0
1395_0001021	1.0	1.0
1395_0001022	1.0	1.0
1395_0001040	0.0	1.0
1395_0001045	2.0	1.0
1395_0001073	2.0	1.0
1395_0001078	0.0	1.0
1395_0001116	2.0	1.0
1395_0001117	1.0	1.0
1395_0001119	1.0	2.0
1395_0001120	1.0	1.0
1395_0001124	0.0	1.0
1395_0001145	2.0	2.0
1395_0001161	1.0	2.0
1395_0001164	2.0	2.0
LANGUAGE: IT, 3th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.23
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.11
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.50      0.27      0.35        49
         2.0       0.51      0.92      0.66        75
         3.0       0.00      0.00      0.00        22

    accuracy                           0.51       160
   macro avg       0.25      0.30      0.25       160
weighted avg       0.39      0.51      0.42       160

[[ 0  7  7  0]
 [ 0 13 36  0]
 [ 0  6 69  0]
 [ 0  0 22  0]]
0.4156762360446571
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.05
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.05
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.48      0.33      0.39        49
         2.0       0.54      0.92      0.68        75
         3.0       0.00      0.00      0.00        22

    accuracy                           0.53       160
   macro avg       0.26      0.31      0.27       160
weighted avg       0.40      0.53      0.44       160

[[ 0 11  3  0]
 [ 0 16 33  0]
 [ 0  6 69  0]
 [ 0  0 22  0]]
0.4397473436368028
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.94
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.92
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.51      0.86      0.64        49
         2.0       0.65      0.68      0.67        75
         3.0       0.00      0.00      0.00        22

    accuracy                           0.58       160
   macro avg       0.29      0.38      0.33       160
weighted avg       0.46      0.58      0.51       160

[[ 0 14  0  0]
 [ 0 42  7  0]
 [ 0 24 51  0]
 [ 0  2 20  0]]
0.5088740458015267
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.87
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.91
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.52      0.57      0.54        49
         2.0       0.59      0.84      0.70        75
         3.0       0.00      0.00      0.00        22

    accuracy                           0.57       160
   macro avg       0.28      0.35      0.31       160
weighted avg       0.44      0.57      0.49       160

[[ 0 13  1  0]
 [ 0 28 21  0]
 [ 0 12 63  0]
 [ 0  1 21  0]]
0.4928170090650646
160 160 160
Filename	True Label	Prediction
1325_1001010	2.0	2.0
1325_1001014	3.0	2.0
1325_1001018	2.0	2.0
1325_1001028	2.0	2.0
1325_1001039	3.0	2.0
1325_1001052	2.0	2.0
1325_1001054	2.0	2.0
1325_1001088	2.0	2.0
1325_1001094	2.0	2.0
1325_1001097	1.0	2.0
1325_1001101	3.0	2.0
1325_1001108	3.0	2.0
1325_1001130	2.0	2.0
1325_1001138	2.0	2.0
1325_1001157	2.0	2.0
1325_1001159	3.0	2.0
1325_1001160	2.0	2.0
1325_1001167	3.0	2.0
1325_1001169	2.0	2.0
1325_1001170	3.0	2.0
1325_9000107	2.0	2.0
1325_9000215	3.0	2.0
1325_9000240	2.0	2.0
1325_9000314	2.0	2.0
1325_9000319	2.0	2.0
1325_9000323	2.0	2.0
1325_9000504	3.0	2.0
1325_9000533	3.0	2.0
1325_9000536	2.0	2.0
1325_9000602	3.0	2.0
1325_9000676	3.0	2.0
1365_0100011	1.0	2.0
1365_0100027	2.0	2.0
1365_0100029	1.0	2.0
1365_0100056	2.0	2.0
1365_0100064	2.0	2.0
1365_0100079	2.0	2.0
1365_0100094	2.0	2.0
1365_0100101	3.0	2.0
1365_0100104	2.0	2.0
1365_0100105	3.0	2.0
1365_0100123	2.0	2.0
1365_0100125	3.0	2.0
1365_0100133	2.0	2.0
1365_0100135	2.0	2.0
1365_0100148	2.0	2.0
1365_0100166	2.0	2.0
1365_0100171	2.0	2.0
1365_0100173	2.0	2.0
1365_0100179	2.0	2.0
1365_0100185	1.0	2.0
1365_0100186	2.0	2.0
1365_0100187	2.0	2.0
1365_0100188	2.0	2.0
1365_0100191	2.0	2.0
1365_0100195	1.0	2.0
1365_0100199	2.0	2.0
1365_0100203	2.0	2.0
1365_0100205	3.0	2.0
1365_0100212	3.0	2.0
1365_0100221	2.0	2.0
1365_0100225	2.0	2.0
1365_0100227	3.0	2.0
1365_0100252	2.0	2.0
1365_0100255	2.0	2.0
1365_0100267	2.0	2.0
1365_0100270	2.0	2.0
1365_0100276	3.0	2.0
1365_0100280	1.0	2.0
1365_0100281	1.0	2.0
1365_0100286	1.0	2.0
1365_0100457	2.0	2.0
1365_0100474	2.0	2.0
1365_0100476	2.0	2.0
1385_0000017	1.0	1.0
1385_0000021	2.0	2.0
1385_0000033	1.0	2.0
1385_0000037	1.0	1.0
1385_0000054	2.0	2.0
1385_0000114	2.0	2.0
1385_0000125	1.0	2.0
1385_0000128	1.0	1.0
1385_0001103	2.0	1.0
1385_0001109	2.0	1.0
1385_0001112	2.0	1.0
1385_0001123	2.0	1.0
1385_0001133	2.0	1.0
1385_0001135	1.0	1.0
1385_0001153	3.0	2.0
1385_0001154	2.0	2.0
1385_0001157	1.0	2.0
1385_0001160	3.0	2.0
1385_0001163	2.0	1.0
1385_0001164	1.0	1.0
1385_0001172	0.0	1.0
1385_0001175	0.0	1.0
1385_0001178	0.0	1.0
1385_0001189	0.0	1.0
1385_0001196	1.0	1.0
1385_0001199	1.0	1.0
1385_0001503	1.0	2.0
1385_0001523	1.0	1.0
1385_0001718	0.0	1.0
1385_0001728	1.0	2.0
1385_0001729	1.0	1.0
1385_0001730	1.0	1.0
1385_0001733	2.0	2.0
1385_0001737	2.0	2.0
1385_0001744	0.0	1.0
1385_0001753	1.0	1.0
1385_0001754	1.0	1.0
1385_0001758	1.0	1.0
1385_0001764	0.0	1.0
1385_0001786	1.0	2.0
1385_0001787	0.0	2.0
1385_0001788	1.0	2.0
1385_0001791	0.0	1.0
1395_0000360	3.0	1.0
1395_0000392	2.0	2.0
1395_0000399	1.0	1.0
1395_0000413	2.0	2.0
1395_0000446	2.0	2.0
1395_0000448	1.0	1.0
1395_0000454	2.0	1.0
1395_0000455	1.0	1.0
1395_0000458	2.0	1.0
1395_0000528	2.0	1.0
1395_0000531	2.0	2.0
1395_0000534	2.0	2.0
1395_0000548	2.0	2.0
1395_0000549	2.0	2.0
1395_0000552	2.0	2.0
1395_0000555	1.0	1.0
1395_0000556	1.0	1.0
1395_0000560	2.0	2.0
1395_0000564	1.0	1.0
1395_0000581	1.0	2.0
1395_0000582	0.0	1.0
1395_0000583	1.0	2.0
1395_0000596	2.0	1.0
1395_0000604	1.0	1.0
1395_0000609	0.0	1.0
1395_0000612	1.0	1.0
1395_0000627	1.0	1.0
1395_0000642	0.0	1.0
1395_0001010	1.0	1.0
1395_0001028	1.0	2.0
1395_0001034	1.0	1.0
1395_0001064	2.0	2.0
1395_0001065	1.0	2.0
1395_0001069	2.0	1.0
1395_0001074	1.0	2.0
1395_0001080	1.0	1.0
1395_0001104	0.0	1.0
1395_0001118	0.0	1.0
1395_0001126	1.0	1.0
1395_0001141	1.0	1.0
1395_0001147	1.0	2.0
1395_0001150	1.0	1.0
1395_0001160	2.0	1.0
LANGUAGE: IT, 4th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.24
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.15
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        15
         1.0       0.29      0.04      0.07        48
         2.0       0.48      0.97      0.64        75
         3.0       0.00      0.00      0.00        22

    accuracy                           0.47       160
   macro avg       0.19      0.25      0.18       160
weighted avg       0.31      0.47      0.32       160

[[ 0  3 12  0]
 [ 0  2 46  0]
 [ 0  2 73  0]
 [ 0  0 22  0]]
0.32198265550239236
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.05
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.96
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        15
         1.0       0.54      0.90      0.67        48
         2.0       0.68      0.72      0.70        75
         3.0       0.00      0.00      0.00        22

    accuracy                           0.61       160
   macro avg       0.30      0.40      0.34       160
weighted avg       0.48      0.61      0.53       160

[[ 0 15  0  0]
 [ 0 43  5  0]
 [ 0 21 54  0]
 [ 0  1 21  0]]
0.5281754032258064
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.95
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.92
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        15
         1.0       0.57      0.56      0.57        48
         2.0       0.61      0.92      0.73        75
         3.0       0.00      0.00      0.00        22

    accuracy                           0.60       160
   macro avg       0.30      0.37      0.33       160
weighted avg       0.46      0.60      0.51       160

[[ 0 14  1  0]
 [ 0 27 21  0]
 [ 0  6 69  0]
 [ 0  0 22  0]]
0.5146087625979844
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.82
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.91
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        15
         1.0       0.57      0.60      0.59        48
         2.0       0.62      0.91      0.74        75
         3.0       0.00      0.00      0.00        22

    accuracy                           0.61       160
   macro avg       0.30      0.38      0.33       160
weighted avg       0.46      0.61      0.52       160

[[ 0 15  0  0]
 [ 0 29 19  0]
 [ 0  7 68  0]
 [ 0  0 22  0]]
0.5222249670619236
160 160 160
Filename	True Label	Prediction
1325_1001008	2.0	2.0
1325_1001012	2.0	2.0
1325_1001015	2.0	2.0
1325_1001016	2.0	2.0
1325_1001029	2.0	2.0
1325_1001055	2.0	2.0
1325_1001056	2.0	2.0
1325_1001059	2.0	2.0
1325_1001078	2.0	2.0
1325_1001085	2.0	2.0
1325_1001089	2.0	2.0
1325_1001107	3.0	2.0
1325_1001119	3.0	2.0
1325_1001127	3.0	2.0
1325_1001143	2.0	2.0
1325_1001153	2.0	2.0
1325_1001155	2.0	2.0
1325_9000090	2.0	2.0
1325_9000095	2.0	2.0
1325_9000105	1.0	2.0
1325_9000106	2.0	2.0
1325_9000136	2.0	2.0
1325_9000143	3.0	2.0
1325_9000209	2.0	2.0
1325_9000210	2.0	2.0
1325_9000213	3.0	2.0
1325_9000279	3.0	2.0
1325_9000296	2.0	2.0
1325_9000302	2.0	2.0
1325_9000315	2.0	2.0
1325_9000320	3.0	2.0
1325_9000321	3.0	2.0
1325_9000503	3.0	2.0
1325_9000505	2.0	2.0
1325_9000611	2.0	2.0
1325_9000612	2.0	2.0
1325_9000675	3.0	2.0
1365_0100010	1.0	1.0
1365_0100013	3.0	2.0
1365_0100016	2.0	2.0
1365_0100022	2.0	2.0
1365_0100067	2.0	2.0
1365_0100071	2.0	2.0
1365_0100073	2.0	2.0
1365_0100095	2.0	2.0
1365_0100096	2.0	2.0
1365_0100116	2.0	2.0
1365_0100117	3.0	2.0
1365_0100119	3.0	2.0
1365_0100120	3.0	2.0
1365_0100136	2.0	2.0
1365_0100139	2.0	2.0
1365_0100151	2.0	2.0
1365_0100165	3.0	2.0
1365_0100176	2.0	2.0
1365_0100190	2.0	2.0
1365_0100204	2.0	2.0
1365_0100217	3.0	2.0
1365_0100222	1.0	2.0
1365_0100223	2.0	2.0
1365_0100256	2.0	2.0
1365_0100257	2.0	2.0
1365_0100260	2.0	2.0
1365_0100268	2.0	2.0
1365_0100269	2.0	2.0
1365_0100274	2.0	2.0
1365_0100277	3.0	2.0
1365_0100278	2.0	2.0
1365_0100279	1.0	2.0
1365_0100289	2.0	2.0
1365_0100299	3.0	2.0
1365_0100447	3.0	2.0
1365_0100448	1.0	2.0
1365_0100459	2.0	2.0
1365_0100461	2.0	2.0
1365_0100471	2.0	2.0
1365_0100479	3.0	2.0
1365_0100481	2.0	2.0
1385_0000012	2.0	2.0
1385_0000016	2.0	1.0
1385_0000041	2.0	2.0
1385_0000051	2.0	2.0
1385_0000052	1.0	2.0
1385_0000059	2.0	2.0
1385_0000104	2.0	1.0
1385_0000129	2.0	1.0
1385_0001104	1.0	1.0
1385_0001118	1.0	1.0
1385_0001119	1.0	1.0
1385_0001120	1.0	1.0
1385_0001124	1.0	1.0
1385_0001128	0.0	1.0
1385_0001129	1.0	1.0
1385_0001151	2.0	2.0
1385_0001158	1.0	2.0
1385_0001167	1.0	1.0
1385_0001174	0.0	1.0
1385_0001195	1.0	2.0
1385_0001501	1.0	2.0
1385_0001522	0.0	1.0
1385_0001525	1.0	1.0
1385_0001712	1.0	2.0
1385_0001720	0.0	1.0
1385_0001724	1.0	1.0
1385_0001732	1.0	2.0
1385_0001739	0.0	1.0
1385_0001740	1.0	1.0
1385_0001747	1.0	1.0
1385_0001757	2.0	1.0
1385_0001759	0.0	1.0
1385_0001765	0.0	1.0
1385_0001771	0.0	1.0
1385_0001772	0.0	1.0
1385_0001774	0.0	1.0
1385_0001789	1.0	2.0
1385_0001790	2.0	2.0
1385_0001793	1.0	2.0
1385_0001796	2.0	2.0
1385_0001798	1.0	2.0
1395_0000341	1.0	1.0
1395_0000354	1.0	1.0
1395_0000359	2.0	2.0
1395_0000361	2.0	2.0
1395_0000366	2.0	2.0
1395_0000379	1.0	1.0
1395_0000387	3.0	2.0
1395_0000390	1.0	1.0
1395_0000391	3.0	2.0
1395_0000432	2.0	1.0
1395_0000443	2.0	2.0
1395_0000465	1.0	1.0
1395_0000471	1.0	1.0
1395_0000504	1.0	1.0
1395_0000512	2.0	2.0
1395_0000513	2.0	2.0
1395_0000515	2.0	1.0
1395_0000553	2.0	1.0
1395_0000554	2.0	2.0
1395_0000572	1.0	1.0
1395_0000575	1.0	1.0
1395_0000593	1.0	1.0
1395_0000597	1.0	1.0
1395_0000602	1.0	1.0
1395_0000631	1.0	2.0
1395_0000635	0.0	1.0
1395_0000644	1.0	2.0
1395_0001015	1.0	1.0
1395_0001019	0.0	1.0
1395_0001058	1.0	1.0
1395_0001066	1.0	2.0
1395_0001068	1.0	2.0
1395_0001075	1.0	1.0
1395_0001121	0.0	1.0
1395_0001122	1.0	1.0
1395_0001131	0.0	1.0
1395_0001132	1.0	2.0
1395_0001146	0.0	1.0
1395_0001167	1.0	2.0
1395_0001169	2.0	2.0
1395_0001171	1.0	1.0
Averaged weighted F1-scores 0.5101583055136254
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
LANGUAGE: IT, 0th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.60
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.36
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.00      0.00      0.00        29
         2.0       0.48      0.70      0.57        44
         3.0       0.00      0.00      0.00        15
         4.0       0.51      0.92      0.65        52
         5.0       0.00      0.00      0.00        16

    accuracy                           0.49       160
   macro avg       0.16      0.27      0.20       160
weighted avg       0.30      0.49      0.37       160

[[ 0  0  4  0  0  0]
 [ 0  0 24  0  5  0]
 [ 0  0 31  0 13  0]
 [ 0  0  2  0 13  0]
 [ 0  0  4  0 48  0]
 [ 0  0  0  0 16  0]]
0.36866691630780757
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.28
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.21
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.00      0.00      0.00        29
         2.0       0.47      0.84      0.60        44
         3.0       0.00      0.00      0.00        15
         4.0       0.58      0.90      0.71        52
         5.0       0.00      0.00      0.00        16

    accuracy                           0.53       160
   macro avg       0.17      0.29      0.22       160
weighted avg       0.32      0.53      0.40       160

[[ 0  0  4  0  0  0]
 [ 0  0 28  0  1  0]
 [ 0  0 37  0  7  0]
 [ 0  0  3  0 12  0]
 [ 0  0  5  0 47  0]
 [ 0  0  2  0 14  0]]
0.39514640259184547
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.14
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.25
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.00      0.00      0.00        29
         2.0       0.47      0.84      0.61        44
         3.0       0.00      0.00      0.00        15
         4.0       0.57      0.90      0.70        52
         5.0       0.00      0.00      0.00        16

    accuracy                           0.53       160
   macro avg       0.17      0.29      0.22       160
weighted avg       0.32      0.53      0.39       160

[[ 0  0  4  0  0  0]
 [ 0  0 27  0  2  0]
 [ 0  0 37  0  7  0]
 [ 0  0  3  0 12  0]
 [ 0  0  5  0 47  0]
 [ 0  0  2  0 14  0]]
0.3947883533153903
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.08
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.23
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.00      0.00      0.00        29
         2.0       0.49      0.82      0.61        44
         3.0       0.00      0.00      0.00        15
         4.0       0.57      0.94      0.71        52
         5.0       0.00      0.00      0.00        16

    accuracy                           0.53       160
   macro avg       0.18      0.29      0.22       160
weighted avg       0.32      0.53      0.40       160

[[ 0  0  4  0  0  0]
 [ 0  0 27  0  2  0]
 [ 0  0 36  0  8  0]
 [ 0  0  3  0 12  0]
 [ 0  0  3  0 49  0]
 [ 0  0  1  0 15  0]]
0.3985937116187669
160 160 160
Filename	True Label	Prediction
1325_1001012	4.0	4.0
1325_1001019	4.0	4.0
1325_1001021	5.0	4.0
1325_1001027	4.0	4.0
1325_1001028	4.0	4.0
1325_1001032	4.0	4.0
1325_1001047	4.0	4.0
1325_1001052	5.0	4.0
1325_1001076	5.0	4.0
1325_1001077	4.0	4.0
1325_1001087	4.0	4.0
1325_1001089	3.0	4.0
1325_1001108	4.0	4.0
1325_1001111	4.0	4.0
1325_1001125	4.0	4.0
1325_1001134	4.0	4.0
1325_1001136	3.0	4.0
1325_1001143	4.0	4.0
1325_1001160	4.0	4.0
1325_1001163	4.0	4.0
1325_1001169	4.0	4.0
1325_9000105	4.0	4.0
1325_9000186	4.0	4.0
1325_9000187	4.0	4.0
1325_9000209	4.0	4.0
1325_9000278	3.0	4.0
1325_9000317	4.0	4.0
1325_9000319	4.0	4.0
1325_9000322	4.0	4.0
1325_9000602	4.0	4.0
1325_9000686	3.0	4.0
1365_0100005	3.0	4.0
1365_0100007	2.0	4.0
1365_0100015	4.0	4.0
1365_0100016	4.0	4.0
1365_0100022	4.0	4.0
1365_0100026	2.0	4.0
1365_0100029	1.0	4.0
1365_0100031	5.0	4.0
1365_0100056	4.0	4.0
1365_0100069	4.0	4.0
1365_0100072	4.0	4.0
1365_0100073	3.0	4.0
1365_0100092	4.0	4.0
1365_0100104	3.0	4.0
1365_0100105	5.0	4.0
1365_0100107	4.0	4.0
1365_0100119	5.0	4.0
1365_0100120	5.0	4.0
1365_0100135	4.0	4.0
1365_0100136	4.0	4.0
1365_0100137	5.0	4.0
1365_0100147	4.0	4.0
1365_0100163	5.0	4.0
1365_0100165	4.0	4.0
1365_0100167	2.0	4.0
1365_0100168	4.0	4.0
1365_0100171	4.0	4.0
1365_0100180	5.0	4.0
1365_0100182	4.0	4.0
1365_0100185	3.0	4.0
1365_0100186	5.0	4.0
1365_0100188	5.0	4.0
1365_0100190	4.0	4.0
1365_0100202	3.0	4.0
1365_0100205	5.0	4.0
1365_0100211	4.0	4.0
1365_0100224	3.0	4.0
1365_0100230	4.0	4.0
1365_0100231	4.0	4.0
1365_0100256	4.0	4.0
1365_0100257	3.0	4.0
1365_0100258	5.0	4.0
1365_0100259	4.0	4.0
1365_0100262	4.0	4.0
1365_0100270	3.0	4.0
1365_0100280	2.0	4.0
1365_0100281	5.0	4.0
1365_0100290	4.0	4.0
1365_0100451	2.0	4.0
1365_0100456	2.0	4.0
1365_0100457	2.0	4.0
1365_0100476	4.0	4.0
1385_0000011	1.0	2.0
1385_0000012	2.0	2.0
1385_0000016	1.0	2.0
1385_0000033	1.0	2.0
1385_0000037	2.0	2.0
1385_0000043	2.0	2.0
1385_0000049	2.0	2.0
1385_0000050	2.0	2.0
1385_0000051	2.0	2.0
1385_0000057	1.0	2.0
1385_0000058	2.0	2.0
1385_0000104	2.0	2.0
1385_0000123	2.0	2.0
1385_0000124	2.0	2.0
1385_0001103	2.0	2.0
1385_0001105	2.0	2.0
1385_0001129	1.0	2.0
1385_0001131	2.0	2.0
1385_0001136	2.0	2.0
1385_0001138	2.0	2.0
1385_0001159	1.0	2.0
1385_0001167	2.0	2.0
1385_0001169	2.0	2.0
1385_0001192	1.0	2.0
1385_0001193	2.0	2.0
1385_0001197	2.0	2.0
1385_0001524	1.0	2.0
1385_0001726	1.0	2.0
1385_0001728	2.0	2.0
1385_0001733	1.0	2.0
1385_0001737	2.0	2.0
1385_0001739	1.0	2.0
1385_0001750	0.0	2.0
1385_0001766	2.0	2.0
1385_0001775	2.0	2.0
1385_0001785	1.0	2.0
1385_0001786	2.0	2.0
1385_0001788	1.0	2.0
1385_0001790	2.0	2.0
1385_0001791	1.0	2.0
1385_0001796	1.0	2.0
1395_0000360	3.0	2.0
1395_0000378	2.0	2.0
1395_0000390	1.0	2.0
1395_0000403	5.0	2.0
1395_0000414	2.0	2.0
1395_0000452	2.0	2.0
1395_0000470	2.0	2.0
1395_0000514	4.0	4.0
1395_0000515	4.0	2.0
1395_0000518	4.0	2.0
1395_0000535	2.0	2.0
1395_0000552	3.0	2.0
1395_0000556	2.0	2.0
1395_0000557	4.0	2.0
1395_0000575	2.0	2.0
1395_0000581	1.0	4.0
1395_0000582	0.0	2.0
1395_0000584	0.0	2.0
1395_0000596	3.0	2.0
1395_0000599	1.0	2.0
1395_0000627	1.0	2.0
1395_0000628	1.0	2.0
1395_0000630	1.0	2.0
1395_0001013	2.0	2.0
1395_0001022	2.0	2.0
1395_0001040	1.0	2.0
1395_0001064	2.0	2.0
1395_0001066	1.0	2.0
1395_0001074	1.0	2.0
1395_0001076	1.0	2.0
1395_0001084	2.0	2.0
1395_0001109	0.0	2.0
1395_0001119	2.0	4.0
1395_0001120	1.0	2.0
1395_0001126	1.0	2.0
1395_0001150	1.0	2.0
LANGUAGE: IT, 1th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.58
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.41
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.00      0.00      0.00        29
         2.0       0.45      0.91      0.61        44
         3.0       0.00      0.00      0.00        15
         4.0       0.64      0.87      0.74        53
         5.0       0.00      0.00      0.00        15

    accuracy                           0.54       160
   macro avg       0.18      0.30      0.22       160
weighted avg       0.34      0.54      0.41       160

[[ 0  0  4  0  0  0]
 [ 0  0 28  0  1  0]
 [ 0  0 40  0  4  0]
 [ 0  0  4  0 11  0]
 [ 0  0  7  0 46  0]
 [ 0  0  5  0 10  0]]
0.41046666666666665
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.23
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.22
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.00      0.00      0.00        29
         2.0       0.50      0.91      0.65        44
         3.0       0.00      0.00      0.00        15
         4.0       0.60      0.91      0.72        53
         5.0       0.00      0.00      0.00        15

    accuracy                           0.55       160
   macro avg       0.18      0.30      0.23       160
weighted avg       0.34      0.55      0.42       160

[[ 0  0  4  0  0  0]
 [ 0  0 26  0  3  0]
 [ 0  0 40  0  4  0]
 [ 0  0  1  0 14  0]
 [ 0  0  5  0 48  0]
 [ 0  0  4  0 11  0]]
0.416517099199612
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.11
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.19
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.00      0.00      0.00        29
         2.0       0.51      0.91      0.65        44
         3.0       0.00      0.00      0.00        15
         4.0       0.59      0.91      0.72        53
         5.0       0.00      0.00      0.00        15

    accuracy                           0.55       160
   macro avg       0.18      0.30      0.23       160
weighted avg       0.34      0.55      0.42       160

[[ 0  0  4  0  0  0]
 [ 0  0 26  0  3  0]
 [ 0  0 40  0  4  0]
 [ 0  0  1  0 14  0]
 [ 0  0  5  0 48  0]
 [ 0  0  3  0 12  0]]
0.41617522145370706
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.06
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.19
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.00      0.00      0.00        29
         2.0       0.50      0.89      0.64        44
         3.0       0.00      0.00      0.00        15
         4.0       0.59      0.91      0.71        53
         5.0       0.00      0.00      0.00        15

    accuracy                           0.54       160
   macro avg       0.18      0.30      0.23       160
weighted avg       0.33      0.54      0.41       160

[[ 0  0  4  0  0  0]
 [ 0  0 26  0  3  0]
 [ 0  0 39  0  5  0]
 [ 0  0  1  0 14  0]
 [ 0  0  5  0 48  0]
 [ 0  0  3  0 12  0]]
0.41137522768670304
160 160 160
Filename	True Label	Prediction
1325_1001010	5.0	4.0
1325_1001011	4.0	4.0
1325_1001014	4.0	4.0
1325_1001015	4.0	4.0
1325_1001029	4.0	4.0
1325_1001042	5.0	4.0
1325_1001050	4.0	4.0
1325_1001053	5.0	4.0
1325_1001056	4.0	4.0
1325_1001059	4.0	4.0
1325_1001062	4.0	4.0
1325_1001075	3.0	4.0
1325_1001086	4.0	4.0
1325_1001088	4.0	4.0
1325_1001090	4.0	4.0
1325_1001091	4.0	4.0
1325_1001095	5.0	4.0
1325_1001097	1.0	4.0
1325_1001098	4.0	4.0
1325_1001099	4.0	4.0
1325_1001100	4.0	4.0
1325_1001101	4.0	4.0
1325_1001120	5.0	4.0
1325_1001121	4.0	4.0
1325_1001128	4.0	4.0
1325_1001131	3.0	4.0
1325_1001133	5.0	4.0
1325_1001138	4.0	4.0
1325_1001152	4.0	4.0
1325_1001153	4.0	4.0
1325_1001154	4.0	4.0
1325_1001162	4.0	4.0
1325_9000089	4.0	4.0
1325_9000099	5.0	4.0
1325_9000152	4.0	4.0
1325_9000211	4.0	4.0
1325_9000240	2.0	4.0
1325_9000241	4.0	4.0
1325_9000302	3.0	4.0
1325_9000303	4.0	4.0
1325_9000315	3.0	4.0
1325_9000316	3.0	4.0
1325_9000323	4.0	4.0
1325_9000536	4.0	4.0
1325_9000611	3.0	4.0
1325_9000674	4.0	4.0
1365_0100010	1.0	4.0
1365_0100058	4.0	4.0
1365_0100066	3.0	4.0
1365_0100099	3.0	4.0
1365_0100100	4.0	4.0
1365_0100106	3.0	4.0
1365_0100121	4.0	4.0
1365_0100125	4.0	4.0
1365_0100133	5.0	4.0
1365_0100145	5.0	4.0
1365_0100146	4.0	4.0
1365_0100148	4.0	4.0
1365_0100162	4.0	4.0
1365_0100166	3.0	4.0
1365_0100179	4.0	4.0
1365_0100181	3.0	4.0
1365_0100184	4.0	4.0
1365_0100194	4.0	4.0
1365_0100203	4.0	4.0
1365_0100221	3.0	4.0
1365_0100222	3.0	4.0
1365_0100223	4.0	4.0
1365_0100227	4.0	4.0
1365_0100228	3.0	4.0
1365_0100261	5.0	4.0
1365_0100282	5.0	4.0
1365_0100455	2.0	4.0
1365_0100473	4.0	4.0
1365_0100474	4.0	4.0
1365_0100478	4.0	4.0
1365_0100481	4.0	4.0
1385_0000021	1.0	2.0
1385_0000022	0.0	2.0
1385_0000038	2.0	2.0
1385_0000045	2.0	2.0
1385_0000053	1.0	2.0
1385_0000054	2.0	2.0
1385_0000098	2.0	2.0
1385_0000129	2.0	2.0
1385_0001109	2.0	2.0
1385_0001112	2.0	2.0
1385_0001118	2.0	2.0
1385_0001135	2.0	2.0
1385_0001137	2.0	2.0
1385_0001149	2.0	2.0
1385_0001151	2.0	2.0
1385_0001156	2.0	2.0
1385_0001170	1.0	2.0
1385_0001172	1.0	2.0
1385_0001174	1.0	2.0
1385_0001188	2.0	2.0
1385_0001190	1.0	2.0
1385_0001196	2.0	2.0
1385_0001522	1.0	2.0
1385_0001738	0.0	2.0
1385_0001748	1.0	2.0
1385_0001752	1.0	2.0
1385_0001756	2.0	2.0
1385_0001761	1.0	2.0
1385_0001767	1.0	2.0
1385_0001771	1.0	2.0
1385_0001772	1.0	2.0
1385_0001792	1.0	2.0
1385_0001795	1.0	2.0
1385_0001798	1.0	2.0
1395_0000337	1.0	2.0
1395_0000340	2.0	2.0
1395_0000355	2.0	2.0
1395_0000356	2.0	2.0
1395_0000368	1.0	2.0
1395_0000369	4.0	2.0
1395_0000383	5.0	2.0
1395_0000387	5.0	4.0
1395_0000392	5.0	2.0
1395_0000399	2.0	2.0
1395_0000404	4.0	2.0
1395_0000432	5.0	2.0
1395_0000446	3.0	2.0
1395_0000449	4.0	2.0
1395_0000460	2.0	2.0
1395_0000462	4.0	2.0
1395_0000513	4.0	2.0
1395_0000526	2.0	2.0
1395_0000529	2.0	2.0
1395_0000549	2.0	4.0
1395_0000550	2.0	2.0
1395_0000559	2.0	2.0
1395_0000564	2.0	2.0
1395_0000587	0.0	2.0
1395_0000602	1.0	2.0
1395_0000606	0.0	2.0
1395_0000609	1.0	2.0
1395_0000610	2.0	2.0
1395_0000626	2.0	4.0
1395_0000639	1.0	2.0
1395_0001010	2.0	2.0
1395_0001017	2.0	2.0
1395_0001023	2.0	2.0
1395_0001033	2.0	2.0
1395_0001045	2.0	2.0
1395_0001058	2.0	2.0
1395_0001061	2.0	4.0
1395_0001068	1.0	2.0
1395_0001078	2.0	2.0
1395_0001080	2.0	2.0
1395_0001101	2.0	2.0
1395_0001115	2.0	2.0
1395_0001132	1.0	2.0
1395_0001149	1.0	2.0
1395_0001158	2.0	2.0
1395_0001161	1.0	2.0
1395_0001169	2.0	2.0
1395_0001170	1.0	4.0
1395_0001171	1.0	2.0
LANGUAGE: IT, 2th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.54
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.18
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.00      0.00      0.00        29
         2.0       0.46      0.95      0.62        44
         3.0       0.00      0.00      0.00        15
         4.0       0.69      0.89      0.78        53
         5.0       0.00      0.00      0.00        15

    accuracy                           0.56       160
   macro avg       0.19      0.31      0.23       160
weighted avg       0.35      0.56      0.43       160

[[ 0  0  4  0  0  0]
 [ 0  0 29  0  0  0]
 [ 0  0 42  0  2  0]
 [ 0  0  4  0 11  0]
 [ 0  0  6  0 47  0]
 [ 0  0  7  0  8  0]]
0.42718765192027225
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.22
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.07
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.00      0.00      0.00        29
         2.0       0.50      0.93      0.65        44
         3.0       0.00      0.00      0.00        15
         4.0       0.65      0.96      0.78        53
         5.0       0.00      0.00      0.00        15

    accuracy                           0.57       160
   macro avg       0.19      0.32      0.24       160
weighted avg       0.35      0.57      0.44       160

[[ 0  0  4  0  0  0]
 [ 0  0 29  0  0  0]
 [ 0  0 41  0  3  0]
 [ 0  0  4  0 11  0]
 [ 0  0  2  0 51  0]
 [ 0  0  2  0 13  0]]
0.4368881012964983
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.12
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.02
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.00      0.00      0.00        29
         2.0       0.51      0.93      0.66        44
         3.0       0.00      0.00      0.00        15
         4.0       0.65      0.98      0.78        53
         5.0       0.00      0.00      0.00        15

    accuracy                           0.58       160
   macro avg       0.19      0.32      0.24       160
weighted avg       0.36      0.58      0.44       160

[[ 0  0  4  0  0  0]
 [ 0  0 29  0  0  0]
 [ 0  0 41  0  3  0]
 [ 0  0  3  0 12  0]
 [ 0  0  1  0 52  0]
 [ 0  0  2  0 13  0]]
0.4408773951006547
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.09
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.03
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.00      0.00      0.00        29
         2.0       0.53      0.93      0.68        44
         3.0       0.00      0.00      0.00        15
         4.0       0.64      1.00      0.78        53
         5.0       0.00      0.00      0.00        15

    accuracy                           0.59       160
   macro avg       0.20      0.32      0.24       160
weighted avg       0.36      0.59      0.44       160

[[ 0  0  4  0  0  0]
 [ 0  0 29  0  0  0]
 [ 0  0 41  0  3  0]
 [ 0  0  1  0 14  0]
 [ 0  0  0  0 53  0]
 [ 0  0  2  0 13  0]]
0.44454378342245987
160 160 160
Filename	True Label	Prediction
1325_1001018	4.0	4.0
1325_1001022	4.0	4.0
1325_1001024	4.0	4.0
1325_1001025	4.0	4.0
1325_1001033	5.0	4.0
1325_1001037	5.0	4.0
1325_1001039	5.0	4.0
1325_1001040	4.0	4.0
1325_1001041	4.0	4.0
1325_1001043	4.0	4.0
1325_1001045	5.0	4.0
1325_1001063	4.0	4.0
1325_1001094	4.0	4.0
1325_1001096	4.0	4.0
1325_1001107	4.0	4.0
1325_1001129	3.0	4.0
1325_1001139	4.0	4.0
1325_1001144	4.0	4.0
1325_1001155	4.0	4.0
1325_1001165	4.0	4.0
1325_1001167	4.0	4.0
1325_1001170	4.0	4.0
1325_9000087	5.0	4.0
1325_9000088	4.0	4.0
1325_9000095	4.0	4.0
1325_9000102	3.0	4.0
1325_9000107	4.0	4.0
1325_9000136	4.0	4.0
1325_9000144	4.0	4.0
1325_9000188	4.0	4.0
1325_9000213	4.0	4.0
1325_9000215	4.0	4.0
1325_9000279	3.0	4.0
1325_9000318	4.0	4.0
1325_9000504	4.0	4.0
1325_9000534	4.0	4.0
1325_9000676	3.0	4.0
1325_9000700	4.0	4.0
1325_9000750	4.0	4.0
1365_0100013	4.0	4.0
1365_0100020	3.0	4.0
1365_0100021	3.0	4.0
1365_0100023	2.0	4.0
1365_0100063	4.0	4.0
1365_0100065	2.0	4.0
1365_0100071	4.0	4.0
1365_0100074	3.0	4.0
1365_0100094	5.0	4.0
1365_0100101	4.0	4.0
1365_0100103	5.0	4.0
1365_0100118	4.0	4.0
1365_0100123	4.0	4.0
1365_0100134	5.0	4.0
1365_0100151	4.0	4.0
1365_0100164	3.0	4.0
1365_0100173	5.0	4.0
1365_0100177	5.0	4.0
1365_0100187	5.0	4.0
1365_0100196	4.0	4.0
1365_0100198	4.0	4.0
1365_0100199	4.0	4.0
1365_0100201	5.0	4.0
1365_0100204	5.0	4.0
1365_0100213	4.0	4.0
1365_0100218	4.0	4.0
1365_0100220	4.0	4.0
1365_0100225	4.0	4.0
1365_0100232	4.0	4.0
1365_0100251	3.0	4.0
1365_0100252	4.0	4.0
1365_0100255	3.0	4.0
1365_0100260	4.0	4.0
1365_0100266	3.0	4.0
1365_0100269	4.0	4.0
1365_0100279	4.0	4.0
1365_0100286	3.0	4.0
1365_0100459	4.0	4.0
1365_0100461	2.0	4.0
1365_0100475	4.0	4.0
1365_0100477	4.0	4.0
1385_0000013	1.0	2.0
1385_0000023	2.0	2.0
1385_0000034	2.0	2.0
1385_0000035	2.0	2.0
1385_0000052	1.0	2.0
1385_0000095	1.0	2.0
1385_0000099	1.0	2.0
1385_0000114	2.0	2.0
1385_0000122	2.0	2.0
1385_0000127	2.0	2.0
1385_0001123	2.0	2.0
1385_0001127	2.0	2.0
1385_0001130	1.0	2.0
1385_0001132	2.0	2.0
1385_0001133	2.0	2.0
1385_0001148	2.0	2.0
1385_0001152	2.0	2.0
1385_0001153	2.0	2.0
1385_0001157	2.0	2.0
1385_0001163	1.0	2.0
1385_0001166	2.0	2.0
1385_0001175	1.0	2.0
1385_0001189	1.0	2.0
1385_0001503	2.0	2.0
1385_0001523	2.0	2.0
1385_0001527	2.0	2.0
1385_0001712	1.0	2.0
1385_0001714	1.0	2.0
1385_0001719	1.0	2.0
1385_0001724	2.0	2.0
1385_0001727	1.0	2.0
1385_0001729	2.0	2.0
1385_0001732	1.0	2.0
1385_0001736	2.0	2.0
1385_0001740	2.0	2.0
1385_0001741	1.0	2.0
1385_0001742	0.0	2.0
1385_0001744	0.0	2.0
1385_0001754	1.0	2.0
1385_0001757	2.0	2.0
1385_0001758	1.0	2.0
1385_0001759	1.0	2.0
1385_0001764	1.0	2.0
1385_0001800	1.0	2.0
1395_0000333	2.0	2.0
1395_0000353	2.0	2.0
1395_0000359	3.0	4.0
1395_0000361	4.0	4.0
1395_0000380	5.0	2.0
1395_0000389	1.0	2.0
1395_0000391	3.0	4.0
1395_0000443	5.0	2.0
1395_0000450	2.0	2.0
1395_0000458	2.0	2.0
1395_0000465	2.0	2.0
1395_0000499	2.0	2.0
1395_0000527	1.0	2.0
1395_0000531	2.0	2.0
1395_0000548	2.0	2.0
1395_0000560	3.0	2.0
1395_0000563	2.0	2.0
1395_0000565	2.0	2.0
1395_0000597	1.0	2.0
1395_0000598	1.0	2.0
1395_0000646	2.0	2.0
1395_0000649	2.0	2.0
1395_0001016	2.0	2.0
1395_0001019	2.0	2.0
1395_0001021	2.0	2.0
1395_0001065	1.0	2.0
1395_0001067	1.0	2.0
1395_0001093	2.0	2.0
1395_0001116	2.0	2.0
1395_0001122	1.0	2.0
1395_0001124	0.0	2.0
1395_0001131	1.0	2.0
1395_0001133	1.0	2.0
1395_0001146	0.0	2.0
1395_0001164	2.0	2.0
1395_0001167	1.0	2.0
LANGUAGE: IT, 3th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.57
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.27
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.52      0.46      0.49        28
         2.0       0.49      0.59      0.54        44
         3.0       0.00      0.00      0.00        16
         4.0       0.59      0.92      0.72        52
         5.0       0.00      0.00      0.00        15

    accuracy                           0.54       160
   macro avg       0.27      0.33      0.29       160
weighted avg       0.42      0.54      0.47       160

[[ 0  2  1  0  2  0]
 [ 0 13 13  0  2  0]
 [ 0  9 26  0  9  0]
 [ 0  0  6  0 10  0]
 [ 0  0  4  0 48  0]
 [ 0  1  3  0 11  0]]
0.46610755791166714
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.22
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.21
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.46      0.39      0.42        28
         2.0       0.52      0.57      0.54        44
         3.0       0.00      0.00      0.00        16
         4.0       0.57      0.96      0.71        52
         5.0       0.00      0.00      0.00        15

    accuracy                           0.54       160
   macro avg       0.26      0.32      0.28       160
weighted avg       0.41      0.54      0.46       160

[[ 0  3  1  0  1  0]
 [ 0 11 15  0  2  0]
 [ 0  9 25  0 10  0]
 [ 0  0  4  0 12  0]
 [ 0  0  2  0 50  0]
 [ 0  1  1  0 13  0]]
0.4556378404204491
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.11
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.22
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.50      0.04      0.07        28
         2.0       0.49      0.80      0.61        44
         3.0       0.00      0.00      0.00        16
         4.0       0.59      0.98      0.73        52
         5.0       0.00      0.00      0.00        15

    accuracy                           0.54       160
   macro avg       0.26      0.30      0.23       160
weighted avg       0.41      0.54      0.42       160

[[ 0  1  3  0  1  0]
 [ 0  1 26  0  1  0]
 [ 0  0 35  0  9  0]
 [ 0  0  4  0 12  0]
 [ 0  0  1  0 51  0]
 [ 0  0  2  0 13  0]]
0.4175471796475863
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.07
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.19
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.00      0.00      0.00        28
         2.0       0.47      0.82      0.60        44
         3.0       0.00      0.00      0.00        16
         4.0       0.60      0.96      0.74        52
         5.0       0.00      0.00      0.00        15

    accuracy                           0.54       160
   macro avg       0.18      0.30      0.22       160
weighted avg       0.32      0.54      0.40       160

[[ 0  0  4  0  1  0]
 [ 0  0 27  0  1  0]
 [ 0  0 36  0  8  0]
 [ 0  0  5  0 11  0]
 [ 0  0  2  0 50  0]
 [ 0  0  2  0 13  0]]
0.4039705882352941
160 160 160
Filename	True Label	Prediction
1325_1001017	4.0	4.0
1325_1001023	4.0	4.0
1325_1001035	5.0	4.0
1325_1001036	5.0	4.0
1325_1001044	3.0	4.0
1325_1001046	4.0	4.0
1325_1001054	4.0	4.0
1325_1001058	4.0	4.0
1325_1001078	4.0	4.0
1325_1001080	4.0	4.0
1325_1001092	4.0	4.0
1325_1001093	4.0	4.0
1325_1001109	4.0	4.0
1325_1001122	4.0	4.0
1325_1001123	4.0	4.0
1325_1001124	3.0	4.0
1325_1001135	3.0	4.0
1325_1001141	2.0	4.0
1325_1001142	4.0	4.0
1325_1001156	4.0	4.0
1325_1001157	4.0	4.0
1325_1001158	4.0	4.0
1325_1001166	4.0	4.0
1325_9000090	4.0	4.0
1325_9000106	4.0	4.0
1325_9000138	4.0	4.0
1325_9000139	4.0	4.0
1325_9000185	4.0	4.0
1325_9000210	3.0	4.0
1325_9000214	4.0	4.0
1325_9000237	3.0	4.0
1325_9000304	3.0	4.0
1325_9000503	4.0	4.0
1325_9000612	2.0	4.0
1325_9000677	4.0	4.0
1325_9000685	4.0	4.0
1365_0100003	2.0	4.0
1365_0100006	4.0	4.0
1365_0100009	3.0	4.0
1365_0100011	4.0	4.0
1365_0100014	4.0	4.0
1365_0100024	4.0	4.0
1365_0100030	4.0	4.0
1365_0100051	2.0	4.0
1365_0100057	4.0	4.0
1365_0100067	3.0	4.0
1365_0100096	4.0	4.0
1365_0100097	4.0	4.0
1365_0100098	2.0	4.0
1365_0100116	4.0	4.0
1365_0100117	5.0	4.0
1365_0100138	5.0	4.0
1365_0100139	4.0	4.0
1365_0100169	5.0	4.0
1365_0100172	5.0	4.0
1365_0100174	3.0	4.0
1365_0100175	4.0	4.0
1365_0100192	4.0	4.0
1365_0100195	4.0	2.0
1365_0100212	5.0	4.0
1365_0100215	4.0	4.0
1365_0100217	5.0	4.0
1365_0100219	4.0	4.0
1365_0100233	4.0	4.0
1365_0100253	3.0	2.0
1365_0100263	4.0	4.0
1365_0100275	4.0	4.0
1365_0100276	4.0	4.0
1365_0100277	4.0	4.0
1365_0100287	4.0	4.0
1365_0100288	4.0	4.0
1365_0100289	5.0	4.0
1365_0100299	5.0	4.0
1365_0100447	5.0	4.0
1365_0100469	5.0	4.0
1365_0100470	4.0	4.0
1365_0100471	3.0	4.0
1365_0100479	4.0	4.0
1365_0100482	4.0	4.0
1385_0000020	2.0	2.0
1385_0000100	2.0	2.0
1385_0000101	1.0	2.0
1385_0000102	2.0	2.0
1385_0000103	2.0	2.0
1385_0000126	2.0	2.0
1385_0000130	2.0	2.0
1385_0001104	1.0	2.0
1385_0001107	2.0	2.0
1385_0001108	2.0	2.0
1385_0001110	2.0	2.0
1385_0001113	1.0	2.0
1385_0001120	2.0	2.0
1385_0001122	2.0	2.0
1385_0001124	1.0	2.0
1385_0001125	2.0	2.0
1385_0001126	1.0	2.0
1385_0001150	2.0	2.0
1385_0001158	2.0	2.0
1385_0001160	1.0	2.0
1385_0001161	2.0	2.0
1385_0001162	1.0	2.0
1385_0001165	2.0	2.0
1385_0001171	1.0	2.0
1385_0001173	0.0	2.0
1385_0001194	2.0	2.0
1385_0001195	2.0	2.0
1385_0001198	2.0	2.0
1385_0001528	2.0	2.0
1385_0001715	1.0	2.0
1385_0001720	1.0	2.0
1385_0001725	1.0	2.0
1385_0001730	2.0	2.0
1385_0001753	1.0	2.0
1385_0001760	0.0	2.0
1385_0001774	1.0	2.0
1385_0001799	2.0	2.0
1395_0000338	2.0	2.0
1395_0000364	2.0	2.0
1395_0000366	3.0	4.0
1395_0000379	1.0	2.0
1395_0000396	2.0	2.0
1395_0000398	5.0	2.0
1395_0000409	3.0	2.0
1395_0000413	2.0	4.0
1395_0000415	2.0	2.0
1395_0000438	5.0	4.0
1395_0000447	2.0	4.0
1395_0000454	2.0	2.0
1395_0000455	2.0	2.0
1395_0000469	2.0	2.0
1395_0000471	2.0	2.0
1395_0000512	3.0	2.0
1395_0000516	1.0	2.0
1395_0000525	5.0	2.0
1395_0000528	4.0	2.0
1395_0000534	2.0	2.0
1395_0000537	2.0	2.0
1395_0000547	3.0	2.0
1395_0000551	3.0	2.0
1395_0000553	2.0	2.0
1395_0000572	2.0	2.0
1395_0000583	1.0	4.0
1395_0000591	0.0	2.0
1395_0000593	0.0	2.0
1395_0000604	1.0	2.0
1395_0000607	1.0	2.0
1395_0000608	1.0	2.0
1395_0000611	1.0	2.0
1395_0000612	0.0	4.0
1395_0000635	1.0	2.0
1395_0000642	1.0	2.0
1395_0001020	1.0	2.0
1395_0001028	2.0	2.0
1395_0001070	2.0	4.0
1395_0001075	1.0	2.0
1395_0001103	1.0	2.0
1395_0001117	1.0	2.0
1395_0001118	1.0	2.0
1395_0001147	1.0	2.0
1395_0001160	2.0	2.0
LANGUAGE: IT, 4th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.57
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.25
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.00      0.00      0.00        28
         2.0       0.45      0.98      0.62        44
         3.0       0.00      0.00      0.00        16
         4.0       0.62      0.77      0.68        52
         5.0       0.00      0.00      0.00        15

    accuracy                           0.52       160
   macro avg       0.18      0.29      0.22       160
weighted avg       0.32      0.52      0.39       160

[[ 0  0  5  0  0  0]
 [ 0  0 28  0  0  0]
 [ 0  0 43  0  1  0]
 [ 0  0  4  0 12  0]
 [ 0  0 12  0 40  0]
 [ 0  0  3  0 12  0]]
0.3923661071143086
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.26
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.11
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.35      0.61      0.45        28
         2.0       0.52      0.36      0.43        44
         3.0       0.00      0.00      0.00        16
         4.0       0.62      0.96      0.75        52
         5.0       0.00      0.00      0.00        15

    accuracy                           0.52       160
   macro avg       0.25      0.32      0.27       160
weighted avg       0.40      0.52      0.44       160

[[ 0  5  0  0  0  0]
 [ 0 17 11  0  0  0]
 [ 0 26 16  0  2  0]
 [ 0  0  1  0 15  0]
 [ 0  0  2  0 50  0]
 [ 0  0  1  0 14  0]]
0.4399837092731829
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.14
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.08
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.00      0.00      0.00        28
         2.0       0.54      0.95      0.69        44
         3.0       0.00      0.00      0.00        16
         4.0       0.61      0.96      0.75        52
         5.0       0.00      0.00      0.00        15

    accuracy                           0.57       160
   macro avg       0.19      0.32      0.24       160
weighted avg       0.35      0.57      0.43       160

[[ 0  0  5  0  0  0]
 [ 0  0 27  0  1  0]
 [ 0  0 42  0  2  0]
 [ 0  0  1  0 15  0]
 [ 0  0  2  0 50  0]
 [ 0  0  1  0 14  0]]
0.4318815757279177
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.10
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.03
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.50      0.14      0.22        28
         2.0       0.58      0.93      0.71        44
         3.0       0.00      0.00      0.00        16
         4.0       0.62      0.96      0.75        52
         5.0       0.00      0.00      0.00        15

    accuracy                           0.59       160
   macro avg       0.28      0.34      0.28       160
weighted avg       0.45      0.59      0.48       160

[[ 0  3  2  0  0  0]
 [ 0  4 24  0  0  0]
 [ 0  1 41  0  2  0]
 [ 0  0  1  0 15  0]
 [ 0  0  2  0 50  0]
 [ 0  0  1  0 14  0]]
0.4793367476662671
160 160 160
Filename	True Label	Prediction
1325_1001008	5.0	4.0
1325_1001009	5.0	4.0
1325_1001013	4.0	4.0
1325_1001016	5.0	4.0
1325_1001020	4.0	4.0
1325_1001048	4.0	4.0
1325_1001051	4.0	4.0
1325_1001055	5.0	4.0
1325_1001057	4.0	4.0
1325_1001079	4.0	4.0
1325_1001081	4.0	4.0
1325_1001082	5.0	4.0
1325_1001083	5.0	4.0
1325_1001084	4.0	4.0
1325_1001085	4.0	4.0
1325_1001110	4.0	4.0
1325_1001113	4.0	4.0
1325_1001119	4.0	4.0
1325_1001126	3.0	4.0
1325_1001127	4.0	4.0
1325_1001130	4.0	4.0
1325_1001132	4.0	4.0
1325_1001159	5.0	4.0
1325_1001161	4.0	4.0
1325_1001164	3.0	4.0
1325_1001168	4.0	4.0
1325_9000059	5.0	4.0
1325_9000104	3.0	4.0
1325_9000137	4.0	4.0
1325_9000140	4.0	4.0
1325_9000143	4.0	4.0
1325_9000239	3.0	4.0
1325_9000296	3.0	4.0
1325_9000314	4.0	4.0
1325_9000320	3.0	4.0
1325_9000321	4.0	4.0
1325_9000505	4.0	4.0
1325_9000533	4.0	4.0
1325_9000554	3.0	4.0
1325_9000601	4.0	4.0
1325_9000675	4.0	4.0
1325_9000678	4.0	4.0
1325_9000684	4.0	4.0
1365_0100002	4.0	4.0
1365_0100004	3.0	4.0
1365_0100008	4.0	4.0
1365_0100012	4.0	4.0
1365_0100017	4.0	4.0
1365_0100018	3.0	4.0
1365_0100019	4.0	4.0
1365_0100027	4.0	4.0
1365_0100028	4.0	4.0
1365_0100061	5.0	4.0
1365_0100064	4.0	4.0
1365_0100070	4.0	4.0
1365_0100079	5.0	4.0
1365_0100080	4.0	4.0
1365_0100093	4.0	4.0
1365_0100095	4.0	4.0
1365_0100102	4.0	4.0
1365_0100170	4.0	4.0
1365_0100176	4.0	4.0
1365_0100178	4.0	4.0
1365_0100183	4.0	4.0
1365_0100191	3.0	4.0
1365_0100200	5.0	4.0
1365_0100226	5.0	4.0
1365_0100229	5.0	4.0
1365_0100265	4.0	4.0
1365_0100267	4.0	4.0
1365_0100268	3.0	4.0
1365_0100274	4.0	4.0
1365_0100278	4.0	4.0
1365_0100285	3.0	4.0
1365_0100448	2.0	4.0
1365_0100458	4.0	4.0
1365_0100472	3.0	4.0
1365_0100480	5.0	4.0
1385_0000017	1.0	2.0
1385_0000036	2.0	2.0
1385_0000039	2.0	2.0
1385_0000040	1.0	1.0
1385_0000041	2.0	2.0
1385_0000042	2.0	2.0
1385_0000044	2.0	2.0
1385_0000047	2.0	2.0
1385_0000048	2.0	2.0
1385_0000059	2.0	2.0
1385_0000097	2.0	2.0
1385_0000119	2.0	2.0
1385_0000120	1.0	2.0
1385_0000125	2.0	2.0
1385_0000128	1.0	2.0
1385_0001111	2.0	2.0
1385_0001119	2.0	2.0
1385_0001121	2.0	2.0
1385_0001128	1.0	2.0
1385_0001134	2.0	1.0
1385_0001147	2.0	2.0
1385_0001154	2.0	2.0
1385_0001155	2.0	2.0
1385_0001164	1.0	2.0
1385_0001178	1.0	1.0
1385_0001191	2.0	2.0
1385_0001199	2.0	2.0
1385_0001501	1.0	2.0
1385_0001525	2.0	2.0
1385_0001526	0.0	1.0
1385_0001716	2.0	2.0
1385_0001717	2.0	2.0
1385_0001718	1.0	1.0
1385_0001723	0.0	1.0
1385_0001734	2.0	2.0
1385_0001746	1.0	2.0
1385_0001747	0.0	2.0
1385_0001749	1.0	2.0
1385_0001751	1.0	2.0
1385_0001762	2.0	2.0
1385_0001765	0.0	1.0
1385_0001768	2.0	2.0
1385_0001773	1.0	1.0
1385_0001787	1.0	2.0
1385_0001789	2.0	2.0
1385_0001793	1.0	2.0
1385_0001794	1.0	2.0
1395_0000341	2.0	2.0
1395_0000354	1.0	2.0
1395_0000357	3.0	4.0
1395_0000365	3.0	2.0
1395_0000376	5.0	2.0
1395_0000388	4.0	2.0
1395_0000402	2.0	2.0
1395_0000448	2.0	2.0
1395_0000451	2.0	2.0
1395_0000500	2.0	2.0
1395_0000504	2.0	2.0
1395_0000533	4.0	2.0
1395_0000554	2.0	2.0
1395_0000555	2.0	2.0
1395_0000579	1.0	2.0
1395_0000585	1.0	2.0
1395_0000595	0.0	2.0
1395_0000631	1.0	2.0
1395_0000636	1.0	2.0
1395_0000644	2.0	4.0
1395_0001015	2.0	2.0
1395_0001024	1.0	2.0
1395_0001034	2.0	2.0
1395_0001060	2.0	2.0
1395_0001069	2.0	2.0
1395_0001071	2.0	2.0
1395_0001073	1.0	2.0
1395_0001090	2.0	2.0
1395_0001104	1.0	2.0
1395_0001108	1.0	2.0
1395_0001114	1.0	2.0
1395_0001121	1.0	2.0
1395_0001123	1.0	2.0
1395_0001141	2.0	2.0
1395_0001145	3.0	4.0
Averaged weighted F1-scores 0.4275640117258982
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']
LANGUAGE: IT, 0th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.16
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.89
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         7
         1.0       0.65      0.96      0.77        48
         2.0       0.61      0.70      0.65        66
         3.0       0.86      0.32      0.46        38
         4.0       0.00      0.00      0.00         1

    accuracy                           0.65       160
   macro avg       0.42      0.39      0.38       160
weighted avg       0.65      0.65      0.61       160

[[ 0  6  1  0  0]
 [ 0 46  2  0  0]
 [ 0 19 46  1  0]
 [ 0  0 26 12  0]
 [ 0  0  0  1  0]]
0.6106970938948411
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.82
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.80
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         7
         1.0       0.61      0.94      0.74        48
         2.0       0.72      0.50      0.59        66
         3.0       0.70      0.74      0.72        38
         4.0       0.00      0.00      0.00         1

    accuracy                           0.66       160
   macro avg       0.41      0.43      0.41       160
weighted avg       0.64      0.66      0.63       160

[[ 0  7  0  0  0]
 [ 0 45  3  0  0]
 [ 0 22 33 11  0]
 [ 0  0 10 28  0]
 [ 0  0  0  1  0]]
0.6349046530655137
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.71
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.83
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         7
         1.0       0.67      0.85      0.75        48
         2.0       0.79      0.47      0.59        66
         3.0       0.62      0.97      0.76        38
         4.0       0.00      0.00      0.00         1

    accuracy                           0.68       160
   macro avg       0.42      0.46      0.42       160
weighted avg       0.68      0.68      0.65       160

[[ 0  7  0  0  0]
 [ 0 41  7  0  0]
 [ 0 13 31 22  0]
 [ 0  0  1 37  0]
 [ 0  0  0  1  0]]
0.6485962366598016
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.66
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.78
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         7
         1.0       0.66      0.88      0.75        48
         2.0       0.74      0.53      0.62        66
         3.0       0.65      0.84      0.74        38
         4.0       0.00      0.00      0.00         1

    accuracy                           0.68       160
   macro avg       0.41      0.45      0.42       160
weighted avg       0.66      0.68      0.66       160

[[ 0  7  0  0  0]
 [ 0 42  6  0  0]
 [ 0 15 35 16  0]
 [ 0  0  6 32  0]
 [ 0  0  0  1  0]]
0.6552436171294882
160 160 160
Filename	True Label	Prediction
1325_1001015	3.0	3.0
1325_1001017	3.0	3.0
1325_1001025	2.0	3.0
1325_1001028	3.0	3.0
1325_1001033	3.0	3.0
1325_1001037	2.0	2.0
1325_1001041	3.0	3.0
1325_1001045	3.0	3.0
1325_1001046	2.0	3.0
1325_1001048	2.0	3.0
1325_1001081	3.0	3.0
1325_1001085	3.0	3.0
1325_1001086	3.0	3.0
1325_1001087	2.0	3.0
1325_1001108	3.0	3.0
1325_1001120	3.0	3.0
1325_1001127	3.0	3.0
1325_1001132	3.0	3.0
1325_1001139	3.0	3.0
1325_1001141	2.0	3.0
1325_1001159	3.0	3.0
1325_1001161	3.0	3.0
1325_1001166	3.0	3.0
1325_9000088	2.0	3.0
1325_9000090	2.0	3.0
1325_9000095	3.0	3.0
1325_9000107	3.0	3.0
1325_9000138	4.0	3.0
1325_9000185	3.0	3.0
1325_9000186	3.0	3.0
1325_9000209	3.0	3.0
1325_9000303	3.0	3.0
1325_9000304	3.0	3.0
1325_9000314	3.0	3.0
1325_9000317	3.0	3.0
1325_9000504	3.0	3.0
1325_9000601	3.0	3.0
1325_9000685	3.0	3.0
1365_0100009	2.0	2.0
1365_0100017	2.0	2.0
1365_0100030	2.0	2.0
1365_0100057	2.0	3.0
1365_0100061	3.0	2.0
1365_0100064	2.0	2.0
1365_0100072	2.0	2.0
1365_0100074	2.0	3.0
1365_0100096	2.0	2.0
1365_0100097	2.0	2.0
1365_0100099	2.0	2.0
1365_0100100	2.0	3.0
1365_0100102	3.0	2.0
1365_0100105	3.0	2.0
1365_0100116	3.0	2.0
1365_0100121	2.0	2.0
1365_0100139	2.0	2.0
1365_0100164	2.0	3.0
1365_0100165	3.0	2.0
1365_0100166	2.0	2.0
1365_0100185	2.0	2.0
1365_0100188	2.0	2.0
1365_0100190	2.0	3.0
1365_0100192	3.0	3.0
1365_0100195	2.0	1.0
1365_0100219	2.0	3.0
1365_0100227	3.0	3.0
1365_0100228	2.0	2.0
1365_0100251	2.0	3.0
1365_0100252	2.0	2.0
1365_0100259	2.0	2.0
1365_0100266	2.0	3.0
1365_0100267	2.0	3.0
1365_0100269	2.0	2.0
1365_0100276	3.0	3.0
1365_0100278	3.0	2.0
1365_0100280	1.0	2.0
1365_0100287	2.0	2.0
1365_0100290	2.0	2.0
1365_0100299	2.0	2.0
1365_0100461	2.0	2.0
1365_0100478	2.0	2.0
1365_0100482	2.0	2.0
1385_0000013	1.0	1.0
1385_0000021	1.0	1.0
1385_0000022	1.0	1.0
1385_0000033	1.0	1.0
1385_0000035	1.0	1.0
1385_0000036	1.0	1.0
1385_0000038	1.0	1.0
1385_0000047	1.0	1.0
1385_0000051	2.0	1.0
1385_0000053	1.0	1.0
1385_0000097	2.0	1.0
1385_0000099	1.0	1.0
1385_0000100	1.0	1.0
1385_0000101	1.0	1.0
1385_0000119	1.0	1.0
1385_0001103	1.0	1.0
1385_0001109	1.0	1.0
1385_0001110	2.0	1.0
1385_0001120	2.0	1.0
1385_0001130	1.0	1.0
1385_0001148	2.0	1.0
1385_0001160	1.0	2.0
1385_0001165	1.0	1.0
1385_0001166	1.0	1.0
1385_0001167	1.0	1.0
1385_0001178	1.0	1.0
1385_0001199	1.0	1.0
1385_0001501	1.0	1.0
1385_0001522	0.0	1.0
1385_0001524	1.0	1.0
1385_0001526	0.0	1.0
1385_0001527	2.0	1.0
1385_0001715	1.0	1.0
1385_0001733	1.0	1.0
1385_0001740	1.0	1.0
1385_0001742	0.0	1.0
1385_0001751	1.0	1.0
1385_0001753	1.0	1.0
1385_0001754	1.0	1.0
1385_0001757	1.0	1.0
1385_0001766	2.0	1.0
1385_0001775	1.0	1.0
1385_0001790	1.0	1.0
1385_0001798	1.0	1.0
1395_0000361	2.0	2.0
1395_0000366	2.0	2.0
1395_0000376	2.0	2.0
1395_0000389	1.0	1.0
1395_0000399	2.0	2.0
1395_0000446	2.0	2.0
1395_0000448	2.0	1.0
1395_0000449	2.0	1.0
1395_0000455	2.0	1.0
1395_0000525	2.0	1.0
1395_0000526	1.0	1.0
1395_0000537	2.0	2.0
1395_0000549	2.0	2.0
1395_0000553	2.0	1.0
1395_0000557	2.0	2.0
1395_0000560	2.0	1.0
1395_0000582	0.0	1.0
1395_0000587	0.0	1.0
1395_0000595	1.0	1.0
1395_0000596	2.0	1.0
1395_0000611	1.0	1.0
1395_0000636	1.0	1.0
1395_0001017	1.0	1.0
1395_0001028	1.0	2.0
1395_0001066	1.0	2.0
1395_0001068	1.0	2.0
1395_0001069	2.0	2.0
1395_0001076	1.0	2.0
1395_0001119	2.0	2.0
1395_0001122	0.0	1.0
1395_0001123	1.0	1.0
1395_0001126	1.0	1.0
1395_0001131	0.0	1.0
1395_0001141	2.0	2.0
1395_0001167	1.0	1.0
LANGUAGE: IT, 1th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.24
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.93
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.60      0.82      0.69        49
         2.0       0.65      0.55      0.60        65
         3.0       0.74      0.74      0.74        38

    accuracy                           0.65       160
   macro avg       0.50      0.53      0.51       160
weighted avg       0.62      0.65      0.63       160

[[ 0  7  1  0]
 [ 0 40  9  0]
 [ 0 19 36 10]
 [ 0  1  9 28]]
0.6299568965517242
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.84
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.76
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.67      0.63      0.65        49
         2.0       0.63      0.63      0.63        65
         3.0       0.65      0.84      0.74        38

    accuracy                           0.65       160
   macro avg       0.49      0.53      0.50       160
weighted avg       0.62      0.65      0.63       160

[[ 0  8  0  0]
 [ 0 31 18  0]
 [ 0  7 41 17]
 [ 0  0  6 32]]
0.6308310647307925
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.72
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.78
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.60      0.92      0.73        49
         2.0       0.71      0.54      0.61        65
         3.0       0.78      0.74      0.76        38

    accuracy                           0.68       160
   macro avg       0.52      0.55      0.52       160
weighted avg       0.66      0.68      0.65       160

[[ 0  8  0  0]
 [ 0 45  4  0]
 [ 0 22 35  8]
 [ 0  0 10 28]]
0.6514597099221462
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.68
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.72
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.68      0.82      0.74        49
         2.0       0.73      0.68      0.70        65
         3.0       0.76      0.82      0.78        38

    accuracy                           0.72       160
   macro avg       0.54      0.58      0.56       160
weighted avg       0.69      0.72      0.70       160

[[ 0  8  0  0]
 [ 0 40  9  0]
 [ 0 11 44 10]
 [ 0  0  7 31]]
0.699244256915143
160 160 160
Filename	True Label	Prediction
1325_1001009	3.0	3.0
1325_1001010	3.0	3.0
1325_1001022	3.0	3.0
1325_1001027	3.0	3.0
1325_1001036	3.0	3.0
1325_1001042	2.0	3.0
1325_1001044	3.0	3.0
1325_1001052	2.0	3.0
1325_1001057	2.0	3.0
1325_1001059	3.0	3.0
1325_1001062	3.0	3.0
1325_1001076	3.0	3.0
1325_1001079	3.0	3.0
1325_1001095	2.0	3.0
1325_1001100	2.0	3.0
1325_1001110	3.0	3.0
1325_1001113	3.0	3.0
1325_1001122	3.0	3.0
1325_1001123	3.0	3.0
1325_1001135	3.0	3.0
1325_1001138	2.0	3.0
1325_1001143	3.0	3.0
1325_1001144	3.0	3.0
1325_1001152	3.0	3.0
1325_1001160	3.0	3.0
1325_1001164	3.0	3.0
1325_9000059	3.0	3.0
1325_9000087	2.0	3.0
1325_9000105	2.0	3.0
1325_9000139	3.0	3.0
1325_9000140	3.0	3.0
1325_9000144	3.0	3.0
1325_9000152	3.0	3.0
1325_9000215	3.0	3.0
1325_9000278	3.0	3.0
1325_9000534	3.0	3.0
1325_9000536	3.0	3.0
1325_9000554	3.0	3.0
1325_9000611	3.0	3.0
1365_0100006	2.0	2.0
1365_0100007	1.0	2.0
1365_0100008	2.0	2.0
1365_0100013	3.0	2.0
1365_0100014	2.0	2.0
1365_0100018	2.0	2.0
1365_0100019	2.0	2.0
1365_0100024	2.0	2.0
1365_0100051	2.0	2.0
1365_0100058	2.0	2.0
1365_0100079	2.0	2.0
1365_0100092	2.0	2.0
1365_0100098	2.0	2.0
1365_0100103	2.0	3.0
1365_0100120	3.0	2.0
1365_0100133	2.0	2.0
1365_0100151	2.0	2.0
1365_0100167	2.0	2.0
1365_0100168	2.0	2.0
1365_0100169	2.0	2.0
1365_0100172	2.0	2.0
1365_0100183	2.0	2.0
1365_0100194	2.0	2.0
1365_0100203	2.0	2.0
1365_0100211	3.0	2.0
1365_0100220	3.0	2.0
1365_0100230	2.0	2.0
1365_0100231	2.0	2.0
1365_0100255	2.0	2.0
1365_0100256	2.0	2.0
1365_0100257	2.0	2.0
1365_0100261	2.0	2.0
1365_0100268	2.0	2.0
1365_0100279	2.0	2.0
1365_0100448	2.0	2.0
1365_0100457	2.0	3.0
1365_0100469	2.0	2.0
1365_0100470	2.0	2.0
1365_0100475	2.0	2.0
1365_0100477	2.0	2.0
1385_0000034	1.0	1.0
1385_0000042	1.0	1.0
1385_0000048	1.0	1.0
1385_0000054	2.0	1.0
1385_0000058	1.0	1.0
1385_0000103	1.0	1.0
1385_0000120	0.0	1.0
1385_0000126	1.0	1.0
1385_0001111	2.0	1.0
1385_0001126	0.0	1.0
1385_0001128	1.0	1.0
1385_0001133	2.0	1.0
1385_0001135	1.0	1.0
1385_0001138	1.0	1.0
1385_0001147	1.0	1.0
1385_0001149	2.0	1.0
1385_0001157	1.0	1.0
1385_0001164	1.0	1.0
1385_0001172	1.0	1.0
1385_0001189	1.0	1.0
1385_0001192	1.0	1.0
1385_0001503	1.0	1.0
1385_0001712	1.0	1.0
1385_0001714	0.0	1.0
1385_0001739	1.0	1.0
1385_0001746	1.0	1.0
1385_0001747	1.0	1.0
1385_0001759	1.0	1.0
1385_0001761	1.0	1.0
1385_0001767	1.0	1.0
1385_0001771	1.0	1.0
1385_0001786	1.0	1.0
1385_0001787	0.0	1.0
1385_0001795	0.0	1.0
1395_0000338	2.0	2.0
1395_0000341	2.0	1.0
1395_0000353	1.0	1.0
1395_0000355	2.0	2.0
1395_0000368	0.0	1.0
1395_0000369	2.0	2.0
1395_0000378	2.0	2.0
1395_0000388	2.0	2.0
1395_0000391	3.0	2.0
1395_0000438	3.0	2.0
1395_0000454	2.0	2.0
1395_0000460	1.0	1.0
1395_0000465	1.0	1.0
1395_0000500	1.0	1.0
1395_0000504	2.0	1.0
1395_0000514	3.0	2.0
1395_0000515	2.0	2.0
1395_0000529	2.0	1.0
1395_0000551	2.0	2.0
1395_0000554	2.0	1.0
1395_0000555	1.0	1.0
1395_0000556	1.0	1.0
1395_0000559	2.0	1.0
1395_0000563	2.0	2.0
1395_0000564	2.0	1.0
1395_0000581	1.0	2.0
1395_0000584	1.0	1.0
1395_0000597	1.0	2.0
1395_0000598	1.0	1.0
1395_0000609	1.0	1.0
1395_0000628	1.0	2.0
1395_0000631	1.0	2.0
1395_0000646	1.0	1.0
1395_0001010	1.0	2.0
1395_0001013	1.0	1.0
1395_0001016	1.0	1.0
1395_0001022	1.0	2.0
1395_0001023	1.0	1.0
1395_0001045	2.0	2.0
1395_0001058	1.0	1.0
1395_0001060	1.0	2.0
1395_0001080	2.0	2.0
1395_0001116	2.0	1.0
1395_0001121	0.0	1.0
1395_0001124	1.0	1.0
1395_0001150	0.0	1.0
1395_0001161	1.0	2.0
LANGUAGE: IT, 2th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.17
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.77
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.61      0.88      0.72        49
         2.0       0.82      0.48      0.60        65
         3.0       0.71      0.97      0.82        38

    accuracy                           0.69       160
   macro avg       0.54      0.58      0.54       160
weighted avg       0.69      0.69      0.66       160

[[ 0  8  0  0]
 [ 0 43  6  0]
 [ 0 19 31 15]
 [ 0  0  1 37]]
0.6611401421409988
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.86
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.76
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.66      0.67      0.67        49
         2.0       0.69      0.58      0.63        65
         3.0       0.67      0.97      0.80        38

    accuracy                           0.68       160
   macro avg       0.51      0.56      0.52       160
weighted avg       0.64      0.68      0.65       160

[[ 0  8  0  0]
 [ 0 33 16  0]
 [ 0  9 38 18]
 [ 0  0  1 37]]
0.6504368279569892
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.72
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.72
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.61      0.94      0.74        49
         2.0       0.74      0.52      0.61        65
         3.0       0.76      0.76      0.76        38

    accuracy                           0.68       160
   macro avg       0.53      0.56      0.53       160
weighted avg       0.67      0.68      0.66       160

[[ 0  8  0  0]
 [ 0 46  3  0]
 [ 0 22 34  9]
 [ 0  0  9 29]]
0.6555238738738739
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.67
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.68
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.64      0.71      0.67        49
         2.0       0.67      0.66      0.67        65
         3.0       0.76      0.82      0.78        38

    accuracy                           0.68       160
   macro avg       0.52      0.55      0.53       160
weighted avg       0.65      0.68      0.66       160

[[ 0  8  0  0]
 [ 0 35 14  0]
 [ 0 12 43 10]
 [ 0  0  7 31]]
0.6633555460889322
160 160 160
Filename	True Label	Prediction
1325_1001008	3.0	3.0
1325_1001021	3.0	3.0
1325_1001040	3.0	3.0
1325_1001047	3.0	2.0
1325_1001054	3.0	2.0
1325_1001055	3.0	3.0
1325_1001058	2.0	3.0
1325_1001077	3.0	3.0
1325_1001080	2.0	3.0
1325_1001084	3.0	2.0
1325_1001089	3.0	3.0
1325_1001096	3.0	3.0
1325_1001101	3.0	3.0
1325_1001119	3.0	3.0
1325_1001121	2.0	3.0
1325_1001128	3.0	3.0
1325_1001130	3.0	2.0
1325_1001133	3.0	3.0
1325_1001136	3.0	3.0
1325_1001153	2.0	3.0
1325_1001154	3.0	3.0
1325_1001162	3.0	3.0
1325_1001163	2.0	3.0
1325_1001167	3.0	3.0
1325_9000089	2.0	3.0
1325_9000187	3.0	3.0
1325_9000188	3.0	3.0
1325_9000210	2.0	3.0
1325_9000211	3.0	3.0
1325_9000240	3.0	3.0
1325_9000296	3.0	3.0
1325_9000302	3.0	3.0
1325_9000315	2.0	3.0
1325_9000322	3.0	3.0
1325_9000503	3.0	3.0
1325_9000533	3.0	3.0
1325_9000612	2.0	3.0
1325_9000674	3.0	3.0
1325_9000675	3.0	3.0
1325_9000676	3.0	3.0
1325_9000678	3.0	3.0
1325_9000686	3.0	3.0
1325_9000700	3.0	3.0
1365_0100003	2.0	2.0
1365_0100010	2.0	2.0
1365_0100012	2.0	2.0
1365_0100016	2.0	2.0
1365_0100026	2.0	2.0
1365_0100027	2.0	2.0
1365_0100029	1.0	2.0
1365_0100063	3.0	2.0
1365_0100066	2.0	2.0
1365_0100069	2.0	2.0
1365_0100080	2.0	2.0
1365_0100093	2.0	2.0
1365_0100095	2.0	2.0
1365_0100117	2.0	2.0
1365_0100118	2.0	2.0
1365_0100134	2.0	2.0
1365_0100136	2.0	2.0
1365_0100138	2.0	2.0
1365_0100147	2.0	2.0
1365_0100148	2.0	2.0
1365_0100177	2.0	2.0
1365_0100178	2.0	2.0
1365_0100179	2.0	2.0
1365_0100186	2.0	2.0
1365_0100196	2.0	2.0
1365_0100198	2.0	2.0
1365_0100200	3.0	2.0
1365_0100204	2.0	2.0
1365_0100205	2.0	2.0
1365_0100217	3.0	2.0
1365_0100222	3.0	3.0
1365_0100225	2.0	2.0
1365_0100233	2.0	2.0
1365_0100260	2.0	2.0
1365_0100281	2.0	2.0
1365_0100471	2.0	3.0
1365_0100479	2.0	2.0
1385_0000011	0.0	1.0
1385_0000012	1.0	1.0
1385_0000017	1.0	1.0
1385_0000037	1.0	1.0
1385_0000040	1.0	1.0
1385_0000041	1.0	1.0
1385_0000127	2.0	1.0
1385_0000129	1.0	1.0
1385_0000130	1.0	1.0
1385_0001107	1.0	1.0
1385_0001112	2.0	1.0
1385_0001118	2.0	1.0
1385_0001125	1.0	1.0
1385_0001127	2.0	1.0
1385_0001129	1.0	1.0
1385_0001134	1.0	1.0
1385_0001159	1.0	1.0
1385_0001161	1.0	1.0
1385_0001162	1.0	1.0
1385_0001170	1.0	1.0
1385_0001171	0.0	1.0
1385_0001174	1.0	1.0
1385_0001188	1.0	1.0
1385_0001193	1.0	1.0
1385_0001195	2.0	2.0
1385_0001197	1.0	1.0
1385_0001718	0.0	1.0
1385_0001719	1.0	1.0
1385_0001720	0.0	1.0
1385_0001724	1.0	1.0
1385_0001732	1.0	1.0
1385_0001734	1.0	1.0
1385_0001748	1.0	2.0
1385_0001764	1.0	1.0
1385_0001768	2.0	1.0
1385_0001785	1.0	1.0
1385_0001794	1.0	1.0
1395_0000333	1.0	2.0
1395_0000337	1.0	2.0
1395_0000354	1.0	1.0
1395_0000359	2.0	2.0
1395_0000364	2.0	1.0
1395_0000365	2.0	2.0
1395_0000392	2.0	1.0
1395_0000402	2.0	1.0
1395_0000404	2.0	2.0
1395_0000413	2.0	2.0
1395_0000443	2.0	2.0
1395_0000450	1.0	1.0
1395_0000462	2.0	1.0
1395_0000469	2.0	1.0
1395_0000499	2.0	1.0
1395_0000512	2.0	2.0
1395_0000513	2.0	1.0
1395_0000534	2.0	2.0
1395_0000550	2.0	2.0
1395_0000565	1.0	1.0
1395_0000583	1.0	2.0
1395_0000585	1.0	2.0
1395_0000593	1.0	2.0
1395_0000602	1.0	1.0
1395_0000606	0.0	1.0
1395_0000607	0.0	1.0
1395_0000608	1.0	2.0
1395_0000635	1.0	1.0
1395_0000644	1.0	2.0
1395_0001034	1.0	1.0
1395_0001040	0.0	1.0
1395_0001064	2.0	2.0
1395_0001065	1.0	2.0
1395_0001070	2.0	2.0
1395_0001103	1.0	2.0
1395_0001108	1.0	1.0
1395_0001109	1.0	2.0
1395_0001120	1.0	1.0
1395_0001133	1.0	2.0
1395_0001145	2.0	2.0
1395_0001146	0.0	1.0
1395_0001160	1.0	1.0
1395_0001170	1.0	2.0
LANGUAGE: IT, 3th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.21
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.84
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.59      0.92      0.72        49
         2.0       0.86      0.38      0.53        66
         3.0       0.62      0.92      0.74        37

    accuracy                           0.65       160
   macro avg       0.52      0.55      0.50       160
weighted avg       0.68      0.65      0.61       160

[[ 0  8  0  0]
 [ 0 45  1  3]
 [ 0 23 25 18]
 [ 0  0  3 34]]
0.608529176201373
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.84
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.73
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.67      0.86      0.75        49
         2.0       0.71      0.68      0.70        66
         3.0       0.79      0.73      0.76        37

    accuracy                           0.71       160
   macro avg       0.54      0.57      0.55       160
weighted avg       0.68      0.71      0.69       160

[[ 0  7  1  0]
 [ 0 42  7  0]
 [ 0 14 45  7]
 [ 0  0 10 27]]
0.6933584793645595
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.71
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.76
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.66      0.71      0.69        49
         2.0       0.69      0.68      0.69        66
         3.0       0.74      0.84      0.78        37

    accuracy                           0.69       160
   macro avg       0.52      0.56      0.54       160
weighted avg       0.66      0.69      0.68       160

[[ 0  7  1  0]
 [ 0 35 13  1]
 [ 0 11 45 10]
 [ 0  0  6 31]]
0.6750558569644883
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.64
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.72
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.68      0.78      0.72        49
         2.0       0.68      0.74      0.71        66
         3.0       0.81      0.70      0.75        37

    accuracy                           0.71       160
   macro avg       0.54      0.56      0.55       160
weighted avg       0.68      0.71      0.69       160

[[ 0  7  1  0]
 [ 0 38 11  0]
 [ 0 11 49  6]
 [ 0  0 11 26]]
0.688876811594203
160 160 160
Filename	True Label	Prediction
1325_1001012	3.0	3.0
1325_1001013	3.0	3.0
1325_1001014	3.0	3.0
1325_1001016	2.0	3.0
1325_1001018	3.0	3.0
1325_1001019	3.0	3.0
1325_1001024	3.0	3.0
1325_1001029	3.0	3.0
1325_1001039	3.0	3.0
1325_1001043	3.0	3.0
1325_1001050	3.0	3.0
1325_1001051	3.0	3.0
1325_1001075	2.0	3.0
1325_1001078	3.0	3.0
1325_1001082	3.0	3.0
1325_1001083	3.0	3.0
1325_1001088	2.0	3.0
1325_1001090	2.0	3.0
1325_1001091	3.0	3.0
1325_1001097	1.0	2.0
1325_1001099	3.0	3.0
1325_1001124	3.0	3.0
1325_1001125	3.0	3.0
1325_1001131	3.0	3.0
1325_1001155	3.0	3.0
1325_1001157	3.0	3.0
1325_1001158	3.0	3.0
1325_1001165	2.0	3.0
1325_9000099	2.0	3.0
1325_9000137	3.0	3.0
1325_9000143	3.0	3.0
1325_9000213	3.0	2.0
1325_9000318	3.0	3.0
1325_9000320	3.0	3.0
1365_0100002	2.0	2.0
1365_0100011	2.0	2.0
1365_0100020	2.0	2.0
1365_0100021	2.0	2.0
1365_0100028	2.0	2.0
1365_0100031	2.0	2.0
1365_0100056	2.0	2.0
1365_0100065	1.0	2.0
1365_0100071	3.0	2.0
1365_0100101	3.0	2.0
1365_0100104	2.0	2.0
1365_0100125	3.0	2.0
1365_0100135	2.0	2.0
1365_0100137	2.0	2.0
1365_0100146	2.0	2.0
1365_0100163	3.0	2.0
1365_0100170	2.0	2.0
1365_0100171	2.0	2.0
1365_0100176	2.0	2.0
1365_0100187	2.0	2.0
1365_0100212	3.0	2.0
1365_0100213	2.0	2.0
1365_0100215	2.0	2.0
1365_0100224	3.0	2.0
1365_0100226	3.0	2.0
1365_0100232	2.0	2.0
1365_0100253	2.0	2.0
1365_0100258	2.0	2.0
1365_0100262	3.0	2.0
1365_0100270	2.0	2.0
1365_0100282	2.0	2.0
1365_0100285	2.0	2.0
1365_0100286	2.0	2.0
1365_0100289	2.0	2.0
1365_0100451	2.0	2.0
1365_0100456	2.0	2.0
1365_0100458	2.0	2.0
1365_0100472	2.0	2.0
1365_0100473	2.0	2.0
1365_0100474	2.0	2.0
1365_0100476	2.0	2.0
1365_0100480	2.0	2.0
1365_0100481	2.0	2.0
1385_0000016	1.0	1.0
1385_0000020	1.0	1.0
1385_0000039	1.0	1.0
1385_0000043	1.0	1.0
1385_0000045	2.0	1.0
1385_0000050	1.0	1.0
1385_0000102	2.0	1.0
1385_0000114	2.0	1.0
1385_0000123	1.0	1.0
1385_0001105	1.0	1.0
1385_0001119	2.0	1.0
1385_0001122	2.0	1.0
1385_0001124	1.0	1.0
1385_0001137	1.0	1.0
1385_0001151	2.0	1.0
1385_0001153	2.0	1.0
1385_0001154	1.0	1.0
1385_0001169	1.0	1.0
1385_0001190	1.0	1.0
1385_0001191	1.0	1.0
1385_0001194	1.0	1.0
1385_0001196	1.0	1.0
1385_0001523	1.0	1.0
1385_0001525	1.0	1.0
1385_0001716	1.0	1.0
1385_0001727	0.0	1.0
1385_0001728	1.0	1.0
1385_0001729	1.0	2.0
1385_0001736	1.0	2.0
1385_0001741	0.0	1.0
1385_0001744	0.0	1.0
1385_0001750	0.0	1.0
1385_0001752	1.0	1.0
1385_0001758	1.0	1.0
1385_0001760	1.0	1.0
1385_0001772	1.0	1.0
1385_0001788	1.0	1.0
1385_0001791	1.0	1.0
1385_0001792	1.0	1.0
1385_0001793	1.0	1.0
1385_0001799	2.0	1.0
1385_0001800	1.0	1.0
1395_0000340	2.0	2.0
1395_0000356	1.0	1.0
1395_0000357	3.0	2.0
1395_0000379	2.0	2.0
1395_0000380	2.0	2.0
1395_0000387	3.0	2.0
1395_0000390	1.0	1.0
1395_0000396	2.0	2.0
1395_0000398	2.0	2.0
1395_0000409	2.0	2.0
1395_0000415	2.0	2.0
1395_0000432	2.0	2.0
1395_0000458	1.0	2.0
1395_0000470	2.0	1.0
1395_0000471	2.0	2.0
1395_0000516	1.0	1.0
1395_0000527	1.0	1.0
1395_0000533	2.0	2.0
1395_0000548	2.0	2.0
1395_0000552	2.0	2.0
1395_0000591	0.0	1.0
1395_0000604	0.0	1.0
1395_0000610	2.0	1.0
1395_0000627	1.0	2.0
1395_0000639	1.0	2.0
1395_0001019	1.0	1.0
1395_0001020	1.0	1.0
1395_0001024	2.0	2.0
1395_0001033	1.0	2.0
1395_0001067	1.0	2.0
1395_0001071	2.0	1.0
1395_0001074	1.0	1.0
1395_0001078	0.0	2.0
1395_0001090	1.0	2.0
1395_0001104	1.0	1.0
1395_0001114	0.0	1.0
1395_0001147	2.0	2.0
1395_0001149	1.0	1.0
1395_0001158	1.0	2.0
1395_0001164	2.0	2.0
1395_0001171	1.0	1.0
LANGUAGE: IT, 4th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.19
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.19
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.71      0.71      0.71        48
         2.0       0.56      0.08      0.13        66
         3.0       0.35      0.97      0.51        37
         4.0       0.00      0.00      0.00         1

    accuracy                           0.47       160
   macro avg       0.32      0.35      0.27       160
weighted avg       0.52      0.47      0.39       160

[[ 0  7  0  1  0]
 [ 0 34  3 11  0]
 [ 0  7  5 54  0]
 [ 0  0  1 36  0]
 [ 0  0  0  1  0]]
0.38642857142857145
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.85
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.75
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.68      0.81      0.74        48
         2.0       0.72      0.64      0.68        66
         3.0       0.67      0.81      0.73        37
         4.0       0.00      0.00      0.00         1

    accuracy                           0.69       160
   macro avg       0.42      0.45      0.43       160
weighted avg       0.66      0.69      0.67       160

[[ 0  7  1  0  0]
 [ 0 39  9  0  0]
 [ 0 10 42 14  0]
 [ 0  1  6 30  0]
 [ 0  0  0  1  0]]
0.6714999438012814
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.71
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.74      0.88      0.80        48
         2.0       0.76      0.47      0.58        66
         3.0       0.55      0.92      0.69        37
         4.0       0.00      0.00      0.00         1

    accuracy                           0.67       160
   macro avg       0.41      0.45      0.41       160
weighted avg       0.66      0.67      0.64       160

[[ 0  7  1  0  0]
 [ 0 42  6  0  0]
 [ 0  8 31 27  0]
 [ 0  0  3 34  0]
 [ 0  0  0  1  0]]
0.637857075427169
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.65
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.68
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.71      0.92      0.80        48
         2.0       0.78      0.64      0.70        66
         3.0       0.68      0.81      0.74        37
         4.0       0.00      0.00      0.00         1

    accuracy                           0.73       160
   macro avg       0.43      0.47      0.45       160
weighted avg       0.69      0.72      0.70       160

[[ 0  7  1  0  0]
 [ 0 44  4  0  0]
 [ 0 11 42 13  0]
 [ 0  0  7 30  0]
 [ 0  0  0  1  0]]
0.7000462962962962
160 160 160
Filename	True Label	Prediction
1325_1001011	3.0	3.0
1325_1001020	3.0	3.0
1325_1001023	3.0	2.0
1325_1001032	3.0	3.0
1325_1001035	3.0	3.0
1325_1001053	2.0	2.0
1325_1001056	3.0	3.0
1325_1001063	2.0	3.0
1325_1001092	2.0	3.0
1325_1001093	2.0	3.0
1325_1001094	2.0	3.0
1325_1001098	3.0	3.0
1325_1001107	3.0	3.0
1325_1001109	2.0	3.0
1325_1001111	3.0	3.0
1325_1001126	2.0	3.0
1325_1001129	2.0	3.0
1325_1001134	2.0	3.0
1325_1001142	3.0	3.0
1325_1001156	3.0	3.0
1325_1001168	3.0	3.0
1325_1001169	3.0	3.0
1325_1001170	3.0	3.0
1325_9000102	3.0	2.0
1325_9000104	3.0	3.0
1325_9000106	3.0	3.0
1325_9000136	3.0	3.0
1325_9000214	3.0	3.0
1325_9000237	3.0	3.0
1325_9000239	3.0	3.0
1325_9000241	3.0	3.0
1325_9000279	3.0	3.0
1325_9000316	3.0	3.0
1325_9000319	3.0	3.0
1325_9000321	3.0	3.0
1325_9000323	3.0	3.0
1325_9000505	3.0	3.0
1325_9000602	4.0	3.0
1325_9000677	3.0	3.0
1325_9000684	3.0	3.0
1325_9000750	3.0	2.0
1365_0100004	2.0	2.0
1365_0100005	2.0	2.0
1365_0100015	2.0	2.0
1365_0100022	2.0	2.0
1365_0100023	2.0	2.0
1365_0100067	2.0	2.0
1365_0100070	2.0	2.0
1365_0100073	2.0	2.0
1365_0100094	2.0	2.0
1365_0100106	2.0	3.0
1365_0100107	2.0	3.0
1365_0100119	3.0	3.0
1365_0100123	2.0	2.0
1365_0100145	2.0	2.0
1365_0100162	2.0	2.0
1365_0100173	2.0	2.0
1365_0100174	2.0	2.0
1365_0100175	2.0	2.0
1365_0100180	2.0	2.0
1365_0100181	2.0	2.0
1365_0100182	2.0	2.0
1365_0100184	2.0	2.0
1365_0100191	2.0	2.0
1365_0100199	2.0	2.0
1365_0100201	2.0	2.0
1365_0100202	2.0	2.0
1365_0100218	2.0	2.0
1365_0100221	2.0	2.0
1365_0100223	2.0	3.0
1365_0100229	2.0	2.0
1365_0100263	3.0	2.0
1365_0100265	2.0	2.0
1365_0100274	2.0	3.0
1365_0100275	3.0	2.0
1365_0100277	3.0	3.0
1365_0100288	2.0	2.0
1365_0100447	2.0	2.0
1365_0100455	2.0	3.0
1365_0100459	3.0	2.0
1385_0000023	1.0	1.0
1385_0000044	2.0	1.0
1385_0000049	1.0	1.0
1385_0000052	1.0	1.0
1385_0000057	1.0	1.0
1385_0000059	1.0	1.0
1385_0000095	1.0	1.0
1385_0000098	2.0	1.0
1385_0000104	2.0	1.0
1385_0000122	1.0	1.0
1385_0000124	2.0	1.0
1385_0000125	2.0	1.0
1385_0000128	1.0	1.0
1385_0001104	1.0	1.0
1385_0001108	1.0	1.0
1385_0001113	1.0	1.0
1385_0001121	2.0	1.0
1385_0001123	2.0	1.0
1385_0001131	1.0	1.0
1385_0001132	1.0	1.0
1385_0001136	1.0	1.0
1385_0001150	1.0	1.0
1385_0001152	2.0	1.0
1385_0001155	1.0	1.0
1385_0001156	1.0	1.0
1385_0001158	1.0	1.0
1385_0001163	1.0	1.0
1385_0001173	0.0	1.0
1385_0001175	0.0	1.0
1385_0001198	1.0	1.0
1385_0001528	1.0	1.0
1385_0001717	1.0	1.0
1385_0001723	0.0	1.0
1385_0001725	1.0	1.0
1385_0001726	1.0	1.0
1385_0001730	1.0	1.0
1385_0001737	1.0	1.0
1385_0001738	0.0	1.0
1385_0001749	1.0	1.0
1385_0001756	1.0	1.0
1385_0001762	1.0	1.0
1385_0001765	0.0	1.0
1385_0001773	0.0	1.0
1385_0001774	0.0	1.0
1385_0001789	1.0	1.0
1385_0001796	1.0	1.0
1395_0000360	3.0	2.0
1395_0000383	2.0	2.0
1395_0000403	2.0	2.0
1395_0000414	2.0	2.0
1395_0000447	2.0	2.0
1395_0000451	2.0	2.0
1395_0000452	1.0	1.0
1395_0000518	2.0	2.0
1395_0000528	2.0	1.0
1395_0000531	2.0	1.0
1395_0000535	1.0	1.0
1395_0000547	2.0	2.0
1395_0000572	1.0	1.0
1395_0000575	1.0	1.0
1395_0000579	1.0	1.0
1395_0000599	1.0	1.0
1395_0000612	0.0	2.0
1395_0000626	2.0	2.0
1395_0000630	1.0	2.0
1395_0000642	1.0	1.0
1395_0000649	1.0	2.0
1395_0001015	1.0	2.0
1395_0001021	1.0	1.0
1395_0001061	2.0	2.0
1395_0001073	1.0	2.0
1395_0001075	1.0	1.0
1395_0001084	1.0	1.0
1395_0001093	1.0	1.0
1395_0001101	2.0	1.0
1395_0001115	2.0	2.0
1395_0001117	1.0	1.0
1395_0001118	1.0	1.0
1395_0001132	2.0	2.0
1395_0001169	2.0	2.0
Averaged weighted F1-scores 0.6813533056048126
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']
LANGUAGE: IT, 0th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.24
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.97
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.64      0.51      0.57        41
         2.0       0.51      0.80      0.63        65
         3.0       0.73      0.47      0.58        40

    accuracy                           0.57       160
   macro avg       0.47      0.45      0.44       160
weighted avg       0.55      0.57      0.54       160

[[ 0  6  8  0]
 [ 0 21 20  0]
 [ 0  6 52  7]
 [ 0  0 21 19]]
0.5438966554177398
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.98
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.90
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.58      0.73      0.65        41
         2.0       0.55      0.63      0.59        65
         3.0       0.61      0.50      0.55        40

    accuracy                           0.57       160
   macro avg       0.43      0.47      0.44       160
weighted avg       0.52      0.57      0.54       160

[[ 0 10  4  0]
 [ 0 30 10  1]
 [ 0 12 41 12]
 [ 0  0 20 20]]
0.5402553105864529
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.88
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.93
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.54      0.88      0.67        41
         2.0       0.67      0.31      0.42        65
         3.0       0.57      0.90      0.70        40

    accuracy                           0.57       160
   macro avg       0.44      0.52      0.45       160
weighted avg       0.55      0.57      0.52       160

[[ 0 12  2  0]
 [ 0 36  4  1]
 [ 0 19 20 26]
 [ 0  0  4 36]]
0.5166432464656787
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.82
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.86
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.60      0.78      0.68        41
         2.0       0.61      0.57      0.59        65
         3.0       0.65      0.75      0.70        40

    accuracy                           0.62       160
   macro avg       0.47      0.52      0.49       160
weighted avg       0.56      0.62      0.59       160

[[ 0  9  5  0]
 [ 0 32  9  0]
 [ 0 12 37 16]
 [ 0  0 10 30]]
0.5874779595988157
160 160 160
Filename	True Label	Prediction
1325_1001016	2.0	3.0
1325_1001018	2.0	3.0
1325_1001022	3.0	3.0
1325_1001024	2.0	3.0
1325_1001025	2.0	3.0
1325_1001036	3.0	3.0
1325_1001040	3.0	3.0
1325_1001043	3.0	3.0
1325_1001054	3.0	2.0
1325_1001057	2.0	3.0
1325_1001062	3.0	3.0
1325_1001095	2.0	3.0
1325_1001100	3.0	3.0
1325_1001101	3.0	3.0
1325_1001111	3.0	3.0
1325_1001113	3.0	3.0
1325_1001126	2.0	2.0
1325_1001128	3.0	3.0
1325_1001135	3.0	3.0
1325_1001136	2.0	2.0
1325_1001153	3.0	3.0
1325_1001160	3.0	3.0
1325_1001169	3.0	3.0
1325_9000087	2.0	3.0
1325_9000137	3.0	3.0
1325_9000139	3.0	2.0
1325_9000143	3.0	3.0
1325_9000185	3.0	2.0
1325_9000214	3.0	3.0
1325_9000241	3.0	3.0
1325_9000296	2.0	3.0
1325_9000322	3.0	3.0
1325_9000503	3.0	3.0
1325_9000533	3.0	3.0
1365_0100003	2.0	2.0
1365_0100005	1.0	2.0
1365_0100008	2.0	2.0
1365_0100010	2.0	2.0
1365_0100015	1.0	2.0
1365_0100016	2.0	2.0
1365_0100018	2.0	2.0
1365_0100026	2.0	2.0
1365_0100027	3.0	2.0
1365_0100028	2.0	2.0
1365_0100031	2.0	2.0
1365_0100051	2.0	2.0
1365_0100058	3.0	3.0
1365_0100061	3.0	3.0
1365_0100074	2.0	3.0
1365_0100107	3.0	3.0
1365_0100118	2.0	3.0
1365_0100123	2.0	2.0
1365_0100125	3.0	2.0
1365_0100165	3.0	3.0
1365_0100178	2.0	2.0
1365_0100181	2.0	2.0
1365_0100182	2.0	2.0
1365_0100185	2.0	2.0
1365_0100194	3.0	3.0
1365_0100205	2.0	2.0
1365_0100217	3.0	3.0
1365_0100218	3.0	2.0
1365_0100220	3.0	3.0
1365_0100221	2.0	2.0
1365_0100231	2.0	3.0
1365_0100232	2.0	3.0
1365_0100251	3.0	3.0
1365_0100252	3.0	2.0
1365_0100259	2.0	2.0
1365_0100267	2.0	3.0
1365_0100268	2.0	2.0
1365_0100270	2.0	3.0
1365_0100289	2.0	2.0
1365_0100447	3.0	2.0
1365_0100457	3.0	3.0
1365_0100458	2.0	3.0
1365_0100461	3.0	2.0
1365_0100469	2.0	2.0
1365_0100477	2.0	3.0
1385_0000016	1.0	1.0
1385_0000017	1.0	1.0
1385_0000022	1.0	1.0
1385_0000036	1.0	1.0
1385_0000038	1.0	1.0
1385_0000039	1.0	1.0
1385_0000043	1.0	1.0
1385_0000047	1.0	2.0
1385_0000048	1.0	1.0
1385_0000053	2.0	1.0
1385_0000057	1.0	1.0
1385_0000098	2.0	1.0
1385_0000103	2.0	1.0
1385_0000114	2.0	1.0
1385_0000119	1.0	1.0
1385_0001104	1.0	1.0
1385_0001109	2.0	1.0
1385_0001119	2.0	1.0
1385_0001125	2.0	1.0
1385_0001126	0.0	1.0
1385_0001151	2.0	1.0
1385_0001152	2.0	2.0
1385_0001158	1.0	1.0
1385_0001159	1.0	1.0
1385_0001166	1.0	1.0
1385_0001167	1.0	1.0
1385_0001174	0.0	1.0
1385_0001191	1.0	1.0
1385_0001522	1.0	1.0
1385_0001524	1.0	1.0
1385_0001716	1.0	1.0
1385_0001719	1.0	1.0
1385_0001720	0.0	1.0
1385_0001724	1.0	1.0
1385_0001736	1.0	2.0
1385_0001742	0.0	1.0
1385_0001744	0.0	1.0
1385_0001750	0.0	1.0
1385_0001752	1.0	1.0
1385_0001766	2.0	1.0
1385_0001771	1.0	1.0
1385_0001772	1.0	1.0
1385_0001795	0.0	1.0
1385_0001799	1.0	1.0
1395_0000337	1.0	1.0
1395_0000340	2.0	2.0
1395_0000355	2.0	2.0
1395_0000379	2.0	2.0
1395_0000389	1.0	1.0
1395_0000413	2.0	2.0
1395_0000432	2.0	2.0
1395_0000438	2.0	2.0
1395_0000447	2.0	2.0
1395_0000460	1.0	1.0
1395_0000512	2.0	2.0
1395_0000514	3.0	2.0
1395_0000529	2.0	1.0
1395_0000537	2.0	2.0
1395_0000550	2.0	2.0
1395_0000552	2.0	2.0
1395_0000554	2.0	1.0
1395_0000560	2.0	1.0
1395_0000579	1.0	1.0
1395_0000595	0.0	1.0
1395_0000604	0.0	1.0
1395_0000626	1.0	2.0
1395_0000630	0.0	2.0
1395_0000636	0.0	2.0
1395_0001010	1.0	1.0
1395_0001022	1.0	2.0
1395_0001033	1.0	2.0
1395_0001060	2.0	2.0
1395_0001067	0.0	2.0
1395_0001074	1.0	1.0
1395_0001076	0.0	2.0
1395_0001080	2.0	2.0
1395_0001109	0.0	2.0
1395_0001119	2.0	2.0
1395_0001133	1.0	2.0
1395_0001147	1.0	2.0
1395_0001167	1.0	1.0
LANGUAGE: IT, 1th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.32
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.01
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.52      0.85      0.65        41
         2.0       0.62      0.52      0.56        66
         3.0       0.63      0.62      0.62        39

    accuracy                           0.58       160
   macro avg       0.44      0.50      0.46       160
weighted avg       0.54      0.58      0.55       160

[[ 0 13  1  0]
 [ 0 35  6  0]
 [ 0 18 34 14]
 [ 0  1 14 24]]
0.5498541967291968
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.00
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.88
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.50      0.68      0.58        41
         2.0       0.58      0.52      0.54        66
         3.0       0.62      0.72      0.67        39

    accuracy                           0.56       160
   macro avg       0.42      0.48      0.45       160
weighted avg       0.52      0.56      0.53       160

[[ 0 13  1  0]
 [ 0 28 13  0]
 [ 0 15 34 17]
 [ 0  0 11 28]]
0.5348381443298968
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.93
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.86
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.51      0.76      0.61        41
         2.0       0.60      0.45      0.52        66
         3.0       0.61      0.77      0.68        39

    accuracy                           0.57       160
   macro avg       0.43      0.49      0.45       160
weighted avg       0.53      0.57      0.54       160

[[ 0 13  1  0]
 [ 0 31 10  0]
 [ 0 17 30 19]
 [ 0  0  9 30]]
0.5353150547052677
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.85
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.85
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.50      0.80      0.62        41
         2.0       0.67      0.39      0.50        66
         3.0       0.62      0.87      0.72        39

    accuracy                           0.58       160
   macro avg       0.45      0.52      0.46       160
weighted avg       0.55      0.58      0.54       160

[[ 0 14  0  0]
 [ 0 33  8  0]
 [ 0 19 26 21]
 [ 0  0  5 34]]
0.5386762491833083
160 160 160
Filename	True Label	Prediction
1325_1001009	3.0	3.0
1325_1001013	3.0	3.0
1325_1001020	2.0	3.0
1325_1001027	3.0	3.0
1325_1001028	3.0	3.0
1325_1001029	3.0	3.0
1325_1001032	2.0	3.0
1325_1001041	3.0	3.0
1325_1001047	2.0	3.0
1325_1001077	3.0	3.0
1325_1001080	3.0	3.0
1325_1001082	2.0	3.0
1325_1001083	2.0	3.0
1325_1001084	3.0	2.0
1325_1001087	3.0	3.0
1325_1001094	2.0	3.0
1325_1001122	2.0	3.0
1325_1001130	3.0	3.0
1325_1001132	3.0	3.0
1325_1001139	3.0	3.0
1325_1001141	2.0	3.0
1325_1001143	3.0	3.0
1325_1001166	3.0	3.0
1325_1001168	2.0	3.0
1325_9000059	2.0	3.0
1325_9000089	3.0	3.0
1325_9000090	3.0	3.0
1325_9000099	3.0	3.0
1325_9000140	3.0	3.0
1325_9000152	3.0	3.0
1325_9000210	2.0	3.0
1325_9000213	3.0	3.0
1325_9000302	2.0	3.0
1325_9000303	2.0	3.0
1325_9000304	3.0	3.0
1325_9000316	3.0	3.0
1325_9000320	3.0	3.0
1325_9000321	3.0	3.0
1325_9000554	2.0	3.0
1325_9000674	3.0	3.0
1325_9000677	3.0	3.0
1325_9000684	3.0	3.0
1365_0100006	2.0	2.0
1365_0100009	2.0	2.0
1365_0100019	2.0	2.0
1365_0100021	2.0	2.0
1365_0100030	2.0	2.0
1365_0100056	2.0	2.0
1365_0100080	2.0	2.0
1365_0100097	2.0	2.0
1365_0100104	2.0	3.0
1365_0100119	3.0	3.0
1365_0100120	3.0	3.0
1365_0100121	3.0	3.0
1365_0100133	2.0	2.0
1365_0100134	2.0	2.0
1365_0100135	2.0	2.0
1365_0100138	2.0	2.0
1365_0100145	3.0	3.0
1365_0100175	2.0	2.0
1365_0100184	2.0	2.0
1365_0100192	3.0	3.0
1365_0100198	2.0	2.0
1365_0100223	2.0	3.0
1365_0100225	3.0	2.0
1365_0100228	2.0	3.0
1365_0100255	2.0	2.0
1365_0100256	2.0	2.0
1365_0100257	2.0	2.0
1365_0100261	2.0	2.0
1365_0100262	3.0	2.0
1365_0100263	3.0	3.0
1365_0100269	2.0	2.0
1365_0100276	3.0	3.0
1365_0100282	2.0	2.0
1365_0100471	2.0	3.0
1365_0100475	2.0	3.0
1365_0100478	2.0	3.0
1365_0100479	2.0	3.0
1385_0000021	1.0	1.0
1385_0000034	1.0	1.0
1385_0000035	1.0	2.0
1385_0000044	2.0	1.0
1385_0000051	2.0	1.0
1385_0000101	1.0	2.0
1385_0000104	2.0	1.0
1385_0000122	1.0	1.0
1385_0000124	1.0	1.0
1385_0001110	2.0	1.0
1385_0001124	2.0	1.0
1385_0001127	2.0	1.0
1385_0001129	1.0	1.0
1385_0001131	1.0	1.0
1385_0001134	1.0	1.0
1385_0001156	1.0	1.0
1385_0001157	1.0	2.0
1385_0001162	1.0	1.0
1385_0001169	1.0	1.0
1385_0001171	0.0	1.0
1385_0001178	0.0	1.0
1385_0001189	0.0	1.0
1385_0001194	1.0	1.0
1385_0001196	1.0	1.0
1385_0001527	2.0	1.0
1385_0001712	1.0	1.0
1385_0001727	0.0	1.0
1385_0001734	1.0	1.0
1385_0001737	2.0	1.0
1385_0001747	1.0	1.0
1385_0001756	2.0	1.0
1385_0001757	1.0	1.0
1385_0001761	0.0	1.0
1385_0001765	0.0	1.0
1385_0001767	0.0	1.0
1385_0001775	0.0	1.0
1385_0001789	1.0	1.0
1385_0001793	0.0	1.0
1385_0001794	1.0	1.0
1395_0000333	1.0	1.0
1395_0000341	2.0	1.0
1395_0000354	1.0	1.0
1395_0000359	2.0	1.0
1395_0000369	2.0	1.0
1395_0000391	3.0	2.0
1395_0000446	2.0	2.0
1395_0000450	1.0	1.0
1395_0000462	2.0	1.0
1395_0000469	1.0	1.0
1395_0000499	1.0	1.0
1395_0000500	2.0	1.0
1395_0000515	2.0	1.0
1395_0000533	3.0	2.0
1395_0000548	2.0	2.0
1395_0000551	2.0	1.0
1395_0000553	2.0	1.0
1395_0000555	1.0	1.0
1395_0000557	2.0	2.0
1395_0000559	2.0	1.0
1395_0000564	1.0	1.0
1395_0000583	1.0	2.0
1395_0000585	0.0	1.0
1395_0000591	0.0	1.0
1395_0000596	2.0	1.0
1395_0000597	1.0	2.0
1395_0000606	0.0	1.0
1395_0000607	0.0	1.0
1395_0001019	1.0	1.0
1395_0001020	1.0	1.0
1395_0001021	1.0	1.0
1395_0001024	1.0	1.0
1395_0001028	1.0	2.0
1395_0001058	1.0	1.0
1395_0001065	1.0	2.0
1395_0001069	2.0	2.0
1395_0001073	2.0	2.0
1395_0001078	1.0	1.0
1395_0001093	1.0	1.0
1395_0001104	0.0	1.0
1395_0001145	1.0	2.0
1395_0001161	1.0	1.0
LANGUAGE: IT, 2th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.32
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.07
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.53      0.50      0.51        40
         2.0       0.49      0.47      0.48        66
         3.0       0.51      0.77      0.61        39
         4.0       0.00      0.00      0.00         1

    accuracy                           0.51       160
   macro avg       0.31      0.35      0.32       160
weighted avg       0.46      0.51      0.48       160

[[ 0 10  4  0  0]
 [ 0 20 19  1  0]
 [ 0  8 31 27  0]
 [ 0  0  9 30  0]
 [ 0  0  0  1  0]]
0.4756956360361676
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.96
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.98
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.52      0.70      0.60        40
         2.0       0.53      0.47      0.50        66
         3.0       0.55      0.67      0.60        39
         4.0       0.00      0.00      0.00         1

    accuracy                           0.53       160
   macro avg       0.32      0.37      0.34       160
weighted avg       0.48      0.53      0.50       160

[[ 0 11  3  0  0]
 [ 0 28 12  0  0]
 [ 0 15 31 20  0]
 [ 0  0 13 26  0]
 [ 0  0  0  1  0]]
0.5009198911429985
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.87
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.98
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.55      0.68      0.61        40
         2.0       0.54      0.53      0.53        66
         3.0       0.54      0.64      0.59        39
         4.0       0.00      0.00      0.00         1

    accuracy                           0.54       160
   macro avg       0.33      0.37      0.35       160
weighted avg       0.49      0.54      0.52       160

[[ 0 11  3  0  0]
 [ 0 27 13  0  0]
 [ 0 11 35 20  0]
 [ 0  0 14 25  0]
 [ 0  0  0  1  0]]
0.5154875935278478
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.78
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.98
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.54      0.75      0.63        40
         2.0       0.56      0.53      0.55        66
         3.0       0.57      0.62      0.59        39
         4.0       0.00      0.00      0.00         1

    accuracy                           0.56       160
   macro avg       0.33      0.38      0.35       160
weighted avg       0.51      0.56      0.53       160

[[ 0 12  2  0  0]
 [ 0 30 10  0  0]
 [ 0 14 35 17  0]
 [ 0  0 15 24  0]
 [ 0  0  0  1  0]]
0.5262803819444446
160 160 160
Filename	True Label	Prediction
1325_1001008	3.0	3.0
1325_1001012	3.0	3.0
1325_1001021	3.0	3.0
1325_1001033	3.0	3.0
1325_1001037	3.0	2.0
1325_1001042	3.0	3.0
1325_1001044	3.0	2.0
1325_1001045	3.0	3.0
1325_1001048	2.0	3.0
1325_1001056	2.0	3.0
1325_1001058	3.0	3.0
1325_1001078	3.0	3.0
1325_1001079	3.0	3.0
1325_1001089	2.0	2.0
1325_1001090	3.0	3.0
1325_1001096	2.0	3.0
1325_1001097	1.0	2.0
1325_1001098	2.0	3.0
1325_1001110	3.0	3.0
1325_1001119	3.0	3.0
1325_1001124	3.0	2.0
1325_1001125	3.0	3.0
1325_1001129	2.0	3.0
1325_1001142	2.0	3.0
1325_1001144	3.0	3.0
1325_1001152	3.0	3.0
1325_1001156	2.0	3.0
1325_1001157	2.0	3.0
1325_1001162	2.0	3.0
1325_9000088	2.0	3.0
1325_9000102	3.0	2.0
1325_9000105	2.0	2.0
1325_9000138	4.0	3.0
1325_9000144	3.0	3.0
1325_9000240	2.0	3.0
1325_9000504	3.0	3.0
1325_9000505	3.0	3.0
1325_9000534	3.0	3.0
1325_9000611	2.0	3.0
1325_9000612	2.0	3.0
1325_9000750	3.0	2.0
1365_0100002	3.0	2.0
1365_0100011	2.0	2.0
1365_0100013	2.0	3.0
1365_0100020	2.0	2.0
1365_0100022	2.0	2.0
1365_0100024	2.0	2.0
1365_0100065	1.0	2.0
1365_0100070	2.0	2.0
1365_0100072	2.0	2.0
1365_0100092	2.0	2.0
1365_0100094	2.0	2.0
1365_0100096	2.0	2.0
1365_0100100	3.0	3.0
1365_0100101	2.0	2.0
1365_0100102	2.0	2.0
1365_0100105	3.0	2.0
1365_0100117	3.0	2.0
1365_0100146	3.0	2.0
1365_0100147	3.0	2.0
1365_0100148	3.0	2.0
1365_0100163	3.0	3.0
1365_0100168	3.0	2.0
1365_0100169	2.0	2.0
1365_0100170	2.0	2.0
1365_0100171	2.0	2.0
1365_0100177	3.0	2.0
1365_0100179	2.0	2.0
1365_0100186	2.0	2.0
1365_0100188	2.0	2.0
1365_0100199	2.0	2.0
1365_0100200	3.0	2.0
1365_0100201	2.0	2.0
1365_0100211	3.0	3.0
1365_0100219	2.0	3.0
1365_0100230	2.0	3.0
1365_0100253	2.0	2.0
1365_0100260	2.0	2.0
1365_0100275	3.0	3.0
1365_0100277	3.0	3.0
1365_0100285	2.0	2.0
1365_0100480	2.0	3.0
1385_0000011	1.0	1.0
1385_0000037	1.0	1.0
1385_0000050	1.0	1.0
1385_0000054	2.0	1.0
1385_0000128	1.0	1.0
1385_0001105	1.0	1.0
1385_0001108	2.0	1.0
1385_0001111	2.0	1.0
1385_0001112	2.0	2.0
1385_0001113	1.0	1.0
1385_0001123	2.0	1.0
1385_0001130	1.0	1.0
1385_0001132	1.0	1.0
1385_0001135	1.0	1.0
1385_0001136	1.0	1.0
1385_0001149	2.0	1.0
1385_0001153	2.0	1.0
1385_0001161	1.0	1.0
1385_0001163	1.0	1.0
1385_0001165	1.0	1.0
1385_0001173	0.0	1.0
1385_0001193	1.0	1.0
1385_0001525	1.0	1.0
1385_0001714	1.0	1.0
1385_0001715	0.0	1.0
1385_0001718	0.0	1.0
1385_0001728	1.0	2.0
1385_0001732	0.0	1.0
1385_0001740	1.0	1.0
1385_0001741	0.0	1.0
1385_0001748	1.0	2.0
1385_0001759	0.0	1.0
1385_0001762	2.0	1.0
1385_0001768	2.0	1.0
1385_0001773	0.0	1.0
1385_0001774	0.0	1.0
1385_0001791	0.0	1.0
1385_0001796	1.0	1.0
1395_0000338	2.0	2.0
1395_0000353	1.0	1.0
1395_0000356	1.0	1.0
1395_0000376	2.0	2.0
1395_0000380	2.0	1.0
1395_0000387	3.0	2.0
1395_0000388	2.0	2.0
1395_0000396	1.0	2.0
1395_0000398	2.0	2.0
1395_0000409	2.0	2.0
1395_0000448	2.0	1.0
1395_0000449	2.0	2.0
1395_0000451	1.0	2.0
1395_0000452	1.0	1.0
1395_0000458	2.0	2.0
1395_0000470	2.0	1.0
1395_0000471	2.0	1.0
1395_0000528	2.0	2.0
1395_0000547	2.0	1.0
1395_0000572	1.0	1.0
1395_0000593	0.0	1.0
1395_0000598	0.0	2.0
1395_0000599	1.0	1.0
1395_0000602	1.0	1.0
1395_0000609	1.0	1.0
1395_0000612	0.0	2.0
1395_0000635	0.0	1.0
1395_0000642	1.0	1.0
1395_0000646	1.0	1.0
1395_0000649	1.0	2.0
1395_0001016	1.0	1.0
1395_0001034	1.0	1.0
1395_0001045	2.0	1.0
1395_0001061	2.0	2.0
1395_0001066	1.0	2.0
1395_0001103	1.0	2.0
1395_0001118	0.0	1.0
1395_0001132	2.0	2.0
1395_0001149	1.0	1.0
1395_0001160	1.0	2.0
LANGUAGE: IT, 3th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.30
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.54      0.88      0.67        41
         2.0       0.59      0.71      0.64        65
         3.0       0.87      0.33      0.48        39
         4.0       0.00      0.00      0.00         1

    accuracy                           0.59       160
   macro avg       0.40      0.38      0.36       160
weighted avg       0.59      0.59      0.55       160

[[ 0 11  3  0  0]
 [ 0 36  5  0  0]
 [ 0 18 46  1  0]
 [ 0  2 24 13  0]
 [ 0  0  0  1  0]]
0.5495580808080807
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.98
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.89
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.51      0.95      0.67        41
         2.0       0.79      0.40      0.53        65
         3.0       0.63      0.82      0.71        39
         4.0       0.00      0.00      0.00         1

    accuracy                           0.61       160
   macro avg       0.39      0.43      0.38       160
weighted avg       0.60      0.61      0.56       160

[[ 0 14  0  0  0]
 [ 0 39  2  0  0]
 [ 0 21 26 18  0]
 [ 0  2  5 32  0]
 [ 0  0  0  1  0]]
0.5597278911564626
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.88
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.86
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.55      0.90      0.69        41
         2.0       0.66      0.51      0.57        65
         3.0       0.65      0.72      0.68        39
         4.0       0.00      0.00      0.00         1

    accuracy                           0.61       160
   macro avg       0.37      0.43      0.39       160
weighted avg       0.57      0.61      0.58       160

[[ 0 11  3  0  0]
 [ 0 37  4  0  0]
 [ 0 18 33 14  0]
 [ 0  1 10 28  0]
 [ 0  0  0  1  0]]
0.5751942922508936
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.81
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.87
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.56      0.88      0.69        41
         2.0       0.65      0.51      0.57        65
         3.0       0.64      0.74      0.69        39
         4.0       0.00      0.00      0.00         1

    accuracy                           0.61       160
   macro avg       0.37      0.43      0.39       160
weighted avg       0.56      0.61      0.58       160

[[ 0 11  3  0  0]
 [ 0 36  5  0  0]
 [ 0 17 33 15  0]
 [ 0  0 10 29  0]
 [ 0  0  0  1  0]]
0.5751600985221675
160 160 160
Filename	True Label	Prediction
1325_1001010	3.0	3.0
1325_1001011	3.0	3.0
1325_1001015	2.0	3.0
1325_1001023	3.0	2.0
1325_1001035	3.0	3.0
1325_1001039	3.0	3.0
1325_1001050	3.0	3.0
1325_1001053	2.0	2.0
1325_1001059	2.0	3.0
1325_1001063	2.0	3.0
1325_1001085	3.0	3.0
1325_1001086	3.0	3.0
1325_1001088	2.0	3.0
1325_1001107	3.0	3.0
1325_1001108	3.0	3.0
1325_1001109	2.0	3.0
1325_1001120	3.0	3.0
1325_1001127	3.0	3.0
1325_1001133	3.0	3.0
1325_1001134	3.0	3.0
1325_1001138	2.0	3.0
1325_1001154	3.0	3.0
1325_1001155	3.0	3.0
1325_1001161	2.0	3.0
1325_1001170	3.0	3.0
1325_9000095	2.0	3.0
1325_9000104	3.0	3.0
1325_9000106	3.0	2.0
1325_9000187	3.0	3.0
1325_9000278	3.0	3.0
1325_9000279	3.0	3.0
1325_9000314	2.0	3.0
1325_9000315	2.0	2.0
1325_9000317	3.0	3.0
1325_9000318	3.0	3.0
1325_9000536	3.0	3.0
1325_9000685	4.0	3.0
1325_9000686	2.0	3.0
1325_9000700	3.0	3.0
1365_0100004	2.0	2.0
1365_0100007	2.0	2.0
1365_0100023	2.0	2.0
1365_0100057	2.0	3.0
1365_0100069	3.0	2.0
1365_0100071	3.0	3.0
1365_0100093	2.0	2.0
1365_0100095	2.0	2.0
1365_0100099	2.0	2.0
1365_0100136	2.0	2.0
1365_0100137	2.0	2.0
1365_0100167	2.0	2.0
1365_0100180	2.0	2.0
1365_0100187	3.0	2.0
1365_0100202	2.0	2.0
1365_0100203	2.0	2.0
1365_0100212	3.0	3.0
1365_0100226	3.0	2.0
1365_0100227	3.0	2.0
1365_0100229	2.0	3.0
1365_0100258	2.0	2.0
1365_0100274	3.0	3.0
1365_0100278	3.0	2.0
1365_0100279	2.0	2.0
1365_0100286	2.0	2.0
1365_0100287	2.0	2.0
1365_0100299	3.0	2.0
1365_0100448	2.0	2.0
1365_0100451	3.0	3.0
1365_0100459	3.0	3.0
1365_0100472	2.0	2.0
1365_0100474	2.0	3.0
1365_0100476	2.0	3.0
1365_0100481	2.0	3.0
1365_0100482	2.0	2.0
1385_0000013	1.0	1.0
1385_0000023	1.0	1.0
1385_0000033	2.0	1.0
1385_0000041	1.0	1.0
1385_0000045	2.0	1.0
1385_0000049	1.0	1.0
1385_0000052	1.0	1.0
1385_0000059	2.0	1.0
1385_0000095	1.0	1.0
1385_0000097	2.0	1.0
1385_0000102	2.0	1.0
1385_0000126	1.0	1.0
1385_0000127	2.0	1.0
1385_0000130	2.0	1.0
1385_0001118	2.0	1.0
1385_0001121	2.0	1.0
1385_0001133	2.0	1.0
1385_0001138	1.0	1.0
1385_0001148	2.0	1.0
1385_0001154	1.0	1.0
1385_0001155	1.0	1.0
1385_0001160	1.0	2.0
1385_0001164	1.0	1.0
1385_0001172	0.0	1.0
1385_0001192	1.0	1.0
1385_0001195	2.0	2.0
1385_0001197	1.0	1.0
1385_0001501	1.0	1.0
1385_0001717	1.0	1.0
1385_0001723	0.0	1.0
1385_0001725	1.0	1.0
1385_0001729	1.0	2.0
1385_0001730	2.0	1.0
1385_0001733	1.0	1.0
1385_0001746	1.0	1.0
1385_0001749	1.0	1.0
1385_0001751	0.0	1.0
1385_0001753	1.0	1.0
1385_0001758	0.0	1.0
1385_0001760	1.0	1.0
1385_0001785	1.0	1.0
1385_0001787	1.0	1.0
1385_0001788	1.0	1.0
1395_0000360	3.0	2.0
1395_0000361	2.0	2.0
1395_0000364	2.0	1.0
1395_0000365	3.0	2.0
1395_0000368	1.0	1.0
1395_0000378	2.0	2.0
1395_0000390	1.0	1.0
1395_0000392	2.0	1.0
1395_0000403	2.0	2.0
1395_0000404	2.0	2.0
1395_0000415	1.0	1.0
1395_0000454	2.0	2.0
1395_0000455	2.0	1.0
1395_0000516	1.0	1.0
1395_0000525	2.0	1.0
1395_0000534	2.0	2.0
1395_0000549	2.0	2.0
1395_0000556	1.0	1.0
1395_0000563	2.0	2.0
1395_0000575	1.0	1.0
1395_0000581	2.0	2.0
1395_0000582	0.0	1.0
1395_0000608	0.0	1.0
1395_0000611	0.0	1.0
1395_0000627	1.0	2.0
1395_0000628	0.0	2.0
1395_0000639	0.0	2.0
1395_0001015	1.0	2.0
1395_0001023	1.0	1.0
1395_0001040	0.0	1.0
1395_0001064	2.0	2.0
1395_0001068	0.0	2.0
1395_0001101	1.0	1.0
1395_0001114	1.0	1.0
1395_0001115	1.0	1.0
1395_0001117	1.0	1.0
1395_0001124	0.0	1.0
1395_0001126	1.0	1.0
1395_0001131	0.0	1.0
1395_0001141	2.0	1.0
1395_0001146	0.0	1.0
1395_0001164	2.0	2.0
1395_0001170	1.0	2.0
LANGUAGE: IT, 4th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.31
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.02
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.47      0.95      0.63        41
         2.0       0.45      0.54      0.49        65
         3.0       0.00      0.00      0.00        39
         4.0       0.00      0.00      0.00         1

    accuracy                           0.46       160
   macro avg       0.18      0.30      0.22       160
weighted avg       0.31      0.46      0.36       160

[[ 0 14  0  0  0]
 [ 0 39  2  0  0]
 [ 0 30 35  0  0]
 [ 0  0 39  0  0]
 [ 0  0  1  0  0]]
0.3614536006360745
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.00
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.87
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.58      0.83      0.68        41
         2.0       0.70      0.46      0.56        65
         3.0       0.60      0.90      0.72        39
         4.0       0.00      0.00      0.00         1

    accuracy                           0.62       160
   macro avg       0.38      0.44      0.39       160
weighted avg       0.58      0.62      0.58       160

[[ 0 12  2  0  0]
 [ 0 34  7  0  0]
 [ 0 13 30 22  0]
 [ 0  0  4 35  0]
 [ 0  0  0  1  0]]
0.5758465063001145
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.91
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.85
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.53      0.95      0.68        41
         2.0       0.72      0.35      0.47        65
         3.0       0.61      0.85      0.71        39
         4.0       0.00      0.00      0.00         1

    accuracy                           0.59       160
   macro avg       0.37      0.43      0.37       160
weighted avg       0.58      0.59      0.54       160

[[ 0 13  1  0  0]
 [ 0 39  2  0  0]
 [ 0 22 23 20  0]
 [ 0  0  6 33  0]
 [ 0  0  0  1  0]]
0.5394428579690866
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.83
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.84
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.57      0.88      0.69        41
         2.0       0.72      0.48      0.57        65
         3.0       0.61      0.85      0.71        39
         4.0       0.00      0.00      0.00         1

    accuracy                           0.62       160
   macro avg       0.38      0.44      0.40       160
weighted avg       0.59      0.62      0.58       160

[[ 0 13  1  0  0]
 [ 0 36  5  0  0]
 [ 0 14 31 20  0]
 [ 0  0  6 33  0]
 [ 0  0  0  1  0]]
0.5836053097141807
160 160 160
Filename	True Label	Prediction
1325_1001014	3.0	3.0
1325_1001017	3.0	3.0
1325_1001019	3.0	3.0
1325_1001046	2.0	3.0
1325_1001051	3.0	3.0
1325_1001052	2.0	3.0
1325_1001055	3.0	3.0
1325_1001075	2.0	3.0
1325_1001076	2.0	3.0
1325_1001081	2.0	3.0
1325_1001091	2.0	3.0
1325_1001092	2.0	3.0
1325_1001093	2.0	3.0
1325_1001099	3.0	3.0
1325_1001121	3.0	3.0
1325_1001123	3.0	3.0
1325_1001131	3.0	3.0
1325_1001158	3.0	3.0
1325_1001159	3.0	3.0
1325_1001163	2.0	3.0
1325_1001164	3.0	3.0
1325_1001165	2.0	3.0
1325_1001167	3.0	3.0
1325_9000107	3.0	3.0
1325_9000136	3.0	3.0
1325_9000186	3.0	3.0
1325_9000188	3.0	3.0
1325_9000209	3.0	3.0
1325_9000211	2.0	3.0
1325_9000215	3.0	3.0
1325_9000237	3.0	3.0
1325_9000239	3.0	3.0
1325_9000319	2.0	3.0
1325_9000323	2.0	3.0
1325_9000601	3.0	3.0
1325_9000602	4.0	3.0
1325_9000675	3.0	3.0
1325_9000676	3.0	3.0
1325_9000678	3.0	3.0
1365_0100012	2.0	2.0
1365_0100014	2.0	2.0
1365_0100017	2.0	2.0
1365_0100029	1.0	2.0
1365_0100063	3.0	3.0
1365_0100064	3.0	2.0
1365_0100066	2.0	2.0
1365_0100067	2.0	2.0
1365_0100073	3.0	2.0
1365_0100079	2.0	2.0
1365_0100098	2.0	2.0
1365_0100103	3.0	3.0
1365_0100106	2.0	3.0
1365_0100116	3.0	2.0
1365_0100139	2.0	2.0
1365_0100151	2.0	2.0
1365_0100162	3.0	3.0
1365_0100164	3.0	3.0
1365_0100166	2.0	2.0
1365_0100172	2.0	2.0
1365_0100173	2.0	2.0
1365_0100174	2.0	2.0
1365_0100176	2.0	2.0
1365_0100183	2.0	2.0
1365_0100190	3.0	2.0
1365_0100191	2.0	3.0
1365_0100195	2.0	2.0
1365_0100196	2.0	3.0
1365_0100204	2.0	2.0
1365_0100213	2.0	2.0
1365_0100215	2.0	2.0
1365_0100222	2.0	3.0
1365_0100224	3.0	3.0
1365_0100233	3.0	3.0
1365_0100265	3.0	2.0
1365_0100266	2.0	3.0
1365_0100280	1.0	2.0
1365_0100281	2.0	2.0
1365_0100288	2.0	2.0
1365_0100290	2.0	2.0
1365_0100455	3.0	3.0
1365_0100456	3.0	3.0
1365_0100470	2.0	3.0
1365_0100473	2.0	3.0
1385_0000012	1.0	1.0
1385_0000020	1.0	1.0
1385_0000040	1.0	1.0
1385_0000042	1.0	1.0
1385_0000058	1.0	1.0
1385_0000099	1.0	1.0
1385_0000100	2.0	1.0
1385_0000120	0.0	1.0
1385_0000123	1.0	1.0
1385_0000125	2.0	2.0
1385_0000129	2.0	1.0
1385_0001103	2.0	1.0
1385_0001107	2.0	1.0
1385_0001120	2.0	1.0
1385_0001122	2.0	1.0
1385_0001128	1.0	2.0
1385_0001137	2.0	1.0
1385_0001147	1.0	1.0
1385_0001150	1.0	1.0
1385_0001170	0.0	1.0
1385_0001175	0.0	1.0
1385_0001188	0.0	1.0
1385_0001190	0.0	1.0
1385_0001198	1.0	1.0
1385_0001199	1.0	1.0
1385_0001503	1.0	1.0
1385_0001523	1.0	1.0
1385_0001526	0.0	1.0
1385_0001528	1.0	1.0
1385_0001726	1.0	1.0
1385_0001738	0.0	1.0
1385_0001739	1.0	1.0
1385_0001754	1.0	1.0
1385_0001764	1.0	1.0
1385_0001786	1.0	1.0
1385_0001790	1.0	1.0
1385_0001792	1.0	1.0
1385_0001798	1.0	1.0
1385_0001800	1.0	1.0
1395_0000357	3.0	2.0
1395_0000366	2.0	2.0
1395_0000383	2.0	1.0
1395_0000399	2.0	2.0
1395_0000402	1.0	1.0
1395_0000414	2.0	2.0
1395_0000443	2.0	2.0
1395_0000465	1.0	2.0
1395_0000504	2.0	1.0
1395_0000513	2.0	2.0
1395_0000518	2.0	2.0
1395_0000526	1.0	1.0
1395_0000527	1.0	1.0
1395_0000531	2.0	2.0
1395_0000535	1.0	1.0
1395_0000565	2.0	1.0
1395_0000584	0.0	1.0
1395_0000587	0.0	1.0
1395_0000610	2.0	1.0
1395_0000631	0.0	2.0
1395_0000644	1.0	1.0
1395_0001013	1.0	1.0
1395_0001017	1.0	2.0
1395_0001070	2.0	2.0
1395_0001071	2.0	1.0
1395_0001075	0.0	1.0
1395_0001084	1.0	1.0
1395_0001090	1.0	1.0
1395_0001108	0.0	1.0
1395_0001116	2.0	1.0
1395_0001120	1.0	1.0
1395_0001121	0.0	1.0
1395_0001122	0.0	1.0
1395_0001123	1.0	1.0
1395_0001150	1.0	1.0
1395_0001158	1.0	1.0
1395_0001169	2.0	1.0
1395_0001171	1.0	1.0
Averaged weighted F1-scores 0.5622399997925834
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2']
LANGUAGE: IT, 0th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.10
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.83
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        21
         1.0       0.64      0.97      0.77        67
         2.0       0.76      0.75      0.76        60
         3.0       0.00      0.00      0.00        12

    accuracy                           0.69       160
   macro avg       0.35      0.43      0.38       160
weighted avg       0.56      0.69      0.61       160

[[ 0 21  0  0]
 [ 0 65  2  0]
 [ 0 15 45  0]
 [ 0  0 12  0]]
0.6076461834733894
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.89
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        21
         1.0       0.64      0.79      0.71        67
         2.0       0.66      0.85      0.74        60
         3.0       0.00      0.00      0.00        12

    accuracy                           0.65       160
   macro avg       0.33      0.41      0.36       160
weighted avg       0.52      0.65      0.58       160

[[ 0 21  0  0]
 [ 0 53 14  0]
 [ 0  9 51  0]
 [ 0  0 12  0]]
0.5751137469586375
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.80
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.84
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        21
         1.0       0.67      0.70      0.69        67
         2.0       0.64      0.97      0.77        60
         3.0       0.00      0.00      0.00        12

    accuracy                           0.66       160
   macro avg       0.33      0.42      0.36       160
weighted avg       0.52      0.66      0.58       160

[[ 0 21  0  0]
 [ 0 47 20  0]
 [ 0  2 58  0]
 [ 0  0 12  0]]
0.5773175182481752
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.73
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.71
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        21
         1.0       0.66      0.87      0.75        67
         2.0       0.71      0.85      0.77        60
         3.0       0.00      0.00      0.00        12

    accuracy                           0.68       160
   macro avg       0.34      0.43      0.38       160
weighted avg       0.54      0.68      0.60       160

[[ 0 21  0  0]
 [ 0 58  9  0]
 [ 0  9 51  0]
 [ 0  0 12  0]]
0.6031598240469208
160 160 160
Filename	True Label	Prediction
1325_1001009	3.0	2.0
1325_1001025	2.0	2.0
1325_1001027	3.0	2.0
1325_1001035	3.0	2.0
1325_1001042	2.0	2.0
1325_1001043	2.0	2.0
1325_1001056	2.0	2.0
1325_1001075	1.0	2.0
1325_1001076	2.0	2.0
1325_1001077	2.0	2.0
1325_1001083	2.0	2.0
1325_1001089	2.0	2.0
1325_1001107	2.0	2.0
1325_1001108	3.0	2.0
1325_1001111	3.0	2.0
1325_1001119	2.0	2.0
1325_1001125	3.0	2.0
1325_1001130	2.0	2.0
1325_1001139	2.0	2.0
1325_1001143	2.0	2.0
1325_1001156	2.0	2.0
1325_1001157	2.0	2.0
1325_1001159	2.0	2.0
1325_1001160	2.0	2.0
1325_1001170	2.0	2.0
1325_9000090	2.0	2.0
1325_9000106	2.0	2.0
1325_9000136	2.0	2.0
1325_9000137	2.0	2.0
1325_9000140	3.0	2.0
1325_9000188	2.0	2.0
1325_9000209	2.0	2.0
1325_9000240	3.0	2.0
1325_9000278	3.0	2.0
1325_9000316	2.0	2.0
1325_9000533	2.0	2.0
1325_9000536	3.0	2.0
1325_9000602	3.0	2.0
1325_9000612	2.0	2.0
1325_9000675	2.0	2.0
1325_9000685	3.0	2.0
1365_0100002	2.0	2.0
1365_0100009	1.0	1.0
1365_0100015	2.0	1.0
1365_0100019	1.0	1.0
1365_0100027	2.0	2.0
1365_0100057	2.0	2.0
1365_0100071	2.0	2.0
1365_0100073	2.0	2.0
1365_0100092	2.0	2.0
1365_0100093	1.0	1.0
1365_0100094	2.0	1.0
1365_0100095	2.0	1.0
1365_0100099	1.0	2.0
1365_0100104	2.0	2.0
1365_0100133	2.0	2.0
1365_0100134	2.0	1.0
1365_0100148	2.0	2.0
1365_0100172	1.0	2.0
1365_0100178	1.0	1.0
1365_0100180	1.0	1.0
1365_0100183	2.0	2.0
1365_0100200	2.0	2.0
1365_0100223	2.0	2.0
1365_0100227	2.0	2.0
1365_0100229	2.0	2.0
1365_0100230	2.0	2.0
1365_0100231	2.0	2.0
1365_0100261	2.0	2.0
1365_0100265	2.0	2.0
1365_0100270	2.0	2.0
1365_0100275	2.0	2.0
1365_0100455	2.0	2.0
1365_0100456	2.0	2.0
1365_0100457	2.0	2.0
1385_0000020	1.0	1.0
1385_0000040	1.0	1.0
1385_0000048	1.0	1.0
1385_0000054	1.0	1.0
1385_0000059	1.0	1.0
1385_0000122	1.0	1.0
1385_0000124	1.0	1.0
1385_0001113	1.0	1.0
1385_0001125	1.0	1.0
1385_0001129	0.0	1.0
1385_0001135	1.0	1.0
1385_0001137	1.0	1.0
1385_0001153	2.0	1.0
1385_0001154	1.0	1.0
1385_0001155	1.0	1.0
1385_0001159	1.0	1.0
1385_0001164	1.0	1.0
1385_0001169	1.0	1.0
1385_0001189	0.0	1.0
1385_0001191	1.0	1.0
1385_0001522	0.0	1.0
1385_0001712	1.0	1.0
1385_0001723	0.0	1.0
1385_0001725	0.0	1.0
1385_0001737	1.0	1.0
1385_0001744	0.0	1.0
1385_0001748	2.0	1.0
1385_0001750	0.0	1.0
1385_0001751	0.0	1.0
1385_0001756	1.0	1.0
1385_0001757	1.0	1.0
1385_0001758	0.0	1.0
1385_0001762	1.0	1.0
1385_0001789	1.0	1.0
1385_0001795	0.0	1.0
1385_0001798	1.0	1.0
1395_0000340	1.0	1.0
1395_0000355	1.0	1.0
1395_0000359	1.0	1.0
1395_0000361	1.0	1.0
1395_0000383	1.0	1.0
1395_0000409	2.0	1.0
1395_0000413	1.0	1.0
1395_0000414	1.0	1.0
1395_0000415	1.0	1.0
1395_0000447	1.0	2.0
1395_0000458	1.0	1.0
1395_0000515	2.0	1.0
1395_0000518	1.0	1.0
1395_0000525	1.0	1.0
1395_0000527	0.0	1.0
1395_0000529	1.0	1.0
1395_0000531	1.0	1.0
1395_0000535	1.0	1.0
1395_0000556	1.0	1.0
1395_0000564	1.0	1.0
1395_0000581	1.0	2.0
1395_0000584	0.0	1.0
1395_0000607	0.0	1.0
1395_0000611	0.0	1.0
1395_0000612	1.0	1.0
1395_0000626	1.0	2.0
1395_0000644	1.0	1.0
1395_0001010	1.0	1.0
1395_0001016	1.0	1.0
1395_0001017	0.0	1.0
1395_0001020	1.0	1.0
1395_0001024	1.0	1.0
1395_0001028	1.0	1.0
1395_0001045	1.0	1.0
1395_0001064	1.0	1.0
1395_0001070	1.0	2.0
1395_0001090	1.0	1.0
1395_0001104	0.0	1.0
1395_0001115	1.0	1.0
1395_0001120	0.0	1.0
1395_0001122	0.0	1.0
1395_0001132	2.0	1.0
1395_0001145	1.0	2.0
1395_0001146	0.0	1.0
1395_0001149	0.0	1.0
1395_0001150	0.0	1.0
1395_0001161	1.0	1.0
1395_0001164	1.0	2.0
1395_0001169	1.0	1.0
LANGUAGE: IT, 1th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 11

  Average training loss: 1.14
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.81
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.69      0.94      0.79        66
         2.0       0.79      0.92      0.85        60
         3.0       0.00      0.00      0.00        12

    accuracy                           0.73       160
   macro avg       0.37      0.46      0.41       160
weighted avg       0.58      0.73      0.65       160

[[ 0 22  0  0]
 [ 0 62  4  0]
 [ 0  5 55  0]
 [ 0  1 11  0]]
0.6451923076923076
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.89
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.73
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.68      0.95      0.79        66
         2.0       0.79      0.88      0.83        60
         3.0       0.00      0.00      0.00        12

    accuracy                           0.73       160
   macro avg       0.37      0.46      0.41       160
weighted avg       0.58      0.72      0.64       160

[[ 0 22  0  0]
 [ 0 63  3  0]
 [ 0  7 53  0]
 [ 0  1 11  0]]
0.6398789184370821
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.82
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.65
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.70      0.95      0.81        66
         2.0       0.80      0.93      0.86        60
         3.0       0.00      0.00      0.00        12

    accuracy                           0.74       160
   macro avg       0.38      0.47      0.42       160
weighted avg       0.59      0.74      0.66       160

[[ 0 22  0  0]
 [ 0 63  3  0]
 [ 0  4 56  0]
 [ 0  1 11  0]]
0.65625
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.75
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.66
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.69      0.95      0.80        66
         2.0       0.80      0.92      0.85        60
         3.0       0.00      0.00      0.00        12

    accuracy                           0.74       160
   macro avg       0.37      0.47      0.41       160
weighted avg       0.58      0.74      0.65       160

[[ 0 22  0  0]
 [ 0 63  3  0]
 [ 0  5 55  0]
 [ 0  1 11  0]]
0.6508183972744778
160 160 160
Filename	True Label	Prediction
1325_1001011	2.0	2.0
1325_1001013	2.0	2.0
1325_1001019	2.0	2.0
1325_1001021	2.0	2.0
1325_1001022	2.0	2.0
1325_1001023	2.0	2.0
1325_1001033	2.0	2.0
1325_1001036	2.0	2.0
1325_1001040	2.0	2.0
1325_1001046	2.0	2.0
1325_1001051	2.0	2.0
1325_1001053	1.0	1.0
1325_1001078	2.0	2.0
1325_1001080	2.0	2.0
1325_1001088	2.0	2.0
1325_1001092	2.0	2.0
1325_1001094	2.0	2.0
1325_1001096	2.0	2.0
1325_1001113	3.0	2.0
1325_1001124	2.0	2.0
1325_1001126	2.0	2.0
1325_1001127	3.0	2.0
1325_1001131	2.0	2.0
1325_1001136	2.0	2.0
1325_1001144	2.0	2.0
1325_1001152	2.0	2.0
1325_1001154	3.0	2.0
1325_1001161	2.0	2.0
1325_1001166	2.0	2.0
1325_1001167	2.0	2.0
1325_9000059	2.0	2.0
1325_9000087	2.0	2.0
1325_9000088	2.0	2.0
1325_9000099	3.0	2.0
1325_9000317	3.0	2.0
1325_9000505	3.0	2.0
1325_9000676	3.0	2.0
1365_0100013	2.0	2.0
1365_0100021	2.0	2.0
1365_0100026	1.0	1.0
1365_0100028	1.0	2.0
1365_0100030	1.0	1.0
1365_0100056	2.0	2.0
1365_0100063	2.0	2.0
1365_0100066	1.0	1.0
1365_0100080	2.0	2.0
1365_0100096	2.0	2.0
1365_0100105	3.0	2.0
1365_0100120	3.0	2.0
1365_0100121	2.0	2.0
1365_0100125	2.0	2.0
1365_0100137	1.0	1.0
1365_0100166	1.0	2.0
1365_0100176	2.0	2.0
1365_0100184	1.0	1.0
1365_0100185	1.0	1.0
1365_0100188	2.0	2.0
1365_0100191	2.0	2.0
1365_0100194	2.0	2.0
1365_0100203	2.0	1.0
1365_0100212	3.0	2.0
1365_0100218	2.0	2.0
1365_0100222	2.0	2.0
1365_0100225	2.0	1.0
1365_0100226	2.0	2.0
1365_0100252	2.0	2.0
1365_0100253	1.0	1.0
1365_0100262	2.0	2.0
1365_0100269	2.0	2.0
1365_0100276	3.0	2.0
1365_0100282	2.0	2.0
1365_0100289	2.0	1.0
1365_0100451	2.0	2.0
1365_0100458	2.0	2.0
1365_0100459	2.0	2.0
1365_0100472	2.0	2.0
1365_0100474	2.0	2.0
1365_0100475	2.0	2.0
1365_0100476	2.0	2.0
1385_0000011	0.0	1.0
1385_0000021	1.0	1.0
1385_0000034	1.0	1.0
1385_0000039	1.0	1.0
1385_0000045	1.0	1.0
1385_0000051	1.0	1.0
1385_0000058	1.0	1.0
1385_0000095	1.0	1.0
1385_0000097	1.0	1.0
1385_0000103	1.0	1.0
1385_0000123	1.0	1.0
1385_0000125	1.0	1.0
1385_0000129	1.0	1.0
1385_0000130	1.0	1.0
1385_0001104	0.0	1.0
1385_0001105	1.0	1.0
1385_0001107	1.0	1.0
1385_0001110	1.0	1.0
1385_0001112	1.0	1.0
1385_0001118	1.0	1.0
1385_0001128	0.0	1.0
1385_0001130	1.0	1.0
1385_0001138	1.0	1.0
1385_0001147	1.0	1.0
1385_0001157	1.0	1.0
1385_0001175	0.0	1.0
1385_0001188	1.0	1.0
1385_0001192	1.0	1.0
1385_0001194	1.0	1.0
1385_0001195	1.0	1.0
1385_0001196	0.0	1.0
1385_0001198	2.0	1.0
1385_0001501	0.0	1.0
1385_0001526	0.0	1.0
1385_0001528	1.0	1.0
1385_0001715	1.0	1.0
1385_0001719	1.0	1.0
1385_0001726	1.0	1.0
1385_0001727	0.0	1.0
1385_0001728	0.0	1.0
1385_0001742	0.0	1.0
1385_0001760	1.0	1.0
1385_0001761	0.0	1.0
1385_0001766	1.0	1.0
1385_0001774	0.0	1.0
1385_0001787	0.0	1.0
1385_0001792	1.0	1.0
1385_0001793	0.0	1.0
1385_0001794	1.0	1.0
1395_0000337	0.0	1.0
1395_0000338	1.0	1.0
1395_0000376	2.0	1.0
1395_0000387	3.0	1.0
1395_0000390	1.0	1.0
1395_0000392	1.0	1.0
1395_0000402	1.0	1.0
1395_0000432	1.0	1.0
1395_0000448	1.0	1.0
1395_0000455	1.0	1.0
1395_0000465	1.0	1.0
1395_0000499	1.0	1.0
1395_0000500	1.0	1.0
1395_0000526	1.0	1.0
1395_0000548	1.0	1.0
1395_0000550	1.0	1.0
1395_0000552	1.0	1.0
1395_0000553	1.0	1.0
1395_0000575	1.0	1.0
1395_0000598	1.0	1.0
1395_0000599	1.0	1.0
1395_0000606	0.0	1.0
1395_0000628	0.0	1.0
1395_0000646	0.0	1.0
1395_0001015	0.0	1.0
1395_0001023	1.0	1.0
1395_0001060	1.0	1.0
1395_0001061	1.0	2.0
1395_0001065	0.0	1.0
1395_0001078	0.0	1.0
1395_0001114	0.0	1.0
1395_0001170	1.0	1.0
LANGUAGE: IT, 2th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.15
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.99
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.63      0.85      0.73        67
         2.0       0.69      0.80      0.74        60
         3.0       0.00      0.00      0.00        11

    accuracy                           0.66       160
   macro avg       0.33      0.41      0.37       160
weighted avg       0.52      0.66      0.58       160

[[ 0 21  1  0]
 [ 0 57 10  0]
 [ 0 12 48  0]
 [ 0  0 11  0]]
0.580983586477217
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 11

  Average training loss: 0.92
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.87
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.62      0.87      0.72        67
         2.0       0.70      0.77      0.73        60
         3.0       0.00      0.00      0.00        11

    accuracy                           0.65       160
   macro avg       0.33      0.41      0.36       160
weighted avg       0.52      0.65      0.58       160

[[ 0 22  0  0]
 [ 0 58  9  0]
 [ 0 14 46  0]
 [ 0  0 11  0]]
0.5755175983436853
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 11

  Average training loss: 0.82
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.82
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.62      0.84      0.71        67
         2.0       0.69      0.80      0.74        60
         3.0       0.00      0.00      0.00        11

    accuracy                           0.65       160
   macro avg       0.33      0.41      0.36       160
weighted avg       0.52      0.65      0.58       160

[[ 0 22  0  0]
 [ 0 56 11  0]
 [ 0 12 48  0]
 [ 0  0 11  0]]
0.5756491915727585
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.76
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.83
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.63      0.85      0.72        67
         2.0       0.70      0.80      0.74        60
         3.0       0.00      0.00      0.00        11

    accuracy                           0.66       160
   macro avg       0.33      0.41      0.37       160
weighted avg       0.52      0.66      0.58       160

[[ 0 22  0  0]
 [ 0 57 10  0]
 [ 0 12 48  0]
 [ 0  0 11  0]]
0.5812058433912275
160 160 160
Filename	True Label	Prediction
1325_1001008	2.0	2.0
1325_1001012	2.0	2.0
1325_1001017	2.0	2.0
1325_1001029	2.0	2.0
1325_1001032	2.0	2.0
1325_1001037	2.0	2.0
1325_1001044	2.0	2.0
1325_1001052	2.0	2.0
1325_1001058	2.0	2.0
1325_1001079	2.0	2.0
1325_1001081	2.0	2.0
1325_1001085	2.0	2.0
1325_1001095	2.0	2.0
1325_1001098	2.0	2.0
1325_1001099	3.0	2.0
1325_1001101	3.0	2.0
1325_1001121	2.0	2.0
1325_1001132	2.0	2.0
1325_1001134	2.0	2.0
1325_1001155	2.0	2.0
1325_1001162	2.0	2.0
1325_1001164	2.0	2.0
1325_9000089	2.0	2.0
1325_9000107	2.0	2.0
1325_9000139	2.0	2.0
1325_9000143	3.0	2.0
1325_9000185	3.0	2.0
1325_9000187	2.0	2.0
1325_9000210	1.0	2.0
1325_9000211	2.0	2.0
1325_9000215	3.0	2.0
1325_9000241	3.0	2.0
1325_9000296	1.0	2.0
1325_9000303	2.0	2.0
1325_9000315	2.0	2.0
1325_9000321	3.0	2.0
1325_9000322	3.0	2.0
1325_9000503	3.0	2.0
1325_9000534	2.0	2.0
1325_9000601	2.0	2.0
1365_0100007	1.0	1.0
1365_0100008	2.0	1.0
1365_0100012	1.0	1.0
1365_0100016	2.0	2.0
1365_0100017	2.0	2.0
1365_0100029	1.0	1.0
1365_0100065	1.0	1.0
1365_0100097	2.0	1.0
1365_0100101	2.0	2.0
1365_0100102	2.0	2.0
1365_0100116	2.0	2.0
1365_0100118	2.0	2.0
1365_0100119	3.0	2.0
1365_0100123	2.0	2.0
1365_0100138	2.0	1.0
1365_0100147	2.0	2.0
1365_0100151	1.0	1.0
1365_0100162	2.0	2.0
1365_0100181	1.0	1.0
1365_0100186	1.0	2.0
1365_0100190	2.0	2.0
1365_0100195	1.0	1.0
1365_0100198	1.0	2.0
1365_0100199	2.0	2.0
1365_0100211	3.0	2.0
1365_0100219	2.0	2.0
1365_0100220	2.0	2.0
1365_0100232	1.0	2.0
1365_0100251	2.0	2.0
1365_0100255	1.0	2.0
1365_0100257	2.0	2.0
1365_0100260	2.0	2.0
1365_0100267	2.0	2.0
1365_0100268	1.0	2.0
1365_0100281	2.0	2.0
1365_0100286	1.0	1.0
1365_0100290	1.0	2.0
1365_0100469	2.0	2.0
1365_0100471	1.0	2.0
1385_0000035	1.0	1.0
1385_0000038	1.0	1.0
1385_0000041	1.0	1.0
1385_0000044	1.0	1.0
1385_0000049	1.0	1.0
1385_0000057	1.0	1.0
1385_0000101	1.0	1.0
1385_0000102	1.0	1.0
1385_0000119	1.0	1.0
1385_0000126	1.0	1.0
1385_0001109	1.0	1.0
1385_0001119	1.0	1.0
1385_0001120	1.0	1.0
1385_0001124	1.0	1.0
1385_0001148	1.0	1.0
1385_0001149	1.0	1.0
1385_0001151	1.0	1.0
1385_0001156	1.0	1.0
1385_0001158	1.0	1.0
1385_0001161	1.0	1.0
1385_0001165	1.0	1.0
1385_0001166	1.0	1.0
1385_0001167	0.0	1.0
1385_0001174	0.0	1.0
1385_0001190	0.0	1.0
1385_0001503	1.0	1.0
1385_0001523	1.0	1.0
1385_0001525	1.0	1.0
1385_0001716	1.0	1.0
1385_0001720	0.0	1.0
1385_0001732	0.0	1.0
1385_0001734	0.0	1.0
1385_0001741	0.0	1.0
1385_0001747	0.0	1.0
1385_0001749	0.0	1.0
1385_0001772	1.0	1.0
1385_0001775	1.0	1.0
1385_0001785	0.0	1.0
1385_0001786	1.0	1.0
1385_0001788	0.0	1.0
1395_0000353	1.0	1.0
1395_0000360	2.0	1.0
1395_0000364	1.0	1.0
1395_0000365	2.0	1.0
1395_0000378	1.0	1.0
1395_0000404	1.0	1.0
1395_0000443	2.0	1.0
1395_0000469	1.0	1.0
1395_0000470	1.0	1.0
1395_0000471	1.0	1.0
1395_0000504	1.0	1.0
1395_0000513	2.0	1.0
1395_0000514	2.0	1.0
1395_0000533	2.0	1.0
1395_0000549	1.0	1.0
1395_0000555	1.0	1.0
1395_0000559	1.0	1.0
1395_0000560	2.0	1.0
1395_0000563	1.0	1.0
1395_0000582	0.0	1.0
1395_0000591	0.0	1.0
1395_0000596	2.0	1.0
1395_0000597	1.0	1.0
1395_0000604	0.0	1.0
1395_0000631	0.0	1.0
1395_0000635	0.0	1.0
1395_0000649	2.0	1.0
1395_0001022	1.0	1.0
1395_0001040	0.0	1.0
1395_0001067	0.0	1.0
1395_0001068	0.0	1.0
1395_0001071	1.0	1.0
1395_0001074	1.0	1.0
1395_0001080	1.0	1.0
1395_0001084	0.0	1.0
1395_0001119	1.0	2.0
1395_0001121	0.0	1.0
1395_0001124	1.0	1.0
1395_0001133	1.0	1.0
1395_0001147	0.0	1.0
1395_0001160	1.0	1.0
LANGUAGE: IT, 3th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.12
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.91
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.61      0.76      0.68        67
         2.0       0.63      0.80      0.71        60
         3.0       0.00      0.00      0.00        11

    accuracy                           0.62       160
   macro avg       0.31      0.39      0.35       160
weighted avg       0.49      0.62      0.55       160

[[ 0 20  2  0]
 [ 0 51 16  0]
 [ 0 12 48  0]
 [ 0  1 10  0]]
0.5475701207635373
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.93
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.89
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.64      0.81      0.71        67
         2.0       0.67      0.83      0.74        60
         3.0       0.00      0.00      0.00        11

    accuracy                           0.65       160
   macro avg       0.33      0.41      0.36       160
weighted avg       0.52      0.65      0.58       160

[[ 0 21  1  0]
 [ 0 54 13  0]
 [ 0 10 50  0]
 [ 0  0 11  0]]
0.5753106725146199
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.82
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.85
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.63      0.90      0.74        67
         2.0       0.74      0.80      0.77        60
         3.0       0.00      0.00      0.00        11

    accuracy                           0.68       160
   macro avg       0.34      0.42      0.38       160
weighted avg       0.54      0.68      0.60       160

[[ 0 22  0  0]
 [ 0 60  7  0]
 [ 0 12 48  0]
 [ 0  1 10  0]]
0.5981851851851852
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.77
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.84
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.65      0.88      0.75        67
         2.0       0.71      0.82      0.76        60
         3.0       0.00      0.00      0.00        11

    accuracy                           0.68       160
   macro avg       0.34      0.42      0.38       160
weighted avg       0.54      0.68      0.60       160

[[ 0 21  1  0]
 [ 0 59  8  0]
 [ 0 11 49  0]
 [ 0  0 11  0]]
0.5976210627023846
160 160 160
Filename	True Label	Prediction
1325_1001010	2.0	2.0
1325_1001014	3.0	2.0
1325_1001018	2.0	2.0
1325_1001024	2.0	2.0
1325_1001039	3.0	2.0
1325_1001041	3.0	2.0
1325_1001045	2.0	2.0
1325_1001048	2.0	2.0
1325_1001050	2.0	2.0
1325_1001054	2.0	2.0
1325_1001055	2.0	2.0
1325_1001057	1.0	2.0
1325_1001059	2.0	2.0
1325_1001082	2.0	2.0
1325_1001084	2.0	2.0
1325_1001090	2.0	2.0
1325_1001091	2.0	2.0
1325_1001093	2.0	2.0
1325_1001128	2.0	2.0
1325_1001138	2.0	2.0
1325_1001153	2.0	2.0
1325_1001158	2.0	2.0
1325_1001163	2.0	2.0
1325_1001165	2.0	2.0
1325_1001168	2.0	2.0
1325_1001169	2.0	2.0
1325_9000102	2.0	2.0
1325_9000104	2.0	2.0
1325_9000138	3.0	2.0
1325_9000152	2.0	2.0
1325_9000237	2.0	2.0
1325_9000239	2.0	2.0
1325_9000279	3.0	2.0
1325_9000302	2.0	2.0
1325_9000314	2.0	2.0
1325_9000318	3.0	2.0
1325_9000320	3.0	2.0
1325_9000323	2.0	2.0
1325_9000504	2.0	2.0
1325_9000677	2.0	2.0
1325_9000678	3.0	2.0
1325_9000686	2.0	2.0
1325_9000700	2.0	2.0
1325_9000750	3.0	2.0
1365_0100003	1.0	1.0
1365_0100006	2.0	2.0
1365_0100011	2.0	1.0
1365_0100031	2.0	1.0
1365_0100051	0.0	2.0
1365_0100061	3.0	2.0
1365_0100067	1.0	2.0
1365_0100072	2.0	2.0
1365_0100074	2.0	2.0
1365_0100079	2.0	2.0
1365_0100098	1.0	2.0
1365_0100135	2.0	1.0
1365_0100136	1.0	1.0
1365_0100163	2.0	2.0
1365_0100164	2.0	2.0
1365_0100167	1.0	2.0
1365_0100168	2.0	1.0
1365_0100169	1.0	1.0
1365_0100170	2.0	2.0
1365_0100174	1.0	1.0
1365_0100177	2.0	1.0
1365_0100179	1.0	2.0
1365_0100182	2.0	2.0
1365_0100187	2.0	2.0
1365_0100204	1.0	1.0
1365_0100215	2.0	2.0
1365_0100228	1.0	2.0
1365_0100259	2.0	2.0
1365_0100266	2.0	2.0
1365_0100279	2.0	2.0
1365_0100280	1.0	1.0
1365_0100288	1.0	1.0
1365_0100447	2.0	2.0
1365_0100473	2.0	2.0
1365_0100478	1.0	2.0
1365_0100481	1.0	2.0
1385_0000013	1.0	1.0
1385_0000016	1.0	1.0
1385_0000022	1.0	1.0
1385_0000033	1.0	1.0
1385_0000037	1.0	1.0
1385_0000042	1.0	1.0
1385_0000043	1.0	1.0
1385_0000052	1.0	1.0
1385_0000053	1.0	1.0
1385_0000098	1.0	1.0
1385_0000100	1.0	1.0
1385_0000120	0.0	1.0
1385_0000128	1.0	1.0
1385_0001103	1.0	1.0
1385_0001108	1.0	1.0
1385_0001127	1.0	1.0
1385_0001132	1.0	1.0
1385_0001133	1.0	1.0
1385_0001134	1.0	1.0
1385_0001136	1.0	1.0
1385_0001150	1.0	1.0
1385_0001152	1.0	1.0
1385_0001160	1.0	1.0
1385_0001170	0.0	1.0
1385_0001171	0.0	1.0
1385_0001178	0.0	1.0
1385_0001197	0.0	1.0
1385_0001524	0.0	1.0
1385_0001714	1.0	1.0
1385_0001717	1.0	1.0
1385_0001730	1.0	1.0
1385_0001740	1.0	1.0
1385_0001746	1.0	1.0
1385_0001764	0.0	1.0
1385_0001768	1.0	1.0
1385_0001771	0.0	1.0
1385_0001773	0.0	1.0
1385_0001791	1.0	1.0
1385_0001796	1.0	1.0
1385_0001800	1.0	1.0
1395_0000333	1.0	1.0
1395_0000356	1.0	1.0
1395_0000366	2.0	1.0
1395_0000368	0.0	1.0
1395_0000369	2.0	1.0
1395_0000379	1.0	1.0
1395_0000380	1.0	1.0
1395_0000391	3.0	2.0
1395_0000449	2.0	1.0
1395_0000450	1.0	1.0
1395_0000454	1.0	1.0
1395_0000460	1.0	1.0
1395_0000516	1.0	1.0
1395_0000537	1.0	1.0
1395_0000551	2.0	1.0
1395_0000554	1.0	1.0
1395_0000565	1.0	1.0
1395_0000572	1.0	1.0
1395_0000579	1.0	1.0
1395_0000583	1.0	1.0
1395_0000587	0.0	1.0
1395_0000593	0.0	1.0
1395_0000595	0.0	1.0
1395_0000627	1.0	1.0
1395_0000630	0.0	1.0
1395_0000639	0.0	1.0
1395_0001013	1.0	1.0
1395_0001033	1.0	1.0
1395_0001034	0.0	1.0
1395_0001069	2.0	1.0
1395_0001073	1.0	1.0
1395_0001076	1.0	1.0
1395_0001103	0.0	1.0
1395_0001108	0.0	1.0
1395_0001118	0.0	1.0
1395_0001123	0.0	1.0
1395_0001131	0.0	1.0
1395_0001141	1.0	1.0
1395_0001167	2.0	1.0
1395_0001171	1.0	1.0
LANGUAGE: IT, 4th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.18
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.07
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.59      0.85      0.70        67
         2.0       0.65      0.68      0.67        60
         3.0       0.00      0.00      0.00        11

    accuracy                           0.61       160
   macro avg       0.31      0.38      0.34       160
weighted avg       0.49      0.61      0.54       160

[[ 0 21  1  0]
 [ 0 57 10  0]
 [ 0 19 41  0]
 [ 0  0 11  0]]
0.5410823170731707
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.91
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.95
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.63      0.78      0.69        67
         2.0       0.65      0.83      0.73        60
         3.0       0.00      0.00      0.00        11

    accuracy                           0.64       160
   macro avg       0.32      0.40      0.36       160
weighted avg       0.51      0.64      0.56       160

[[ 0 21  1  0]
 [ 0 52 15  0]
 [ 0 10 50  0]
 [ 0  0 11  0]]
0.5640559610705596
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.80
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.90
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.64      0.72      0.68        67
         2.0       0.64      0.90      0.74        60
         3.0       0.00      0.00      0.00        11

    accuracy                           0.64       160
   macro avg       0.32      0.40      0.36       160
weighted avg       0.51      0.64      0.56       160

[[ 0 21  1  0]
 [ 0 48 19  0]
 [ 0  6 54  0]
 [ 0  0 11  0]]
0.5624089363768819
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.71
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.93
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.61      0.76      0.68        67
         2.0       0.64      0.82      0.72        60
         3.0       0.00      0.00      0.00        11

    accuracy                           0.62       160
   macro avg       0.31      0.39      0.35       160
weighted avg       0.50      0.62      0.55       160

[[ 0 21  1  0]
 [ 0 51 16  0]
 [ 0 11 49  0]
 [ 0  0 11  0]]
0.5529981751824817
160 160 160
Filename	True Label	Prediction
1325_1001015	2.0	2.0
1325_1001016	2.0	2.0
1325_1001020	2.0	2.0
1325_1001028	2.0	2.0
1325_1001047	1.0	2.0
1325_1001062	2.0	2.0
1325_1001063	2.0	2.0
1325_1001086	2.0	2.0
1325_1001087	2.0	2.0
1325_1001097	0.0	2.0
1325_1001100	2.0	2.0
1325_1001109	2.0	2.0
1325_1001110	2.0	2.0
1325_1001120	2.0	2.0
1325_1001122	2.0	2.0
1325_1001123	2.0	2.0
1325_1001129	1.0	2.0
1325_1001133	2.0	2.0
1325_1001135	2.0	2.0
1325_1001141	2.0	2.0
1325_1001142	2.0	2.0
1325_9000095	2.0	2.0
1325_9000105	2.0	2.0
1325_9000144	3.0	2.0
1325_9000186	3.0	2.0
1325_9000213	3.0	2.0
1325_9000214	3.0	2.0
1325_9000304	2.0	2.0
1325_9000319	2.0	2.0
1325_9000554	2.0	2.0
1325_9000611	2.0	2.0
1325_9000674	3.0	2.0
1325_9000684	3.0	2.0
1365_0100004	2.0	2.0
1365_0100005	1.0	1.0
1365_0100010	1.0	1.0
1365_0100014	2.0	1.0
1365_0100018	2.0	2.0
1365_0100020	2.0	2.0
1365_0100022	2.0	2.0
1365_0100023	1.0	2.0
1365_0100024	1.0	2.0
1365_0100058	2.0	2.0
1365_0100064	2.0	2.0
1365_0100069	1.0	2.0
1365_0100070	2.0	2.0
1365_0100100	2.0	2.0
1365_0100103	2.0	2.0
1365_0100106	2.0	2.0
1365_0100107	2.0	2.0
1365_0100117	2.0	2.0
1365_0100139	1.0	2.0
1365_0100145	2.0	2.0
1365_0100146	2.0	1.0
1365_0100165	2.0	2.0
1365_0100171	1.0	2.0
1365_0100173	1.0	1.0
1365_0100175	1.0	1.0
1365_0100192	3.0	2.0
1365_0100196	1.0	2.0
1365_0100201	1.0	2.0
1365_0100202	1.0	2.0
1365_0100205	2.0	1.0
1365_0100213	1.0	1.0
1365_0100217	3.0	2.0
1365_0100221	3.0	2.0
1365_0100224	2.0	2.0
1365_0100233	2.0	2.0
1365_0100256	2.0	1.0
1365_0100258	2.0	2.0
1365_0100263	3.0	2.0
1365_0100274	2.0	2.0
1365_0100277	3.0	2.0
1365_0100278	2.0	2.0
1365_0100285	2.0	1.0
1365_0100287	1.0	2.0
1365_0100299	2.0	2.0
1365_0100448	1.0	2.0
1365_0100461	2.0	2.0
1365_0100470	1.0	2.0
1365_0100477	1.0	2.0
1365_0100479	2.0	2.0
1365_0100480	1.0	2.0
1365_0100482	2.0	2.0
1385_0000012	1.0	1.0
1385_0000017	1.0	1.0
1385_0000023	1.0	1.0
1385_0000036	1.0	1.0
1385_0000047	1.0	1.0
1385_0000050	1.0	1.0
1385_0000099	1.0	1.0
1385_0000104	1.0	1.0
1385_0000114	1.0	1.0
1385_0000127	1.0	1.0
1385_0001111	1.0	1.0
1385_0001121	1.0	1.0
1385_0001122	1.0	1.0
1385_0001123	1.0	1.0
1385_0001126	0.0	1.0
1385_0001131	1.0	1.0
1385_0001162	1.0	1.0
1385_0001163	1.0	1.0
1385_0001172	0.0	1.0
1385_0001173	0.0	1.0
1385_0001193	1.0	1.0
1385_0001199	1.0	1.0
1385_0001527	1.0	1.0
1385_0001718	0.0	1.0
1385_0001724	2.0	1.0
1385_0001729	1.0	1.0
1385_0001733	0.0	1.0
1385_0001736	1.0	2.0
1385_0001738	0.0	1.0
1385_0001739	1.0	1.0
1385_0001752	1.0	1.0
1385_0001753	1.0	1.0
1385_0001754	0.0	1.0
1385_0001759	1.0	1.0
1385_0001765	0.0	1.0
1385_0001767	0.0	1.0
1385_0001790	0.0	1.0
1385_0001799	1.0	1.0
1395_0000341	1.0	1.0
1395_0000354	1.0	1.0
1395_0000357	2.0	2.0
1395_0000388	2.0	1.0
1395_0000389	0.0	1.0
1395_0000396	1.0	1.0
1395_0000398	2.0	1.0
1395_0000399	1.0	1.0
1395_0000403	1.0	1.0
1395_0000438	2.0	2.0
1395_0000446	2.0	1.0
1395_0000451	1.0	1.0
1395_0000452	1.0	1.0
1395_0000462	1.0	1.0
1395_0000512	1.0	1.0
1395_0000528	1.0	1.0
1395_0000534	1.0	1.0
1395_0000547	1.0	1.0
1395_0000557	2.0	1.0
1395_0000585	0.0	1.0
1395_0000602	1.0	1.0
1395_0000608	0.0	1.0
1395_0000609	1.0	1.0
1395_0000610	1.0	1.0
1395_0000636	0.0	1.0
1395_0000642	0.0	1.0
1395_0001019	0.0	1.0
1395_0001021	1.0	1.0
1395_0001058	1.0	1.0
1395_0001066	0.0	1.0
1395_0001075	0.0	1.0
1395_0001093	0.0	1.0
1395_0001101	1.0	1.0
1395_0001109	0.0	1.0
1395_0001116	1.0	1.0
1395_0001117	0.0	1.0
1395_0001126	1.0	1.0
1395_0001158	2.0	1.0
Averaged weighted F1-scores 0.5971606605194986
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2']
LANGUAGE: IT, 0th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.08
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.94
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.58      0.91      0.71        67
         2.0       0.78      0.57      0.66        75
         3.0       0.00      0.00      0.00         7

    accuracy                           0.65       160
   macro avg       0.34      0.37      0.34       160
weighted avg       0.61      0.65      0.61       160

[[ 0 11  0  0]
 [ 0 61  6  0]
 [ 0 32 43  0]
 [ 0  1  6  0]]
0.6071165026833631
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.86
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.80
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.69      0.76      0.72        67
         2.0       0.73      0.84      0.78        75
         3.0       0.00      0.00      0.00         7

    accuracy                           0.71       160
   macro avg       0.36      0.40      0.38       160
weighted avg       0.63      0.71      0.67       160

[[ 0 11  0  0]
 [ 0 51 16  0]
 [ 0 12 63  0]
 [ 0  0  7  0]]
0.66977335800185
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.80
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.75
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.71      0.75      0.73        67
         2.0       0.73      0.88      0.80        75
         3.0       0.00      0.00      0.00         7

    accuracy                           0.73       160
   macro avg       0.36      0.41      0.38       160
weighted avg       0.64      0.72      0.68       160

[[ 0 11  0  0]
 [ 0 50 17  0]
 [ 0  9 66  0]
 [ 0  0  7  0]]
0.6806569343065693
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.71
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.74
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.71      0.73      0.72        67
         2.0       0.73      0.88      0.80        75
         3.0       0.00      0.00      0.00         7

    accuracy                           0.72       160
   macro avg       0.36      0.40      0.38       160
weighted avg       0.64      0.72      0.67       160

[[ 0 11  0  0]
 [ 0 49 18  0]
 [ 0  9 66  0]
 [ 0  0  7  0]]
0.6744872873848334
160 160 160
Filename	True Label	Prediction
1325_1001015	2.0	2.0
1325_1001016	1.0	2.0
1325_1001021	2.0	2.0
1325_1001022	2.0	2.0
1325_1001033	2.0	2.0
1325_1001045	2.0	2.0
1325_1001048	2.0	2.0
1325_1001052	2.0	2.0
1325_1001058	2.0	2.0
1325_1001079	2.0	2.0
1325_1001082	2.0	2.0
1325_1001083	2.0	2.0
1325_1001085	2.0	2.0
1325_1001090	2.0	2.0
1325_1001091	2.0	2.0
1325_1001094	2.0	2.0
1325_1001099	3.0	2.0
1325_1001101	3.0	2.0
1325_1001110	2.0	2.0
1325_1001122	2.0	2.0
1325_1001124	1.0	2.0
1325_1001138	2.0	2.0
1325_1001139	2.0	2.0
1325_1001142	1.0	2.0
1325_1001158	2.0	2.0
1325_1001159	2.0	2.0
1325_9000102	2.0	2.0
1325_9000136	2.0	2.0
1325_9000138	2.0	2.0
1325_9000187	2.0	2.0
1325_9000239	3.0	2.0
1325_9000240	2.0	2.0
1325_9000296	2.0	2.0
1325_9000316	2.0	2.0
1325_9000533	2.0	2.0
1325_9000611	2.0	2.0
1325_9000612	1.0	2.0
1325_9000678	3.0	2.0
1365_0100005	2.0	2.0
1365_0100008	1.0	2.0
1365_0100009	2.0	1.0
1365_0100012	2.0	1.0
1365_0100019	1.0	2.0
1365_0100024	1.0	2.0
1365_0100026	2.0	2.0
1365_0100030	1.0	2.0
1365_0100070	2.0	2.0
1365_0100072	2.0	2.0
1365_0100079	1.0	2.0
1365_0100093	2.0	2.0
1365_0100097	2.0	2.0
1365_0100099	2.0	2.0
1365_0100100	2.0	2.0
1365_0100105	3.0	2.0
1365_0100120	2.0	2.0
1365_0100166	2.0	1.0
1365_0100174	1.0	2.0
1365_0100175	2.0	2.0
1365_0100178	2.0	2.0
1365_0100179	2.0	2.0
1365_0100187	2.0	2.0
1365_0100194	2.0	2.0
1365_0100198	1.0	2.0
1365_0100201	2.0	2.0
1365_0100211	2.0	2.0
1365_0100212	3.0	2.0
1365_0100215	2.0	2.0
1365_0100227	2.0	2.0
1365_0100228	2.0	2.0
1365_0100233	2.0	2.0
1365_0100256	2.0	2.0
1365_0100261	2.0	2.0
1365_0100263	3.0	2.0
1365_0100269	2.0	2.0
1365_0100276	2.0	2.0
1365_0100285	2.0	2.0
1365_0100448	2.0	2.0
1365_0100451	2.0	2.0
1365_0100459	2.0	2.0
1365_0100478	2.0	2.0
1385_0000012	1.0	1.0
1385_0000021	2.0	1.0
1385_0000034	1.0	2.0
1385_0000035	1.0	1.0
1385_0000037	0.0	1.0
1385_0000041	2.0	2.0
1385_0000042	2.0	2.0
1385_0000045	2.0	2.0
1385_0000057	2.0	2.0
1385_0000100	1.0	1.0
1385_0000120	0.0	1.0
1385_0000123	1.0	2.0
1385_0000124	2.0	2.0
1385_0000125	1.0	1.0
1385_0000127	1.0	1.0
1385_0000129	2.0	1.0
1385_0000130	2.0	1.0
1385_0001104	1.0	1.0
1385_0001121	1.0	1.0
1385_0001124	1.0	1.0
1385_0001129	1.0	1.0
1385_0001130	1.0	1.0
1385_0001133	1.0	1.0
1385_0001160	1.0	2.0
1385_0001161	2.0	2.0
1385_0001170	0.0	1.0
1385_0001171	1.0	1.0
1385_0001172	1.0	1.0
1385_0001174	1.0	1.0
1385_0001199	0.0	1.0
1385_0001503	1.0	2.0
1385_0001524	1.0	1.0
1385_0001714	0.0	1.0
1385_0001728	1.0	1.0
1385_0001734	1.0	2.0
1385_0001742	0.0	1.0
1385_0001744	0.0	1.0
1385_0001751	1.0	1.0
1385_0001756	1.0	1.0
1385_0001761	1.0	1.0
1385_0001773	1.0	1.0
1385_0001794	1.0	1.0
1385_0001799	1.0	2.0
1395_0000357	2.0	2.0
1395_0000359	1.0	1.0
1395_0000378	1.0	1.0
1395_0000403	1.0	1.0
1395_0000443	1.0	1.0
1395_0000449	1.0	1.0
1395_0000452	1.0	1.0
1395_0000458	1.0	1.0
1395_0000512	2.0	1.0
1395_0000527	1.0	1.0
1395_0000535	1.0	1.0
1395_0000550	1.0	1.0
1395_0000552	2.0	1.0
1395_0000553	1.0	1.0
1395_0000559	1.0	1.0
1395_0000560	1.0	1.0
1395_0000565	1.0	1.0
1395_0000583	1.0	1.0
1395_0000587	0.0	1.0
1395_0000609	0.0	1.0
1395_0000628	1.0	1.0
1395_0000631	1.0	1.0
1395_0000636	1.0	1.0
1395_0000639	0.0	1.0
1395_0000646	1.0	1.0
1395_0000649	1.0	1.0
1395_0001016	1.0	1.0
1395_0001019	1.0	1.0
1395_0001040	1.0	1.0
1395_0001045	2.0	1.0
1395_0001075	1.0	1.0
1395_0001101	1.0	1.0
1395_0001104	1.0	1.0
1395_0001115	1.0	1.0
1395_0001119	1.0	2.0
1395_0001131	1.0	1.0
1395_0001147	0.0	1.0
LANGUAGE: IT, 1th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.03
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.65      0.93      0.77        67
         2.0       0.82      0.71      0.76        75
         3.0       0.00      0.00      0.00         7

    accuracy                           0.72       160
   macro avg       0.37      0.41      0.38       160
weighted avg       0.66      0.72      0.68       160

[[ 0 11  0  0]
 [ 0 62  5  0]
 [ 0 22 53  0]
 [ 0  0  7  0]]
0.6754354056437389
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.82
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.76
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.68      0.88      0.77        67
         2.0       0.79      0.77      0.78        75
         3.0       0.00      0.00      0.00         7

    accuracy                           0.73       160
   macro avg       0.37      0.41      0.39       160
weighted avg       0.66      0.73      0.69       160

[[ 0 11  0  0]
 [ 0 59  8  0]
 [ 0 17 58  0]
 [ 0  0  7  0]]
0.6882590382590382
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.78
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.70
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.73      0.78      0.75        67
         2.0       0.75      0.89      0.82        75
         3.0       0.00      0.00      0.00         7

    accuracy                           0.74       160
   macro avg       0.37      0.42      0.39       160
weighted avg       0.66      0.74      0.70       160

[[ 0 11  0  0]
 [ 0 52 15  0]
 [ 0  8 67  0]
 [ 0  0  7  0]]
0.6985827589254153
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.69
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.69
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.71      0.79      0.75        67
         2.0       0.75      0.85      0.80        75
         3.0       0.00      0.00      0.00         7

    accuracy                           0.73       160
   macro avg       0.36      0.41      0.39       160
weighted avg       0.65      0.73      0.69       160

[[ 0 11  0  0]
 [ 0 53 14  0]
 [ 0 11 64  0]
 [ 0  0  7  0]]
0.687588028169014
160 160 160
Filename	True Label	Prediction
1325_1001012	2.0	2.0
1325_1001017	1.0	2.0
1325_1001018	2.0	2.0
1325_1001020	2.0	2.0
1325_1001027	2.0	2.0
1325_1001036	2.0	2.0
1325_1001037	2.0	2.0
1325_1001043	2.0	2.0
1325_1001047	2.0	2.0
1325_1001050	2.0	2.0
1325_1001055	2.0	2.0
1325_1001076	2.0	2.0
1325_1001089	2.0	2.0
1325_1001097	1.0	2.0
1325_1001109	2.0	2.0
1325_1001113	3.0	2.0
1325_1001121	2.0	2.0
1325_1001125	2.0	2.0
1325_1001126	2.0	2.0
1325_1001133	2.0	2.0
1325_1001134	2.0	2.0
1325_1001136	2.0	2.0
1325_1001143	2.0	2.0
1325_1001152	2.0	2.0
1325_1001154	3.0	2.0
1325_1001155	2.0	2.0
1325_1001160	2.0	2.0
1325_9000059	3.0	2.0
1325_9000095	2.0	2.0
1325_9000137	2.0	2.0
1325_9000140	3.0	2.0
1325_9000185	3.0	2.0
1325_9000186	3.0	2.0
1325_9000237	2.0	2.0
1325_9000241	3.0	2.0
1325_9000279	2.0	2.0
1325_9000304	2.0	2.0
1325_9000317	2.0	2.0
1325_9000319	2.0	2.0
1325_9000601	2.0	2.0
1325_9000677	2.0	2.0
1365_0100004	1.0	2.0
1365_0100006	2.0	2.0
1365_0100016	1.0	2.0
1365_0100017	2.0	2.0
1365_0100023	1.0	2.0
1365_0100028	2.0	2.0
1365_0100029	2.0	2.0
1365_0100058	2.0	2.0
1365_0100069	2.0	2.0
1365_0100080	2.0	2.0
1365_0100096	2.0	2.0
1365_0100117	2.0	2.0
1365_0100125	2.0	2.0
1365_0100151	2.0	1.0
1365_0100171	1.0	2.0
1365_0100188	1.0	2.0
1365_0100191	1.0	2.0
1365_0100200	2.0	2.0
1365_0100203	2.0	2.0
1365_0100222	2.0	2.0
1365_0100230	2.0	2.0
1365_0100253	1.0	2.0
1365_0100255	1.0	2.0
1365_0100258	2.0	2.0
1365_0100267	2.0	2.0
1365_0100275	2.0	2.0
1365_0100278	2.0	2.0
1365_0100281	2.0	2.0
1365_0100290	2.0	2.0
1365_0100458	2.0	2.0
1365_0100461	2.0	2.0
1365_0100470	2.0	2.0
1365_0100474	2.0	2.0
1365_0100476	2.0	2.0
1365_0100482	2.0	2.0
1385_0000013	0.0	1.0
1385_0000017	1.0	1.0
1385_0000020	2.0	1.0
1385_0000036	1.0	1.0
1385_0000049	2.0	2.0
1385_0000053	1.0	2.0
1385_0000054	2.0	2.0
1385_0000097	1.0	1.0
1385_0000098	1.0	1.0
1385_0000101	1.0	1.0
1385_0000114	2.0	2.0
1385_0001107	2.0	1.0
1385_0001111	2.0	1.0
1385_0001120	1.0	1.0
1385_0001123	1.0	1.0
1385_0001125	1.0	1.0
1385_0001126	1.0	1.0
1385_0001127	1.0	1.0
1385_0001137	1.0	1.0
1385_0001138	1.0	1.0
1385_0001148	1.0	1.0
1385_0001151	2.0	1.0
1385_0001153	2.0	2.0
1385_0001154	2.0	2.0
1385_0001156	2.0	2.0
1385_0001164	1.0	1.0
1385_0001166	1.0	1.0
1385_0001167	1.0	1.0
1385_0001189	0.0	1.0
1385_0001193	1.0	1.0
1385_0001195	2.0	1.0
1385_0001198	1.0	1.0
1385_0001522	1.0	1.0
1385_0001527	1.0	1.0
1385_0001715	0.0	1.0
1385_0001725	1.0	1.0
1385_0001736	1.0	2.0
1385_0001746	1.0	1.0
1385_0001747	0.0	1.0
1385_0001758	1.0	1.0
1385_0001759	1.0	1.0
1385_0001760	0.0	1.0
1385_0001765	0.0	1.0
1385_0001768	1.0	1.0
1385_0001771	1.0	1.0
1385_0001785	0.0	1.0
1385_0001789	1.0	2.0
1385_0001793	1.0	1.0
1385_0001800	1.0	1.0
1395_0000338	1.0	1.0
1395_0000340	1.0	1.0
1395_0000353	1.0	1.0
1395_0000354	0.0	1.0
1395_0000369	2.0	1.0
1395_0000376	2.0	1.0
1395_0000383	1.0	1.0
1395_0000415	1.0	1.0
1395_0000432	1.0	1.0
1395_0000451	2.0	1.0
1395_0000455	1.0	1.0
1395_0000469	1.0	1.0
1395_0000471	1.0	1.0
1395_0000513	1.0	2.0
1395_0000515	1.0	1.0
1395_0000526	1.0	1.0
1395_0000528	2.0	1.0
1395_0000547	1.0	1.0
1395_0000582	1.0	1.0
1395_0000591	0.0	1.0
1395_0000593	1.0	1.0
1395_0000595	0.0	1.0
1395_0000596	2.0	1.0
1395_0000598	1.0	1.0
1395_0000599	1.0	1.0
1395_0000630	1.0	1.0
1395_0001023	0.0	1.0
1395_0001034	1.0	1.0
1395_0001066	1.0	1.0
1395_0001108	1.0	1.0
1395_0001120	1.0	1.0
1395_0001126	1.0	1.0
1395_0001141	1.0	1.0
1395_0001150	1.0	1.0
1395_0001169	1.0	1.0
LANGUAGE: IT, 2th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.01
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.83
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.67      0.82      0.74        67
         2.0       0.73      0.77      0.75        74
         3.0       0.00      0.00      0.00         8

    accuracy                           0.70       160
   macro avg       0.35      0.40      0.37       160
weighted avg       0.62      0.70      0.66       160

[[ 0 10  1  0]
 [ 0 55 12  0]
 [ 0 17 57  0]
 [ 0  0  8  0]]
0.6560192953020134
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.83
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.73      0.73      0.73        67
         2.0       0.71      0.89      0.79        74
         3.0       0.00      0.00      0.00         8

    accuracy                           0.72       160
   macro avg       0.36      0.41      0.38       160
weighted avg       0.63      0.72      0.67       160

[[ 0 10  1  0]
 [ 0 49 18  0]
 [ 0  8 66  0]
 [ 0  0  8  0]]
0.6718188622754491
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.78
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.70      0.78      0.74        67
         2.0       0.72      0.84      0.77        74
         3.0       0.00      0.00      0.00         8

    accuracy                           0.71       160
   macro avg       0.36      0.40      0.38       160
weighted avg       0.63      0.71      0.67       160

[[ 0 10  1  0]
 [ 0 52 15  0]
 [ 0 12 62  0]
 [ 0  0  8  0]]
0.6673027482269503
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.70
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.80
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.73      0.73      0.73        67
         2.0       0.70      0.88      0.78        74
         3.0       0.00      0.00      0.00         8

    accuracy                           0.71       160
   macro avg       0.36      0.40      0.38       160
weighted avg       0.63      0.71      0.67       160

[[ 0  9  2  0]
 [ 0 49 18  0]
 [ 0  9 65  0]
 [ 0  0  8  0]]
0.6662799401197604
160 160 160
Filename	True Label	Prediction
1325_1001009	2.0	2.0
1325_1001010	2.0	2.0
1325_1001011	3.0	2.0
1325_1001013	3.0	2.0
1325_1001029	2.0	2.0
1325_1001032	2.0	2.0
1325_1001051	1.0	2.0
1325_1001056	2.0	2.0
1325_1001062	2.0	2.0
1325_1001063	1.0	2.0
1325_1001077	2.0	2.0
1325_1001078	2.0	2.0
1325_1001080	2.0	2.0
1325_1001081	2.0	2.0
1325_1001084	2.0	2.0
1325_1001086	2.0	2.0
1325_1001092	2.0	2.0
1325_1001095	2.0	2.0
1325_1001096	2.0	2.0
1325_1001098	2.0	2.0
1325_1001100	2.0	2.0
1325_1001107	2.0	2.0
1325_1001120	2.0	2.0
1325_1001127	2.0	2.0
1325_1001162	2.0	2.0
1325_1001164	2.0	2.0
1325_1001166	2.0	2.0
1325_1001169	2.0	2.0
1325_9000106	2.0	2.0
1325_9000139	2.0	2.0
1325_9000152	2.0	2.0
1325_9000213	3.0	2.0
1325_9000314	2.0	2.0
1325_9000320	3.0	2.0
1325_9000321	3.0	2.0
1325_9000323	1.0	2.0
1325_9000503	3.0	2.0
1325_9000602	2.0	2.0
1325_9000675	2.0	2.0
1325_9000676	2.0	2.0
1325_9000685	3.0	2.0
1365_0100003	2.0	2.0
1365_0100013	2.0	2.0
1365_0100018	2.0	2.0
1365_0100063	2.0	2.0
1365_0100065	1.0	2.0
1365_0100066	1.0	2.0
1365_0100067	1.0	2.0
1365_0100073	2.0	2.0
1365_0100074	1.0	2.0
1365_0100106	1.0	2.0
1365_0100119	3.0	2.0
1365_0100133	2.0	2.0
1365_0100134	2.0	2.0
1365_0100136	2.0	2.0
1365_0100137	2.0	2.0
1365_0100145	2.0	2.0
1365_0100147	2.0	1.0
1365_0100148	1.0	2.0
1365_0100162	2.0	2.0
1365_0100163	2.0	2.0
1365_0100173	1.0	2.0
1365_0100176	2.0	2.0
1365_0100181	1.0	2.0
1365_0100199	2.0	2.0
1365_0100205	2.0	2.0
1365_0100219	2.0	2.0
1365_0100220	2.0	2.0
1365_0100223	2.0	2.0
1365_0100225	2.0	2.0
1365_0100251	2.0	2.0
1365_0100262	2.0	2.0
1365_0100266	1.0	2.0
1365_0100268	2.0	2.0
1365_0100270	2.0	2.0
1365_0100274	2.0	2.0
1365_0100280	1.0	2.0
1365_0100289	2.0	2.0
1365_0100299	1.0	2.0
1365_0100447	2.0	2.0
1365_0100456	2.0	2.0
1365_0100471	2.0	2.0
1365_0100477	2.0	2.0
1365_0100480	2.0	2.0
1385_0000011	1.0	1.0
1385_0000016	1.0	1.0
1385_0000033	1.0	1.0
1385_0000043	2.0	2.0
1385_0000052	2.0	1.0
1385_0000102	1.0	1.0
1385_0000104	1.0	1.0
1385_0000126	1.0	1.0
1385_0000128	1.0	1.0
1385_0001103	1.0	1.0
1385_0001112	1.0	1.0
1385_0001132	1.0	1.0
1385_0001135	1.0	1.0
1385_0001150	2.0	2.0
1385_0001152	2.0	2.0
1385_0001162	1.0	1.0
1385_0001175	1.0	1.0
1385_0001188	1.0	1.0
1385_0001191	1.0	1.0
1385_0001192	0.0	1.0
1385_0001197	1.0	1.0
1385_0001525	1.0	1.0
1385_0001712	1.0	2.0
1385_0001727	0.0	1.0
1385_0001732	1.0	1.0
1385_0001741	0.0	1.0
1385_0001748	1.0	1.0
1385_0001752	0.0	1.0
1385_0001754	0.0	1.0
1385_0001762	1.0	1.0
1385_0001772	0.0	1.0
1385_0001786	1.0	2.0
1385_0001787	0.0	2.0
1385_0001790	1.0	2.0
1385_0001796	0.0	2.0
1385_0001798	1.0	2.0
1395_0000337	1.0	1.0
1395_0000356	1.0	1.0
1395_0000361	1.0	1.0
1395_0000379	1.0	1.0
1395_0000390	1.0	1.0
1395_0000409	2.0	1.0
1395_0000413	1.0	1.0
1395_0000414	1.0	1.0
1395_0000447	1.0	1.0
1395_0000448	1.0	1.0
1395_0000465	1.0	1.0
1395_0000470	1.0	1.0
1395_0000516	1.0	1.0
1395_0000533	2.0	2.0
1395_0000537	1.0	1.0
1395_0000548	1.0	1.0
1395_0000551	2.0	1.0
1395_0000554	2.0	1.0
1395_0000556	1.0	1.0
1395_0000563	2.0	1.0
1395_0000575	1.0	1.0
1395_0000602	1.0	1.0
1395_0000604	0.0	1.0
1395_0000608	1.0	1.0
1395_0000610	2.0	1.0
1395_0000611	1.0	1.0
1395_0000612	1.0	1.0
1395_0001010	2.0	1.0
1395_0001017	1.0	1.0
1395_0001021	1.0	1.0
1395_0001022	1.0	1.0
1395_0001024	0.0	1.0
1395_0001065	1.0	1.0
1395_0001069	1.0	1.0
1395_0001078	1.0	1.0
1395_0001084	1.0	1.0
1395_0001116	2.0	1.0
1395_0001124	0.0	1.0
1395_0001145	1.0	1.0
1395_0001167	1.0	1.0
LANGUAGE: IT, 3th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.05
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.85
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.72      0.75      0.73        68
         2.0       0.71      0.85      0.77        74
         3.0       0.00      0.00      0.00         7

    accuracy                           0.71       160
   macro avg       0.36      0.40      0.38       160
weighted avg       0.63      0.71      0.67       160

[[ 0  9  2  0]
 [ 0 51 17  0]
 [ 0 11 63  0]
 [ 0  0  7  0]]
0.6693858410204353
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.83
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.83
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.72      0.74      0.73        68
         2.0       0.70      0.86      0.78        74
         3.0       0.00      0.00      0.00         7

    accuracy                           0.71       160
   macro avg       0.36      0.40      0.38       160
weighted avg       0.63      0.71      0.67       160

[[ 0  9  2  0]
 [ 0 50 18  0]
 [ 0 10 64  0]
 [ 0  0  7  0]]
0.6690068568900686
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.78
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.87
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.62      0.82      0.70        68
         2.0       0.72      0.68      0.70        74
         3.0       0.00      0.00      0.00         7

    accuracy                           0.66       160
   macro avg       0.34      0.37      0.35       160
weighted avg       0.60      0.66      0.62       160

[[ 0 11  0  0]
 [ 0 56 12  0]
 [ 0 24 50  0]
 [ 0  0  7  0]]
0.6227976426089634
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.71
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.84
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.66      0.81      0.73        68
         2.0       0.73      0.76      0.74        74
         3.0       0.00      0.00      0.00         7

    accuracy                           0.69       160
   macro avg       0.35      0.39      0.37       160
weighted avg       0.62      0.69      0.65       160

[[ 0 10  1  0]
 [ 0 55 13  0]
 [ 0 18 56  0]
 [ 0  0  7  0]]
0.6526490066225166
160 160 160
Filename	True Label	Prediction
1325_1001008	2.0	2.0
1325_1001019	2.0	2.0
1325_1001023	2.0	2.0
1325_1001040	2.0	2.0
1325_1001041	3.0	2.0
1325_1001046	1.0	2.0
1325_1001053	2.0	2.0
1325_1001057	2.0	2.0
1325_1001059	2.0	2.0
1325_1001087	2.0	2.0
1325_1001088	2.0	2.0
1325_1001108	2.0	2.0
1325_1001123	2.0	2.0
1325_1001128	2.0	2.0
1325_1001130	2.0	2.0
1325_1001132	2.0	2.0
1325_1001135	2.0	2.0
1325_1001156	2.0	2.0
1325_1001161	2.0	2.0
1325_1001165	1.0	2.0
1325_1001168	1.0	2.0
1325_9000087	2.0	2.0
1325_9000099	2.0	2.0
1325_9000143	3.0	2.0
1325_9000144	3.0	2.0
1325_9000210	1.0	2.0
1325_9000215	2.0	2.0
1325_9000278	3.0	2.0
1325_9000318	2.0	2.0
1325_9000322	2.0	2.0
1325_9000505	2.0	2.0
1325_9000536	3.0	2.0
1325_9000674	3.0	2.0
1325_9000684	2.0	2.0
1325_9000686	2.0	2.0
1325_9000750	3.0	2.0
1365_0100002	1.0	2.0
1365_0100014	2.0	2.0
1365_0100015	2.0	1.0
1365_0100022	1.0	2.0
1365_0100056	1.0	1.0
1365_0100057	1.0	2.0
1365_0100064	2.0	2.0
1365_0100071	2.0	2.0
1365_0100092	2.0	2.0
1365_0100098	2.0	2.0
1365_0100101	2.0	2.0
1365_0100102	2.0	2.0
1365_0100104	1.0	2.0
1365_0100116	2.0	2.0
1365_0100118	2.0	2.0
1365_0100121	2.0	2.0
1365_0100138	2.0	2.0
1365_0100139	2.0	2.0
1365_0100146	2.0	1.0
1365_0100164	1.0	2.0
1365_0100165	2.0	2.0
1365_0100168	2.0	2.0
1365_0100177	2.0	2.0
1365_0100182	2.0	2.0
1365_0100184	2.0	2.0
1365_0100195	1.0	1.0
1365_0100196	1.0	2.0
1365_0100217	2.0	2.0
1365_0100221	2.0	2.0
1365_0100257	2.0	2.0
1365_0100259	2.0	2.0
1365_0100260	1.0	2.0
1365_0100279	2.0	2.0
1365_0100286	1.0	2.0
1365_0100288	2.0	2.0
1365_0100457	2.0	2.0
1365_0100469	2.0	1.0
1365_0100472	2.0	2.0
1365_0100475	2.0	2.0
1365_0100481	2.0	2.0
1385_0000038	2.0	1.0
1385_0000039	1.0	1.0
1385_0000044	2.0	1.0
1385_0000047	2.0	1.0
1385_0000099	1.0	1.0
1385_0000122	1.0	1.0
1385_0001105	1.0	1.0
1385_0001109	1.0	1.0
1385_0001110	1.0	1.0
1385_0001113	1.0	1.0
1385_0001118	1.0	1.0
1385_0001128	1.0	1.0
1385_0001136	1.0	1.0
1385_0001147	1.0	1.0
1385_0001149	2.0	2.0
1385_0001155	2.0	2.0
1385_0001159	2.0	1.0
1385_0001163	1.0	1.0
1385_0001178	0.0	1.0
1385_0001190	0.0	1.0
1385_0001194	1.0	1.0
1385_0001196	1.0	1.0
1385_0001501	0.0	1.0
1385_0001528	1.0	1.0
1385_0001718	1.0	1.0
1385_0001723	0.0	1.0
1385_0001724	1.0	1.0
1385_0001730	1.0	1.0
1385_0001733	1.0	1.0
1385_0001737	2.0	1.0
1385_0001738	0.0	1.0
1385_0001739	0.0	1.0
1385_0001740	1.0	1.0
1385_0001753	1.0	1.0
1385_0001757	1.0	1.0
1385_0001766	2.0	1.0
1385_0001767	1.0	1.0
1385_0001774	0.0	1.0
1385_0001775	1.0	1.0
1385_0001788	0.0	2.0
1385_0001795	1.0	1.0
1395_0000364	1.0	1.0
1395_0000388	1.0	1.0
1395_0000391	2.0	2.0
1395_0000396	1.0	1.0
1395_0000399	1.0	1.0
1395_0000402	1.0	1.0
1395_0000438	2.0	2.0
1395_0000446	2.0	1.0
1395_0000454	2.0	1.0
1395_0000460	1.0	1.0
1395_0000462	2.0	1.0
1395_0000499	1.0	1.0
1395_0000500	1.0	1.0
1395_0000504	1.0	1.0
1395_0000514	2.0	1.0
1395_0000529	2.0	1.0
1395_0000531	2.0	1.0
1395_0000549	1.0	1.0
1395_0000555	2.0	1.0
1395_0000557	2.0	1.0
1395_0000585	1.0	1.0
1395_0000607	0.0	1.0
1395_0000626	0.0	1.0
1395_0000627	1.0	1.0
1395_0000642	1.0	1.0
1395_0001020	1.0	1.0
1395_0001060	1.0	1.0
1395_0001061	1.0	2.0
1395_0001064	1.0	1.0
1395_0001073	1.0	1.0
1395_0001074	1.0	1.0
1395_0001076	1.0	1.0
1395_0001080	2.0	1.0
1395_0001090	1.0	1.0
1395_0001093	1.0	1.0
1395_0001109	1.0	1.0
1395_0001118	1.0	1.0
1395_0001121	0.0	1.0
1395_0001122	1.0	1.0
1395_0001132	1.0	1.0
1395_0001133	1.0	1.0
1395_0001158	1.0	1.0
1395_0001170	1.0	1.0
LANGUAGE: IT, 4th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.08
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.89
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        12
         1.0       0.71      0.55      0.62        67
         2.0       0.64      0.93      0.76        74
         3.0       0.00      0.00      0.00         7

    accuracy                           0.66       160
   macro avg       0.34      0.37      0.35       160
weighted avg       0.59      0.66      0.61       160

[[ 0 10  2  0]
 [ 0 37 30  0]
 [ 0  5 69  0]
 [ 0  0  7  0]]
0.6110859728506787
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.87
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.87
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        12
         1.0       0.75      0.49      0.59        67
         2.0       0.63      0.99      0.77        74
         3.0       0.00      0.00      0.00         7

    accuracy                           0.66       160
   macro avg       0.34      0.37      0.34       160
weighted avg       0.61      0.66      0.60       160

[[ 0 10  2  0]
 [ 0 33 34  0]
 [ 0  1 73  0]
 [ 0  0  7  0]]
0.6043812233285918
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.76
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        12
         1.0       0.74      0.69      0.71        67
         2.0       0.70      0.93      0.80        74
         3.0       0.00      0.00      0.00         7

    accuracy                           0.72       160
   macro avg       0.36      0.40      0.38       160
weighted avg       0.64      0.72      0.67       160

[[ 0 11  1  0]
 [ 0 46 21  0]
 [ 0  5 69  0]
 [ 0  0  7  0]]
0.669718992248062
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.71
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.72
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        12
         1.0       0.72      0.81      0.76        67
         2.0       0.78      0.89      0.83        74
         3.0       0.00      0.00      0.00         7

    accuracy                           0.75       160
   macro avg       0.37      0.42      0.40       160
weighted avg       0.66      0.75      0.70       160

[[ 0 12  0  0]
 [ 0 54 13  0]
 [ 0  8 66  0]
 [ 0  1  6  0]]
0.702448179643901
160 160 160
Filename	True Label	Prediction
1325_1001014	2.0	2.0
1325_1001024	2.0	2.0
1325_1001025	2.0	2.0
1325_1001028	1.0	2.0
1325_1001035	3.0	2.0
1325_1001039	2.0	2.0
1325_1001042	2.0	2.0
1325_1001044	2.0	2.0
1325_1001054	3.0	2.0
1325_1001075	2.0	2.0
1325_1001093	2.0	2.0
1325_1001111	3.0	2.0
1325_1001119	2.0	2.0
1325_1001129	1.0	2.0
1325_1001131	2.0	2.0
1325_1001141	1.0	2.0
1325_1001144	2.0	2.0
1325_1001153	2.0	2.0
1325_1001157	2.0	2.0
1325_1001163	1.0	2.0
1325_1001167	2.0	2.0
1325_1001170	2.0	2.0
1325_9000088	2.0	2.0
1325_9000089	2.0	2.0
1325_9000090	2.0	2.0
1325_9000104	2.0	2.0
1325_9000105	2.0	2.0
1325_9000107	3.0	2.0
1325_9000188	2.0	2.0
1325_9000209	2.0	2.0
1325_9000211	2.0	2.0
1325_9000214	2.0	2.0
1325_9000302	2.0	2.0
1325_9000303	2.0	2.0
1325_9000315	1.0	2.0
1325_9000504	2.0	2.0
1325_9000534	1.0	2.0
1325_9000554	2.0	2.0
1325_9000700	2.0	2.0
1365_0100007	1.0	1.0
1365_0100010	1.0	1.0
1365_0100011	2.0	1.0
1365_0100020	2.0	2.0
1365_0100021	2.0	2.0
1365_0100027	2.0	2.0
1365_0100031	1.0	2.0
1365_0100051	1.0	2.0
1365_0100061	3.0	2.0
1365_0100094	2.0	2.0
1365_0100095	2.0	2.0
1365_0100103	2.0	2.0
1365_0100107	2.0	2.0
1365_0100123	2.0	2.0
1365_0100135	1.0	2.0
1365_0100167	1.0	2.0
1365_0100169	2.0	2.0
1365_0100170	1.0	2.0
1365_0100172	2.0	2.0
1365_0100180	1.0	2.0
1365_0100183	2.0	2.0
1365_0100185	2.0	2.0
1365_0100186	2.0	2.0
1365_0100190	2.0	2.0
1365_0100192	3.0	2.0
1365_0100202	2.0	2.0
1365_0100204	1.0	2.0
1365_0100213	2.0	2.0
1365_0100218	2.0	2.0
1365_0100224	2.0	2.0
1365_0100226	2.0	2.0
1365_0100229	2.0	2.0
1365_0100231	2.0	2.0
1365_0100232	2.0	2.0
1365_0100252	2.0	2.0
1365_0100265	2.0	2.0
1365_0100277	2.0	2.0
1365_0100282	2.0	2.0
1365_0100287	2.0	2.0
1365_0100455	2.0	2.0
1365_0100473	2.0	2.0
1365_0100479	2.0	2.0
1385_0000022	0.0	1.0
1385_0000023	1.0	1.0
1385_0000040	1.0	1.0
1385_0000048	2.0	2.0
1385_0000050	2.0	2.0
1385_0000051	2.0	2.0
1385_0000058	2.0	2.0
1385_0000059	2.0	2.0
1385_0000095	1.0	1.0
1385_0000103	1.0	1.0
1385_0000119	2.0	2.0
1385_0001108	1.0	1.0
1385_0001119	1.0	1.0
1385_0001122	1.0	1.0
1385_0001131	1.0	1.0
1385_0001134	1.0	1.0
1385_0001157	2.0	2.0
1385_0001158	2.0	1.0
1385_0001165	1.0	1.0
1385_0001169	1.0	1.0
1385_0001173	0.0	1.0
1385_0001523	1.0	1.0
1385_0001526	0.0	1.0
1385_0001716	1.0	1.0
1385_0001717	1.0	1.0
1385_0001719	1.0	1.0
1385_0001720	0.0	1.0
1385_0001726	0.0	1.0
1385_0001729	1.0	1.0
1385_0001749	0.0	1.0
1385_0001750	0.0	1.0
1385_0001764	1.0	1.0
1385_0001791	1.0	1.0
1385_0001792	1.0	1.0
1395_0000333	1.0	1.0
1395_0000341	1.0	1.0
1395_0000355	1.0	1.0
1395_0000360	2.0	1.0
1395_0000365	2.0	1.0
1395_0000366	2.0	1.0
1395_0000368	0.0	1.0
1395_0000380	1.0	1.0
1395_0000387	3.0	1.0
1395_0000389	1.0	1.0
1395_0000392	1.0	1.0
1395_0000398	1.0	1.0
1395_0000404	1.0	1.0
1395_0000450	1.0	1.0
1395_0000518	2.0	1.0
1395_0000525	2.0	1.0
1395_0000534	1.0	1.0
1395_0000564	1.0	1.0
1395_0000572	1.0	1.0
1395_0000579	1.0	1.0
1395_0000581	1.0	1.0
1395_0000584	0.0	1.0
1395_0000597	1.0	1.0
1395_0000606	1.0	1.0
1395_0000635	1.0	1.0
1395_0000644	1.0	1.0
1395_0001013	1.0	1.0
1395_0001015	1.0	1.0
1395_0001028	1.0	1.0
1395_0001033	1.0	1.0
1395_0001058	1.0	1.0
1395_0001067	1.0	1.0
1395_0001068	0.0	1.0
1395_0001070	1.0	1.0
1395_0001071	1.0	1.0
1395_0001103	1.0	1.0
1395_0001114	1.0	1.0
1395_0001117	1.0	1.0
1395_0001123	0.0	1.0
1395_0001146	0.0	1.0
1395_0001149	1.0	1.0
1395_0001160	1.0	1.0
1395_0001161	1.0	1.0
1395_0001164	2.0	1.0
1395_0001171	1.0	1.0
Averaged weighted F1-scores 0.6766904883880052
MULTILINGUAL EXPERIMENTS
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
144.90552995391704 65.35717405024758
LANGUAGE: IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']
1 Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.88
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.78      0.75      0.76       174
         2.0       0.72      0.66      0.69       177
         3.0       0.59      0.99      0.74        75
         4.0       0.00      0.00      0.00         9

    accuracy                           0.71       452
   macro avg       0.42      0.48      0.44       452
weighted avg       0.68      0.71      0.69       452

[[  0  16   1   0   0]
 [  0 130  43   1   0]
 [  0  20 116  41   0]
 [  0   0   1  74   0]
 [  0   0   0   9   0]]
0.6859512119242629
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.62
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.70
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.82      0.71      0.76       174
         2.0       0.72      0.80      0.76       177
         3.0       0.69      0.96      0.80        75
         4.0       0.00      0.00      0.00         9

    accuracy                           0.75       452
   macro avg       0.44      0.49      0.46       452
weighted avg       0.71      0.75      0.72       452

[[  0  16   1   0   0]
 [  0 124  50   0   0]
 [  0  12 141  24   0]
 [  0   0   3  72   0]
 [  0   0   0   9   0]]
0.7224458352087343
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.49
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.73
              precision    recall  f1-score   support

         0.0       1.00      0.12      0.21        17
         1.0       0.76      0.82      0.79       174
         2.0       0.78      0.71      0.75       177
         3.0       0.71      0.96      0.81        75
         4.0       0.00      0.00      0.00         9

    accuracy                           0.76       452
   macro avg       0.65      0.52      0.51       452
weighted avg       0.76      0.76      0.74       452

[[  2  14   1   0   0]
 [  0 143  31   0   0]
 [  0  30 126  21   0]
 [  0   0   3  72   0]
 [  0   0   0   9   0]]
0.7398469227319665
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.38
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.78
              precision    recall  f1-score   support

         0.0       0.50      0.12      0.19        17
         1.0       0.81      0.70      0.75       174
         2.0       0.72      0.80      0.76       177
         3.0       0.71      0.96      0.81        75
         4.0       0.00      0.00      0.00         9

    accuracy                           0.75       452
   macro avg       0.55      0.52      0.50       452
weighted avg       0.73      0.75      0.73       452

[[  2  14   1   0   0]
 [  2 121  51   0   0]
 [  0  14 142  21   0]
 [  0   0   3  72   0]
 [  0   0   0   9   0]]
0.7279353685405673
452 452 452
Filename	True Label	Prediction
1023_0001418	3.0	3.0
1023_0001419	3.0	3.0
1023_0001423	2.0	2.0
1023_0001575	3.0	3.0
1023_0101675	3.0	3.0
1023_0101684	2.0	2.0
1023_0101688	3.0	3.0
1023_0101694	3.0	3.0
1023_0101843	3.0	3.0
1023_0101844	2.0	3.0
1023_0101845	3.0	3.0
1023_0101856	2.0	3.0
1023_0101897	2.0	3.0
1023_0102118	3.0	3.0
1023_0103822	2.0	2.0
1023_0103826	3.0	3.0
1023_0103829	2.0	3.0
1023_0103833	4.0	3.0
1023_0103834	3.0	3.0
1023_0103836	3.0	3.0
1023_0103844	4.0	3.0
1023_0103883	3.0	3.0
1023_0107244	3.0	3.0
1023_0107726	3.0	3.0
1023_0108306	3.0	3.0
1023_0108648	3.0	3.0
1023_0108766	2.0	3.0
1023_0108886	3.0	3.0
1023_0108888	3.0	3.0
1023_0108993	3.0	3.0
1023_0109022	3.0	3.0
1023_0109395	2.0	3.0
1023_0109396	2.0	3.0
1023_0109402	2.0	3.0
1023_0109505	3.0	3.0
1023_0109518	2.0	2.0
1023_0109524	3.0	3.0
1023_0109528	3.0	3.0
1023_0109590	2.0	3.0
1023_0109606	3.0	3.0
1023_0109880	3.0	3.0
1023_0109945	3.0	3.0
1031_0001951	2.0	3.0
1031_0001997	3.0	3.0
1031_0002006	4.0	3.0
1031_0002036	4.0	3.0
1031_0002061	3.0	3.0
1031_0002086	3.0	3.0
1031_0002187	3.0	3.0
1031_0002196	3.0	3.0
1031_0002198	3.0	3.0
1031_0003053	3.0	3.0
1031_0003085	3.0	3.0
1031_0003095	3.0	3.0
1031_0003121	3.0	3.0
1031_0003132	3.0	3.0
1031_0003133	4.0	3.0
1031_0003140	3.0	3.0
1031_0003141	3.0	3.0
1031_0003146	4.0	3.0
1031_0003160	3.0	3.0
1031_0003166	2.0	3.0
1031_0003180	4.0	3.0
1031_0003191	3.0	3.0
1031_0003216	3.0	3.0
1031_0003217	4.0	3.0
1031_0003220	3.0	3.0
1031_0003224	3.0	3.0
1031_0003234	3.0	3.0
1031_0003236	3.0	3.0
1031_0003239	4.0	3.0
1031_0003240	2.0	3.0
1031_0003242	3.0	3.0
1031_0003249	3.0	3.0
1031_0003261	3.0	3.0
1031_0003331	3.0	3.0
1031_0003336	3.0	3.0
1031_0003355	3.0	3.0
1031_0003365	3.0	3.0
1031_0003368	3.0	3.0
1031_0003386	2.0	3.0
1031_0003389	3.0	3.0
1031_0003407	3.0	3.0
1061_0120282	0.0	1.0
1061_0120286	1.0	1.0
1061_0120295	0.0	2.0
1061_0120297	2.0	2.0
1061_0120298	2.0	2.0
1061_0120302	1.0	2.0
1061_0120311	3.0	2.0
1061_0120314	2.0	2.0
1061_0120318	2.0	2.0
1061_0120324	2.0	2.0
1061_0120328	1.0	1.0
1061_0120329	2.0	2.0
1061_0120371	3.0	3.0
1061_0120373	2.0	2.0
1061_0120375	2.0	1.0
1061_0120391	1.0	1.0
1061_0120403	3.0	2.0
1061_0120404	1.0	1.0
1061_0120430	2.0	2.0
1061_0120439	1.0	1.0
1061_0120460	2.0	2.0
1061_0120481	3.0	3.0
1061_0120482	2.0	2.0
1061_0120484	2.0	2.0
1061_0120491	2.0	2.0
1061_0120853	2.0	2.0
1061_0120855	1.0	2.0
1061_0120857	2.0	2.0
1061_0120881	2.0	3.0
1061_0120885	2.0	2.0
1061_1029119	1.0	2.0
1061_1202910	2.0	2.0
1061_1202911	1.0	2.0
1061_1202913	2.0	1.0
1071_0024680	2.0	2.0
1071_0024682	2.0	2.0
1071_0024686	2.0	2.0
1071_0024688	1.0	1.0
1071_0024690	1.0	2.0
1071_0024693	1.0	1.0
1071_0024694	1.0	2.0
1071_0024704	1.0	1.0
1071_0024705	1.0	2.0
1071_0024712	1.0	1.0
1071_0024714	2.0	2.0
1071_0024756	1.0	1.0
1071_0024765	0.0	1.0
1071_0024770	1.0	1.0
1071_0024779	1.0	1.0
1071_0024801	1.0	1.0
1071_0024813	0.0	1.0
1071_0024815	0.0	1.0
1071_0024825	1.0	1.0
1071_0024831	0.0	1.0
1071_0024840	1.0	1.0
1071_0024855	1.0	1.0
1071_0024856	1.0	1.0
1071_0024859	1.0	1.0
1071_0024874	1.0	1.0
1071_0024876	1.0	1.0
1071_0242022	0.0	1.0
1071_0242073	1.0	1.0
1071_0243581	1.0	1.0
1071_0248310	1.0	1.0
1071_0248317	0.0	0.0
1071_0248327	0.0	1.0
1071_0248329	1.0	1.0
1071_0248338	1.0	1.0
1071_0248347	1.0	0.0
1071_0248350	1.0	0.0
1091_0000001	1.0	2.0
1091_0000002	2.0	2.0
1091_0000009	0.0	1.0
1091_0000015	1.0	2.0
1091_0000016	0.0	1.0
1091_0000017	2.0	2.0
1091_0000019	1.0	2.0
1091_0000020	1.0	2.0
1091_0000021	2.0	2.0
1091_0000023	1.0	1.0
1091_0000027	2.0	1.0
1091_0000030	1.0	1.0
1091_0000035	1.0	1.0
1091_0000042	0.0	1.0
1091_0000043	1.0	1.0
1091_0000050	1.0	1.0
1091_0000054	1.0	1.0
1091_0000071	2.0	2.0
1091_0000116	2.0	2.0
1091_0000146	1.0	1.0
1091_0000151	1.0	1.0
1091_0000153	2.0	2.0
1091_0000155	2.0	3.0
1091_0000158	2.0	2.0
1091_0000159	2.0	2.0
1091_0000164	1.0	1.0
1091_0000165	1.0	2.0
1091_0000168	2.0	2.0
1091_0000172	2.0	1.0
1091_0000190	1.0	2.0
1091_0000195	1.0	1.0
1091_0000197	2.0	2.0
1091_0000204	1.0	2.0
1091_0000206	1.0	1.0
1091_0000207	2.0	2.0
1091_0000211	2.0	2.0
1091_0000213	1.0	1.0
1091_0000214	1.0	1.0
1091_0000215	2.0	2.0
1091_0000217	1.0	1.0
1091_0000218	1.0	2.0
1091_0000224	1.0	1.0
1091_0000233	1.0	2.0
1091_0000237	2.0	2.0
1091_0000248	1.0	1.0
1091_0000251	2.0	2.0
1091_0000252	2.0	2.0
1091_0000258	2.0	2.0
1091_0000262	1.0	2.0
1091_0000263	2.0	2.0
1091_0000264	1.0	2.0
1091_0000269	2.0	2.0
1091_0000272	1.0	2.0
1091_0000275	1.0	2.0
0611	2.0	1.0
0613	1.0	1.0
0623	1.0	2.0
0625	1.0	1.0
0633	2.0	2.0
0642	1.0	2.0
0644	1.0	2.0
0714	2.0	2.0
0715	2.0	2.0
0716	2.0	2.0
0719	2.0	1.0
0723	1.0	2.0
0802	1.0	1.0
0812	1.0	1.0
0813	1.0	1.0
0816	2.0	2.0
0825	1.0	2.0
0827	1.0	1.0
0924	1.0	1.0
0930	1.0	2.0
1003	1.0	1.0
1005	1.0	1.0
1014	2.0	2.0
1022	2.0	1.0
1023	1.0	2.0
BER0609003	2.0	2.0
LIB0611004A	1.0	1.0
MOS0509004	2.0	1.0
MOS0611015	2.0	3.0
PAR1011008A	1.0	1.0
PAR1011009A	1.0	2.0
PAR1011013	2.0	2.0
PAR1011014	2.0	3.0
PAR1011015	2.0	2.0
PAR1011016	3.0	3.0
PHA0111001A	1.0	1.0
PHA0111014	1.0	2.0
PHA0112003A	1.0	1.0
PHA0112003B	1.0	1.0
PHA0112006A	2.0	2.0
PHA0112006B	2.0	2.0
PHA0112009A	2.0	1.0
PHA0209013	1.0	1.0
PHA0209026	3.0	3.0
PHA0209034	2.0	2.0
PHA0209038	3.0	3.0
PHA0411008A	1.0	1.0
PHA0411011B	1.0	1.0
PHA0411012A	1.0	1.0
PHA0411030	3.0	3.0
PHA0411033	2.0	2.0
PHA0411036	3.0	2.0
PHA0411042	2.0	3.0
PHA0411045	2.0	2.0
PHA0411056	3.0	3.0
PHA0509017	2.0	3.0
PHA0509018	3.0	3.0
PHA0509019	2.0	2.0
PHA0509020	3.0	3.0
PHA0509022	3.0	3.0
PHA0509031	2.0	2.0
PHA0509044	2.0	3.0
PHA0510002A	1.0	1.0
PHA0510003A	1.0	1.0
PHA0510013B	1.0	1.0
PHA0510035	3.0	3.0
PHA0610005B	1.0	1.0
PHA0610006A	1.0	1.0
PHA0610017	3.0	3.0
PHA0710019	3.0	3.0
PHA0809010	2.0	2.0
PHA0810006	2.0	2.0
PHA0810008	2.0	3.0
PHA0811010	2.0	2.0
PHA0811014	2.0	2.0
PHA1109002	3.0	3.0
PHA1109007	2.0	2.0
PHA1110001A	1.0	1.0
PHA1110004A	1.0	1.0
PHA1111002A	1.0	1.0
PHA1111003A	1.0	1.0
PHA1111004B	1.0	1.0
PHA1111008B	1.0	1.0
VAR0909005	2.0	2.0
VAR0909008	2.0	2.0
VAR0909009	3.0	3.0
1325_1001014	2.0	2.0
1325_1001018	2.0	2.0
1325_1001024	2.0	2.0
1325_1001025	2.0	2.0
1325_1001035	2.0	2.0
1325_1001036	2.0	2.0
1325_1001040	2.0	2.0
1325_1001051	2.0	2.0
1325_1001057	2.0	2.0
1325_1001063	2.0	2.0
1325_1001075	1.0	2.0
1325_1001084	2.0	2.0
1325_1001085	2.0	2.0
1325_1001091	2.0	2.0
1325_1001093	2.0	2.0
1325_1001096	2.0	2.0
1325_1001123	2.0	2.0
1325_1001125	2.0	2.0
1325_1001127	2.0	2.0
1325_1001129	1.0	2.0
1325_1001134	2.0	2.0
1325_1001141	1.0	2.0
1325_1001159	2.0	2.0
1325_1001160	2.0	2.0
1325_1001162	2.0	2.0
1325_1001167	2.0	2.0
1325_1001170	2.0	2.0
1325_9000088	2.0	2.0
1325_9000089	2.0	2.0
1325_9000095	2.0	2.0
1325_9000105	2.0	2.0
1325_9000106	2.0	2.0
1325_9000137	2.0	2.0
1325_9000140	2.0	2.0
1325_9000211	2.0	2.0
1325_9000213	2.0	2.0
1325_9000214	2.0	2.0
1325_9000240	2.0	2.0
1325_9000296	2.0	2.0
1325_9000315	1.0	2.0
1325_9000316	2.0	2.0
1325_9000323	2.0	2.0
1325_9000505	2.0	2.0
1325_9000554	2.0	2.0
1325_9000675	2.0	2.0
1365_0100003	1.0	1.0
1365_0100004	1.0	2.0
1365_0100008	1.0	2.0
1365_0100009	1.0	1.0
1365_0100011	2.0	2.0
1365_0100014	2.0	2.0
1365_0100016	2.0	2.0
1365_0100017	2.0	2.0
1365_0100021	2.0	2.0
1365_0100024	1.0	2.0
1365_0100061	2.0	2.0
1365_0100065	1.0	2.0
1365_0100071	2.0	2.0
1365_0100079	2.0	2.0
1365_0100095	2.0	2.0
1365_0100096	2.0	2.0
1365_0100104	1.0	2.0
1365_0100106	1.0	2.0
1365_0100133	2.0	2.0
1365_0100139	2.0	2.0
1365_0100145	2.0	2.0
1365_0100169	2.0	2.0
1365_0100170	1.0	2.0
1365_0100191	1.0	2.0
1365_0100195	1.0	1.0
1365_0100196	1.0	2.0
1365_0100201	2.0	2.0
1365_0100202	1.0	2.0
1365_0100217	2.0	2.0
1365_0100226	2.0	2.0
1365_0100251	2.0	2.0
1365_0100255	1.0	2.0
1365_0100265	2.0	2.0
1365_0100270	2.0	2.0
1365_0100275	2.0	2.0
1365_0100277	2.0	2.0
1365_0100286	1.0	2.0
1365_0100447	2.0	2.0
1365_0100457	2.0	2.0
1365_0100458	2.0	2.0
1365_0100461	2.0	2.0
1365_0100477	1.0	2.0
1365_0100481	2.0	2.0
1385_0000021	1.0	1.0
1385_0000033	1.0	1.0
1385_0000039	1.0	1.0
1385_0000051	1.0	1.0
1385_0000057	1.0	1.0
1385_0000098	1.0	1.0
1385_0000101	1.0	1.0
1385_0000120	0.0	1.0
1385_0000127	1.0	1.0
1385_0000130	1.0	1.0
1385_0001111	1.0	1.0
1385_0001126	0.0	1.0
1385_0001130	1.0	1.0
1385_0001132	1.0	1.0
1385_0001149	1.0	1.0
1385_0001153	2.0	2.0
1385_0001158	1.0	1.0
1385_0001160	1.0	2.0
1385_0001161	1.0	1.0
1385_0001164	1.0	1.0
1385_0001178	0.0	1.0
1385_0001190	1.0	1.0
1385_0001522	1.0	1.0
1385_0001526	0.0	1.0
1385_0001712	1.0	2.0
1385_0001715	1.0	1.0
1385_0001730	2.0	1.0
1385_0001737	1.0	1.0
1385_0001742	0.0	0.0
1385_0001759	1.0	1.0
1385_0001766	1.0	2.0
1385_0001787	1.0	1.0
1385_0001789	1.0	1.0
1385_0001792	1.0	1.0
1395_0000353	1.0	1.0
1395_0000365	2.0	2.0
1395_0000366	2.0	1.0
1395_0000387	2.0	2.0
1395_0000388	2.0	2.0
1395_0000390	1.0	1.0
1395_0000403	1.0	1.0
1395_0000413	1.0	2.0
1395_0000449	2.0	2.0
1395_0000451	1.0	1.0
1395_0000452	1.0	1.0
1395_0000462	2.0	1.0
1395_0000512	1.0	1.0
1395_0000528	2.0	1.0
1395_0000552	1.0	2.0
1395_0000555	1.0	1.0
1395_0000564	1.0	1.0
1395_0000572	1.0	1.0
1395_0000597	1.0	1.0
1395_0000606	1.0	1.0
1395_0000607	1.0	1.0
1395_0000610	1.0	1.0
1395_0000636	1.0	1.0
1395_0000642	1.0	1.0
1395_0001010	2.0	1.0
1395_0001015	1.0	1.0
1395_0001023	1.0	1.0
1395_0001058	1.0	1.0
1395_0001070	2.0	2.0
1395_0001078	1.0	1.0
1395_0001080	1.0	1.0
1395_0001101	1.0	1.0
1395_0001103	1.0	1.0
1395_0001119	2.0	2.0
1395_0001141	1.0	1.0
1395_0001145	2.0	2.0
1395_0001171	1.0	1.0
2 Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.94
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.68
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.78      0.81      0.79       175
         2.0       0.72      0.77      0.74       177
         3.0       0.67      0.72      0.69        75
         4.0       0.00      0.00      0.00         8

    accuracy                           0.73       452
   macro avg       0.43      0.46      0.45       452
weighted avg       0.69      0.73      0.71       452

[[  0  17   0   0   0]
 [  0 141  34   0   0]
 [  0  21 137  19   0]
 [  0   1  20  54   0]
 [  0   0   0   8   0]]
0.7139938161689824
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.62
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.54
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.82      0.87      0.84       175
         2.0       0.78      0.82      0.80       177
         3.0       0.70      0.76      0.73        75
         4.0       0.00      0.00      0.00         8

    accuracy                           0.78       452
   macro avg       0.46      0.49      0.47       452
weighted avg       0.74      0.78      0.76       452

[[  0  17   0   0   0]
 [  0 152  23   0   0]
 [  0  16 145  16   0]
 [  0   0  18  57   0]
 [  0   0   0   8   0]]
0.7610407715999028
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.47
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.62
              precision    recall  f1-score   support

         0.0       0.50      0.29      0.37        17
         1.0       0.86      0.75      0.80       175
         2.0       0.75      0.79      0.77       177
         3.0       0.66      0.91      0.76        75
         4.0       0.00      0.00      0.00         8

    accuracy                           0.76       452
   macro avg       0.55      0.55      0.54       452
weighted avg       0.75      0.76      0.75       452

[[  5  12   0   0   0]
 [  5 131  39   0   0]
 [  0  10 140  27   0]
 [  0   0   7  68   0]
 [  0   0   0   8   0]]
0.7520247302400476
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.37
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.64
              precision    recall  f1-score   support

         0.0       0.40      0.12      0.18        17
         1.0       0.86      0.79      0.82       175
         2.0       0.78      0.81      0.79       177
         3.0       0.66      0.92      0.77        75
         4.0       0.00      0.00      0.00         8

    accuracy                           0.78       452
   macro avg       0.54      0.53      0.51       452
weighted avg       0.77      0.78      0.76       452

[[  2  15   0   0   0]
 [  3 138  34   0   0]
 [  0   7 143  27   0]
 [  0   0   6  69   0]
 [  0   0   0   8   0]]
0.7648405072687324
452 452 452
Filename	True Label	Prediction
1023_0001420	3.0	3.0
1023_0101683	3.0	2.0
1023_0101689	2.0	2.0
1023_0101690	2.0	2.0
1023_0101691	3.0	3.0
1023_0101693	3.0	3.0
1023_0101749	3.0	3.0
1023_0101847	3.0	3.0
1023_0101851	3.0	3.0
1023_0101893	3.0	3.0
1023_0101894	2.0	3.0
1023_0101907	3.0	3.0
1023_0103821	3.0	3.0
1023_0103823	3.0	3.0
1023_0103824	3.0	3.0
1023_0103830	3.0	3.0
1023_0103831	3.0	3.0
1023_0103839	3.0	3.0
1023_0107075	2.0	3.0
1023_0107682	2.0	2.0
1023_0107725	2.0	3.0
1023_0107729	3.0	3.0
1023_0107784	2.0	2.0
1023_0107787	2.0	2.0
1023_0108423	3.0	2.0
1023_0108426	2.0	3.0
1023_0108641	4.0	3.0
1023_0108815	3.0	3.0
1023_0108889	3.0	3.0
1023_0108931	3.0	3.0
1023_0108935	2.0	3.0
1023_0109248	2.0	3.0
1023_0109249	3.0	3.0
1023_0109267	2.0	3.0
1023_0109399	2.0	3.0
1023_0109495	3.0	3.0
1023_0109500	2.0	3.0
1023_0109516	3.0	3.0
1023_0109614	2.0	2.0
1023_0109717	3.0	3.0
1023_0109890	3.0	3.0
1023_0109891	3.0	3.0
1023_0109947	2.0	3.0
1023_0109951	2.0	3.0
1031_0001949	3.0	3.0
1031_0002003	3.0	3.0
1031_0002005	3.0	3.0
1031_0002032	3.0	3.0
1031_0002040	4.0	3.0
1031_0002043	4.0	3.0
1031_0002084	3.0	3.0
1031_0002089	3.0	3.0
1031_0002195	3.0	3.0
1031_0003029	3.0	3.0
1031_0003063	4.0	3.0
1031_0003065	3.0	3.0
1031_0003071	3.0	3.0
1031_0003072	3.0	3.0
1031_0003078	3.0	3.0
1031_0003092	2.0	3.0
1031_0003098	4.0	3.0
1031_0003099	3.0	3.0
1031_0003128	4.0	3.0
1031_0003131	3.0	3.0
1031_0003136	4.0	3.0
1031_0003155	3.0	3.0
1031_0003163	3.0	3.0
1031_0003182	4.0	3.0
1031_0003186	3.0	3.0
1031_0003211	2.0	3.0
1031_0003212	2.0	3.0
1031_0003221	2.0	3.0
1031_0003238	3.0	3.0
1031_0003245	3.0	3.0
1031_0003310	3.0	3.0
1031_0003338	3.0	3.0
1031_0003367	3.0	3.0
1031_0003384	2.0	2.0
1031_0003387	3.0	3.0
1031_0003393	3.0	3.0
1031_0003414	3.0	3.0
1061_0012029	3.0	2.0
1061_0120274	1.0	1.0
1061_0120275	2.0	2.0
1061_0120276	2.0	2.0
1061_0120277	1.0	1.0
1061_0120279	1.0	1.0
1061_0120280	1.0	1.0
1061_0120285	1.0	2.0
1061_0120290	1.0	2.0
1061_0120312	1.0	1.0
1061_0120327	2.0	3.0
1061_0120334	2.0	2.0
1061_0120335	2.0	3.0
1061_0120343	2.0	2.0
1061_0120347	1.0	2.0
1061_0120352	1.0	1.0
1061_0120354	1.0	1.0
1061_0120355	1.0	1.0
1061_0120358	1.0	1.0
1061_0120359	1.0	2.0
1061_0120366	3.0	3.0
1061_0120374	2.0	2.0
1061_0120388	1.0	2.0
1061_0120410	2.0	2.0
1061_0120421	2.0	3.0
1061_0120438	2.0	3.0
1061_0120443	0.0	1.0
1061_0120449	2.0	2.0
1061_0120457	2.0	2.0
1061_0120458	3.0	3.0
1061_0120486	2.0	2.0
1061_0120492	2.0	3.0
1061_0120494	2.0	2.0
1061_0120495	2.0	2.0
1061_0120497	3.0	3.0
1061_0120498	2.0	2.0
1061_0120500	2.0	2.0
1061_0120858	2.0	2.0
1061_0120874	2.0	2.0
1061_0120880	3.0	3.0
1061_0120887	1.0	2.0
1061_1029111	2.0	2.0
1061_1029114	1.0	1.0
1061_1029116	1.0	2.0
1061_1202912	2.0	2.0
1071_0020001	1.0	1.0
1071_0024683	0.0	1.0
1071_0024689	1.0	1.0
1071_0024702	1.0	2.0
1071_0024709	2.0	2.0
1071_0024759	0.0	1.0
1071_0024775	0.0	1.0
1071_0024777	1.0	1.0
1071_0024778	0.0	1.0
1071_0024783	0.0	0.0
1071_0024799	2.0	2.0
1071_0024810	1.0	1.0
1071_0024819	1.0	1.0
1071_0024820	0.0	1.0
1071_0024821	1.0	1.0
1071_0024822	0.0	1.0
1071_0024823	1.0	1.0
1071_0024827	1.0	1.0
1071_0024837	0.0	1.0
1071_0024852	0.0	1.0
1071_0024860	1.0	1.0
1071_0024866	2.0	2.0
1071_0024871	1.0	1.0
1071_0241831	1.0	2.0
1071_0242021	1.0	1.0
1071_0242091	1.0	0.0
1071_0243501	1.0	1.0
1071_0248304	1.0	1.0
1071_0248316	1.0	1.0
1071_0248318	0.0	0.0
1071_0248319	0.0	1.0
1071_0248322	1.0	1.0
1071_0248339	1.0	1.0
1071_0248343	1.0	1.0
1071_0248345	1.0	2.0
1091_0000004	1.0	1.0
1091_0000007	2.0	2.0
1091_0000012	1.0	1.0
1091_0000025	1.0	0.0
1091_0000031	1.0	1.0
1091_0000038	1.0	1.0
1091_0000044	1.0	2.0
1091_0000053	1.0	2.0
1091_0000055	2.0	2.0
1091_0000062	1.0	1.0
1091_0000068	2.0	2.0
1091_0000073	1.0	1.0
1091_0000075	2.0	2.0
1091_0000076	3.0	2.0
1091_0000078	1.0	1.0
1091_0000079	2.0	2.0
1091_0000102	1.0	1.0
1091_0000113	2.0	2.0
1091_0000123	2.0	2.0
1091_0000144	1.0	0.0
1091_0000145	1.0	1.0
1091_0000185	1.0	1.0
1091_0000191	2.0	1.0
1091_0000193	1.0	1.0
1091_0000208	2.0	1.0
1091_0000209	1.0	1.0
1091_0000210	1.0	1.0
1091_0000216	2.0	2.0
1091_0000225	1.0	1.0
1091_0000227	1.0	2.0
1091_0000229	2.0	2.0
1091_0000238	1.0	2.0
1091_0000240	1.0	1.0
1091_0000241	1.0	1.0
1091_0000254	1.0	1.0
1091_0000255	1.0	1.0
1091_0000260	2.0	2.0
1091_0000266	2.0	2.0
1091_0000274	2.0	2.0
0608	1.0	1.0
0610	1.0	2.0
0615	1.0	1.0
0618	1.0	1.0
0619	2.0	1.0
0622	1.0	1.0
0624	2.0	2.0
0627	2.0	2.0
0804	1.0	1.0
0806	1.0	2.0
0808	1.0	2.0
0809	2.0	2.0
0818	1.0	1.0
0819	3.0	2.0
0823	2.0	2.0
0824	1.0	1.0
0826	1.0	1.0
0828	2.0	2.0
0901	2.0	2.0
0906	2.0	2.0
0922	1.0	1.0
0923	2.0	2.0
0928	2.0	2.0
1001	1.0	1.0
1002	2.0	1.0
1007	2.0	2.0
1010	1.0	1.0
1117	1.0	1.0
BER0611007	2.0	2.0
KYJ0611005A	1.0	1.0
KYJ0611006A	1.0	1.0
KYJ0611006B	1.0	1.0
LIB0611001B	1.0	1.0
MOS0509001	2.0	2.0
PHA0111001B	1.0	1.0
PHA0111004B	1.0	1.0
PHA0111005A	1.0	1.0
PHA0111005B	1.0	1.0
PHA0111015	3.0	3.0
PHA0111016	3.0	3.0
PHA0112009B	2.0	1.0
PHA0112012B	1.0	2.0
PHA0209028	2.0	2.0
PHA0210007	1.0	1.0
PHA0411010B	1.0	1.0
PHA0411011A	1.0	1.0
PHA0411012B	1.0	1.0
PHA0411032	2.0	3.0
PHA0411037	2.0	2.0
PHA0411039	2.0	3.0
PHA0411043	2.0	2.0
PHA0411044	3.0	3.0
PHA0411055	3.0	3.0
PHA0411060	2.0	3.0
PHA0411061	3.0	3.0
PHA0509007	1.0	1.0
PHA0509025	3.0	3.0
PHA0509026	3.0	3.0
PHA0509027	2.0	2.0
PHA0509034	2.0	3.0
PHA0509036	3.0	3.0
PHA0509039	3.0	3.0
PHA0509042	3.0	3.0
PHA0510002B	1.0	1.0
PHA0510003B	1.0	1.0
PHA0510023	3.0	3.0
PHA0510030	2.0	3.0
PHA0510036	3.0	3.0
PHA0510046	2.0	2.0
PHA0510050	2.0	3.0
PHA0610005A	1.0	1.0
PHA0610019A	1.0	1.0
PHA0709008	3.0	3.0
PHA0710011	3.0	3.0
PHA0710016	3.0	2.0
PHA0809009	2.0	2.0
PHA0810003	2.0	3.0
PHA0810004	2.0	2.0
PHA0810015	3.0	3.0
PHA1110002B	2.0	1.0
PHA1110015	3.0	3.0
PHA1110021	2.0	2.0
PHA1111001B	1.0	1.0
VAR0909007	2.0	2.0
1325_1001013	2.0	2.0
1325_1001017	2.0	2.0
1325_1001019	2.0	2.0
1325_1001032	2.0	2.0
1325_1001033	2.0	2.0
1325_1001037	2.0	2.0
1325_1001043	2.0	2.0
1325_1001044	2.0	2.0
1325_1001048	2.0	2.0
1325_1001055	2.0	2.0
1325_1001080	2.0	2.0
1325_1001086	2.0	2.0
1325_1001088	2.0	2.0
1325_1001092	2.0	2.0
1325_1001100	2.0	2.0
1325_1001108	2.0	2.0
1325_1001121	2.0	2.0
1325_1001124	2.0	2.0
1325_1001126	2.0	2.0
1325_1001128	2.0	2.0
1325_1001131	2.0	2.0
1325_1001139	2.0	2.0
1325_1001142	2.0	2.0
1325_1001156	2.0	2.0
1325_1001163	2.0	2.0
1325_1001164	2.0	2.0
1325_1001165	2.0	2.0
1325_9000059	2.0	2.0
1325_9000090	2.0	2.0
1325_9000102	2.0	2.0
1325_9000144	2.0	2.0
1325_9000186	2.0	2.0
1325_9000187	2.0	2.0
1325_9000210	1.0	2.0
1325_9000302	2.0	2.0
1325_9000318	2.0	2.0
1325_9000319	2.0	2.0
1325_9000678	2.0	2.0
1325_9000750	2.0	2.0
1365_0100005	1.0	2.0
1365_0100015	1.0	1.0
1365_0100018	1.0	2.0
1365_0100023	1.0	2.0
1365_0100029	1.0	1.0
1365_0100031	2.0	2.0
1365_0100066	1.0	2.0
1365_0100067	1.0	2.0
1365_0100073	2.0	2.0
1365_0100098	1.0	2.0
1365_0100107	2.0	2.0
1365_0100136	2.0	2.0
1365_0100146	2.0	2.0
1365_0100147	2.0	2.0
1365_0100148	2.0	2.0
1365_0100162	2.0	2.0
1365_0100168	2.0	2.0
1365_0100177	2.0	2.0
1365_0100179	2.0	2.0
1365_0100183	2.0	2.0
1365_0100187	2.0	2.0
1365_0100203	2.0	2.0
1365_0100223	2.0	2.0
1365_0100224	2.0	2.0
1365_0100229	2.0	2.0
1365_0100230	2.0	2.0
1365_0100252	2.0	2.0
1365_0100259	2.0	2.0
1365_0100260	2.0	2.0
1365_0100263	2.0	2.0
1365_0100266	2.0	2.0
1365_0100267	2.0	2.0
1365_0100269	2.0	2.0
1365_0100278	2.0	2.0
1365_0100279	2.0	2.0
1365_0100280	1.0	1.0
1365_0100282	2.0	2.0
1365_0100285	2.0	2.0
1365_0100290	2.0	2.0
1365_0100451	2.0	2.0
1365_0100456	2.0	2.0
1365_0100470	2.0	2.0
1365_0100471	1.0	2.0
1365_0100472	2.0	2.0
1365_0100473	2.0	2.0
1365_0100480	2.0	2.0
1385_0000016	1.0	1.0
1385_0000023	1.0	1.0
1385_0000036	1.0	1.0
1385_0000040	1.0	1.0
1385_0000042	1.0	1.0
1385_0000043	1.0	1.0
1385_0000044	1.0	1.0
1385_0000054	1.0	1.0
1385_0000099	1.0	1.0
1385_0000125	1.0	1.0
1385_0000126	1.0	1.0
1385_0001105	1.0	1.0
1385_0001109	1.0	1.0
1385_0001110	1.0	1.0
1385_0001118	1.0	1.0
1385_0001124	1.0	1.0
1385_0001127	1.0	1.0
1385_0001131	1.0	1.0
1385_0001138	1.0	1.0
1385_0001154	1.0	1.0
1385_0001167	1.0	1.0
1385_0001171	1.0	1.0
1385_0001191	1.0	1.0
1385_0001195	1.0	2.0
1385_0001197	1.0	1.0
1385_0001524	1.0	1.0
1385_0001527	1.0	1.0
1385_0001716	1.0	1.0
1385_0001719	1.0	1.0
1385_0001725	1.0	1.0
1385_0001736	2.0	2.0
1385_0001738	0.0	1.0
1385_0001739	1.0	1.0
1385_0001746	1.0	1.0
1385_0001747	1.0	1.0
1385_0001749	1.0	1.0
1385_0001760	1.0	1.0
1385_0001774	0.0	1.0
1385_0001785	1.0	1.0
1385_0001790	1.0	1.0
1395_0000337	0.0	1.0
1395_0000354	1.0	1.0
1395_0000361	1.0	2.0
1395_0000364	1.0	1.0
1395_0000376	2.0	2.0
1395_0000391	2.0	2.0
1395_0000398	2.0	2.0
1395_0000402	1.0	1.0
1395_0000438	2.0	2.0
1395_0000450	1.0	1.0
1395_0000454	1.0	2.0
1395_0000460	1.0	1.0
1395_0000465	1.0	1.0
1395_0000471	1.0	1.0
1395_0000513	2.0	2.0
1395_0000514	2.0	2.0
1395_0000531	1.0	1.0
1395_0000534	1.0	2.0
1395_0000549	1.0	2.0
1395_0000551	2.0	1.0
1395_0000554	1.0	1.0
1395_0000556	1.0	1.0
1395_0000557	2.0	2.0
1395_0000565	1.0	1.0
1395_0000575	1.0	1.0
1395_0000583	1.0	2.0
1395_0000591	0.0	1.0
1395_0000604	1.0	1.0
1395_0000611	1.0	1.0
1395_0000627	1.0	1.0
1395_0000631	1.0	2.0
1395_0001019	1.0	1.0
1395_0001020	1.0	1.0
1395_0001034	1.0	1.0
1395_0001067	1.0	1.0
1395_0001069	1.0	2.0
1395_0001074	1.0	1.0
1395_0001075	1.0	1.0
1395_0001084	1.0	1.0
1395_0001121	0.0	1.0
1395_0001132	2.0	2.0
1395_0001149	1.0	1.0
1395_0001150	1.0	1.0
3 Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.91
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.63
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.81      0.75      0.78       175
         2.0       0.76      0.78      0.77       177
         3.0       0.68      0.97      0.80        75
         4.0       0.00      0.00      0.00         8

    accuracy                           0.76       452
   macro avg       0.45      0.50      0.47       452
weighted avg       0.72      0.76      0.74       452

[[  0  17   0   0   0]
 [  0 132  42   1   0]
 [  0  14 138  25   0]
 [  0   0   2  73   0]
 [  0   0   0   8   0]]
0.7365690710759932
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.59
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.56
              precision    recall  f1-score   support

         0.0       1.00      0.18      0.30        17
         1.0       0.83      0.85      0.84       175
         2.0       0.83      0.76      0.79       177
         3.0       0.68      0.97      0.80        75
         4.0       0.00      0.00      0.00         8

    accuracy                           0.79       452
   macro avg       0.67      0.55      0.55       452
weighted avg       0.80      0.79      0.78       452

[[  3  14   0   0   0]
 [  0 149  25   1   0]
 [  0  17 134  26   0]
 [  0   0   2  73   0]
 [  0   0   0   8   0]]
0.7791607727671391
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.46
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.55
              precision    recall  f1-score   support

         0.0       0.67      0.24      0.35        17
         1.0       0.82      0.82      0.82       175
         2.0       0.81      0.82      0.82       177
         3.0       0.75      0.93      0.83        75
         4.0       0.00      0.00      0.00         8

    accuracy                           0.80       452
   macro avg       0.61      0.56      0.56       452
weighted avg       0.79      0.80      0.79       452

[[  4  13   0   0   0]
 [  2 144  28   1   0]
 [  0  18 145  14   0]
 [  0   0   5  70   0]
 [  0   0   0   8   0]]
0.7898331698540609
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.34
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.64
              precision    recall  f1-score   support

         0.0       0.75      0.18      0.29        17
         1.0       0.87      0.71      0.78       175
         2.0       0.75      0.89      0.82       177
         3.0       0.76      0.95      0.84        75
         4.0       0.00      0.00      0.00         8

    accuracy                           0.79       452
   macro avg       0.63      0.55      0.55       452
weighted avg       0.78      0.79      0.77       452

[[  3  14   0   0   0]
 [  1 125  48   1   0]
 [  0   5 158  14   0]
 [  0   0   4  71   0]
 [  0   0   0   8   0]]
0.773339295448177
452 452 452
Filename	True Label	Prediction
1023_0101695	2.0	2.0
1023_0101700	3.0	3.0
1023_0101701	2.0	3.0
1023_0101841	2.0	3.0
1023_0101846	4.0	3.0
1023_0101848	2.0	2.0
1023_0101852	3.0	3.0
1023_0101854	2.0	2.0
1023_0101855	2.0	2.0
1023_0101898	3.0	3.0
1023_0101906	2.0	3.0
1023_0101909	4.0	3.0
1023_0103827	3.0	3.0
1023_0103841	3.0	3.0
1023_0103843	2.0	2.0
1023_0103880	3.0	3.0
1023_0103955	3.0	3.0
1023_0104203	3.0	3.0
1023_0104209	3.0	3.0
1023_0107672	2.0	3.0
1023_0107740	3.0	3.0
1023_0107780	3.0	3.0
1023_0107781	3.0	3.0
1023_0107788	3.0	3.0
1023_0108304	3.0	3.0
1023_0108307	3.0	3.0
1023_0108422	3.0	3.0
1023_0108649	3.0	3.0
1023_0108751	3.0	3.0
1023_0108811	3.0	3.0
1023_0108813	3.0	3.0
1023_0108885	2.0	2.0
1023_0108887	2.0	2.0
1023_0109029	1.0	2.0
1023_0109033	4.0	3.0
1023_0109038	3.0	3.0
1023_0109039	3.0	3.0
1023_0109096	3.0	3.0
1023_0109151	3.0	3.0
1023_0109247	3.0	3.0
1023_0109400	3.0	3.0
1023_0109522	3.0	3.0
1023_0109591	3.0	3.0
1023_0109651	3.0	3.0
1023_0109721	2.0	3.0
1023_0109917	3.0	3.0
1023_0109946	2.0	3.0
1031_0001703	4.0	3.0
1031_0001950	3.0	3.0
1031_0002088	3.0	3.0
1031_0002091	3.0	3.0
1031_0002092	4.0	3.0
1031_0002185	3.0	3.0
1031_0002199	3.0	3.0
1031_0003035	3.0	3.0
1031_0003043	4.0	3.0
1031_0003052	3.0	3.0
1031_0003054	3.0	3.0
1031_0003074	3.0	3.0
1031_0003076	4.0	3.0
1031_0003164	3.0	3.0
1031_0003170	3.0	3.0
1031_0003174	3.0	3.0
1031_0003185	3.0	3.0
1031_0003187	3.0	3.0
1031_0003189	3.0	3.0
1031_0003218	3.0	3.0
1031_0003237	3.0	3.0
1031_0003246	3.0	3.0
1031_0003273	3.0	3.0
1031_0003274	3.0	3.0
1031_0003313	3.0	3.0
1031_0003315	3.0	3.0
1031_0003327	2.0	3.0
1031_0003352	2.0	3.0
1031_0003356	3.0	3.0
1031_0003369	3.0	3.0
1031_0003388	3.0	3.0
1031_0003390	3.0	3.0
1031_0003392	3.0	3.0
1031_0003408	2.0	3.0
1031_0003409	4.0	3.0
1061_0120271	2.0	2.0
1061_0120278	2.0	2.0
1061_0120283	1.0	1.0
1061_0120287	1.0	2.0
1061_0120288	2.0	2.0
1061_0120291	1.0	1.0
1061_0120296	2.0	2.0
1061_0120299	2.0	2.0
1061_0120308	2.0	3.0
1061_0120313	2.0	2.0
1061_0120316	2.0	2.0
1061_0120317	2.0	2.0
1061_0120323	1.0	2.0
1061_0120326	2.0	2.0
1061_0120331	1.0	1.0
1061_0120336	1.0	2.0
1061_0120337	2.0	2.0
1061_0120346	2.0	2.0
1061_0120348	1.0	1.0
1061_0120356	2.0	2.0
1061_0120360	3.0	3.0
1061_0120367	3.0	2.0
1061_0120369	2.0	2.0
1061_0120370	2.0	2.0
1061_0120372	2.0	2.0
1061_0120383	2.0	3.0
1061_0120390	2.0	2.0
1061_0120411	3.0	3.0
1061_0120423	2.0	3.0
1061_0120441	2.0	2.0
1061_0120450	2.0	2.0
1061_0120455	2.0	2.0
1061_0120456	2.0	2.0
1061_0120478	2.0	3.0
1061_0120490	2.0	2.0
1061_0120499	2.0	2.0
1061_0120876	2.0	2.0
1061_0120877	2.0	2.0
1061_0120882	3.0	3.0
1061_0120883	2.0	2.0
1061_0120886	2.0	2.0
1061_0120889	1.0	1.0
1061_0120894	2.0	2.0
1061_1029117	1.0	2.0
1061_1029118	1.0	2.0
1061_1202914	1.0	1.0
1061_1202915	1.0	2.0
1061_1202918	1.0	2.0
1061_1202919	2.0	2.0
1071_0024678	1.0	1.0
1071_0024681	2.0	2.0
1071_0024685	2.0	2.0
1071_0024687	1.0	1.0
1071_0024692	2.0	2.0
1071_0024710	1.0	2.0
1071_0024711	1.0	1.0
1071_0024713	1.0	2.0
1071_0024757	2.0	2.0
1071_0024772	0.0	0.0
1071_0024802	1.0	2.0
1071_0024807	1.0	1.0
1071_0024809	1.0	1.0
1071_0024814	1.0	1.0
1071_0024816	1.0	1.0
1071_0024824	1.0	1.0
1071_0024826	1.0	1.0
1071_0024833	1.0	1.0
1071_0024834	2.0	2.0
1071_0024841	0.0	1.0
1071_0024850	1.0	1.0
1071_0024862	1.0	1.0
1071_0024864	0.0	0.0
1071_0024867	2.0	2.0
1071_0024873	1.0	1.0
1071_0024875	1.0	1.0
1071_0024878	2.0	2.0
1071_0024879	1.0	1.0
1071_0024881	2.0	2.0
1071_0241832	1.0	1.0
1071_0242011	1.0	1.0
1071_0242013	1.0	1.0
1071_0242041	1.0	1.0
1071_0242071	0.0	1.0
1071_0243582	1.0	1.0
1071_0248301	1.0	1.0
1071_0248302	1.0	0.0
1071_0248303	1.0	1.0
1071_0248308	1.0	1.0
1071_0248313	1.0	2.0
1071_0248320	0.0	0.0
1071_0248325	0.0	1.0
1071_0248326	1.0	1.0
1071_0248332	2.0	2.0
1071_0248333	2.0	1.0
1071_0248335	1.0	1.0
1071_0248342	1.0	1.0
1071_0248344	1.0	1.0
1091_0000003	2.0	2.0
1091_0000013	1.0	1.0
1091_0000029	1.0	2.0
1091_0000032	2.0	2.0
1091_0000033	1.0	2.0
1091_0000048	1.0	1.0
1091_0000051	0.0	1.0
1091_0000052	0.0	1.0
1091_0000060	2.0	2.0
1091_0000126	2.0	2.0
1091_0000127	1.0	2.0
1091_0000140	1.0	1.0
1091_0000154	2.0	2.0
1091_0000156	2.0	2.0
1091_0000160	1.0	3.0
1091_0000163	1.0	1.0
1091_0000170	2.0	1.0
1091_0000192	1.0	2.0
1091_0000199	2.0	2.0
1091_0000212	2.0	2.0
1091_0000220	2.0	2.0
1091_0000222	1.0	2.0
1091_0000223	2.0	2.0
1091_0000230	2.0	2.0
1091_0000231	2.0	2.0
1091_0000232	2.0	2.0
1091_0000234	2.0	2.0
1091_0000235	1.0	2.0
1091_0000239	2.0	2.0
1091_0000243	1.0	1.0
1091_0000245	2.0	2.0
1091_0000247	2.0	2.0
1091_0000249	2.0	2.0
1091_0000259	2.0	2.0
1091_0000270	1.0	1.0
0614	2.0	2.0
0620	1.0	2.0
0621	2.0	2.0
0626	2.0	1.0
0630	1.0	1.0
0631	2.0	2.0
0634	2.0	2.0
0636	2.0	2.0
0641	1.0	1.0
0643	2.0	2.0
0645	2.0	2.0
0720	1.0	1.0
0801	1.0	1.0
0807	2.0	2.0
0811	2.0	2.0
0814	1.0	1.0
0817	1.0	2.0
0820	1.0	1.0
0821	2.0	2.0
0822	1.0	1.0
0829	1.0	2.0
0902	2.0	2.0
0903	1.0	2.0
0904	1.0	2.0
0910	1.0	1.0
0911	1.0	2.0
0913	2.0	2.0
0915	2.0	2.0
0920	2.0	2.0
0921	1.0	1.0
0927	2.0	2.0
1006	2.0	2.0
1008	1.0	2.0
1009	2.0	2.0
1015	1.0	2.0
1018	1.0	1.0
1115	1.0	2.0
1116	1.0	2.0
BER0611005	2.0	2.0
KYJ0611003A	1.0	1.0
KYJ0611004A	1.0	1.0
KYJ0611009B	1.0	2.0
LIB0611001A	1.0	1.0
LIB0611002B	1.0	1.0
LIB0611003A	1.0	1.0
LIB0611004B	1.0	2.0
LON0610002A	1.0	1.0
LON0611002A	1.0	1.0
LON0611003	3.0	3.0
LON0611004A	1.0	1.0
LON0611004B	1.0	1.0
MOS0611012	2.0	2.0
PAR1011018	3.0	2.0
PHA0111018	2.0	2.0
PHA0112002A	1.0	1.0
PHA0112002B	1.0	2.0
PHA0411008B	1.0	1.0
PHA0411031	3.0	3.0
PHA0411054	3.0	2.0
PHA0509030	3.0	3.0
PHA0509033	1.0	2.0
PHA0509035	2.0	2.0
PHA0509037	3.0	3.0
PHA0509040	2.0	2.0
PHA0510010B	1.0	1.0
PHA0510031	2.0	2.0
PHA0510039	2.0	3.0
PHA0510047	2.0	2.0
PHA0610006B	1.0	1.0
PHA0610007A	1.0	1.0
PHA0710009	2.0	2.0
PHA0710013	3.0	3.0
PHA0810002	2.0	2.0
PHA0810012	2.0	2.0
PHA0811016	2.0	2.0
PHA0811017	3.0	3.0
PHA0811019	3.0	3.0
PHA1109004	3.0	3.0
PHA1109005	2.0	2.0
PHA1109027	3.0	2.0
PHA1110001B	1.0	1.0
PHA1110016	2.0	2.0
PHA1110022	3.0	3.0
PHA1111003B	1.0	1.0
PHA1111004A	1.0	1.0
PHA1111006A	1.0	1.0
PHA1111006B	1.0	1.0
PHA1111008A	1.0	1.0
ST071122B	1.0	1.0
VAR0909006	3.0	3.0
VAR0910005	2.0	2.0
VAR0910009	3.0	3.0
VAR0910011	2.0	2.0
1325_1001015	2.0	2.0
1325_1001028	2.0	2.0
1325_1001029	2.0	2.0
1325_1001042	2.0	2.0
1325_1001054	2.0	2.0
1325_1001058	2.0	2.0
1325_1001076	2.0	2.0
1325_1001077	2.0	2.0
1325_1001079	2.0	2.0
1325_1001089	2.0	2.0
1325_1001090	2.0	2.0
1325_1001095	2.0	2.0
1325_1001098	2.0	2.0
1325_1001101	2.0	2.0
1325_1001110	2.0	2.0
1325_1001113	2.0	2.0
1325_1001120	2.0	2.0
1325_1001132	2.0	2.0
1325_1001135	2.0	2.0
1325_1001152	2.0	2.0
1325_1001153	2.0	2.0
1325_1001154	2.0	2.0
1325_1001155	2.0	2.0
1325_1001157	2.0	2.0
1325_1001158	2.0	2.0
1325_1001166	2.0	2.0
1325_9000107	2.0	2.0
1325_9000279	2.0	2.0
1325_9000322	2.0	2.0
1325_9000504	2.0	2.0
1325_9000536	2.0	2.0
1325_9000602	2.0	2.0
1325_9000611	2.0	2.0
1365_0100010	1.0	1.0
1365_0100022	2.0	2.0
1365_0100026	1.0	1.0
1365_0100051	1.0	1.0
1365_0100057	2.0	2.0
1365_0100063	2.0	2.0
1365_0100064	2.0	2.0
1365_0100074	1.0	2.0
1365_0100093	2.0	2.0
1365_0100099	1.0	2.0
1365_0100105	2.0	2.0
1365_0100116	2.0	2.0
1365_0100117	2.0	2.0
1365_0100134	2.0	2.0
1365_0100135	2.0	2.0
1365_0100171	1.0	2.0
1365_0100182	2.0	2.0
1365_0100185	1.0	2.0
1365_0100205	2.0	2.0
1365_0100219	2.0	2.0
1365_0100221	2.0	2.0
1365_0100225	2.0	2.0
1365_0100253	1.0	1.0
1365_0100256	2.0	2.0
1365_0100262	2.0	2.0
1365_0100268	1.0	2.0
1365_0100274	2.0	2.0
1365_0100289	2.0	2.0
1365_0100455	2.0	2.0
1365_0100459	2.0	2.0
1365_0100469	2.0	2.0
1365_0100476	2.0	2.0
1365_0100479	2.0	2.0
1385_0000013	0.0	1.0
1385_0000017	1.0	1.0
1385_0000035	1.0	2.0
1385_0000041	1.0	1.0
1385_0000045	1.0	1.0
1385_0000100	1.0	1.0
1385_0000123	1.0	1.0
1385_0000128	1.0	1.0
1385_0001112	1.0	1.0
1385_0001121	1.0	1.0
1385_0001123	1.0	1.0
1385_0001128	0.0	1.0
1385_0001129	1.0	1.0
1385_0001137	1.0	1.0
1385_0001150	1.0	1.0
1385_0001155	1.0	1.0
1385_0001162	1.0	1.0
1385_0001163	1.0	1.0
1385_0001165	1.0	1.0
1385_0001166	1.0	1.0
1385_0001528	1.0	2.0
1385_0001718	1.0	1.0
1385_0001720	0.0	1.0
1385_0001723	0.0	1.0
1385_0001727	0.0	1.0
1385_0001728	1.0	2.0
1385_0001734	1.0	1.0
1385_0001750	0.0	1.0
1385_0001751	1.0	1.0
1385_0001758	1.0	1.0
1385_0001771	1.0	1.0
1385_0001772	1.0	1.0
1385_0001773	1.0	1.0
1385_0001791	1.0	1.0
1385_0001793	1.0	1.0
1385_0001796	1.0	1.0
1395_0000338	1.0	1.0
1395_0000340	1.0	2.0
1395_0000355	1.0	2.0
1395_0000359	1.0	2.0
1395_0000369	2.0	2.0
1395_0000378	1.0	2.0
1395_0000379	1.0	1.0
1395_0000380	1.0	2.0
1395_0000389	0.0	1.0
1395_0000396	1.0	1.0
1395_0000432	1.0	1.0
1395_0000448	1.0	1.0
1395_0000458	1.0	1.0
1395_0000499	1.0	1.0
1395_0000500	1.0	1.0
1395_0000518	2.0	2.0
1395_0000526	1.0	1.0
1395_0000535	1.0	1.0
1395_0000559	1.0	1.0
1395_0000563	1.0	1.0
1395_0000579	1.0	1.0
1395_0000582	0.0	1.0
1395_0000585	1.0	1.0
1395_0000587	0.0	1.0
1395_0000608	1.0	1.0
1395_0000649	2.0	2.0
1395_0001016	2.0	1.0
1395_0001017	1.0	1.0
1395_0001024	1.0	2.0
1395_0001040	1.0	1.0
1395_0001045	1.0	1.0
1395_0001073	2.0	2.0
1395_0001109	1.0	1.0
1395_0001114	1.0	1.0
1395_0001115	2.0	2.0
1395_0001117	1.0	1.0
1395_0001122	1.0	1.0
1395_0001123	1.0	1.0
1395_0001124	1.0	1.0
1395_0001160	2.0	1.0
1395_0001164	2.0	2.0
1395_0001167	1.0	2.0
1395_0001170	1.0	2.0
4 Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.88
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.65
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.74      0.85      0.79       175
         2.0       0.73      0.76      0.74       177
         3.0       0.74      0.65      0.70        75
         4.0       0.00      0.00      0.00         8

    accuracy                           0.74       452
   macro avg       0.44      0.45      0.45       452
weighted avg       0.70      0.74      0.71       452

[[  0  17   0   0   0]
 [  0 149  26   0   0]
 [  0  33 135   9   0]
 [  0   1  25  49   0]
 [  0   0   0   8   0]]
0.7142637584139785
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.61
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.59
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.80      0.75      0.78       175
         2.0       0.72      0.81      0.76       177
         3.0       0.70      0.81      0.75        75
         4.0       0.00      0.00      0.00         8

    accuracy                           0.75       452
   macro avg       0.44      0.48      0.46       452
weighted avg       0.71      0.75      0.72       452

[[  0  17   0   0   0]
 [  0 132  43   0   0]
 [  0  15 144  18   0]
 [  0   1  13  61   0]
 [  0   0   0   8   0]]
0.7247316118010262
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.47
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.59
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.79      0.82      0.80       175
         2.0       0.76      0.80      0.78       177
         3.0       0.73      0.79      0.76        75
         4.0       0.00      0.00      0.00         8

    accuracy                           0.76       452
   macro avg       0.45      0.48      0.47       452
weighted avg       0.72      0.76      0.74       452

[[  0  17   0   0   0]
 [  0 144  31   0   0]
 [  0  21 142  14   0]
 [  0   1  15  59   0]
 [  0   0   0   8   0]]
0.7416669028340767
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.38
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.71
              precision    recall  f1-score   support

         0.0       1.00      0.18      0.30        17
         1.0       0.85      0.69      0.76       175
         2.0       0.71      0.86      0.78       177
         3.0       0.72      0.91      0.80        75
         4.0       0.00      0.00      0.00         8

    accuracy                           0.76       452
   macro avg       0.66      0.53      0.53       452
weighted avg       0.77      0.76      0.74       452

[[  3  14   0   0   0]
 [  0 120  55   0   0]
 [  0   7 152  18   0]
 [  0   0   7  68   0]
 [  0   0   0   8   0]]
0.7433247572506795
452 452 452
Filename	True Label	Prediction
1023_0001416	3.0	3.0
1023_0101751	3.0	3.0
1023_0101752	3.0	3.0
1023_0101849	3.0	3.0
1023_0101853	2.0	3.0
1023_0101896	2.0	2.0
1023_0103832	2.0	2.0
1023_0103837	3.0	3.0
1023_0103838	3.0	3.0
1023_0104206	3.0	2.0
1023_0107042	3.0	3.0
1023_0107074	3.0	3.0
1023_0107783	3.0	2.0
1023_0108305	3.0	3.0
1023_0108650	3.0	3.0
1023_0108752	3.0	3.0
1023_0108753	2.0	2.0
1023_0108812	2.0	3.0
1023_0108814	3.0	3.0
1023_0108932	2.0	3.0
1023_0108933	3.0	3.0
1023_0108934	3.0	3.0
1023_0108955	3.0	3.0
1023_0108958	2.0	3.0
1023_0109027	2.0	2.0
1023_0109030	3.0	3.0
1023_0109391	2.0	3.0
1023_0109392	3.0	3.0
1023_0109401	3.0	3.0
1023_0109515	3.0	3.0
1023_0109588	3.0	3.0
1023_0109716	3.0	3.0
1023_0109878	2.0	2.0
1023_0109914	2.0	3.0
1031_0002004	3.0	3.0
1031_0002011	4.0	3.0
1031_0002042	3.0	3.0
1031_0002085	3.0	3.0
1031_0002200	2.0	3.0
1031_0003012	3.0	3.0
1031_0003013	4.0	3.0
1031_0003023	3.0	3.0
1031_0003048	4.0	3.0
1031_0003088	4.0	3.0
1031_0003097	3.0	3.0
1031_0003106	3.0	3.0
1031_0003135	3.0	3.0
1031_0003144	3.0	3.0
1031_0003145	3.0	3.0
1031_0003149	3.0	3.0
1031_0003156	3.0	3.0
1031_0003157	4.0	3.0
1031_0003161	3.0	3.0
1031_0003167	3.0	3.0
1031_0003172	3.0	3.0
1031_0003173	3.0	3.0
1031_0003179	3.0	3.0
1031_0003183	4.0	3.0
1031_0003190	3.0	3.0
1031_0003205	3.0	3.0
1031_0003219	3.0	3.0
1031_0003230	3.0	3.0
1031_0003232	2.0	3.0
1031_0003244	4.0	3.0
1031_0003309	3.0	3.0
1031_0003314	3.0	3.0
1031_0003330	3.0	3.0
1031_0003353	2.0	3.0
1031_0003366	3.0	3.0
1031_0003415	4.0	3.0
1031_0003419	3.0	3.0
1061_0120273	1.0	2.0
1061_0120281	1.0	2.0
1061_0120289	1.0	2.0
1061_0120303	1.0	2.0
1061_0120304	2.0	2.0
1061_0120309	1.0	1.0
1061_0120315	2.0	1.0
1061_0120325	2.0	2.0
1061_0120332	1.0	2.0
1061_0120338	1.0	2.0
1061_0120345	2.0	2.0
1061_0120350	2.0	2.0
1061_0120357	3.0	3.0
1061_0120368	2.0	2.0
1061_0120376	2.0	2.0
1061_0120386	1.0	2.0
1061_0120387	1.0	2.0
1061_0120394	2.0	2.0
1061_0120406	2.0	2.0
1061_0120413	1.0	1.0
1061_0120424	2.0	2.0
1061_0120431	2.0	2.0
1061_0120432	2.0	2.0
1061_0120442	2.0	2.0
1061_0120448	3.0	2.0
1061_0120459	2.0	2.0
1061_0120479	2.0	2.0
1061_0120480	2.0	2.0
1061_0120483	1.0	2.0
1061_0120489	2.0	2.0
1061_0120496	2.0	2.0
1061_0120856	1.0	2.0
1061_0120859	2.0	2.0
1061_0120875	3.0	3.0
1061_0120884	2.0	2.0
1061_1029112	3.0	3.0
1061_1029115	2.0	2.0
1061_1029120	1.0	2.0
1061_1202916	2.0	2.0
1071_0024701	2.0	2.0
1071_0024706	1.0	1.0
1071_0024715	1.0	1.0
1071_0024716	1.0	1.0
1071_0024758	2.0	2.0
1071_0024763	1.0	1.0
1071_0024766	1.0	1.0
1071_0024769	0.0	1.0
1071_0024774	0.0	0.0
1071_0024781	1.0	1.0
1071_0024782	0.0	0.0
1071_0024784	1.0	1.0
1071_0024798	0.0	1.0
1071_0024800	1.0	1.0
1071_0024804	1.0	1.0
1071_0024808	1.0	1.0
1071_0024818	2.0	1.0
1071_0024835	1.0	1.0
1071_0024843	1.0	1.0
1071_0024844	1.0	1.0
1071_0024851	1.0	1.0
1071_0024853	1.0	1.0
1071_0024857	1.0	1.0
1071_0024863	1.0	1.0
1071_0241833	1.0	1.0
1071_0242042	1.0	1.0
1071_0242043	0.0	1.0
1071_0242072	0.0	1.0
1071_0242092	0.0	1.0
1071_0242093	0.0	1.0
1071_0243593	1.0	2.0
1071_0243621	2.0	2.0
1071_0243622	1.0	1.0
1071_0243623	1.0	1.0
1071_0248309	2.0	2.0
1071_0248311	1.0	1.0
1071_0248312	1.0	1.0
1071_0248314	1.0	1.0
1071_0248323	1.0	1.0
1071_0248324	0.0	1.0
1071_0248328	0.0	1.0
1071_0248330	2.0	2.0
1071_0248337	1.0	2.0
1071_0248340	0.0	0.0
1091_0000010	2.0	2.0
1091_0000011	2.0	2.0
1091_0000018	2.0	2.0
1091_0000022	3.0	2.0
1091_0000046	1.0	1.0
1091_0000047	1.0	1.0
1091_0000049	1.0	1.0
1091_0000059	2.0	2.0
1091_0000061	1.0	1.0
1091_0000064	1.0	1.0
1091_0000070	1.0	1.0
1091_0000077	1.0	1.0
1091_0000087	1.0	2.0
1091_0000092	2.0	2.0
1091_0000095	2.0	2.0
1091_0000125	2.0	2.0
1091_0000148	1.0	1.0
1091_0000152	1.0	1.0
1091_0000157	2.0	2.0
1091_0000162	1.0	2.0
1091_0000167	2.0	2.0
1091_0000169	1.0	2.0
1091_0000171	2.0	2.0
1091_0000194	2.0	2.0
1091_0000196	2.0	1.0
1091_0000200	1.0	2.0
1091_0000201	1.0	2.0
1091_0000203	1.0	1.0
1091_0000221	1.0	1.0
1091_0000236	1.0	2.0
1091_0000246	2.0	2.0
1091_0000250	1.0	2.0
1091_0000253	0.0	1.0
1091_0000256	2.0	2.0
1091_0000257	2.0	2.0
1091_0000265	1.0	2.0
1091_0000267	1.0	2.0
1091_0000271	1.0	1.0
1091_0000273	2.0	2.0
1091_0000276	3.0	2.0
0601	1.0	1.0
0602	1.0	2.0
0604	2.0	2.0
0616	1.0	2.0
0632	1.0	1.0
0635	1.0	2.0
0637	2.0	2.0
0640	2.0	2.0
0717	1.0	1.0
0718	1.0	2.0
0724	2.0	2.0
0805	2.0	2.0
0815	2.0	2.0
0905	2.0	2.0
0907	2.0	2.0
0912	2.0	2.0
0914	1.0	2.0
0919	1.0	2.0
0925	2.0	2.0
0929	1.0	1.0
1016	1.0	1.0
1017	1.0	2.0
1019	1.0	2.0
1020	2.0	2.0
1021	1.0	2.0
1112	1.0	2.0
1113	1.0	2.0
9999	1.0	1.0
KYJ0611005B	1.0	1.0
KYJ0611009A	1.0	1.0
LON0610002B	1.0	1.0
LON0611002B	1.0	1.0
MOS0611013	2.0	3.0
PAR1011009B	1.0	1.0
PHA0111002A	2.0	1.0
PHA0111003A	1.0	1.0
PHA0111010	3.0	3.0
PHA0111012	2.0	2.0
PHA0112007A	1.0	1.0
PHA0209001	1.0	2.0
PHA0209008	1.0	1.0
PHA0209031	3.0	3.0
PHA0411009B	1.0	1.0
PHA0411010A	1.0	1.0
PHA0411034	1.0	2.0
PHA0411035	3.0	2.0
PHA0411038	3.0	3.0
PHA0411047	2.0	3.0
PHA0411062	2.0	3.0
PHA0509002	1.0	1.0
PHA0509021	2.0	2.0
PHA0509024	2.0	2.0
PHA0509028	3.0	3.0
PHA0509043	3.0	3.0
PHA0509045	2.0	2.0
PHA0510004A	1.0	1.0
PHA0510027	2.0	2.0
PHA0510034	3.0	3.0
PHA0510038	3.0	3.0
PHA0510040	2.0	3.0
PHA0510049	2.0	3.0
PHA0610007B	1.0	1.0
PHA0610015	2.0	3.0
PHA0610016	3.0	3.0
PHA0710010	2.0	3.0
PHA0710012	3.0	3.0
PHA0710014	3.0	3.0
PHA0710015	3.0	3.0
PHA0710017	3.0	3.0
PHA0710018	3.0	2.0
PHA0710021	3.0	3.0
PHA0810010	2.0	3.0
PHA0810011	2.0	2.0
PHA0811012	3.0	3.0
PHA0811013	3.0	3.0
PHA0811020	2.0	2.0
PHA1109003	2.0	2.0
PHA1109024	3.0	3.0
PHA1109026	3.0	3.0
PHA1110003A	1.0	1.0
VAR0209036	2.0	3.0
VAR0909004	2.0	2.0
VAR0910004	3.0	3.0
VAR0910006	2.0	2.0
1325_1001008	2.0	2.0
1325_1001010	2.0	2.0
1325_1001011	2.0	2.0
1325_1001012	2.0	2.0
1325_1001016	2.0	2.0
1325_1001020	2.0	2.0
1325_1001021	2.0	2.0
1325_1001022	2.0	2.0
1325_1001027	2.0	2.0
1325_1001045	2.0	2.0
1325_1001046	2.0	2.0
1325_1001047	2.0	2.0
1325_1001050	2.0	2.0
1325_1001052	2.0	2.0
1325_1001059	2.0	2.0
1325_1001081	2.0	2.0
1325_1001097	1.0	2.0
1325_1001107	2.0	2.0
1325_1001109	2.0	2.0
1325_1001119	2.0	2.0
1325_1001130	2.0	2.0
1325_1001144	2.0	2.0
1325_1001169	2.0	2.0
1325_9000087	2.0	2.0
1325_9000138	2.0	2.0
1325_9000152	2.0	2.0
1325_9000188	2.0	2.0
1325_9000209	2.0	2.0
1325_9000215	2.0	2.0
1325_9000241	2.0	2.0
1325_9000278	2.0	2.0
1325_9000303	2.0	2.0
1325_9000304	2.0	2.0
1325_9000314	2.0	2.0
1325_9000533	2.0	2.0
1325_9000534	2.0	2.0
1325_9000601	2.0	2.0
1325_9000676	2.0	2.0
1325_9000684	2.0	2.0
1325_9000685	2.0	2.0
1325_9000686	2.0	2.0
1365_0100002	2.0	2.0
1365_0100007	1.0	1.0
1365_0100012	2.0	2.0
1365_0100027	2.0	2.0
1365_0100028	2.0	2.0
1365_0100058	2.0	2.0
1365_0100069	2.0	2.0
1365_0100070	2.0	2.0
1365_0100072	2.0	2.0
1365_0100080	2.0	2.0
1365_0100094	2.0	2.0
1365_0100102	2.0	2.0
1365_0100118	2.0	2.0
1365_0100123	2.0	2.0
1365_0100137	2.0	2.0
1365_0100138	2.0	2.0
1365_0100151	1.0	2.0
1365_0100163	2.0	2.0
1365_0100165	2.0	2.0
1365_0100166	1.0	2.0
1365_0100167	1.0	2.0
1365_0100172	2.0	2.0
1365_0100173	2.0	2.0
1365_0100174	1.0	2.0
1365_0100175	2.0	2.0
1365_0100178	2.0	2.0
1365_0100180	1.0	1.0
1365_0100181	1.0	2.0
1365_0100184	2.0	2.0
1365_0100188	2.0	2.0
1365_0100204	2.0	2.0
1365_0100213	2.0	2.0
1365_0100215	2.0	2.0
1365_0100220	2.0	2.0
1365_0100222	2.0	2.0
1365_0100227	2.0	2.0
1365_0100257	2.0	2.0
1365_0100276	2.0	2.0
1365_0100281	2.0	2.0
1365_0100287	2.0	2.0
1365_0100288	2.0	2.0
1365_0100299	2.0	2.0
1365_0100474	2.0	2.0
1365_0100482	2.0	2.0
1385_0000012	1.0	1.0
1385_0000020	1.0	1.0
1385_0000037	1.0	1.0
1385_0000038	1.0	1.0
1385_0000049	1.0	1.0
1385_0000052	1.0	1.0
1385_0000053	1.0	1.0
1385_0000059	1.0	1.0
1385_0000097	1.0	1.0
1385_0000102	1.0	1.0
1385_0000103	1.0	1.0
1385_0000104	1.0	1.0
1385_0000114	1.0	1.0
1385_0000122	1.0	1.0
1385_0001103	1.0	1.0
1385_0001104	1.0	1.0
1385_0001108	1.0	1.0
1385_0001113	1.0	1.0
1385_0001120	1.0	1.0
1385_0001122	1.0	1.0
1385_0001134	1.0	1.0
1385_0001136	1.0	1.0
1385_0001151	1.0	1.0
1385_0001152	1.0	1.0
1385_0001156	1.0	1.0
1385_0001157	1.0	2.0
1385_0001159	1.0	1.0
1385_0001169	1.0	1.0
1385_0001173	0.0	1.0
1385_0001174	1.0	1.0
1385_0001194	1.0	1.0
1385_0001196	1.0	1.0
1385_0001198	2.0	1.0
1385_0001523	1.0	1.0
1385_0001714	1.0	1.0
1385_0001717	2.0	2.0
1385_0001726	1.0	1.0
1385_0001733	1.0	1.0
1385_0001744	0.0	1.0
1385_0001753	1.0	1.0
1385_0001754	1.0	1.0
1385_0001761	1.0	1.0
1385_0001762	1.0	1.0
1385_0001764	1.0	1.0
1385_0001765	0.0	1.0
1385_0001768	1.0	1.0
1385_0001775	1.0	1.0
1385_0001786	1.0	1.0
1385_0001794	1.0	1.0
1385_0001795	1.0	1.0
1395_0000333	1.0	2.0
1395_0000357	2.0	2.0
1395_0000360	2.0	2.0
1395_0000383	1.0	2.0
1395_0000399	1.0	1.0
1395_0000404	1.0	2.0
1395_0000415	1.0	1.0
1395_0000443	2.0	2.0
1395_0000446	2.0	2.0
1395_0000447	1.0	1.0
1395_0000470	1.0	1.0
1395_0000504	1.0	1.0
1395_0000515	2.0	1.0
1395_0000547	1.0	1.0
1395_0000548	1.0	2.0
1395_0000550	1.0	2.0
1395_0000584	0.0	1.0
1395_0000609	1.0	1.0
1395_0000626	2.0	2.0
1395_0000630	1.0	2.0
1395_0000635	1.0	1.0
1395_0000639	1.0	2.0
1395_0000644	1.0	2.0
1395_0000646	1.0	1.0
1395_0001013	1.0	1.0
1395_0001021	1.0	1.0
1395_0001033	1.0	2.0
1395_0001060	2.0	2.0
1395_0001064	1.0	2.0
1395_0001065	1.0	2.0
1395_0001066	1.0	2.0
1395_0001090	2.0	2.0
1395_0001104	1.0	1.0
1395_0001118	1.0	1.0
1395_0001126	1.0	1.0
1395_0001133	1.0	1.0
1395_0001146	0.0	1.0
1395_0001158	2.0	1.0
1395_0001161	1.0	2.0
5 Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.94
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.68
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.77      0.81      0.79       175
         2.0       0.76      0.65      0.70       177
         3.0       0.60      0.95      0.74        74
         4.0       0.00      0.00      0.00         9

    accuracy                           0.72       452
   macro avg       0.43      0.48      0.45       452
weighted avg       0.69      0.72      0.70       452

[[  0  17   0   0   0]
 [  0 142  32   1   0]
 [  0  26 115  36   0]
 [  0   0   4  70   0]
 [  0   0   0   9   0]]
0.7006586836362074
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.62
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.53
              precision    recall  f1-score   support

         0.0       1.00      0.18      0.30        17
         1.0       0.79      0.91      0.85       175
         2.0       0.84      0.76      0.80       177
         3.0       0.74      0.86      0.80        74
         4.0       0.00      0.00      0.00         9

    accuracy                           0.80       452
   macro avg       0.67      0.54      0.55       452
weighted avg       0.79      0.80      0.78       452

[[  3  14   0   0   0]
 [  0 159  16   0   0]
 [  0  28 135  14   0]
 [  0   0  10  64   0]
 [  0   0   0   9   0]]
0.781699446526462
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.48
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.77
              precision    recall  f1-score   support

         0.0       1.00      0.24      0.38        17
         1.0       0.87      0.66      0.75       175
         2.0       0.70      0.82      0.76       177
         3.0       0.66      0.95      0.78        74
         4.0       0.00      0.00      0.00         9

    accuracy                           0.74       452
   macro avg       0.65      0.53      0.53       452
weighted avg       0.76      0.74      0.73       452

[[  4  13   0   0   0]
 [  0 116  59   0   0]
 [  0   4 146  27   0]
 [  0   0   4  70   0]
 [  0   0   0   9   0]]
0.7295272143958464
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.38
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.68
              precision    recall  f1-score   support

         0.0       1.00      0.29      0.45        17
         1.0       0.88      0.77      0.82       175
         2.0       0.76      0.86      0.81       177
         3.0       0.71      0.92      0.80        74
         4.0       0.00      0.00      0.00         9

    accuracy                           0.79       452
   macro avg       0.67      0.57      0.58       452
weighted avg       0.79      0.79      0.78       452

[[  5  12   0   0   0]
 [  0 134  41   0   0]
 [  0   6 152  19   0]
 [  0   0   6  68   0]
 [  0   0   0   9   0]]
0.7819883454836184
452 452 452
Filename	True Label	Prediction
1023_0001422	2.0	2.0
1023_0101753	3.0	3.0
1023_0101895	3.0	3.0
1023_0101899	2.0	2.0
1023_0101900	3.0	3.0
1023_0101901	3.0	3.0
1023_0101904	2.0	3.0
1023_0102117	3.0	3.0
1023_0103825	3.0	3.0
1023_0103828	1.0	1.0
1023_0103840	3.0	3.0
1023_0104207	2.0	3.0
1023_0106816	3.0	3.0
1023_0107727	3.0	3.0
1023_0107773	2.0	3.0
1023_0108510	3.0	3.0
1023_0108518	3.0	3.0
1023_0108520	3.0	2.0
1023_0108810	3.0	3.0
1023_0108890	3.0	3.0
1023_0108908	3.0	3.0
1023_0108992	3.0	3.0
1023_0109026	2.0	2.0
1023_0109192	3.0	3.0
1023_0109250	2.0	2.0
1023_0109422	3.0	3.0
1023_0109496	3.0	3.0
1023_0109519	2.0	3.0
1023_0109520	2.0	3.0
1023_0109527	3.0	3.0
1023_0109609	2.0	2.0
1023_0109649	3.0	3.0
1023_0109671	3.0	3.0
1023_0109674	3.0	3.0
1023_0109915	2.0	2.0
1023_0109954	3.0	3.0
1023_0111896	2.0	2.0
1031_0001998	4.0	3.0
1031_0002002	2.0	3.0
1031_0002010	3.0	3.0
1031_0002079	4.0	3.0
1031_0002083	3.0	3.0
1031_0002087	3.0	3.0
1031_0002131	3.0	3.0
1031_0002184	3.0	3.0
1031_0002197	3.0	3.0
1031_0003042	3.0	3.0
1031_0003073	4.0	3.0
1031_0003077	3.0	3.0
1031_0003090	3.0	3.0
1031_0003091	2.0	3.0
1031_0003126	3.0	3.0
1031_0003127	4.0	3.0
1031_0003129	3.0	3.0
1031_0003130	4.0	3.0
1031_0003150	3.0	3.0
1031_0003154	3.0	3.0
1031_0003162	3.0	3.0
1031_0003165	2.0	3.0
1031_0003169	3.0	3.0
1031_0003181	4.0	3.0
1031_0003184	4.0	3.0
1031_0003203	2.0	2.0
1031_0003206	3.0	3.0
1031_0003207	4.0	3.0
1031_0003214	3.0	3.0
1031_0003225	3.0	3.0
1031_0003226	3.0	3.0
1031_0003231	3.0	3.0
1031_0003233	3.0	3.0
1031_0003235	3.0	3.0
1031_0003243	3.0	3.0
1031_0003260	3.0	3.0
1031_0003262	3.0	3.0
1031_0003272	3.0	2.0
1031_0003337	3.0	3.0
1031_0003339	3.0	3.0
1031_0003354	3.0	3.0
1031_0003357	3.0	3.0
1031_0003358	4.0	3.0
1031_0003359	2.0	3.0
1031_0003383	3.0	3.0
1031_0003391	2.0	3.0
1031_0003410	3.0	3.0
1061_0120272	1.0	1.0
1061_0120284	0.0	0.0
1061_0120300	2.0	2.0
1061_0120301	2.0	2.0
1061_0120306	3.0	2.0
1061_0120307	2.0	2.0
1061_0120310	2.0	2.0
1061_0120319	2.0	2.0
1061_0120320	3.0	3.0
1061_0120321	2.0	2.0
1061_0120330	2.0	2.0
1061_0120333	2.0	3.0
1061_0120341	1.0	1.0
1061_0120349	1.0	1.0
1061_0120351	2.0	2.0
1061_0120353	1.0	1.0
1061_0120361	2.0	2.0
1061_0120382	1.0	2.0
1061_0120384	1.0	1.0
1061_0120389	2.0	2.0
1061_0120405	2.0	2.0
1061_0120407	3.0	2.0
1061_0120408	2.0	2.0
1061_0120409	2.0	2.0
1061_0120414	2.0	2.0
1061_0120415	2.0	2.0
1061_0120425	2.0	2.0
1061_0120426	2.0	2.0
1061_0120427	2.0	2.0
1061_0120428	2.0	2.0
1061_0120429	2.0	2.0
1061_0120433	1.0	1.0
1061_0120440	1.0	1.0
1061_0120453	2.0	2.0
1061_0120485	2.0	2.0
1061_0120487	2.0	2.0
1061_0120488	2.0	2.0
1061_0120493	2.0	2.0
1061_0120878	1.0	2.0
1061_0120888	1.0	2.0
1061_0120890	1.0	1.0
1061_1029113	1.0	2.0
1061_1202917	1.0	1.0
1071_0024691	1.0	2.0
1071_0024699	1.0	1.0
1071_0024703	1.0	1.0
1071_0024708	1.0	1.0
1071_0024761	1.0	1.0
1071_0024762	1.0	1.0
1071_0024767	2.0	2.0
1071_0024768	1.0	1.0
1071_0024773	1.0	1.0
1071_0024776	0.0	0.0
1071_0024797	0.0	1.0
1071_0024803	1.0	1.0
1071_0024806	1.0	1.0
1071_0024811	1.0	1.0
1071_0024812	1.0	1.0
1071_0024817	1.0	1.0
1071_0024836	1.0	2.0
1071_0024838	0.0	0.0
1071_0024845	0.0	1.0
1071_0024846	1.0	1.0
1071_0024847	1.0	2.0
1071_0024848	1.0	1.0
1071_0024849	0.0	0.0
1071_0024854	0.0	1.0
1071_0024861	0.0	1.0
1071_0024865	2.0	2.0
1071_0024872	1.0	1.0
1071_0024877	1.0	1.0
1071_0242012	1.0	2.0
1071_0242023	1.0	1.0
1071_0243502	1.0	1.0
1071_0243591	1.0	2.0
1071_0243592	1.0	1.0
1071_0248305	0.0	1.0
1071_0248307	2.0	1.0
1071_0248315	0.0	0.0
1071_0248321	1.0	1.0
1071_0248331	1.0	1.0
1071_0248334	2.0	2.0
1071_0248336	1.0	1.0
1071_0248341	1.0	1.0
1071_0248346	1.0	1.0
1071_0248348	1.0	1.0
1071_0248349	1.0	1.0
1091_0000005	2.0	2.0
1091_0000006	0.0	1.0
1091_0000008	2.0	2.0
1091_0000014	0.0	1.0
1091_0000024	1.0	1.0
1091_0000026	1.0	1.0
1091_0000028	0.0	1.0
1091_0000034	1.0	1.0
1091_0000036	1.0	2.0
1091_0000037	1.0	1.0
1091_0000039	1.0	1.0
1091_0000041	1.0	1.0
1091_0000045	2.0	2.0
1091_0000056	2.0	2.0
1091_0000057	1.0	1.0
1091_0000058	2.0	2.0
1091_0000063	1.0	1.0
1091_0000065	2.0	2.0
1091_0000066	1.0	1.0
1091_0000067	2.0	2.0
1091_0000069	1.0	1.0
1091_0000072	2.0	2.0
1091_0000074	2.0	2.0
1091_0000086	1.0	2.0
1091_0000101	1.0	1.0
1091_0000114	2.0	2.0
1091_0000161	2.0	2.0
1091_0000166	2.0	2.0
1091_0000173	2.0	1.0
1091_0000174	1.0	1.0
1091_0000198	1.0	2.0
1091_0000202	2.0	2.0
1091_0000205	2.0	2.0
1091_0000219	2.0	2.0
1091_0000226	1.0	1.0
1091_0000228	2.0	2.0
1091_0000242	2.0	2.0
1091_0000244	2.0	2.0
1091_0000261	2.0	2.0
1091_0000268	2.0	2.0
0603	2.0	2.0
0605	2.0	2.0
0606	2.0	2.0
0607	2.0	2.0
0609	1.0	2.0
0612	1.0	1.0
0617	1.0	2.0
0628	2.0	2.0
0629	2.0	2.0
0638	1.0	2.0
0639	1.0	2.0
0721	2.0	2.0
0722	2.0	2.0
0725	1.0	2.0
0803	1.0	2.0
0810	1.0	2.0
0916	1.0	1.0
0917	1.0	2.0
0918	1.0	2.0
0926	2.0	2.0
1004	1.0	1.0
1111	1.0	1.0
1114	2.0	2.0
BER0611003	2.0	3.0
BER0611006	2.0	3.0
LIB0611002A	1.0	2.0
LIB0611011	1.0	2.0
MOS0611014	1.0	2.0
PAR1011017	3.0	3.0
PHA0111002B	2.0	2.0
PHA0111003B	1.0	1.0
PHA0111004A	1.0	1.0
PHA0111011	2.0	2.0
PHA0112007B	1.0	1.0
PHA0112012A	1.0	2.0
PHA0209024	2.0	2.0
PHA0209039	2.0	3.0
PHA0210001	1.0	1.0
PHA0210004	1.0	1.0
PHA0210008	1.0	1.0
PHA0411009A	1.0	2.0
PHA0411027	2.0	2.0
PHA0411028	2.0	2.0
PHA0411029	2.0	2.0
PHA0411041	3.0	3.0
PHA0411051	3.0	3.0
PHA0411053	3.0	3.0
PHA0411058	3.0	3.0
PHA0411059	3.0	3.0
PHA0509013	1.0	1.0
PHA0509015	3.0	2.0
PHA0509032	2.0	3.0
PHA0509038	2.0	2.0
PHA0509041	2.0	3.0
PHA0510004B	1.0	1.0
PHA0510010A	1.0	1.0
PHA0510013A	1.0	1.0
PHA0510029	3.0	3.0
PHA0510032	3.0	3.0
PHA0510037	2.0	2.0
PHA0510048	2.0	2.0
PHA0610018	3.0	3.0
PHA0610019B	1.0	1.0
PHA0610025	3.0	3.0
PHA0610026	3.0	3.0
PHA0810001	3.0	3.0
PHA0810009	3.0	3.0
PHA1109001	1.0	1.0
PHA1109006	2.0	2.0
PHA1109008	1.0	1.0
PHA1109023	1.0	1.0
PHA1109025	1.0	1.0
PHA1109028	2.0	3.0
PHA1110002A	2.0	2.0
PHA1110003B	1.0	1.0
PHA1110013	2.0	3.0
PHA1110014	2.0	3.0
PHA1110017	2.0	2.0
PHA1110019	2.0	2.0
PHA1111001A	1.0	1.0
PHA1111002B	1.0	1.0
PHA1111009A	1.0	1.0
TI071122B	1.0	1.0
VAR0909003	2.0	2.0
VAR0909010	2.0	2.0
VAR0910007	2.0	2.0
VAR0910010	3.0	2.0
1325_1001009	2.0	2.0
1325_1001023	2.0	2.0
1325_1001039	2.0	2.0
1325_1001041	2.0	2.0
1325_1001053	1.0	2.0
1325_1001056	2.0	2.0
1325_1001062	2.0	2.0
1325_1001078	2.0	2.0
1325_1001082	2.0	2.0
1325_1001083	2.0	2.0
1325_1001087	2.0	2.0
1325_1001094	2.0	2.0
1325_1001099	2.0	2.0
1325_1001111	2.0	2.0
1325_1001122	2.0	2.0
1325_1001133	2.0	2.0
1325_1001136	2.0	2.0
1325_1001138	2.0	2.0
1325_1001143	2.0	2.0
1325_1001161	2.0	2.0
1325_1001168	2.0	2.0
1325_9000099	2.0	2.0
1325_9000104	2.0	2.0
1325_9000136	2.0	2.0
1325_9000139	2.0	2.0
1325_9000143	2.0	2.0
1325_9000185	2.0	2.0
1325_9000237	2.0	2.0
1325_9000239	2.0	2.0
1325_9000317	2.0	2.0
1325_9000320	2.0	2.0
1325_9000321	2.0	2.0
1325_9000503	2.0	2.0
1325_9000612	1.0	2.0
1325_9000674	2.0	2.0
1325_9000677	2.0	2.0
1325_9000700	2.0	2.0
1365_0100006	2.0	2.0
1365_0100013	2.0	2.0
1365_0100019	1.0	1.0
1365_0100020	2.0	2.0
1365_0100030	1.0	2.0
1365_0100056	2.0	2.0
1365_0100092	2.0	2.0
1365_0100097	2.0	2.0
1365_0100100	2.0	2.0
1365_0100101	2.0	2.0
1365_0100103	2.0	2.0
1365_0100119	2.0	2.0
1365_0100120	2.0	2.0
1365_0100121	2.0	2.0
1365_0100125	2.0	2.0
1365_0100164	2.0	2.0
1365_0100176	2.0	2.0
1365_0100186	2.0	2.0
1365_0100190	2.0	2.0
1365_0100192	2.0	2.0
1365_0100194	2.0	2.0
1365_0100198	1.0	2.0
1365_0100199	2.0	2.0
1365_0100200	2.0	2.0
1365_0100211	2.0	2.0
1365_0100212	2.0	2.0
1365_0100218	2.0	2.0
1365_0100228	1.0	2.0
1365_0100231	2.0	2.0
1365_0100232	2.0	2.0
1365_0100233	2.0	2.0
1365_0100258	2.0	2.0
1365_0100261	2.0	2.0
1365_0100448	1.0	2.0
1365_0100475	2.0	2.0
1365_0100478	2.0	2.0
1385_0000011	0.0	1.0
1385_0000022	1.0	1.0
1385_0000034	1.0	1.0
1385_0000047	1.0	1.0
1385_0000048	1.0	1.0
1385_0000050	1.0	1.0
1385_0000058	1.0	1.0
1385_0000095	1.0	1.0
1385_0000119	1.0	1.0
1385_0000124	1.0	1.0
1385_0000129	1.0	1.0
1385_0001107	1.0	1.0
1385_0001119	1.0	1.0
1385_0001125	1.0	1.0
1385_0001133	1.0	1.0
1385_0001135	1.0	1.0
1385_0001147	1.0	1.0
1385_0001148	1.0	1.0
1385_0001170	1.0	1.0
1385_0001172	1.0	1.0
1385_0001175	1.0	1.0
1385_0001188	1.0	1.0
1385_0001189	1.0	1.0
1385_0001192	1.0	1.0
1385_0001193	2.0	1.0
1385_0001199	1.0	1.0
1385_0001501	1.0	1.0
1385_0001503	1.0	1.0
1385_0001525	1.0	1.0
1385_0001724	2.0	2.0
1385_0001729	1.0	2.0
1385_0001732	1.0	1.0
1385_0001740	1.0	1.0
1385_0001741	0.0	1.0
1385_0001748	1.0	2.0
1385_0001752	1.0	1.0
1385_0001756	1.0	1.0
1385_0001757	2.0	1.0
1385_0001767	1.0	1.0
1385_0001788	1.0	1.0
1385_0001798	1.0	1.0
1385_0001799	1.0	1.0
1385_0001800	1.0	1.0
1395_0000341	1.0	1.0
1395_0000356	1.0	1.0
1395_0000368	0.0	1.0
1395_0000392	1.0	1.0
1395_0000409	2.0	2.0
1395_0000414	1.0	1.0
1395_0000455	1.0	1.0
1395_0000469	1.0	1.0
1395_0000516	1.0	1.0
1395_0000525	2.0	1.0
1395_0000527	1.0	1.0
1395_0000529	1.0	1.0
1395_0000533	2.0	2.0
1395_0000537	1.0	2.0
1395_0000553	1.0	1.0
1395_0000560	1.0	2.0
1395_0000581	1.0	2.0
1395_0000593	1.0	1.0
1395_0000595	0.0	1.0
1395_0000596	2.0	1.0
1395_0000598	1.0	1.0
1395_0000599	1.0	1.0
1395_0000602	1.0	1.0
1395_0000612	1.0	1.0
1395_0000628	1.0	1.0
1395_0001022	1.0	1.0
1395_0001028	1.0	2.0
1395_0001061	2.0	2.0
1395_0001068	1.0	1.0
1395_0001071	1.0	1.0
1395_0001076	1.0	2.0
1395_0001093	1.0	1.0
1395_0001108	1.0	1.0
1395_0001116	1.0	1.0
1395_0001120	1.0	1.0
1395_0001131	1.0	1.0
1395_0001147	1.0	2.0
1395_0001169	1.0	2.0
Averaged weighted F1-scores 0.7582856547983549
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
144.90552995391704 65.35717405024758
LANGUAGE: IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']
1 Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 1.15
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.92
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        33
         1.0       0.57      0.56      0.56       142
         2.0       0.55      0.67      0.61       176
         3.0       0.59      0.67      0.63        90
         4.0       0.00      0.00      0.00        11

    accuracy                           0.57       452
   macro avg       0.34      0.38      0.36       452
weighted avg       0.51      0.57      0.54       452

[[  0  30   3   0   0]
 [  0  79  62   1   0]
 [  0  29 118  29   0]
 [  0   0  30  60   0]
 [  0   0   0  11   0]]
0.5386049196034698
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.88
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.60      0.18      0.28        33
         1.0       0.56      0.40      0.47       142
         2.0       0.51      0.59      0.55       176
         3.0       0.57      0.87      0.68        90
         4.0       0.00      0.00      0.00        11

    accuracy                           0.54       452
   macro avg       0.45      0.41      0.40       452
weighted avg       0.53      0.54      0.52       452

[[  6  23   4   0   0]
 [  2  57  83   0   0]
 [  2  21 104  49   0]
 [  0   0  12  78   0]
 [  0   0   0  11   0]]
0.5176915103282443
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.75
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.95
              precision    recall  f1-score   support

         0.0       0.70      0.21      0.33        33
         1.0       0.59      0.55      0.57       142
         2.0       0.58      0.63      0.60       176
         3.0       0.63      0.82      0.71        90
         4.0       0.00      0.00      0.00        11

    accuracy                           0.60       452
   macro avg       0.50      0.44      0.44       452
weighted avg       0.59      0.60      0.58       452

[[  7  22   4   0   0]
 [  2  78  62   0   0]
 [  1  32 111  32   0]
 [  0   0  16  74   0]
 [  0   0   0  11   0]]
0.5792586423091238
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.59
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.06
              precision    recall  f1-score   support

         0.0       0.71      0.30      0.43        33
         1.0       0.61      0.58      0.60       142
         2.0       0.55      0.50      0.52       176
         3.0       0.56      0.88      0.68        90
         4.0       0.00      0.00      0.00        11

    accuracy                           0.58       452
   macro avg       0.49      0.45      0.45       452
weighted avg       0.57      0.58      0.56       452

[[10 18  5  0  0]
 [ 3 83 56  0  0]
 [ 1 35 88 52  0]
 [ 0  0 11 79  0]
 [ 0  0  0 11  0]]
0.5582245569396377
452 452 452
Filename	True Label	Prediction
1023_0101694	3.0	3.0
1023_0101700	2.0	3.0
1023_0101701	2.0	3.0
1023_0101843	3.0	3.0
1023_0101847	3.0	3.0
1023_0101856	2.0	3.0
1023_0101893	3.0	3.0
1023_0101898	4.0	3.0
1023_0102118	3.0	3.0
1023_0103823	3.0	3.0
1023_0103826	3.0	3.0
1023_0103830	3.0	3.0
1023_0103831	3.0	3.0
1023_0103832	2.0	2.0
1023_0103837	3.0	3.0
1023_0103844	4.0	3.0
1023_0104206	3.0	3.0
1023_0104209	3.0	3.0
1023_0107672	2.0	3.0
1023_0107740	3.0	3.0
1023_0107773	2.0	3.0
1023_0107783	3.0	2.0
1023_0108426	2.0	2.0
1023_0108518	3.0	3.0
1023_0108751	2.0	2.0
1023_0108752	3.0	3.0
1023_0108815	2.0	3.0
1023_0108886	3.0	3.0
1023_0108934	2.0	3.0
1023_0109027	2.0	2.0
1023_0109038	3.0	3.0
1023_0109391	2.0	3.0
1023_0109401	2.0	3.0
1023_0109518	2.0	2.0
1023_0109522	3.0	3.0
1023_0109606	2.0	3.0
1023_0109674	2.0	3.0
1023_0109914	2.0	2.0
1023_0109915	2.0	1.0
1031_0001703	4.0	3.0
1031_0002002	2.0	3.0
1031_0002003	2.0	3.0
1031_0002011	3.0	3.0
1031_0002042	3.0	3.0
1031_0002043	3.0	3.0
1031_0002092	4.0	3.0
1031_0002196	4.0	3.0
1031_0002198	3.0	3.0
1031_0003023	3.0	3.0
1031_0003065	3.0	3.0
1031_0003073	4.0	3.0
1031_0003077	3.0	3.0
1031_0003092	2.0	3.0
1031_0003097	4.0	3.0
1031_0003133	4.0	3.0
1031_0003145	3.0	3.0
1031_0003155	3.0	3.0
1031_0003157	4.0	3.0
1031_0003162	3.0	3.0
1031_0003164	3.0	3.0
1031_0003173	3.0	3.0
1031_0003183	4.0	3.0
1031_0003207	4.0	3.0
1031_0003214	3.0	3.0
1031_0003220	2.0	2.0
1031_0003221	2.0	3.0
1031_0003230	2.0	3.0
1031_0003237	3.0	3.0
1031_0003272	3.0	2.0
1031_0003273	3.0	3.0
1031_0003274	3.0	3.0
1031_0003315	3.0	3.0
1031_0003327	2.0	3.0
1031_0003352	2.0	2.0
1031_0003359	3.0	3.0
1031_0003365	3.0	3.0
1031_0003389	3.0	3.0
1031_0003391	2.0	3.0
1031_0003410	3.0	3.0
1031_0003419	3.0	3.0
1061_0012029	3.0	2.0
1061_0120288	1.0	2.0
1061_0120296	1.0	1.0
1061_0120303	0.0	2.0
1061_0120307	2.0	2.0
1061_0120309	1.0	1.0
1061_0120310	2.0	2.0
1061_0120313	1.0	1.0
1061_0120314	1.0	2.0
1061_0120315	2.0	2.0
1061_0120330	2.0	3.0
1061_0120334	2.0	2.0
1061_0120352	1.0	1.0
1061_0120358	1.0	1.0
1061_0120371	3.0	3.0
1061_0120372	1.0	2.0
1061_0120374	2.0	2.0
1061_0120407	3.0	2.0
1061_0120408	2.0	2.0
1061_0120410	2.0	2.0
1061_0120413	1.0	1.0
1061_0120421	2.0	3.0
1061_0120424	2.0	2.0
1061_0120426	1.0	2.0
1061_0120431	2.0	2.0
1061_0120439	2.0	1.0
1061_0120453	2.0	2.0
1061_0120485	3.0	2.0
1061_0120489	2.0	2.0
1061_0120490	2.0	2.0
1061_0120492	2.0	2.0
1061_0120875	2.0	3.0
1061_0120880	2.0	2.0
1061_1202910	2.0	2.0
1061_1202911	0.0	1.0
1061_1202912	2.0	2.0
1061_1202918	1.0	2.0
1071_0024681	1.0	1.0
1071_0024687	0.0	1.0
1071_0024690	1.0	2.0
1071_0024692	2.0	3.0
1071_0024701	2.0	2.0
1071_0024714	2.0	1.0
1071_0024715	2.0	1.0
1071_0024757	2.0	1.0
1071_0024759	0.0	1.0
1071_0024772	0.0	0.0
1071_0024774	0.0	0.0
1071_0024775	0.0	0.0
1071_0024777	1.0	1.0
1071_0024779	1.0	2.0
1071_0024800	1.0	1.0
1071_0024802	2.0	1.0
1071_0024803	1.0	1.0
1071_0024809	0.0	0.0
1071_0024810	2.0	1.0
1071_0024815	0.0	1.0
1071_0024819	1.0	1.0
1071_0024822	0.0	1.0
1071_0024831	0.0	0.0
1071_0024837	0.0	0.0
1071_0024841	0.0	0.0
1071_0024843	0.0	1.0
1071_0024854	0.0	0.0
1071_0024859	1.0	1.0
1071_0024863	1.0	1.0
1071_0024871	1.0	1.0
1071_0024872	1.0	1.0
1071_0024874	1.0	1.0
1071_0024875	1.0	1.0
1071_0024878	1.0	2.0
1071_0241832	1.0	1.0
1071_0242012	1.0	2.0
1071_0242023	1.0	1.0
1071_0248304	1.0	1.0
1071_0248309	1.0	2.0
1071_0248324	0.0	1.0
1071_0248328	1.0	0.0
1071_0248333	2.0	1.0
1071_0248334	2.0	1.0
1071_0248344	1.0	1.0
1071_0248346	0.0	1.0
1071_0248348	2.0	1.0
1091_0000010	3.0	2.0
1091_0000028	1.0	1.0
1091_0000030	0.0	2.0
1091_0000036	1.0	2.0
1091_0000037	1.0	1.0
1091_0000039	1.0	0.0
1091_0000041	1.0	1.0
1091_0000042	1.0	0.0
1091_0000045	1.0	2.0
1091_0000046	2.0	1.0
1091_0000054	0.0	1.0
1091_0000055	1.0	2.0
1091_0000057	2.0	1.0
1091_0000067	2.0	2.0
1091_0000071	2.0	2.0
1091_0000077	2.0	1.0
1091_0000079	1.0	2.0
1091_0000087	2.0	1.0
1091_0000101	2.0	1.0
1091_0000114	1.0	2.0
1091_0000116	2.0	2.0
1091_0000125	2.0	1.0
1091_0000140	2.0	1.0
1091_0000154	1.0	2.0
1091_0000157	2.0	2.0
1091_0000159	2.0	2.0
1091_0000162	1.0	2.0
1091_0000163	1.0	1.0
1091_0000190	1.0	1.0
1091_0000193	2.0	1.0
1091_0000204	2.0	2.0
1091_0000209	2.0	1.0
1091_0000214	2.0	1.0
1091_0000217	2.0	1.0
1091_0000220	1.0	2.0
1091_0000221	2.0	1.0
1091_0000223	1.0	2.0
1091_0000227	0.0	2.0
1091_0000235	1.0	1.0
1091_0000236	2.0	2.0
1091_0000237	2.0	2.0
1091_0000243	1.0	1.0
1091_0000247	2.0	2.0
1091_0000250	1.0	2.0
1091_0000253	2.0	0.0
1091_0000261	1.0	2.0
1091_0000264	2.0	1.0
1091_0000267	1.0	1.0
1091_0000270	2.0	1.0
1091_0000275	2.0	1.0
0601	1.0	1.0
0608	1.0	1.0
0630	1.0	1.0
0635	1.0	2.0
0641	1.0	1.0
0723	2.0	2.0
0801	1.0	2.0
0810	2.0	2.0
0811	2.0	2.0
0816	2.0	2.0
0824	2.0	2.0
0901	2.0	2.0
0902	2.0	2.0
0911	1.0	1.0
0914	1.0	1.0
0915	2.0	2.0
0920	2.0	2.0
0924	1.0	1.0
0926	2.0	2.0
1001	1.0	2.0
1016	1.0	1.0
1017	1.0	1.0
1018	1.0	1.0
1115	1.0	2.0
1117	2.0	2.0
BER0611003	2.0	3.0
BER0611005	2.0	3.0
BER0611007	2.0	3.0
KYJ0611006A	1.0	1.0
KYJ0611009A	1.0	1.0
LIB0611002A	1.0	2.0
LIB0611004A	1.0	2.0
LIB0611004B	1.0	2.0
MOS0509004	1.0	1.0
PAR1011015	2.0	2.0
PHA0111001B	1.0	1.0
PHA0111003B	2.0	1.0
PHA0111004A	1.0	1.0
PHA0111004B	1.0	1.0
PHA0111012	1.0	2.0
PHA0112002A	2.0	1.0
PHA0112003A	1.0	1.0
PHA0112007A	1.0	1.0
PHA0112012B	1.0	2.0
PHA0209008	1.0	1.0
PHA0411011B	1.0	2.0
PHA0411028	2.0	2.0
PHA0411038	3.0	3.0
PHA0411041	3.0	3.0
PHA0411047	2.0	3.0
PHA0411051	3.0	3.0
PHA0411055	3.0	3.0
PHA0411056	3.0	3.0
PHA0411058	3.0	3.0
PHA0411062	3.0	3.0
PHA0509002	1.0	1.0
PHA0509013	1.0	1.0
PHA0509037	3.0	2.0
PHA0509042	3.0	3.0
PHA0510002A	2.0	1.0
PHA0510004B	0.0	1.0
PHA0510029	2.0	3.0
PHA0510034	3.0	3.0
PHA0610017	3.0	3.0
PHA0710012	3.0	3.0
PHA0710017	3.0	3.0
PHA0809009	2.0	2.0
PHA0810004	1.0	1.0
PHA0810015	3.0	3.0
PHA1109007	1.0	2.0
PHA1109008	1.0	1.0
PHA1109023	1.0	1.0
PHA1110003A	1.0	2.0
PHA1110003B	1.0	2.0
PHA1110017	1.0	2.0
PHA1111003B	1.0	2.0
PHA1111006B	1.0	1.0
PHA1111008B	1.0	2.0
ST071122B	1.0	1.0
VAR0909003	2.0	2.0
VAR0910004	3.0	3.0
VAR0910006	3.0	2.0
VAR0910009	3.0	3.0
VAR0910010	3.0	3.0
1325_1001012	2.0	3.0
1325_1001015	2.0	2.0
1325_1001028	2.0	3.0
1325_1001029	2.0	2.0
1325_1001054	2.0	3.0
1325_1001055	2.0	3.0
1325_1001058	2.0	2.0
1325_1001059	2.0	2.0
1325_1001077	2.0	3.0
1325_1001078	2.0	3.0
1325_1001084	2.0	2.0
1325_1001085	2.0	3.0
1325_1001099	3.0	3.0
1325_1001109	2.0	2.0
1325_1001122	2.0	2.0
1325_1001123	3.0	3.0
1325_1001131	3.0	3.0
1325_1001134	2.0	3.0
1325_1001138	2.0	3.0
1325_1001139	2.0	2.0
1325_1001144	3.0	2.0
1325_1001152	2.0	3.0
1325_1001154	3.0	3.0
1325_1001163	2.0	3.0
1325_1001166	2.0	2.0
1325_1001169	2.0	3.0
1325_9000087	2.0	2.0
1325_9000088	3.0	3.0
1325_9000089	2.0	2.0
1325_9000095	2.0	3.0
1325_9000107	2.0	3.0
1325_9000137	3.0	3.0
1325_9000139	2.0	2.0
1325_9000187	3.0	3.0
1325_9000209	2.0	3.0
1325_9000211	2.0	3.0
1325_9000278	3.0	3.0
1325_9000316	1.0	2.0
1325_9000321	3.0	3.0
1325_9000322	3.0	3.0
1325_9000504	3.0	3.0
1325_9000533	3.0	3.0
1325_9000611	2.0	3.0
1325_9000676	3.0	3.0
1325_9000678	3.0	3.0
1325_9000750	3.0	3.0
1365_0100003	1.0	1.0
1365_0100012	2.0	2.0
1365_0100014	2.0	2.0
1365_0100019	1.0	1.0
1365_0100024	1.0	2.0
1365_0100057	2.0	3.0
1365_0100063	3.0	3.0
1365_0100064	2.0	3.0
1365_0100069	1.0	2.0
1365_0100072	2.0	2.0
1365_0100093	2.0	2.0
1365_0100123	2.0	2.0
1365_0100125	3.0	3.0
1365_0100146	2.0	2.0
1365_0100163	3.0	3.0
1365_0100182	2.0	3.0
1365_0100184	2.0	2.0
1365_0100185	1.0	2.0
1365_0100192	3.0	3.0
1365_0100199	2.0	3.0
1365_0100200	3.0	3.0
1365_0100201	2.0	2.0
1365_0100204	2.0	2.0
1365_0100217	3.0	3.0
1365_0100228	1.0	2.0
1365_0100230	2.0	3.0
1365_0100251	2.0	3.0
1365_0100262	2.0	3.0
1365_0100263	3.0	3.0
1365_0100266	3.0	2.0
1365_0100287	2.0	2.0
1365_0100289	2.0	2.0
1365_0100448	1.0	2.0
1365_0100475	2.0	2.0
1365_0100477	1.0	2.0
1365_0100481	2.0	3.0
1385_0000011	0.0	1.0
1385_0000033	1.0	1.0
1385_0000038	1.0	1.0
1385_0000041	2.0	2.0
1385_0000059	2.0	2.0
1385_0000097	2.0	1.0
1385_0000098	1.0	1.0
1385_0000102	1.0	1.0
1385_0000124	2.0	2.0
1385_0001118	1.0	1.0
1385_0001127	2.0	2.0
1385_0001135	1.0	1.0
1385_0001138	1.0	1.0
1385_0001150	2.0	2.0
1385_0001159	1.0	1.0
1385_0001163	2.0	2.0
1385_0001164	1.0	2.0
1385_0001174	0.0	1.0
1385_0001178	0.0	0.0
1385_0001194	1.0	1.0
1385_0001197	1.0	1.0
1385_0001198	2.0	1.0
1385_0001523	1.0	2.0
1385_0001716	1.0	2.0
1385_0001717	2.0	2.0
1385_0001733	2.0	2.0
1385_0001734	1.0	2.0
1385_0001751	1.0	2.0
1385_0001754	1.0	1.0
1385_0001759	0.0	1.0
1385_0001761	0.0	2.0
1385_0001764	0.0	1.0
1385_0001771	0.0	1.0
1385_0001787	0.0	2.0
1385_0001794	0.0	1.0
1385_0001795	1.0	1.0
1385_0001796	2.0	2.0
1395_0000333	2.0	1.0
1395_0000357	3.0	2.0
1395_0000380	2.0	2.0
1395_0000399	1.0	1.0
1395_0000447	1.0	1.0
1395_0000469	1.0	1.0
1395_0000471	1.0	1.0
1395_0000504	1.0	1.0
1395_0000518	2.0	2.0
1395_0000525	2.0	1.0
1395_0000528	2.0	2.0
1395_0000531	2.0	1.0
1395_0000537	1.0	2.0
1395_0000554	2.0	1.0
1395_0000564	1.0	1.0
1395_0000582	0.0	0.0
1395_0000599	1.0	1.0
1395_0000608	1.0	1.0
1395_0000611	1.0	1.0
1395_0000639	1.0	2.0
1395_0000642	0.0	1.0
1395_0000644	1.0	2.0
1395_0001028	1.0	2.0
1395_0001045	2.0	1.0
1395_0001070	1.0	2.0
1395_0001073	2.0	2.0
1395_0001075	1.0	1.0
1395_0001101	1.0	1.0
1395_0001114	1.0	1.0
1395_0001122	1.0	1.0
1395_0001131	0.0	1.0
1395_0001132	1.0	2.0
1395_0001141	1.0	1.0
1395_0001160	2.0	2.0
1395_0001161	1.0	2.0
1395_0001169	2.0	2.0
2 Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 1.13
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.91
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        33
         1.0       0.54      0.73      0.62       142
         2.0       0.59      0.60      0.59       176
         3.0       0.68      0.61      0.64        90
         4.0       0.00      0.00      0.00        11

    accuracy                           0.58       452
   macro avg       0.36      0.39      0.37       452
weighted avg       0.53      0.58      0.55       452

[[  0  33   0   0   0]
 [  0 103  39   0   0]
 [  0  56 105  15   0]
 [  0   0  35  55   0]
 [  0   0   0  11   0]]
0.5521863965083996
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.89
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.87
              precision    recall  f1-score   support

         0.0       1.00      0.03      0.06        33
         1.0       0.59      0.58      0.58       142
         2.0       0.62      0.77      0.69       176
         3.0       0.74      0.77      0.75        90
         4.0       0.00      0.00      0.00        11

    accuracy                           0.64       452
   macro avg       0.59      0.43      0.42       452
weighted avg       0.65      0.64      0.61       452

[[  1  31   1   0   0]
 [  0  82  60   0   0]
 [  0  27 136  13   0]
 [  0   0  21  69   0]
 [  0   0   0  11   0]]
0.6059604585937137
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.74
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.83
              precision    recall  f1-score   support

         0.0       0.55      0.33      0.42        33
         1.0       0.60      0.67      0.63       142
         2.0       0.65      0.69      0.67       176
         3.0       0.73      0.70      0.72        90
         4.0       0.00      0.00      0.00        11

    accuracy                           0.64       452
   macro avg       0.51      0.48      0.49       452
weighted avg       0.63      0.64      0.63       452

[[ 11  21   1   0   0]
 [  8  95  39   0   0]
 [  1  42 121  12   0]
 [  0   1  26  63   0]
 [  0   0   0  11   0]]
0.6307473810126445
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.58
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.96
              precision    recall  f1-score   support

         0.0       0.56      0.27      0.37        33
         1.0       0.62      0.56      0.59       142
         2.0       0.65      0.69      0.67       176
         3.0       0.67      0.91      0.77        90
         4.0       0.00      0.00      0.00        11

    accuracy                           0.64       452
   macro avg       0.50      0.49      0.48       452
weighted avg       0.62      0.64      0.62       452

[[  9  23   1   0   0]
 [  6  79  56   1   0]
 [  1  25 121  29   0]
 [  0   0   8  82   0]
 [  0   0   0  11   0]]
0.6249577138318176
452 452 452
Filename	True Label	Prediction
1023_0001420	3.0	3.0
1023_0101690	2.0	2.0
1023_0101751	3.0	3.0
1023_0101841	2.0	3.0
1023_0101848	2.0	2.0
1023_0101853	2.0	3.0
1023_0101901	3.0	3.0
1023_0101906	2.0	2.0
1023_0101907	3.0	3.0
1023_0103834	4.0	3.0
1023_0107075	2.0	2.0
1023_0107244	2.0	3.0
1023_0107729	3.0	3.0
1023_0107788	2.0	3.0
1023_0108305	3.0	3.0
1023_0108510	2.0	3.0
1023_0108520	2.0	2.0
1023_0108811	3.0	2.0
1023_0108813	3.0	3.0
1023_0108889	3.0	3.0
1023_0108932	2.0	2.0
1023_0108935	2.0	2.0
1023_0109029	1.0	1.0
1023_0109030	3.0	3.0
1023_0109151	3.0	3.0
1023_0109250	2.0	3.0
1023_0109402	2.0	2.0
1023_0109496	3.0	3.0
1023_0109505	2.0	3.0
1023_0109524	3.0	3.0
1023_0109591	3.0	3.0
1023_0109614	2.0	2.0
1023_0109717	3.0	3.0
1023_0109880	3.0	3.0
1023_0109947	2.0	3.0
1031_0001998	4.0	3.0
1031_0002040	4.0	3.0
1031_0002079	4.0	3.0
1031_0002091	3.0	3.0
1031_0002131	3.0	3.0
1031_0002185	3.0	3.0
1031_0002195	3.0	3.0
1031_0002197	4.0	3.0
1031_0003013	4.0	3.0
1031_0003035	3.0	3.0
1031_0003042	3.0	3.0
1031_0003052	3.0	3.0
1031_0003071	3.0	3.0
1031_0003072	3.0	3.0
1031_0003085	3.0	3.0
1031_0003090	3.0	3.0
1031_0003126	3.0	3.0
1031_0003131	3.0	3.0
1031_0003135	3.0	3.0
1031_0003136	3.0	3.0
1031_0003140	3.0	3.0
1031_0003146	4.0	3.0
1031_0003154	3.0	3.0
1031_0003156	3.0	3.0
1031_0003161	3.0	3.0
1031_0003166	2.0	2.0
1031_0003169	3.0	3.0
1031_0003181	3.0	3.0
1031_0003184	4.0	3.0
1031_0003189	3.0	3.0
1031_0003191	3.0	3.0
1031_0003234	3.0	3.0
1031_0003242	3.0	3.0
1031_0003245	3.0	3.0
1031_0003261	3.0	3.0
1031_0003262	3.0	3.0
1031_0003338	3.0	3.0
1031_0003354	3.0	3.0
1031_0003355	3.0	3.0
1031_0003356	3.0	3.0
1031_0003357	3.0	3.0
1031_0003390	3.0	3.0
1031_0003392	3.0	3.0
1031_0003407	3.0	3.0
1031_0003414	3.0	3.0
1031_0003415	4.0	3.0
1061_0120271	2.0	2.0
1061_0120273	2.0	1.0
1061_0120274	1.0	1.0
1061_0120278	1.0	2.0
1061_0120284	0.0	0.0
1061_0120311	3.0	3.0
1061_0120316	2.0	2.0
1061_0120317	2.0	3.0
1061_0120326	2.0	2.0
1061_0120327	2.0	2.0
1061_0120331	1.0	1.0
1061_0120336	1.0	2.0
1061_0120350	2.0	3.0
1061_0120351	2.0	2.0
1061_0120357	3.0	3.0
1061_0120359	1.0	1.0
1061_0120367	2.0	2.0
1061_0120368	2.0	1.0
1061_0120375	2.0	1.0
1061_0120387	1.0	2.0
1061_0120389	2.0	2.0
1061_0120390	2.0	2.0
1061_0120394	2.0	2.0
1061_0120406	2.0	2.0
1061_0120409	2.0	2.0
1061_0120411	3.0	3.0
1061_0120414	2.0	2.0
1061_0120425	2.0	2.0
1061_0120450	2.0	2.0
1061_0120455	2.0	2.0
1061_0120456	2.0	2.0
1061_0120480	2.0	1.0
1061_0120481	2.0	3.0
1061_0120486	2.0	2.0
1061_0120493	1.0	1.0
1061_0120499	2.0	2.0
1061_0120500	1.0	1.0
1061_0120856	1.0	1.0
1061_0120877	2.0	2.0
1061_0120887	1.0	2.0
1061_1029111	2.0	2.0
1061_1029115	2.0	1.0
1061_1202914	1.0	1.0
1061_1202919	2.0	1.0
1071_0024694	1.0	2.0
1071_0024704	1.0	1.0
1071_0024706	1.0	1.0
1071_0024708	1.0	1.0
1071_0024710	1.0	1.0
1071_0024716	1.0	1.0
1071_0024766	1.0	1.0
1071_0024770	1.0	1.0
1071_0024783	0.0	0.0
1071_0024784	1.0	1.0
1071_0024812	1.0	0.0
1071_0024821	1.0	1.0
1071_0024823	1.0	1.0
1071_0024825	0.0	1.0
1071_0024835	0.0	1.0
1071_0024840	1.0	1.0
1071_0024844	1.0	1.0
1071_0024846	0.0	0.0
1071_0024851	2.0	1.0
1071_0024855	1.0	1.0
1071_0024857	0.0	1.0
1071_0024860	1.0	1.0
1071_0024862	1.0	1.0
1071_0024873	0.0	1.0
1071_0241831	1.0	2.0
1071_0242013	1.0	2.0
1071_0242021	1.0	1.0
1071_0242022	0.0	1.0
1071_0242072	0.0	1.0
1071_0242073	1.0	1.0
1071_0242091	1.0	1.0
1071_0243501	2.0	1.0
1071_0243581	0.0	1.0
1071_0243593	1.0	2.0
1071_0243623	1.0	2.0
1071_0248301	2.0	1.0
1071_0248314	1.0	1.0
1071_0248321	2.0	1.0
1071_0248326	1.0	1.0
1071_0248330	2.0	2.0
1071_0248338	1.0	1.0
1071_0248341	0.0	0.0
1071_0248343	2.0	1.0
1071_0248347	1.0	0.0
1091_0000001	1.0	1.0
1091_0000003	2.0	1.0
1091_0000004	1.0	1.0
1091_0000013	1.0	0.0
1091_0000014	0.0	1.0
1091_0000017	2.0	2.0
1091_0000025	1.0	1.0
1091_0000029	2.0	2.0
1091_0000032	1.0	2.0
1091_0000033	1.0	1.0
1091_0000044	0.0	2.0
1091_0000053	0.0	1.0
1091_0000061	2.0	0.0
1091_0000062	2.0	2.0
1091_0000063	1.0	1.0
1091_0000064	1.0	1.0
1091_0000065	2.0	2.0
1091_0000066	2.0	1.0
1091_0000069	2.0	2.0
1091_0000086	1.0	2.0
1091_0000095	1.0	1.0
1091_0000127	2.0	2.0
1091_0000148	1.0	1.0
1091_0000152	1.0	2.0
1091_0000158	2.0	2.0
1091_0000165	1.0	1.0
1091_0000166	1.0	2.0
1091_0000191	1.0	2.0
1091_0000192	1.0	2.0
1091_0000194	1.0	3.0
1091_0000205	1.0	2.0
1091_0000207	1.0	2.0
1091_0000208	1.0	2.0
1091_0000210	2.0	2.0
1091_0000219	1.0	2.0
1091_0000230	2.0	2.0
1091_0000231	1.0	2.0
1091_0000251	2.0	2.0
1091_0000257	1.0	2.0
1091_0000258	2.0	2.0
1091_0000266	2.0	2.0
0602	2.0	2.0
0603	2.0	2.0
0606	1.0	1.0
0607	2.0	2.0
0610	2.0	2.0
0618	1.0	1.0
0619	2.0	2.0
0627	2.0	2.0
0629	2.0	2.0
0638	2.0	2.0
0719	2.0	1.0
0721	2.0	2.0
0804	1.0	1.0
0817	1.0	2.0
0822	1.0	2.0
0825	1.0	2.0
0910	1.0	2.0
0921	1.0	1.0
0922	1.0	2.0
1005	1.0	1.0
1008	2.0	1.0
1112	1.0	2.0
1114	2.0	2.0
1116	1.0	1.0
BER0609003	2.0	2.0
KYJ0611004A	1.0	1.0
KYJ0611006B	1.0	1.0
LON0610002A	2.0	2.0
MOS0611012	2.0	2.0
MOS0611013	2.0	3.0
PAR1011009B	1.0	1.0
PAR1011013	2.0	3.0
PAR1011016	3.0	3.0
PHA0111002A	2.0	2.0
PHA0111016	3.0	3.0
PHA0209024	1.0	2.0
PHA0209034	2.0	3.0
PHA0209038	4.0	3.0
PHA0210004	1.0	1.0
PHA0210007	1.0	1.0
PHA0411008A	1.0	2.0
PHA0411008B	1.0	1.0
PHA0411011A	1.0	1.0
PHA0411012A	1.0	1.0
PHA0411029	2.0	2.0
PHA0411031	3.0	3.0
PHA0411042	2.0	3.0
PHA0411054	3.0	2.0
PHA0509007	1.0	1.0
PHA0509015	3.0	3.0
PHA0509020	3.0	3.0
PHA0509021	1.0	2.0
PHA0509025	3.0	3.0
PHA0509028	2.0	3.0
PHA0509031	1.0	2.0
PHA0509033	1.0	1.0
PHA0509038	1.0	1.0
PHA0510013B	1.0	1.0
PHA0510039	3.0	3.0
PHA0510050	2.0	3.0
PHA0610006A	1.0	1.0
PHA0610019A	2.0	2.0
PHA0610019B	2.0	1.0
PHA0710013	4.0	3.0
PHA0710019	3.0	3.0
PHA0710021	3.0	3.0
PHA0810006	2.0	2.0
PHA0810011	2.0	3.0
PHA0811010	2.0	1.0
PHA0811019	3.0	3.0
PHA0811020	1.0	2.0
PHA1109025	1.0	1.0
PHA1109028	3.0	3.0
PHA1110002B	2.0	1.0
PHA1110021	2.0	3.0
PHA1111004B	1.0	1.0
PHA1111006A	1.0	2.0
PHA1111009A	1.0	1.0
VAR0909004	2.0	2.0
VAR0909008	2.0	2.0
VAR0909010	1.0	2.0
VAR0910007	2.0	3.0
1325_1001008	2.0	2.0
1325_1001011	2.0	2.0
1325_1001016	2.0	2.0
1325_1001018	2.0	3.0
1325_1001019	2.0	2.0
1325_1001021	2.0	3.0
1325_1001023	2.0	2.0
1325_1001025	2.0	2.0
1325_1001033	3.0	3.0
1325_1001040	3.0	3.0
1325_1001042	2.0	3.0
1325_1001043	2.0	3.0
1325_1001045	2.0	3.0
1325_1001052	2.0	2.0
1325_1001080	2.0	2.0
1325_1001083	2.0	2.0
1325_1001086	2.0	2.0
1325_1001087	2.0	3.0
1325_1001101	3.0	3.0
1325_1001110	3.0	3.0
1325_1001121	2.0	2.0
1325_1001129	1.0	2.0
1325_1001153	2.0	2.0
1325_1001158	2.0	3.0
1325_1001159	3.0	3.0
1325_1001161	2.0	2.0
1325_1001170	3.0	3.0
1325_9000102	2.0	2.0
1325_9000104	2.0	2.0
1325_9000138	3.0	3.0
1325_9000140	3.0	3.0
1325_9000144	3.0	3.0
1325_9000279	3.0	3.0
1325_9000503	3.0	3.0
1325_9000601	3.0	3.0
1325_9000612	2.0	2.0
1325_9000674	2.0	3.0
1365_0100006	2.0	2.0
1365_0100011	1.0	2.0
1365_0100015	1.0	1.0
1365_0100016	2.0	2.0
1365_0100028	2.0	2.0
1365_0100030	1.0	2.0
1365_0100079	2.0	2.0
1365_0100080	2.0	2.0
1365_0100098	1.0	2.0
1365_0100105	3.0	3.0
1365_0100133	2.0	2.0
1365_0100137	2.0	2.0
1365_0100138	2.0	2.0
1365_0100139	2.0	2.0
1365_0100147	2.0	2.0
1365_0100167	1.0	1.0
1365_0100168	3.0	2.0
1365_0100172	2.0	2.0
1365_0100175	2.0	2.0
1365_0100181	1.0	1.0
1365_0100203	2.0	2.0
1365_0100205	3.0	2.0
1365_0100213	2.0	2.0
1365_0100218	3.0	3.0
1365_0100225	2.0	2.0
1365_0100226	2.0	2.0
1365_0100231	2.0	2.0
1365_0100233	3.0	3.0
1365_0100255	2.0	2.0
1365_0100256	2.0	2.0
1365_0100257	2.0	2.0
1365_0100259	2.0	2.0
1365_0100276	3.0	2.0
1365_0100286	1.0	2.0
1365_0100451	2.0	2.0
1365_0100456	2.0	3.0
1365_0100479	3.0	3.0
1365_0100482	2.0	2.0
1385_0000021	2.0	2.0
1385_0000034	2.0	2.0
1385_0000042	2.0	2.0
1385_0000043	2.0	2.0
1385_0000048	2.0	2.0
1385_0000050	2.0	2.0
1385_0000051	2.0	2.0
1385_0000057	1.0	2.0
1385_0000095	1.0	0.0
1385_0000119	2.0	2.0
1385_0000123	1.0	2.0
1385_0000126	2.0	1.0
1385_0000127	2.0	2.0
1385_0000129	2.0	1.0
1385_0001111	2.0	1.0
1385_0001120	1.0	1.0
1385_0001126	0.0	1.0
1385_0001128	0.0	0.0
1385_0001130	1.0	1.0
1385_0001132	1.0	1.0
1385_0001133	2.0	1.0
1385_0001136	1.0	0.0
1385_0001137	1.0	1.0
1385_0001158	1.0	2.0
1385_0001160	3.0	2.0
1385_0001173	0.0	1.0
1385_0001190	0.0	0.0
1385_0001191	1.0	1.0
1385_0001192	1.0	1.0
1385_0001503	1.0	2.0
1385_0001525	1.0	2.0
1385_0001526	0.0	0.0
1385_0001720	0.0	1.0
1385_0001725	1.0	1.0
1385_0001727	0.0	1.0
1385_0001739	0.0	1.0
1385_0001742	0.0	0.0
1385_0001746	0.0	1.0
1385_0001756	1.0	1.0
1385_0001760	1.0	1.0
1385_0001762	1.0	1.0
1385_0001765	0.0	0.0
1385_0001773	0.0	1.0
1385_0001775	0.0	1.0
1385_0001798	1.0	2.0
1395_0000353	1.0	1.0
1395_0000369	2.0	2.0
1395_0000383	2.0	2.0
1395_0000402	1.0	1.0
1395_0000403	2.0	2.0
1395_0000414	2.0	1.0
1395_0000443	2.0	2.0
1395_0000446	2.0	2.0
1395_0000448	1.0	1.0
1395_0000462	2.0	2.0
1395_0000512	2.0	2.0
1395_0000513	2.0	2.0
1395_0000514	3.0	2.0
1395_0000515	2.0	1.0
1395_0000529	2.0	1.0
1395_0000533	3.0	2.0
1395_0000549	2.0	2.0
1395_0000550	1.0	2.0
1395_0000555	1.0	2.0
1395_0000581	1.0	2.0
1395_0000583	1.0	2.0
1395_0000587	0.0	1.0
1395_0000606	1.0	1.0
1395_0000607	1.0	0.0
1395_0000628	0.0	1.0
1395_0000631	1.0	2.0
1395_0000636	0.0	1.0
1395_0001022	1.0	2.0
1395_0001040	0.0	1.0
1395_0001060	1.0	2.0
1395_0001065	1.0	2.0
1395_0001067	1.0	1.0
1395_0001069	2.0	2.0
1395_0001080	1.0	1.0
1395_0001115	1.0	2.0
1395_0001116	2.0	2.0
1395_0001118	0.0	1.0
1395_0001146	0.0	1.0
1395_0001167	1.0	2.0
1395_0001170	1.0	2.0
3 Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 1.17
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.04
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        33
         1.0       0.51      0.88      0.64       142
         2.0       0.58      0.48      0.52       177
         3.0       0.64      0.41      0.50        90
         4.0       0.00      0.00      0.00        10

    accuracy                           0.55       452
   macro avg       0.34      0.35      0.33       452
weighted avg       0.51      0.55      0.51       452

[[  0  33   0   0   0]
 [  0 125  17   0   0]
 [  0  80  85  12   0]
 [  0   9  44  37   0]
 [  0   0   1   9   0]]
0.5069248019318543
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.94
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        33
         1.0       0.52      0.81      0.64       142
         2.0       0.58      0.38      0.46       177
         3.0       0.53      0.68      0.59        90
         4.0       0.00      0.00      0.00        10

    accuracy                           0.54       452
   macro avg       0.33      0.37      0.34       452
weighted avg       0.50      0.54      0.50       452

[[  0  33   0   0   0]
 [  0 115  27   0   0]
 [  0  65  67  45   0]
 [  0   7  22  61   0]
 [  0   0   0  10   0]]
0.4966167495789279
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.83
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.14
              precision    recall  f1-score   support

         0.0       1.00      0.15      0.26        33
         1.0       0.54      0.41      0.47       142
         2.0       0.50      0.53      0.51       177
         3.0       0.50      0.84      0.63        90
         4.0       0.00      0.00      0.00        10

    accuracy                           0.52       452
   macro avg       0.51      0.39      0.37       452
weighted avg       0.54      0.52      0.49       452

[[ 5 26  2  0  0]
 [ 0 58 82  2  0]
 [ 0 20 94 63  0]
 [ 0  3 11 76  0]
 [ 0  0  0 10  0]]
0.4922972106905897
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.73
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.06
              precision    recall  f1-score   support

         0.0       0.61      0.33      0.43        33
         1.0       0.59      0.49      0.54       142
         2.0       0.55      0.59      0.57       177
         3.0       0.55      0.77      0.64        90
         4.0       0.00      0.00      0.00        10

    accuracy                           0.56       452
   macro avg       0.46      0.44      0.44       452
weighted avg       0.56      0.56      0.55       452

[[ 11  21   1   0   0]
 [  4  70  67   1   0]
 [  3  24 105  45   0]
 [  0   3  18  69   0]
 [  0   0   0  10   0]]
0.5519241354474764
452 452 452
Filename	True Label	Prediction
1023_0101675	3.0	3.0
1023_0101684	2.0	3.0
1023_0101749	4.0	3.0
1023_0101852	3.0	3.0
1023_0101894	2.0	3.0
1023_0101896	2.0	2.0
1023_0101897	2.0	3.0
1023_0103824	3.0	3.0
1023_0103828	1.0	2.0
1023_0103829	2.0	3.0
1023_0103840	3.0	3.0
1023_0103883	2.0	3.0
1023_0104207	2.0	3.0
1023_0106816	3.0	3.0
1023_0107042	3.0	2.0
1023_0107074	3.0	3.0
1023_0107781	2.0	3.0
1023_0107784	1.0	2.0
1023_0107787	2.0	3.0
1023_0108304	3.0	3.0
1023_0108641	3.0	3.0
1023_0108753	2.0	2.0
1023_0108766	2.0	3.0
1023_0108810	3.0	3.0
1023_0108812	2.0	3.0
1023_0108885	2.0	2.0
1023_0108933	2.0	3.0
1023_0108955	3.0	3.0
1023_0109096	3.0	3.0
1023_0109248	2.0	3.0
1023_0109392	2.0	2.0
1023_0109399	2.0	3.0
1023_0109500	2.0	3.0
1023_0109515	3.0	3.0
1023_0109520	2.0	3.0
1023_0109588	3.0	3.0
1023_0109649	2.0	3.0
1023_0109651	3.0	3.0
1023_0109716	3.0	3.0
1023_0109721	2.0	3.0
1023_0109890	3.0	3.0
1023_0109917	3.0	3.0
1023_0109945	4.0	3.0
1023_0109954	3.0	3.0
1031_0001949	3.0	3.0
1031_0002061	3.0	3.0
1031_0002083	2.0	3.0
1031_0002086	3.0	3.0
1031_0002088	3.0	3.0
1031_0002187	3.0	3.0
1031_0003012	3.0	3.0
1031_0003043	4.0	3.0
1031_0003048	4.0	3.0
1031_0003053	3.0	3.0
1031_0003063	4.0	3.0
1031_0003099	3.0	3.0
1031_0003128	3.0	3.0
1031_0003129	3.0	3.0
1031_0003132	3.0	3.0
1031_0003144	3.0	3.0
1031_0003160	3.0	3.0
1031_0003180	3.0	3.0
1031_0003185	3.0	3.0
1031_0003186	3.0	3.0
1031_0003203	2.0	2.0
1031_0003212	2.0	3.0
1031_0003217	3.0	3.0
1031_0003219	3.0	3.0
1031_0003224	3.0	3.0
1031_0003225	3.0	3.0
1031_0003232	2.0	3.0
1031_0003233	2.0	3.0
1031_0003236	3.0	3.0
1031_0003239	4.0	3.0
1031_0003243	3.0	3.0
1031_0003310	3.0	3.0
1031_0003313	4.0	3.0
1031_0003314	3.0	3.0
1031_0003330	3.0	3.0
1031_0003353	3.0	3.0
1031_0003358	4.0	3.0
1031_0003368	3.0	3.0
1031_0003369	3.0	3.0
1031_0003386	2.0	3.0
1031_0003409	4.0	3.0
1061_0120277	1.0	2.0
1061_0120279	1.0	2.0
1061_0120280	1.0	2.0
1061_0120282	0.0	0.0
1061_0120283	0.0	1.0
1061_0120285	1.0	2.0
1061_0120298	1.0	2.0
1061_0120300	2.0	1.0
1061_0120319	2.0	2.0
1061_0120320	3.0	3.0
1061_0120321	2.0	2.0
1061_0120329	2.0	2.0
1061_0120341	1.0	1.0
1061_0120343	2.0	2.0
1061_0120345	2.0	2.0
1061_0120348	1.0	1.0
1061_0120353	1.0	1.0
1061_0120361	2.0	2.0
1061_0120373	2.0	2.0
1061_0120376	2.0	2.0
1061_0120388	1.0	2.0
1061_0120423	2.0	2.0
1061_0120432	1.0	2.0
1061_0120438	2.0	2.0
1061_0120440	1.0	1.0
1061_0120441	2.0	2.0
1061_0120449	3.0	2.0
1061_0120478	2.0	2.0
1061_0120479	2.0	2.0
1061_0120484	1.0	2.0
1061_0120488	2.0	2.0
1061_0120496	1.0	2.0
1061_0120497	2.0	3.0
1061_0120498	2.0	3.0
1061_0120855	1.0	2.0
1061_0120857	2.0	2.0
1061_0120859	2.0	2.0
1061_0120882	2.0	3.0
1061_0120883	1.0	1.0
1061_0120886	2.0	2.0
1061_1029113	2.0	2.0
1061_1029119	1.0	2.0
1061_1029120	1.0	2.0
1061_1202913	2.0	1.0
1061_1202916	2.0	2.0
1071_0024685	1.0	2.0
1071_0024688	1.0	1.0
1071_0024689	1.0	1.0
1071_0024691	1.0	2.0
1071_0024711	1.0	1.0
1071_0024713	1.0	1.0
1071_0024756	1.0	1.0
1071_0024758	2.0	2.0
1071_0024767	2.0	2.0
1071_0024781	0.0	1.0
1071_0024782	0.0	0.0
1071_0024797	0.0	1.0
1071_0024799	2.0	2.0
1071_0024807	0.0	1.0
1071_0024814	1.0	1.0
1071_0024818	2.0	1.0
1071_0024834	2.0	2.0
1071_0024836	1.0	2.0
1071_0024845	0.0	1.0
1071_0024852	0.0	0.0
1071_0024861	0.0	1.0
1071_0024864	0.0	0.0
1071_0024865	2.0	2.0
1071_0024867	1.0	2.0
1071_0024879	1.0	1.0
1071_0242041	1.0	1.0
1071_0242093	0.0	0.0
1071_0243502	1.0	1.0
1071_0243592	1.0	1.0
1071_0248305	0.0	0.0
1071_0248311	1.0	1.0
1071_0248320	0.0	0.0
1071_0248323	0.0	1.0
1071_0248332	2.0	2.0
1071_0248336	0.0	1.0
1071_0248339	2.0	1.0
1071_0248340	0.0	1.0
1071_0248345	1.0	2.0
1091_0000008	2.0	2.0
1091_0000009	0.0	1.0
1091_0000016	0.0	1.0
1091_0000019	1.0	2.0
1091_0000022	1.0	2.0
1091_0000023	2.0	0.0
1091_0000024	3.0	1.0
1091_0000026	1.0	1.0
1091_0000027	0.0	2.0
1091_0000047	2.0	1.0
1091_0000049	1.0	1.0
1091_0000051	1.0	1.0
1091_0000058	2.0	2.0
1091_0000072	1.0	2.0
1091_0000074	1.0	2.0
1091_0000076	2.0	2.0
1091_0000078	2.0	1.0
1091_0000113	1.0	2.0
1091_0000151	0.0	1.0
1091_0000155	1.0	3.0
1091_0000156	2.0	2.0
1091_0000160	2.0	3.0
1091_0000161	2.0	2.0
1091_0000164	1.0	1.0
1091_0000168	2.0	2.0
1091_0000169	3.0	1.0
1091_0000170	2.0	1.0
1091_0000172	2.0	1.0
1091_0000174	2.0	0.0
1091_0000197	1.0	2.0
1091_0000211	1.0	2.0
1091_0000212	1.0	2.0
1091_0000218	2.0	2.0
1091_0000225	2.0	1.0
1091_0000229	1.0	2.0
1091_0000234	3.0	2.0
1091_0000245	1.0	2.0
1091_0000246	2.0	2.0
1091_0000248	2.0	2.0
1091_0000263	3.0	2.0
1091_0000268	2.0	2.0
1091_0000269	1.0	2.0
1091_0000273	1.0	2.0
0604	2.0	2.0
0612	1.0	1.0
0614	1.0	2.0
0624	2.0	2.0
0625	2.0	1.0
0632	1.0	1.0
0639	1.0	2.0
0643	2.0	2.0
0644	1.0	2.0
0722	2.0	2.0
0803	2.0	2.0
0807	2.0	2.0
0808	1.0	2.0
0809	2.0	2.0
0813	1.0	2.0
0818	1.0	2.0
0820	1.0	1.0
0827	1.0	2.0
0904	1.0	2.0
0905	2.0	2.0
0907	2.0	2.0
0913	2.0	2.0
0919	1.0	2.0
0927	1.0	1.0
0930	2.0	2.0
1009	2.0	2.0
1019	1.0	1.0
1020	2.0	2.0
1113	1.0	2.0
9999	1.0	1.0
KYJ0611005B	1.0	1.0
KYJ0611009B	1.0	1.0
LIB0611003A	1.0	1.0
LIB0611011	1.0	1.0
LON0610002B	1.0	2.0
LON0611002A	1.0	2.0
LON0611002B	1.0	1.0
PHA0111005A	1.0	2.0
PHA0111010	3.0	3.0
PHA0111014	1.0	2.0
PHA0112009B	2.0	1.0
PHA0112012A	1.0	2.0
PHA0209031	4.0	3.0
PHA0210008	1.0	1.0
PHA0411009A	2.0	2.0
PHA0411034	1.0	1.0
PHA0411036	3.0	2.0
PHA0411045	3.0	2.0
PHA0411053	3.0	3.0
PHA0509017	2.0	3.0
PHA0509019	3.0	2.0
PHA0509026	3.0	3.0
PHA0509040	2.0	2.0
PHA0509044	2.0	2.0
PHA0510002B	2.0	1.0
PHA0510010B	0.0	1.0
PHA0510013A	2.0	2.0
PHA0510023	3.0	3.0
PHA0510027	1.0	1.0
PHA0510035	3.0	3.0
PHA0510036	3.0	2.0
PHA0510037	1.0	2.0
PHA0510049	3.0	3.0
PHA0610005B	0.0	1.0
PHA0610007A	1.0	1.0
PHA0610018	2.0	3.0
PHA0610025	2.0	3.0
PHA0709008	3.0	2.0
PHA0710011	3.0	3.0
PHA0710016	3.0	2.0
PHA0810008	3.0	3.0
PHA0810009	3.0	3.0
PHA1109002	3.0	3.0
PHA1109003	1.0	1.0
PHA1109004	3.0	3.0
PHA1110001B	1.0	1.0
PHA1110013	2.0	3.0
PHA1110016	1.0	2.0
PHA1110022	3.0	3.0
PHA1111001A	1.0	1.0
PHA1111002A	2.0	1.0
PHA1111002B	1.0	1.0
PHA1111004A	1.0	2.0
VAR0910005	3.0	2.0
1325_1001010	2.0	3.0
1325_1001020	2.0	2.0
1325_1001024	2.0	2.0
1325_1001027	3.0	2.0
1325_1001037	2.0	2.0
1325_1001041	3.0	3.0
1325_1001047	1.0	2.0
1325_1001048	1.0	2.0
1325_1001053	1.0	2.0
1325_1001062	2.0	3.0
1325_1001079	3.0	3.0
1325_1001088	2.0	2.0
1325_1001098	2.0	3.0
1325_1001111	3.0	3.0
1325_1001119	3.0	3.0
1325_1001156	2.0	3.0
1325_1001157	2.0	2.0
1325_1001164	2.0	3.0
1325_1001165	2.0	2.0
1325_9000090	2.0	3.0
1325_9000106	2.0	3.0
1325_9000188	2.0	3.0
1325_9000296	2.0	2.0
1325_9000303	3.0	2.0
1325_9000314	2.0	3.0
1325_9000319	2.0	2.0
1325_9000320	3.0	2.0
1325_9000677	3.0	3.0
1325_9000684	3.0	3.0
1325_9000700	2.0	3.0
1365_0100002	2.0	2.0
1365_0100007	1.0	1.0
1365_0100018	1.0	2.0
1365_0100020	2.0	2.0
1365_0100021	2.0	1.0
1365_0100022	2.0	2.0
1365_0100031	2.0	1.0
1365_0100051	1.0	1.0
1365_0100067	2.0	2.0
1365_0100070	2.0	2.0
1365_0100071	2.0	2.0
1365_0100073	2.0	2.0
1365_0100092	2.0	2.0
1365_0100096	2.0	2.0
1365_0100097	2.0	2.0
1365_0100100	2.0	3.0
1365_0100104	2.0	2.0
1365_0100106	2.0	3.0
1365_0100116	2.0	2.0
1365_0100117	3.0	3.0
1365_0100119	3.0	3.0
1365_0100135	2.0	2.0
1365_0100162	2.0	2.0
1365_0100170	1.0	2.0
1365_0100173	2.0	2.0
1365_0100176	2.0	2.0
1365_0100179	2.0	2.0
1365_0100183	1.0	2.0
1365_0100211	2.0	3.0
1365_0100220	2.0	3.0
1365_0100221	2.0	2.0
1365_0100232	3.0	3.0
1365_0100252	2.0	2.0
1365_0100268	2.0	2.0
1365_0100290	2.0	2.0
1365_0100299	3.0	2.0
1365_0100455	3.0	3.0
1365_0100458	2.0	3.0
1365_0100469	2.0	2.0
1365_0100474	2.0	2.0
1365_0100476	2.0	2.0
1365_0100478	2.0	2.0
1385_0000016	2.0	1.0
1385_0000036	1.0	2.0
1385_0000052	1.0	1.0
1385_0000104	2.0	1.0
1385_0000114	2.0	2.0
1385_0001103	2.0	0.0
1385_0001104	1.0	0.0
1385_0001105	2.0	1.0
1385_0001112	2.0	1.0
1385_0001113	1.0	1.0
1385_0001124	1.0	0.0
1385_0001134	1.0	0.0
1385_0001152	2.0	2.0
1385_0001153	3.0	1.0
1385_0001154	2.0	2.0
1385_0001169	1.0	1.0
1385_0001172	0.0	1.0
1385_0001196	1.0	1.0
1385_0001501	1.0	2.0
1385_0001524	0.0	1.0
1385_0001724	1.0	2.0
1385_0001726	1.0	1.0
1385_0001728	1.0	1.0
1385_0001729	1.0	2.0
1385_0001740	1.0	1.0
1385_0001748	1.0	1.0
1385_0001750	0.0	0.0
1385_0001757	2.0	1.0
1385_0001800	1.0	1.0
1395_0000337	0.0	0.0
1395_0000338	1.0	1.0
1395_0000340	2.0	1.0
1395_0000354	1.0	1.0
1395_0000359	2.0	1.0
1395_0000360	3.0	2.0
1395_0000361	2.0	2.0
1395_0000365	2.0	2.0
1395_0000366	2.0	1.0
1395_0000376	2.0	2.0
1395_0000379	1.0	1.0
1395_0000387	3.0	2.0
1395_0000389	0.0	0.0
1395_0000391	3.0	2.0
1395_0000392	2.0	2.0
1395_0000396	2.0	2.0
1395_0000398	2.0	2.0
1395_0000409	2.0	2.0
1395_0000452	1.0	1.0
1395_0000454	2.0	2.0
1395_0000455	1.0	2.0
1395_0000460	1.0	1.0
1395_0000465	1.0	1.0
1395_0000499	1.0	1.0
1395_0000500	1.0	1.0
1395_0000516	1.0	0.0
1395_0000548	2.0	2.0
1395_0000560	2.0	2.0
1395_0000565	1.0	1.0
1395_0000575	1.0	1.0
1395_0000584	0.0	1.0
1395_0000585	1.0	2.0
1395_0000591	0.0	0.0
1395_0000593	1.0	1.0
1395_0000597	1.0	1.0
1395_0000602	1.0	1.0
1395_0000610	2.0	1.0
1395_0000630	1.0	2.0
1395_0000646	1.0	1.0
1395_0001013	1.0	2.0
1395_0001015	1.0	2.0
1395_0001017	1.0	1.0
1395_0001019	0.0	1.0
1395_0001020	1.0	1.0
1395_0001058	1.0	1.0
1395_0001074	1.0	1.0
1395_0001103	1.0	1.0
1395_0001109	0.0	1.0
1395_0001117	1.0	2.0
1395_0001119	1.0	2.0
1395_0001121	0.0	1.0
1395_0001126	1.0	1.0
1395_0001133	0.0	1.0
1395_0001145	2.0	2.0
1395_0001147	1.0	2.0
1395_0001164	2.0	2.0
4 Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 1.13
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.93
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        33
         1.0       0.60      0.65      0.62       142
         2.0       0.61      0.52      0.56       176
         3.0       0.51      0.84      0.64        91
         4.0       0.00      0.00      0.00        10

    accuracy                           0.57       452
   macro avg       0.34      0.40      0.36       452
weighted avg       0.53      0.57      0.54       452

[[ 0 32  1  0  0]
 [ 0 92 43  7  0]
 [ 0 30 91 55  0]
 [ 0  0 15 76  0]
 [ 0  0  0 10  0]]
0.5407133096667576
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.89
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.83
              precision    recall  f1-score   support

         0.0       0.88      0.21      0.34        33
         1.0       0.63      0.75      0.68       142
         2.0       0.67      0.52      0.59       176
         3.0       0.56      0.84      0.67        91
         4.0       0.00      0.00      0.00        10

    accuracy                           0.62       452
   macro avg       0.55      0.46      0.46       452
weighted avg       0.63      0.62      0.60       452

[[  7  26   0   0   0]
 [  1 107  31   3   0]
 [  0  38  92  46   0]
 [  0   0  15  76   0]
 [  0   0   0  10   0]]
0.6033005585144906
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.73
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.99
              precision    recall  f1-score   support

         0.0       1.00      0.27      0.43        33
         1.0       0.70      0.45      0.55       142
         2.0       0.58      0.67      0.62       176
         3.0       0.58      0.92      0.71        91
         4.0       0.00      0.00      0.00        10

    accuracy                           0.61       452
   macro avg       0.57      0.46      0.46       452
weighted avg       0.63      0.61      0.59       452

[[  9  19   5   0   0]
 [  0  64  75   3   0]
 [  0   9 118  49   0]
 [  0   0   7  84   0]
 [  0   0   0  10   0]]
0.5870415323792189
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.59
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.83      0.30      0.44        33
         1.0       0.71      0.49      0.58       142
         2.0       0.59      0.66      0.62       176
         3.0       0.57      0.90      0.70        91
         4.0       0.00      0.00      0.00        10

    accuracy                           0.62       452
   macro avg       0.54      0.47      0.47       452
weighted avg       0.63      0.62      0.60       452

[[ 10  18   5   0   0]
 [  2  70  67   3   0]
 [  0  11 116  49   0]
 [  0   0   9  82   0]
 [  0   0   0  10   0]]
0.5976368254473694
452 452 452
Filename	True Label	Prediction
1023_0001416	4.0	3.0
1023_0001419	3.0	3.0
1023_0001575	3.0	3.0
1023_0101689	1.0	1.0
1023_0101693	3.0	3.0
1023_0101695	2.0	3.0
1023_0101753	3.0	3.0
1023_0101846	4.0	3.0
1023_0101849	3.0	3.0
1023_0101851	3.0	3.0
1023_0101854	2.0	2.0
1023_0101855	2.0	2.0
1023_0101899	3.0	3.0
1023_0101900	3.0	3.0
1023_0101909	3.0	3.0
1023_0103821	3.0	3.0
1023_0103836	3.0	3.0
1023_0103839	3.0	3.0
1023_0107725	2.0	3.0
1023_0107726	2.0	3.0
1023_0107780	3.0	3.0
1023_0108307	3.0	3.0
1023_0108423	2.0	2.0
1023_0108649	3.0	3.0
1023_0108650	3.0	3.0
1023_0108814	3.0	3.0
1023_0108887	2.0	2.0
1023_0108888	3.0	3.0
1023_0108908	2.0	3.0
1023_0108931	3.0	3.0
1023_0108958	2.0	3.0
1023_0108992	3.0	2.0
1023_0108993	3.0	3.0
1023_0109026	2.0	2.0
1023_0109033	3.0	3.0
1023_0109192	2.0	2.0
1023_0109249	2.0	3.0
1023_0109395	2.0	2.0
1023_0109400	3.0	3.0
1023_0109422	3.0	3.0
1023_0109528	3.0	2.0
1023_0109590	3.0	2.0
1023_0109671	3.0	3.0
1023_0109946	2.0	3.0
1031_0001950	3.0	3.0
1031_0002010	2.0	3.0
1031_0002032	3.0	3.0
1031_0002084	3.0	3.0
1031_0002085	3.0	3.0
1031_0002087	3.0	3.0
1031_0002184	3.0	3.0
1031_0002199	3.0	3.0
1031_0002200	2.0	3.0
1031_0003029	3.0	3.0
1031_0003054	3.0	3.0
1031_0003074	3.0	3.0
1031_0003076	4.0	3.0
1031_0003098	4.0	3.0
1031_0003121	4.0	3.0
1031_0003127	4.0	3.0
1031_0003130	4.0	3.0
1031_0003149	3.0	3.0
1031_0003165	2.0	3.0
1031_0003167	3.0	3.0
1031_0003170	2.0	3.0
1031_0003174	4.0	3.0
1031_0003179	3.0	3.0
1031_0003190	3.0	3.0
1031_0003216	3.0	3.0
1031_0003218	3.0	3.0
1031_0003231	3.0	3.0
1031_0003235	3.0	3.0
1031_0003238	3.0	3.0
1031_0003240	2.0	3.0
1031_0003246	3.0	3.0
1031_0003249	3.0	3.0
1031_0003309	3.0	3.0
1031_0003331	2.0	3.0
1031_0003336	2.0	3.0
1031_0003337	3.0	3.0
1031_0003339	3.0	3.0
1031_0003367	4.0	3.0
1031_0003384	2.0	2.0
1031_0003393	3.0	3.0
1061_0120275	2.0	2.0
1061_0120287	1.0	2.0
1061_0120308	3.0	2.0
1061_0120312	1.0	1.0
1061_0120325	2.0	1.0
1061_0120328	1.0	1.0
1061_0120332	1.0	2.0
1061_0120333	3.0	3.0
1061_0120335	3.0	3.0
1061_0120337	2.0	2.0
1061_0120338	2.0	2.0
1061_0120346	2.0	2.0
1061_0120355	1.0	1.0
1061_0120366	3.0	3.0
1061_0120369	1.0	1.0
1061_0120370	2.0	2.0
1061_0120383	2.0	3.0
1061_0120384	1.0	1.0
1061_0120386	0.0	2.0
1061_0120415	2.0	2.0
1061_0120427	1.0	2.0
1061_0120428	2.0	2.0
1061_0120429	2.0	2.0
1061_0120430	1.0	2.0
1061_0120457	3.0	2.0
1061_0120460	2.0	2.0
1061_0120482	1.0	2.0
1061_0120487	2.0	2.0
1061_0120491	2.0	2.0
1061_0120853	1.0	2.0
1061_0120884	1.0	2.0
1061_0120885	2.0	2.0
1061_0120888	1.0	2.0
1061_0120890	1.0	1.0
1061_0120894	2.0	2.0
1061_1029114	1.0	1.0
1061_1029116	1.0	2.0
1061_1029117	1.0	2.0
1061_1029118	1.0	2.0
1061_1202915	1.0	1.0
1071_0020001	1.0	1.0
1071_0024680	2.0	2.0
1071_0024683	0.0	1.0
1071_0024693	1.0	1.0
1071_0024699	1.0	1.0
1071_0024709	2.0	2.0
1071_0024769	0.0	2.0
1071_0024773	1.0	1.0
1071_0024776	0.0	0.0
1071_0024778	0.0	0.0
1071_0024804	1.0	1.0
1071_0024806	1.0	1.0
1071_0024808	0.0	1.0
1071_0024811	1.0	1.0
1071_0024813	0.0	1.0
1071_0024816	1.0	1.0
1071_0024820	0.0	1.0
1071_0024824	1.0	1.0
1071_0024833	1.0	1.0
1071_0024838	0.0	0.0
1071_0024847	1.0	2.0
1071_0024866	2.0	2.0
1071_0242043	0.0	1.0
1071_0242092	0.0	0.0
1071_0243622	1.0	1.0
1071_0248302	0.0	0.0
1071_0248303	0.0	0.0
1071_0248307	2.0	1.0
1071_0248310	0.0	1.0
1071_0248312	1.0	1.0
1071_0248319	0.0	0.0
1071_0248325	0.0	1.0
1071_0248327	0.0	0.0
1071_0248335	1.0	1.0
1071_0248349	1.0	1.0
1091_0000002	2.0	2.0
1091_0000005	2.0	2.0
1091_0000011	1.0	2.0
1091_0000012	1.0	2.0
1091_0000015	2.0	2.0
1091_0000018	2.0	2.0
1091_0000020	1.0	2.0
1091_0000021	1.0	2.0
1091_0000034	2.0	1.0
1091_0000038	1.0	1.0
1091_0000043	1.0	2.0
1091_0000059	1.0	2.0
1091_0000060	2.0	3.0
1091_0000075	2.0	2.0
1091_0000123	2.0	2.0
1091_0000145	1.0	1.0
1091_0000146	1.0	0.0
1091_0000167	2.0	2.0
1091_0000171	1.0	2.0
1091_0000185	2.0	1.0
1091_0000195	1.0	1.0
1091_0000199	2.0	2.0
1091_0000201	2.0	2.0
1091_0000202	1.0	2.0
1091_0000216	1.0	2.0
1091_0000222	2.0	1.0
1091_0000224	1.0	1.0
1091_0000226	1.0	1.0
1091_0000238	1.0	1.0
1091_0000244	2.0	2.0
1091_0000255	0.0	1.0
1091_0000260	1.0	2.0
1091_0000271	2.0	2.0
1091_0000272	1.0	2.0
1091_0000274	1.0	2.0
1091_0000276	2.0	1.0
0611	2.0	2.0
0613	1.0	1.0
0615	1.0	2.0
0616	1.0	2.0
0617	1.0	2.0
0620	1.0	2.0
0631	2.0	2.0
0633	2.0	2.0
0642	1.0	2.0
0645	2.0	2.0
0715	2.0	2.0
0716	2.0	2.0
0724	2.0	2.0
0802	2.0	2.0
0805	2.0	2.0
0806	1.0	2.0
0819	3.0	2.0
0823	2.0	2.0
0829	1.0	2.0
0903	1.0	2.0
0916	1.0	1.0
0923	2.0	2.0
0925	2.0	2.0
0928	1.0	2.0
0929	0.0	1.0
1003	1.0	1.0
1006	2.0	2.0
1007	2.0	2.0
1010	1.0	2.0
1014	2.0	2.0
KYJ0611003A	1.0	2.0
LIB0611001B	1.0	1.0
LON0611004B	1.0	1.0
MOS0611014	1.0	1.0
MOS0611015	2.0	3.0
PAR1011008A	1.0	2.0
PAR1011009A	2.0	2.0
PAR1011014	2.0	3.0
PAR1011018	3.0	3.0
PHA0111001A	1.0	1.0
PHA0111005B	2.0	1.0
PHA0111011	2.0	3.0
PHA0112002B	1.0	2.0
PHA0112003B	1.0	1.0
PHA0112006A	3.0	2.0
PHA0112007B	1.0	1.0
PHA0209026	2.0	3.0
PHA0209028	2.0	2.0
PHA0210001	1.0	2.0
PHA0411010A	0.0	1.0
PHA0411012B	1.0	1.0
PHA0411032	1.0	3.0
PHA0411033	2.0	2.0
PHA0411035	3.0	3.0
PHA0411037	2.0	3.0
PHA0411039	2.0	3.0
PHA0411060	2.0	3.0
PHA0411061	3.0	3.0
PHA0509022	4.0	3.0
PHA0509024	2.0	3.0
PHA0509027	1.0	2.0
PHA0509030	2.0	3.0
PHA0509032	2.0	3.0
PHA0509034	1.0	3.0
PHA0509039	3.0	3.0
PHA0509041	2.0	3.0
PHA0510003A	1.0	1.0
PHA0510003B	1.0	1.0
PHA0510004A	1.0	1.0
PHA0510010A	1.0	1.0
PHA0510030	2.0	3.0
PHA0510031	2.0	2.0
PHA0510032	3.0	3.0
PHA0510038	3.0	3.0
PHA0510040	2.0	3.0
PHA0510048	1.0	2.0
PHA0610005A	1.0	1.0
PHA0610006B	1.0	1.0
PHA0610007B	1.0	1.0
PHA0710010	2.0	3.0
PHA0710014	3.0	3.0
PHA0710015	2.0	3.0
PHA0810002	1.0	3.0
PHA0810012	3.0	3.0
PHA0811014	1.0	2.0
PHA1109006	2.0	2.0
PHA1110015	3.0	3.0
PHA1110019	2.0	2.0
VAR0910011	3.0	3.0
1325_1001009	2.0	2.0
1325_1001013	2.0	3.0
1325_1001014	3.0	3.0
1325_1001022	2.0	2.0
1325_1001035	3.0	3.0
1325_1001036	2.0	2.0
1325_1001039	3.0	3.0
1325_1001051	2.0	2.0
1325_1001057	2.0	3.0
1325_1001081	3.0	2.0
1325_1001082	2.0	2.0
1325_1001093	2.0	2.0
1325_1001094	2.0	2.0
1325_1001095	2.0	2.0
1325_1001096	2.0	2.0
1325_1001097	1.0	1.0
1325_1001100	2.0	2.0
1325_1001107	3.0	3.0
1325_1001108	3.0	3.0
1325_1001120	3.0	3.0
1325_1001126	2.0	2.0
1325_1001128	2.0	3.0
1325_1001130	2.0	2.0
1325_1001133	2.0	3.0
1325_1001135	2.0	2.0
1325_1001141	2.0	2.0
1325_1001167	3.0	3.0
1325_1001168	2.0	3.0
1325_9000059	2.0	3.0
1325_9000099	2.0	3.0
1325_9000105	1.0	2.0
1325_9000136	2.0	3.0
1325_9000210	2.0	2.0
1325_9000213	3.0	3.0
1325_9000239	3.0	3.0
1325_9000240	2.0	2.0
1325_9000302	2.0	2.0
1325_9000315	2.0	2.0
1325_9000317	3.0	3.0
1325_9000318	3.0	3.0
1325_9000505	2.0	3.0
1325_9000536	2.0	3.0
1325_9000602	3.0	3.0
1325_9000675	3.0	2.0
1365_0100017	2.0	3.0
1365_0100027	2.0	2.0
1365_0100029	1.0	1.0
1365_0100056	2.0	3.0
1365_0100058	2.0	3.0
1365_0100065	1.0	2.0
1365_0100066	1.0	2.0
1365_0100094	2.0	2.0
1365_0100099	2.0	2.0
1365_0100118	3.0	3.0
1365_0100120	3.0	3.0
1365_0100121	3.0	3.0
1365_0100134	2.0	2.0
1365_0100136	2.0	2.0
1365_0100145	2.0	3.0
1365_0100164	2.0	2.0
1365_0100178	2.0	2.0
1365_0100180	1.0	1.0
1365_0100186	2.0	2.0
1365_0100190	2.0	2.0
1365_0100191	2.0	2.0
1365_0100194	3.0	3.0
1365_0100195	1.0	2.0
1365_0100212	3.0	3.0
1365_0100215	2.0	3.0
1365_0100219	2.0	3.0
1365_0100223	2.0	2.0
1365_0100258	2.0	2.0
1365_0100260	2.0	2.0
1365_0100261	2.0	2.0
1365_0100265	3.0	3.0
1365_0100269	2.0	3.0
1365_0100270	2.0	2.0
1365_0100275	2.0	3.0
1365_0100281	1.0	2.0
1365_0100282	2.0	2.0
1365_0100285	2.0	2.0
1365_0100447	3.0	3.0
1365_0100457	2.0	2.0
1365_0100461	2.0	2.0
1365_0100480	2.0	2.0
1385_0000012	2.0	2.0
1385_0000020	2.0	1.0
1385_0000023	2.0	1.0
1385_0000035	2.0	2.0
1385_0000037	1.0	2.0
1385_0000040	1.0	0.0
1385_0000044	2.0	2.0
1385_0000049	2.0	2.0
1385_0000053	2.0	2.0
1385_0000054	2.0	2.0
1385_0000099	0.0	1.0
1385_0000100	1.0	1.0
1385_0000122	2.0	2.0
1385_0000128	1.0	2.0
1385_0001107	2.0	1.0
1385_0001109	2.0	2.0
1385_0001110	2.0	2.0
1385_0001119	1.0	1.0
1385_0001121	1.0	2.0
1385_0001122	1.0	1.0
1385_0001125	1.0	1.0
1385_0001129	1.0	1.0
1385_0001148	2.0	2.0
1385_0001151	2.0	2.0
1385_0001157	1.0	1.0
1385_0001161	2.0	2.0
1385_0001165	1.0	2.0
1385_0001166	0.0	1.0
1385_0001170	1.0	1.0
1385_0001171	1.0	1.0
1385_0001195	1.0	2.0
1385_0001199	1.0	1.0
1385_0001723	0.0	1.0
1385_0001741	0.0	1.0
1385_0001744	0.0	1.0
1385_0001749	1.0	1.0
1385_0001752	0.0	1.0
1385_0001758	1.0	1.0
1385_0001772	0.0	1.0
1385_0001785	0.0	0.0
1385_0001786	1.0	2.0
1385_0001788	1.0	2.0
1385_0001792	0.0	2.0
1385_0001793	1.0	1.0
1395_0000355	2.0	2.0
1395_0000364	1.0	2.0
1395_0000368	0.0	0.0
1395_0000378	1.0	2.0
1395_0000390	1.0	1.0
1395_0000413	2.0	2.0
1395_0000415	1.0	2.0
1395_0000432	2.0	2.0
1395_0000449	2.0	2.0
1395_0000458	2.0	2.0
1395_0000527	1.0	2.0
1395_0000534	2.0	2.0
1395_0000535	1.0	1.0
1395_0000547	2.0	2.0
1395_0000551	2.0	2.0
1395_0000556	1.0	1.0
1395_0000557	3.0	3.0
1395_0000563	1.0	2.0
1395_0000572	1.0	1.0
1395_0000579	1.0	1.0
1395_0000598	1.0	2.0
1395_0000612	1.0	1.0
1395_0000626	1.0	2.0
1395_0000627	1.0	2.0
1395_0000635	0.0	1.0
1395_0000649	2.0	2.0
1395_0001016	2.0	1.0
1395_0001023	1.0	1.0
1395_0001024	1.0	2.0
1395_0001033	1.0	2.0
1395_0001061	1.0	2.0
1395_0001064	2.0	2.0
1395_0001078	0.0	2.0
1395_0001084	1.0	2.0
1395_0001093	1.0	2.0
1395_0001120	1.0	1.0
1395_0001123	1.0	1.0
1395_0001149	0.0	2.0
1395_0001171	1.0	2.0
5 Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 1.13
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.05
              precision    recall  f1-score   support

         0.0       1.00      0.03      0.06        33
         1.0       0.58      0.39      0.47       143
         2.0       0.51      0.55      0.53       176
         3.0       0.49      0.92      0.64        90
         4.0       0.00      0.00      0.00        10

    accuracy                           0.52       452
   macro avg       0.52      0.38      0.34       452
weighted avg       0.56      0.52      0.49       452

[[ 1 29  3  0  0]
 [ 0 56 82  5  0]
 [ 0 10 96 70  0]
 [ 0  1  6 83  0]
 [ 0  0  0 10  0]]
0.4866186163870323
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.85
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.92
              precision    recall  f1-score   support

         0.0       0.80      0.12      0.21        33
         1.0       0.56      0.84      0.67       143
         2.0       0.65      0.38      0.48       176
         3.0       0.55      0.80      0.65        90
         4.0       0.00      0.00      0.00        10

    accuracy                           0.58       452
   macro avg       0.51      0.43      0.40       452
weighted avg       0.60      0.58      0.55       452

[[  4  29   0   0   0]
 [  1 120  22   0   0]
 [  0  60  67  49   0]
 [  0   4  14  72   0]
 [  0   0   0  10   0]]
0.5454089882512113
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 48
Elapsed time 59

  Average training loss: 0.71
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.86
              precision    recall  f1-score   support

         0.0       0.69      0.27      0.39        33
         1.0       0.64      0.69      0.66       143
         2.0       0.65      0.60      0.62       176
         3.0       0.60      0.80      0.69        90
         4.0       0.00      0.00      0.00        10

    accuracy                           0.63       452
   macro avg       0.52      0.47      0.47       452
weighted avg       0.62      0.63      0.62       452

[[  9  23   1   0   0]
 [  3  99  41   0   0]
 [  1  31 106  38   0]
 [  0   2  16  72   0]
 [  0   0   0  10   0]]
0.6181016116856787
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.54
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.99
              precision    recall  f1-score   support

         0.0       0.74      0.42      0.54        33
         1.0       0.64      0.57      0.60       143
         2.0       0.58      0.61      0.59       176
         3.0       0.57      0.78      0.66        90
         4.0       0.00      0.00      0.00        10

    accuracy                           0.60       452
   macro avg       0.51      0.48      0.48       452
weighted avg       0.60      0.60      0.59       452

[[ 14  17   2   0   0]
 [  4  81  57   1   0]
 [  1  27 107  41   0]
 [  0   2  18  70   0]
 [  0   0   0  10   0]]
0.59209162667169
452 452 452
Filename	True Label	Prediction
1023_0001418	2.0	2.0
1023_0001422	3.0	2.0
1023_0001423	2.0	2.0
1023_0101683	3.0	3.0
1023_0101688	3.0	3.0
1023_0101691	3.0	3.0
1023_0101752	2.0	3.0
1023_0101844	2.0	3.0
1023_0101845	2.0	3.0
1023_0101895	4.0	3.0
1023_0101904	2.0	2.0
1023_0102117	2.0	3.0
1023_0103822	2.0	3.0
1023_0103825	3.0	3.0
1023_0103827	3.0	3.0
1023_0103833	4.0	3.0
1023_0103838	3.0	3.0
1023_0103841	3.0	3.0
1023_0103843	2.0	2.0
1023_0103880	3.0	3.0
1023_0103955	3.0	3.0
1023_0104203	2.0	2.0
1023_0107682	2.0	2.0
1023_0107727	4.0	3.0
1023_0108306	3.0	3.0
1023_0108422	3.0	3.0
1023_0108648	3.0	3.0
1023_0108890	3.0	2.0
1023_0109022	2.0	2.0
1023_0109039	3.0	3.0
1023_0109247	3.0	2.0
1023_0109267	2.0	2.0
1023_0109396	2.0	3.0
1023_0109495	2.0	2.0
1023_0109516	3.0	3.0
1023_0109519	2.0	3.0
1023_0109527	3.0	3.0
1023_0109609	2.0	2.0
1023_0109878	2.0	2.0
1023_0109891	3.0	3.0
1023_0109951	2.0	2.0
1023_0111896	2.0	2.0
1031_0001951	2.0	3.0
1031_0001997	3.0	3.0
1031_0002004	3.0	3.0
1031_0002005	3.0	3.0
1031_0002006	4.0	3.0
1031_0002036	4.0	3.0
1031_0002089	3.0	3.0
1031_0003078	3.0	3.0
1031_0003088	4.0	3.0
1031_0003091	2.0	2.0
1031_0003095	2.0	3.0
1031_0003106	3.0	3.0
1031_0003141	3.0	3.0
1031_0003150	3.0	3.0
1031_0003163	3.0	3.0
1031_0003172	3.0	3.0
1031_0003182	4.0	3.0
1031_0003187	3.0	3.0
1031_0003205	3.0	3.0
1031_0003206	3.0	3.0
1031_0003211	2.0	3.0
1031_0003226	3.0	3.0
1031_0003244	4.0	3.0
1031_0003260	4.0	3.0
1031_0003366	3.0	3.0
1031_0003383	3.0	3.0
1031_0003387	3.0	3.0
1031_0003388	3.0	3.0
1031_0003408	2.0	3.0
1061_0120272	1.0	2.0
1061_0120276	2.0	2.0
1061_0120281	1.0	2.0
1061_0120286	0.0	2.0
1061_0120289	1.0	1.0
1061_0120290	1.0	1.0
1061_0120291	1.0	1.0
1061_0120295	0.0	2.0
1061_0120297	1.0	2.0
1061_0120299	2.0	2.0
1061_0120301	2.0	2.0
1061_0120302	1.0	1.0
1061_0120304	2.0	1.0
1061_0120306	3.0	2.0
1061_0120318	2.0	2.0
1061_0120323	1.0	1.0
1061_0120324	2.0	2.0
1061_0120347	2.0	1.0
1061_0120349	1.0	1.0
1061_0120354	1.0	1.0
1061_0120356	2.0	2.0
1061_0120360	3.0	2.0
1061_0120382	1.0	1.0
1061_0120391	1.0	1.0
1061_0120403	3.0	3.0
1061_0120404	1.0	1.0
1061_0120405	2.0	2.0
1061_0120433	1.0	1.0
1061_0120442	2.0	3.0
1061_0120443	0.0	1.0
1061_0120448	3.0	2.0
1061_0120458	3.0	2.0
1061_0120459	2.0	2.0
1061_0120483	1.0	2.0
1061_0120494	1.0	1.0
1061_0120495	2.0	2.0
1061_0120858	2.0	2.0
1061_0120874	2.0	2.0
1061_0120876	2.0	2.0
1061_0120878	1.0	2.0
1061_0120881	2.0	2.0
1061_0120889	1.0	1.0
1061_1029112	3.0	3.0
1061_1202917	1.0	1.0
1071_0024678	1.0	1.0
1071_0024682	2.0	2.0
1071_0024686	2.0	2.0
1071_0024702	1.0	1.0
1071_0024703	1.0	2.0
1071_0024705	1.0	2.0
1071_0024712	1.0	1.0
1071_0024761	2.0	1.0
1071_0024762	0.0	1.0
1071_0024763	1.0	1.0
1071_0024765	0.0	0.0
1071_0024768	1.0	1.0
1071_0024798	1.0	1.0
1071_0024801	1.0	2.0
1071_0024817	1.0	1.0
1071_0024826	1.0	1.0
1071_0024827	1.0	1.0
1071_0024848	1.0	1.0
1071_0024849	0.0	0.0
1071_0024850	0.0	0.0
1071_0024853	1.0	1.0
1071_0024856	1.0	0.0
1071_0024876	1.0	1.0
1071_0024877	1.0	1.0
1071_0024881	2.0	1.0
1071_0241833	1.0	1.0
1071_0242011	2.0	2.0
1071_0242042	0.0	0.0
1071_0242071	0.0	0.0
1071_0243582	0.0	1.0
1071_0243591	1.0	2.0
1071_0243621	2.0	2.0
1071_0248308	1.0	2.0
1071_0248313	1.0	2.0
1071_0248315	0.0	0.0
1071_0248316	1.0	0.0
1071_0248317	0.0	0.0
1071_0248318	0.0	0.0
1071_0248322	1.0	1.0
1071_0248329	1.0	1.0
1071_0248331	0.0	1.0
1071_0248337	1.0	2.0
1071_0248342	0.0	1.0
1071_0248350	2.0	0.0
1091_0000006	1.0	1.0
1091_0000007	3.0	2.0
1091_0000031	1.0	1.0
1091_0000035	1.0	1.0
1091_0000048	1.0	1.0
1091_0000050	1.0	1.0
1091_0000052	0.0	1.0
1091_0000056	1.0	2.0
1091_0000068	2.0	2.0
1091_0000070	2.0	1.0
1091_0000073	3.0	1.0
1091_0000092	1.0	1.0
1091_0000102	1.0	1.0
1091_0000126	2.0	1.0
1091_0000144	1.0	1.0
1091_0000153	1.0	2.0
1091_0000173	2.0	1.0
1091_0000196	2.0	1.0
1091_0000198	2.0	2.0
1091_0000200	1.0	1.0
1091_0000203	1.0	1.0
1091_0000206	1.0	1.0
1091_0000213	2.0	2.0
1091_0000215	2.0	2.0
1091_0000228	1.0	2.0
1091_0000232	2.0	2.0
1091_0000233	2.0	1.0
1091_0000239	2.0	1.0
1091_0000240	1.0	1.0
1091_0000241	2.0	1.0
1091_0000242	1.0	2.0
1091_0000249	2.0	2.0
1091_0000252	1.0	2.0
1091_0000254	2.0	1.0
1091_0000256	1.0	1.0
1091_0000259	2.0	2.0
1091_0000262	2.0	2.0
1091_0000265	2.0	2.0
0605	2.0	2.0
0609	1.0	1.0
0621	2.0	2.0
0622	1.0	1.0
0623	2.0	2.0
0626	2.0	1.0
0628	2.0	2.0
0634	2.0	2.0
0636	2.0	2.0
0637	2.0	2.0
0640	2.0	2.0
0714	2.0	2.0
0717	1.0	1.0
0718	1.0	2.0
0720	1.0	1.0
0725	1.0	2.0
0812	2.0	1.0
0814	1.0	2.0
0815	2.0	2.0
0821	2.0	2.0
0826	1.0	2.0
0828	2.0	2.0
0906	2.0	2.0
0912	2.0	2.0
0917	1.0	1.0
0918	1.0	2.0
1002	2.0	1.0
1004	1.0	1.0
1015	1.0	2.0
1021	1.0	2.0
1022	2.0	1.0
1023	1.0	2.0
1111	1.0	1.0
BER0611006	2.0	3.0
KYJ0611005A	1.0	1.0
LIB0611001A	1.0	1.0
LIB0611002B	2.0	2.0
LON0611003	2.0	3.0
LON0611004A	1.0	1.0
MOS0509001	1.0	2.0
PAR1011017	3.0	3.0
PHA0111002B	3.0	1.0
PHA0111003A	1.0	1.0
PHA0111015	3.0	3.0
PHA0111018	1.0	2.0
PHA0112006B	2.0	3.0
PHA0112009A	2.0	2.0
PHA0209001	1.0	1.0
PHA0209013	1.0	1.0
PHA0209039	2.0	3.0
PHA0411009B	1.0	1.0
PHA0411010B	0.0	1.0
PHA0411027	2.0	2.0
PHA0411030	3.0	3.0
PHA0411043	1.0	1.0
PHA0411044	3.0	3.0
PHA0411059	3.0	3.0
PHA0509018	3.0	3.0
PHA0509035	2.0	3.0
PHA0509036	3.0	3.0
PHA0509043	2.0	2.0
PHA0509045	1.0	1.0
PHA0510046	2.0	3.0
PHA0510047	1.0	2.0
PHA0610015	2.0	3.0
PHA0610016	2.0	3.0
PHA0610026	3.0	3.0
PHA0710009	3.0	2.0
PHA0710018	3.0	3.0
PHA0809010	2.0	3.0
PHA0810001	3.0	3.0
PHA0810003	3.0	3.0
PHA0810010	2.0	3.0
PHA0811012	3.0	3.0
PHA0811013	3.0	3.0
PHA0811016	1.0	1.0
PHA0811017	3.0	3.0
PHA1109001	1.0	1.0
PHA1109005	1.0	2.0
PHA1109024	4.0	3.0
PHA1109026	3.0	3.0
PHA1109027	3.0	2.0
PHA1110001A	2.0	1.0
PHA1110002A	2.0	2.0
PHA1110004A	1.0	2.0
PHA1110014	2.0	3.0
PHA1111001B	1.0	1.0
PHA1111003A	2.0	2.0
PHA1111008A	2.0	1.0
TI071122B	1.0	2.0
VAR0209036	3.0	3.0
VAR0909005	1.0	2.0
VAR0909006	3.0	3.0
VAR0909007	2.0	3.0
VAR0909009	3.0	3.0
1325_1001017	2.0	2.0
1325_1001032	2.0	3.0
1325_1001044	3.0	2.0
1325_1001046	2.0	2.0
1325_1001050	2.0	2.0
1325_1001056	2.0	2.0
1325_1001063	2.0	2.0
1325_1001075	1.0	2.0
1325_1001076	3.0	3.0
1325_1001089	2.0	2.0
1325_1001090	2.0	2.0
1325_1001091	3.0	3.0
1325_1001092	2.0	2.0
1325_1001113	3.0	3.0
1325_1001124	2.0	2.0
1325_1001125	2.0	3.0
1325_1001127	3.0	2.0
1325_1001132	2.0	3.0
1325_1001136	2.0	2.0
1325_1001142	2.0	3.0
1325_1001143	2.0	3.0
1325_1001155	2.0	3.0
1325_1001160	2.0	3.0
1325_1001162	2.0	2.0
1325_9000143	3.0	3.0
1325_9000152	3.0	3.0
1325_9000185	3.0	3.0
1325_9000186	3.0	3.0
1325_9000214	2.0	3.0
1325_9000215	3.0	3.0
1325_9000237	3.0	3.0
1325_9000241	3.0	3.0
1325_9000304	2.0	2.0
1325_9000323	2.0	2.0
1325_9000534	2.0	3.0
1325_9000554	2.0	2.0
1325_9000685	3.0	3.0
1325_9000686	3.0	2.0
1365_0100004	2.0	2.0
1365_0100005	2.0	2.0
1365_0100008	1.0	2.0
1365_0100009	2.0	1.0
1365_0100010	1.0	1.0
1365_0100013	3.0	3.0
1365_0100023	2.0	1.0
1365_0100026	1.0	1.0
1365_0100061	3.0	3.0
1365_0100074	2.0	2.0
1365_0100095	2.0	2.0
1365_0100101	3.0	2.0
1365_0100102	3.0	2.0
1365_0100103	3.0	3.0
1365_0100107	3.0	2.0
1365_0100148	2.0	3.0
1365_0100151	2.0	2.0
1365_0100165	3.0	3.0
1365_0100166	2.0	2.0
1365_0100169	2.0	2.0
1365_0100171	2.0	2.0
1365_0100174	2.0	2.0
1365_0100177	2.0	2.0
1365_0100187	2.0	3.0
1365_0100188	2.0	2.0
1365_0100196	1.0	2.0
1365_0100198	2.0	2.0
1365_0100202	1.0	2.0
1365_0100222	1.0	2.0
1365_0100224	2.0	3.0
1365_0100227	3.0	2.0
1365_0100229	2.0	3.0
1365_0100253	2.0	1.0
1365_0100267	2.0	2.0
1365_0100274	2.0	3.0
1365_0100277	3.0	3.0
1365_0100278	2.0	2.0
1365_0100279	1.0	2.0
1365_0100280	1.0	1.0
1365_0100288	2.0	2.0
1365_0100459	2.0	3.0
1365_0100470	2.0	2.0
1365_0100471	2.0	3.0
1365_0100472	2.0	2.0
1365_0100473	2.0	2.0
1385_0000013	1.0	3.0
1385_0000017	1.0	0.0
1385_0000022	1.0	2.0
1385_0000039	1.0	2.0
1385_0000045	2.0	2.0
1385_0000047	1.0	2.0
1385_0000058	2.0	2.0
1385_0000101	1.0	1.0
1385_0000103	1.0	1.0
1385_0000120	0.0	0.0
1385_0000125	1.0	2.0
1385_0000130	2.0	1.0
1385_0001108	2.0	2.0
1385_0001123	2.0	1.0
1385_0001131	1.0	2.0
1385_0001147	1.0	1.0
1385_0001149	2.0	2.0
1385_0001155	2.0	2.0
1385_0001156	2.0	2.0
1385_0001162	1.0	1.0
1385_0001167	1.0	1.0
1385_0001175	0.0	0.0
1385_0001188	1.0	2.0
1385_0001189	0.0	1.0
1385_0001193	2.0	2.0
1385_0001522	0.0	0.0
1385_0001527	1.0	0.0
1385_0001528	1.0	2.0
1385_0001712	1.0	2.0
1385_0001714	1.0	1.0
1385_0001715	1.0	1.0
1385_0001718	0.0	1.0
1385_0001719	0.0	1.0
1385_0001730	1.0	2.0
1385_0001732	1.0	2.0
1385_0001736	2.0	3.0
1385_0001737	2.0	2.0
1385_0001738	0.0	0.0
1385_0001747	1.0	1.0
1385_0001753	1.0	1.0
1385_0001766	2.0	2.0
1385_0001767	0.0	1.0
1385_0001768	1.0	1.0
1385_0001774	0.0	1.0
1385_0001789	1.0	2.0
1385_0001790	2.0	2.0
1385_0001791	0.0	1.0
1385_0001799	1.0	2.0
1395_0000341	1.0	1.0
1395_0000356	1.0	1.0
1395_0000388	2.0	2.0
1395_0000404	2.0	2.0
1395_0000438	3.0	2.0
1395_0000450	1.0	2.0
1395_0000451	2.0	1.0
1395_0000470	1.0	1.0
1395_0000526	1.0	2.0
1395_0000552	2.0	2.0
1395_0000553	2.0	1.0
1395_0000559	2.0	1.0
1395_0000595	0.0	0.0
1395_0000596	2.0	1.0
1395_0000604	1.0	1.0
1395_0000609	0.0	1.0
1395_0001010	1.0	2.0
1395_0001021	1.0	1.0
1395_0001034	1.0	2.0
1395_0001066	1.0	2.0
1395_0001068	1.0	2.0
1395_0001071	1.0	2.0
1395_0001076	0.0	1.0
1395_0001090	2.0	2.0
1395_0001104	0.0	1.0
1395_0001108	0.0	0.0
1395_0001124	0.0	1.0
1395_0001150	1.0	1.0
1395_0001158	2.0	2.0
Averaged weighted F1-scores 0.5849669716675983
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
144.90552995391704 65.35717405024758
LANGUAGE: IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
1 Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 1.33
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.09
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.40      0.03      0.06        67
         2.0       0.56      0.72      0.63       162
         3.0       0.49      0.37      0.42       106
         4.0       0.51      0.93      0.66        86
         5.0       0.00      0.00      0.00        23

    accuracy                           0.53       452
   macro avg       0.33      0.34      0.29       452
weighted avg       0.47      0.53      0.46       452

[[  0   3   4   0   1   0]
 [  0   2  61   1   3   0]
 [  0   0 117  34  11   0]
 [  0   0  27  39  40   0]
 [  0   0   0   6  80   0]
 [  0   0   1   0  22   0]]
0.45730658259998197
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 1.02
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.03
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.52      0.52      0.52        67
         2.0       0.64      0.69      0.66       162
         3.0       0.65      0.45      0.53       106
         4.0       0.57      0.92      0.71        86
         5.0       0.00      0.00      0.00        23

    accuracy                           0.60       452
   macro avg       0.40      0.43      0.40       452
weighted avg       0.57      0.60      0.57       452

[[  0   6   1   0   1   0]
 [  0  35  29   1   2   0]
 [  0  23 111  18  10   0]
 [  0   3  31  48  24   0]
 [  0   0   0   7  79   0]
 [  0   0   1   0  22   0]]
0.5742240519903643
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.87
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.05
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.56      0.37      0.45        67
         2.0       0.63      0.64      0.64       162
         3.0       0.54      0.57      0.55       106
         4.0       0.57      0.86      0.69        86
         5.0       0.00      0.00      0.00        23

    accuracy                           0.58       452
   macro avg       0.38      0.41      0.39       452
weighted avg       0.54      0.58      0.55       452

[[  0   6   2   0   0   0]
 [  0  25  39   2   1   0]
 [  0  12 104  37   9   0]
 [  0   2  20  60  24   0]
 [  0   0   0  12  74   0]
 [  0   0   0   1  22   0]]
0.5536087071338147
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.72
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.09
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.47      0.27      0.34        67
         2.0       0.62      0.69      0.65       162
         3.0       0.54      0.52      0.53       106
         4.0       0.56      0.86      0.68        86
         5.0       0.00      0.00      0.00        23

    accuracy                           0.57       452
   macro avg       0.37      0.39      0.37       452
weighted avg       0.53      0.57      0.54       452

[[  0   6   1   1   0   0]
 [  0  18  43   4   2   0]
 [  0  12 112  29   9   0]
 [  0   2  25  55  24   0]
 [  0   0   0  12  74   0]
 [  0   0   0   1  22   0]]
0.5386712768288764
452 452 452
Filename	True Label	Prediction
1023_0001416	4.0	4.0
1023_0101683	3.0	3.0
1023_0101689	2.0	2.0
1023_0101691	3.0	3.0
1023_0101701	3.0	3.0
1023_0101841	3.0	3.0
1023_0101846	4.0	3.0
1023_0101847	2.0	3.0
1023_0101852	4.0	4.0
1023_0101896	2.0	2.0
1023_0101898	3.0	3.0
1023_0101899	3.0	3.0
1023_0103824	4.0	4.0
1023_0103830	3.0	4.0
1023_0103838	4.0	4.0
1023_0103839	3.0	4.0
1023_0103840	4.0	3.0
1023_0103841	5.0	4.0
1023_0104203	4.0	3.0
1023_0107244	4.0	4.0
1023_0107672	3.0	3.0
1023_0107725	3.0	3.0
1023_0108306	4.0	4.0
1023_0108648	4.0	4.0
1023_0108751	3.0	3.0
1023_0108752	5.0	4.0
1023_0108810	3.0	3.0
1023_0108811	3.0	3.0
1023_0108814	4.0	3.0
1023_0108815	4.0	4.0
1023_0108885	2.0	3.0
1023_0108886	4.0	3.0
1023_0108890	4.0	3.0
1023_0108931	4.0	4.0
1023_0108955	5.0	4.0
1023_0108992	3.0	3.0
1023_0109027	2.0	3.0
1023_0109029	2.0	2.0
1023_0109038	5.0	4.0
1023_0109249	3.0	4.0
1023_0109496	3.0	4.0
1023_0109516	5.0	4.0
1023_0109527	5.0	4.0
1023_0109891	4.0	4.0
1023_0109914	3.0	3.0
1023_0109915	2.0	3.0
1031_0002011	5.0	3.0
1031_0002032	4.0	4.0
1031_0002043	4.0	4.0
1031_0002083	3.0	3.0
1031_0002198	4.0	4.0
1031_0002199	4.0	3.0
1031_0002200	3.0	3.0
1031_0003012	3.0	3.0
1031_0003073	5.0	4.0
1031_0003088	4.0	4.0
1031_0003128	4.0	3.0
1031_0003144	3.0	3.0
1031_0003154	3.0	4.0
1031_0003180	4.0	4.0
1031_0003185	3.0	3.0
1031_0003187	3.0	3.0
1031_0003191	3.0	3.0
1031_0003216	4.0	4.0
1031_0003226	5.0	4.0
1031_0003238	4.0	4.0
1031_0003249	3.0	4.0
1031_0003310	4.0	3.0
1031_0003315	3.0	3.0
1031_0003355	3.0	3.0
1031_0003356	4.0	4.0
1031_0003365	3.0	3.0
1031_0003368	3.0	3.0
1031_0003384	2.0	3.0
1031_0003386	3.0	3.0
1031_0003388	3.0	4.0
1031_0003392	3.0	4.0
1031_0003414	3.0	3.0
1061_0120272	2.0	3.0
1061_0120274	2.0	2.0
1061_0120289	2.0	2.0
1061_0120309	2.0	2.0
1061_0120319	2.0	2.0
1061_0120325	2.0	2.0
1061_0120329	2.0	3.0
1061_0120331	1.0	2.0
1061_0120333	2.0	3.0
1061_0120334	3.0	3.0
1061_0120338	2.0	3.0
1061_0120348	1.0	2.0
1061_0120368	3.0	2.0
1061_0120369	2.0	2.0
1061_0120370	2.0	2.0
1061_0120374	3.0	3.0
1061_0120382	2.0	2.0
1061_0120391	1.0	2.0
1061_0120427	3.0	3.0
1061_0120433	1.0	2.0
1061_0120441	2.0	2.0
1061_0120457	3.0	4.0
1061_0120458	4.0	3.0
1061_0120483	2.0	3.0
1061_0120485	3.0	3.0
1061_0120487	2.0	3.0
1061_0120489	3.0	3.0
1061_0120490	3.0	3.0
1061_0120853	3.0	3.0
1061_0120855	2.0	3.0
1061_0120874	3.0	2.0
1061_0120875	3.0	4.0
1061_0120880	4.0	3.0
1061_0120883	3.0	2.0
1061_0120889	2.0	1.0
1061_1029112	3.0	3.0
1061_1029113	2.0	3.0
1061_1029116	2.0	2.0
1061_1029119	3.0	3.0
1061_1202912	3.0	3.0
1061_1202915	2.0	2.0
1071_0024686	2.0	2.0
1071_0024692	4.0	3.0
1071_0024701	3.0	3.0
1071_0024703	2.0	2.0
1071_0024704	2.0	2.0
1071_0024715	1.0	2.0
1071_0024758	3.0	2.0
1071_0024765	1.0	1.0
1071_0024779	2.0	2.0
1071_0024781	1.0	1.0
1071_0024800	1.0	2.0
1071_0024801	1.0	2.0
1071_0024802	2.0	2.0
1071_0024824	2.0	2.0
1071_0024826	2.0	2.0
1071_0024840	2.0	2.0
1071_0024848	1.0	2.0
1071_0024851	1.0	1.0
1071_0024863	1.0	2.0
1071_0024871	3.0	2.0
1071_0024872	2.0	2.0
1071_0242012	2.0	2.0
1071_0242023	1.0	2.0
1071_0242041	1.0	2.0
1071_0243501	2.0	2.0
1071_0243502	2.0	1.0
1071_0243592	2.0	2.0
1071_0243623	2.0	2.0
1071_0248302	1.0	1.0
1071_0248310	1.0	1.0
1071_0248314	1.0	2.0
1071_0248316	1.0	1.0
1071_0248318	0.0	1.0
1071_0248319	1.0	1.0
1071_0248320	0.0	1.0
1071_0248343	1.0	1.0
1071_0248344	1.0	1.0
1071_0248347	1.0	1.0
1071_0248350	1.0	1.0
1091_0000002	3.0	3.0
1091_0000005	3.0	3.0
1091_0000013	1.0	1.0
1091_0000016	1.0	2.0
1091_0000021	1.0	3.0
1091_0000022	2.0	3.0
1091_0000026	2.0	2.0
1091_0000034	3.0	2.0
1091_0000035	2.0	2.0
1091_0000037	2.0	1.0
1091_0000042	1.0	1.0
1091_0000049	2.0	1.0
1091_0000051	1.0	1.0
1091_0000056	2.0	2.0
1091_0000061	3.0	1.0
1091_0000067	3.0	2.0
1091_0000070	3.0	1.0
1091_0000076	2.0	2.0
1091_0000095	2.0	1.0
1091_0000101	2.0	2.0
1091_0000113	2.0	3.0
1091_0000114	2.0	3.0
1091_0000153	2.0	3.0
1091_0000155	3.0	3.0
1091_0000157	3.0	2.0
1091_0000158	2.0	3.0
1091_0000169	3.0	2.0
1091_0000173	2.0	2.0
1091_0000185	2.0	2.0
1091_0000190	1.0	2.0
1091_0000191	2.0	3.0
1091_0000199	3.0	3.0
1091_0000200	3.0	2.0
1091_0000203	2.0	2.0
1091_0000213	2.0	2.0
1091_0000214	3.0	2.0
1091_0000215	2.0	4.0
1091_0000221	3.0	2.0
1091_0000236	3.0	2.0
1091_0000238	2.0	2.0
1091_0000243	1.0	2.0
1091_0000257	2.0	3.0
1091_0000258	2.0	2.0
1091_0000262	2.0	2.0
1091_0000273	1.0	3.0
1091_0000274	2.0	2.0
0606	2.0	2.0
0612	2.0	2.0
0615	2.0	2.0
0624	3.0	2.0
0633	3.0	2.0
0634	3.0	3.0
0637	2.0	2.0
0644	2.0	2.0
0645	3.0	2.0
0720	2.0	2.0
0723	2.0	2.0
0724	3.0	2.0
0811	3.0	3.0
0812	2.0	2.0
0815	3.0	2.0
0817	2.0	2.0
0819	3.0	2.0
0820	1.0	2.0
0825	2.0	2.0
0827	2.0	2.0
0829	2.0	2.0
0903	2.0	2.0
0907	2.0	2.0
0910	2.0	2.0
0915	2.0	3.0
0919	2.0	2.0
0923	2.0	2.0
0925	2.0	2.0
0926	3.0	2.0
0929	1.0	2.0
1008	1.0	2.0
1009	2.0	2.0
1019	2.0	2.0
1022	2.0	2.0
9999	1.0	2.0
KYJ0611006A	1.0	3.0
KYJ0611009B	1.0	2.0
LIB0611001B	2.0	2.0
LON0610002B	2.0	2.0
MOS0509004	2.0	2.0
PHA0111002B	2.0	2.0
PHA0111004B	2.0	2.0
PHA0111005B	2.0	2.0
PHA0111016	3.0	3.0
PHA0209026	3.0	3.0
PHA0209039	2.0	3.0
PHA0411011A	3.0	2.0
PHA0411012A	2.0	2.0
PHA0411029	2.0	2.0
PHA0411037	2.0	2.0
PHA0411043	2.0	2.0
PHA0411045	2.0	2.0
PHA0509007	2.0	2.0
PHA0509013	1.0	2.0
PHA0509020	3.0	3.0
PHA0509021	3.0	2.0
PHA0509032	3.0	2.0
PHA0509034	2.0	3.0
PHA0509036	3.0	3.0
PHA0509041	3.0	3.0
PHA0509045	2.0	2.0
PHA0510013A	2.0	3.0
PHA0510013B	2.0	2.0
PHA0510027	2.0	2.0
PHA0510035	3.0	3.0
PHA0510039	2.0	3.0
PHA0510040	3.0	2.0
PHA0510048	2.0	2.0
PHA0610007A	2.0	1.0
PHA0610016	3.0	3.0
PHA0610019B	2.0	2.0
PHA0710011	3.0	3.0
PHA0710016	3.0	2.0
PHA0710019	2.0	3.0
PHA0809009	2.0	2.0
PHA0810004	2.0	2.0
PHA0810008	2.0	3.0
PHA0811012	4.0	4.0
PHA1109001	2.0	2.0
PHA1109003	2.0	2.0
PHA1109028	2.0	3.0
PHA1110004A	2.0	2.0
PHA1110013	3.0	3.0
PHA1110017	2.0	2.0
PHA1110022	3.0	3.0
PHA1111001B	1.0	2.0
PHA1111002B	1.0	2.0
VAR0209036	3.0	3.0
VAR0909005	2.0	2.0
VAR0909008	2.0	2.0
VAR0909010	2.0	2.0
1325_1001009	5.0	4.0
1325_1001010	5.0	4.0
1325_1001013	4.0	4.0
1325_1001016	5.0	4.0
1325_1001021	5.0	4.0
1325_1001024	4.0	4.0
1325_1001028	4.0	4.0
1325_1001032	4.0	4.0
1325_1001050	4.0	4.0
1325_1001055	5.0	4.0
1325_1001058	4.0	4.0
1325_1001077	4.0	4.0
1325_1001091	4.0	4.0
1325_1001094	4.0	4.0
1325_1001095	5.0	4.0
1325_1001098	4.0	4.0
1325_1001122	4.0	4.0
1325_1001130	4.0	4.0
1325_1001142	4.0	4.0
1325_1001144	4.0	4.0
1325_1001155	4.0	4.0
1325_1001163	4.0	4.0
1325_1001168	4.0	4.0
1325_1001170	4.0	4.0
1325_9000095	4.0	4.0
1325_9000104	3.0	4.0
1325_9000137	4.0	4.0
1325_9000144	4.0	4.0
1325_9000185	4.0	4.0
1325_9000209	4.0	4.0
1325_9000211	4.0	4.0
1325_9000302	3.0	4.0
1325_9000304	3.0	4.0
1325_9000314	4.0	4.0
1325_9000315	3.0	4.0
1325_9000316	3.0	3.0
1325_9000323	4.0	4.0
1325_9000505	4.0	4.0
1325_9000601	4.0	4.0
1325_9000602	4.0	4.0
1325_9000700	4.0	4.0
1365_0100004	3.0	4.0
1365_0100011	4.0	4.0
1365_0100013	4.0	4.0
1365_0100017	4.0	4.0
1365_0100018	3.0	4.0
1365_0100023	2.0	4.0
1365_0100030	4.0	4.0
1365_0100056	4.0	4.0
1365_0100067	3.0	4.0
1365_0100070	4.0	4.0
1365_0100099	3.0	4.0
1365_0100102	4.0	4.0
1365_0100135	4.0	4.0
1365_0100136	4.0	4.0
1365_0100146	4.0	4.0
1365_0100147	4.0	4.0
1365_0100162	4.0	4.0
1365_0100172	5.0	4.0
1365_0100178	4.0	4.0
1365_0100179	4.0	4.0
1365_0100186	5.0	4.0
1365_0100195	4.0	4.0
1365_0100196	4.0	4.0
1365_0100198	4.0	4.0
1365_0100199	4.0	4.0
1365_0100212	5.0	4.0
1365_0100221	3.0	4.0
1365_0100222	3.0	4.0
1365_0100227	4.0	4.0
1365_0100233	4.0	4.0
1365_0100262	4.0	4.0
1365_0100266	3.0	4.0
1365_0100279	4.0	4.0
1365_0100290	4.0	4.0
1365_0100457	2.0	4.0
1365_0100469	5.0	4.0
1365_0100480	5.0	4.0
1385_0000017	1.0	1.0
1385_0000035	2.0	1.0
1385_0000043	2.0	2.0
1385_0000057	1.0	2.0
1385_0000058	2.0	2.0
1385_0000102	2.0	2.0
1385_0000104	2.0	2.0
1385_0001104	1.0	1.0
1385_0001105	2.0	2.0
1385_0001120	2.0	1.0
1385_0001124	1.0	2.0
1385_0001129	1.0	2.0
1385_0001130	1.0	1.0
1385_0001134	2.0	2.0
1385_0001136	2.0	1.0
1385_0001157	2.0	2.0
1385_0001169	2.0	1.0
1385_0001173	0.0	1.0
1385_0001188	2.0	2.0
1385_0001197	2.0	2.0
1385_0001712	1.0	4.0
1385_0001714	1.0	2.0
1385_0001715	1.0	2.0
1385_0001728	2.0	2.0
1385_0001730	2.0	2.0
1385_0001733	1.0	2.0
1385_0001742	0.0	1.0
1385_0001758	1.0	2.0
1385_0001759	1.0	2.0
1385_0001761	1.0	2.0
1385_0001786	2.0	2.0
1385_0001794	1.0	2.0
1395_0000341	2.0	2.0
1395_0000353	2.0	1.0
1395_0000361	4.0	4.0
1395_0000376	5.0	4.0
1395_0000378	2.0	4.0
1395_0000379	1.0	2.0
1395_0000387	5.0	4.0
1395_0000392	5.0	4.0
1395_0000399	2.0	2.0
1395_0000409	3.0	4.0
1395_0000447	2.0	2.0
1395_0000450	2.0	3.0
1395_0000499	2.0	2.0
1395_0000512	3.0	4.0
1395_0000518	4.0	4.0
1395_0000534	2.0	2.0
1395_0000535	2.0	2.0
1395_0000537	2.0	4.0
1395_0000547	3.0	4.0
1395_0000548	2.0	4.0
1395_0000553	2.0	2.0
1395_0000555	2.0	2.0
1395_0000556	2.0	2.0
1395_0000575	2.0	2.0
1395_0000585	1.0	2.0
1395_0000587	0.0	1.0
1395_0000593	0.0	3.0
1395_0000602	1.0	2.0
1395_0000604	1.0	2.0
1395_0000606	0.0	1.0
1395_0000612	0.0	2.0
1395_0000626	2.0	4.0
1395_0000627	1.0	4.0
1395_0000635	1.0	2.0
1395_0001022	2.0	2.0
1395_0001023	2.0	2.0
1395_0001028	2.0	4.0
1395_0001067	1.0	2.0
1395_0001070	2.0	4.0
1395_0001073	1.0	3.0
1395_0001090	2.0	2.0
1395_0001101	2.0	1.0
1395_0001104	1.0	2.0
1395_0001108	1.0	1.0
1395_0001120	1.0	2.0
1395_0001133	1.0	2.0
1395_0001149	1.0	2.0
2 Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 1.35
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.14
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.00      0.00      0.00        68
         2.0       0.56      0.82      0.67       162
         3.0       0.54      0.66      0.59       106
         4.0       0.56      0.56      0.56        86
         5.0       0.00      0.00      0.00        22

    accuracy                           0.56       452
   macro avg       0.28      0.34      0.30       452
weighted avg       0.43      0.56      0.48       452

[[  0   0   8   0   0   0]
 [  0   0  67   0   1   0]
 [  0   0 133  23   6   0]
 [  0   0  24  70  12   0]
 [  0   0   4  34  48   0]
 [  0   0   1   3  18   0]]
0.48487180904112687
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 1.03
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.99
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.57      0.19      0.29        68
         2.0       0.60      0.90      0.72       162
         3.0       0.66      0.58      0.62       106
         4.0       0.60      0.66      0.63        86
         5.0       0.00      0.00      0.00        22

    accuracy                           0.61       452
   macro avg       0.40      0.39      0.38       452
weighted avg       0.57      0.61      0.57       452

[[  0   5   3   0   0   0]
 [  0  13  53   0   2   0]
 [  0   5 145   8   4   0]
 [  0   0  31  62  13   0]
 [  0   0   6  23  57   0]
 [  0   0   2   1  19   0]]
0.566769887006009
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.90
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.06
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.59      0.24      0.34        68
         2.0       0.61      0.75      0.67       162
         3.0       0.59      0.51      0.55       106
         4.0       0.55      0.87      0.68        86
         5.0       0.00      0.00      0.00        22

    accuracy                           0.59       452
   macro avg       0.39      0.39      0.37       452
weighted avg       0.55      0.59      0.55       452

[[  0   5   3   0   0   0]
 [  0  16  51   0   1   0]
 [  0   6 121  29   6   0]
 [  0   0  19  54  33   0]
 [  0   0   3   8  75   0]
 [  0   0   1   0  21   0]]
0.5487279787453682
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.78
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.11
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.61      0.25      0.35        68
         2.0       0.62      0.71      0.66       162
         3.0       0.56      0.55      0.55       106
         4.0       0.54      0.85      0.66        86
         5.0       0.00      0.00      0.00        22

    accuracy                           0.58       452
   macro avg       0.39      0.39      0.37       452
weighted avg       0.55      0.58      0.55       452

[[  0   6   2   0   0   0]
 [  0  17  47   0   4   0]
 [  0   5 115  34   8   0]
 [  0   0  19  58  29   0]
 [  0   0   2  11  73   0]
 [  0   0   0   1  21   0]]
0.5460790166760093
452 452 452
Filename	True Label	Prediction
1023_0001422	2.0	2.0
1023_0101684	3.0	3.0
1023_0101752	3.0	4.0
1023_0101849	2.0	3.0
1023_0101853	3.0	4.0
1023_0101904	2.0	3.0
1023_0103822	3.0	4.0
1023_0103827	3.0	4.0
1023_0103832	3.0	3.0
1023_0103833	5.0	4.0
1023_0103843	3.0	3.0
1023_0103955	3.0	4.0
1023_0104207	3.0	4.0
1023_0107042	3.0	3.0
1023_0107729	3.0	3.0
1023_0107773	3.0	3.0
1023_0107780	4.0	4.0
1023_0107784	3.0	3.0
1023_0107788	3.0	3.0
1023_0108305	3.0	4.0
1023_0108423	3.0	2.0
1023_0108520	3.0	3.0
1023_0108812	3.0	4.0
1023_0108935	4.0	4.0
1023_0109022	3.0	4.0
1023_0109026	3.0	3.0
1023_0109033	4.0	4.0
1023_0109039	4.0	4.0
1023_0109096	4.0	4.0
1023_0109247	4.0	4.0
1023_0109250	3.0	3.0
1023_0109392	3.0	3.0
1023_0109515	4.0	3.0
1023_0109524	4.0	3.0
1023_0109614	3.0	3.0
1023_0109651	4.0	4.0
1023_0109717	3.0	4.0
1023_0109880	4.0	4.0
1023_0109917	4.0	4.0
1023_0109951	3.0	3.0
1023_0111896	3.0	3.0
1031_0001703	4.0	4.0
1031_0001998	4.0	4.0
1031_0002002	3.0	3.0
1031_0002003	3.0	3.0
1031_0002079	4.0	3.0
1031_0002187	4.0	4.0
1031_0003013	5.0	4.0
1031_0003023	3.0	4.0
1031_0003029	3.0	3.0
1031_0003053	3.0	4.0
1031_0003076	4.0	4.0
1031_0003090	4.0	3.0
1031_0003097	3.0	4.0
1031_0003098	4.0	3.0
1031_0003099	3.0	3.0
1031_0003106	3.0	3.0
1031_0003126	3.0	3.0
1031_0003127	4.0	4.0
1031_0003132	3.0	4.0
1031_0003133	4.0	4.0
1031_0003136	4.0	3.0
1031_0003165	3.0	3.0
1031_0003166	3.0	3.0
1031_0003169	3.0	3.0
1031_0003170	4.0	4.0
1031_0003174	3.0	3.0
1031_0003181	4.0	4.0
1031_0003189	4.0	4.0
1031_0003220	4.0	3.0
1031_0003236	4.0	4.0
1031_0003274	4.0	4.0
1031_0003327	2.0	3.0
1031_0003331	3.0	4.0
1031_0003339	5.0	4.0
1061_0012029	4.0	4.0
1061_0120278	2.0	3.0
1061_0120281	2.0	2.0
1061_0120283	1.0	2.0
1061_0120296	2.0	2.0
1061_0120299	3.0	3.0
1061_0120312	2.0	2.0
1061_0120313	2.0	2.0
1061_0120326	3.0	3.0
1061_0120327	3.0	3.0
1061_0120335	3.0	4.0
1061_0120336	2.0	3.0
1061_0120356	2.0	2.0
1061_0120359	2.0	2.0
1061_0120371	3.0	3.0
1061_0120373	3.0	3.0
1061_0120386	1.0	2.0
1061_0120424	3.0	3.0
1061_0120440	2.0	2.0
1061_0120448	4.0	4.0
1061_0120450	2.0	3.0
1061_0120456	2.0	3.0
1061_0120459	3.0	3.0
1061_0120460	2.0	2.0
1061_0120478	3.0	3.0
1061_0120479	3.0	3.0
1061_0120488	2.0	3.0
1061_0120492	3.0	4.0
1061_0120500	2.0	2.0
1061_0120856	1.0	2.0
1061_0120876	2.0	3.0
1061_0120878	2.0	2.0
1061_0120885	3.0	3.0
1061_0120888	2.0	3.0
1061_1029114	2.0	2.0
1061_1202918	2.0	2.0
1071_0024681	2.0	3.0
1071_0024683	1.0	1.0
1071_0024685	2.0	2.0
1071_0024702	2.0	2.0
1071_0024710	1.0	2.0
1071_0024714	2.0	2.0
1071_0024716	3.0	2.0
1071_0024759	1.0	1.0
1071_0024777	1.0	2.0
1071_0024778	0.0	1.0
1071_0024783	0.0	1.0
1071_0024784	1.0	1.0
1071_0024799	3.0	3.0
1071_0024806	1.0	1.0
1071_0024812	1.0	1.0
1071_0024814	1.0	2.0
1071_0024816	1.0	2.0
1071_0024822	1.0	2.0
1071_0024834	3.0	3.0
1071_0024836	2.0	2.0
1071_0024845	1.0	2.0
1071_0024847	2.0	2.0
1071_0024853	2.0	2.0
1071_0024857	1.0	1.0
1071_0024862	2.0	2.0
1071_0024866	5.0	3.0
1071_0024873	2.0	2.0
1071_0024881	3.0	2.0
1071_0241831	1.0	2.0
1071_0241833	2.0	2.0
1071_0242043	1.0	2.0
1071_0242072	0.0	1.0
1071_0242073	1.0	2.0
1071_0243581	1.0	1.0
1071_0243621	2.0	2.0
1071_0248308	2.0	2.0
1071_0248309	2.0	2.0
1071_0248322	2.0	2.0
1071_0248324	0.0	1.0
1071_0248326	2.0	2.0
1071_0248328	1.0	1.0
1071_0248331	2.0	2.0
1071_0248334	2.0	2.0
1071_0248335	2.0	2.0
1071_0248342	1.0	1.0
1091_0000004	1.0	2.0
1091_0000009	1.0	1.0
1091_0000010	3.0	2.0
1091_0000011	2.0	2.0
1091_0000015	2.0	3.0
1091_0000017	3.0	3.0
1091_0000020	3.0	2.0
1091_0000029	3.0	2.0
1091_0000036	3.0	3.0
1091_0000038	2.0	2.0
1091_0000039	2.0	1.0
1091_0000043	2.0	2.0
1091_0000055	2.0	3.0
1091_0000057	3.0	2.0
1091_0000063	2.0	2.0
1091_0000065	1.0	2.0
1091_0000068	2.0	2.0
1091_0000069	3.0	2.0
1091_0000086	2.0	2.0
1091_0000127	2.0	3.0
1091_0000140	2.0	1.0
1091_0000145	1.0	1.0
1091_0000148	2.0	1.0
1091_0000168	2.0	3.0
1091_0000174	2.0	1.0
1091_0000193	2.0	2.0
1091_0000196	3.0	2.0
1091_0000209	3.0	2.0
1091_0000216	2.0	3.0
1091_0000220	2.0	2.0
1091_0000223	2.0	3.0
1091_0000226	3.0	2.0
1091_0000229	2.0	2.0
1091_0000240	2.0	2.0
1091_0000246	3.0	2.0
1091_0000252	2.0	3.0
1091_0000253	2.0	1.0
1091_0000254	3.0	2.0
1091_0000255	1.0	2.0
1091_0000263	4.0	3.0
1091_0000265	3.0	2.0
1091_0000268	2.0	2.0
1091_0000269	2.0	2.0
1091_0000271	2.0	2.0
1091_0000272	1.0	2.0
0613	2.0	2.0
0619	2.0	2.0
0622	2.0	2.0
0628	2.0	2.0
0632	2.0	2.0
0640	2.0	3.0
0716	2.0	2.0
0725	2.0	2.0
0803	2.0	3.0
0808	2.0	2.0
0816	3.0	3.0
0818	2.0	3.0
0822	2.0	2.0
0901	3.0	3.0
0904	2.0	2.0
0913	2.0	3.0
0916	2.0	2.0
0921	2.0	2.0
0928	3.0	2.0
1001	2.0	2.0
1002	2.0	2.0
1003	1.0	2.0
1004	2.0	2.0
1006	2.0	3.0
1007	2.0	2.0
1015	2.0	2.0
1016	2.0	2.0
1021	2.0	3.0
1112	2.0	2.0
LIB0611002B	2.0	2.0
LON0610002A	2.0	2.0
LON0611002A	2.0	2.0
LON0611004B	1.0	2.0
MOS0611014	1.0	2.0
MOS0611015	3.0	3.0
PAR1011015	3.0	3.0
PHA0111001B	2.0	2.0
PHA0111010	3.0	3.0
PHA0111012	2.0	2.0
PHA0112002A	2.0	2.0
PHA0209001	2.0	2.0
PHA0209008	2.0	2.0
PHA0209013	2.0	2.0
PHA0209024	1.0	2.0
PHA0209031	3.0	3.0
PHA0411009B	1.0	2.0
PHA0411012B	2.0	2.0
PHA0411030	4.0	3.0
PHA0411031	3.0	3.0
PHA0411039	2.0	2.0
PHA0411044	4.0	4.0
PHA0411054	3.0	2.0
PHA0509019	2.0	3.0
PHA0509028	3.0	3.0
PHA0509030	3.0	3.0
PHA0509037	3.0	3.0
PHA0509039	3.0	3.0
PHA0510002B	2.0	2.0
PHA0510034	3.0	3.0
PHA0510036	3.0	3.0
PHA0510046	2.0	3.0
PHA0510047	2.0	2.0
PHA0510050	3.0	3.0
PHA0610006A	1.0	2.0
PHA0610025	3.0	3.0
PHA0709008	2.0	3.0
PHA0710009	2.0	2.0
PHA0710013	3.0	4.0
PHA0710015	3.0	2.0
PHA0710021	3.0	3.0
PHA0810002	2.0	2.0
PHA0810015	4.0	4.0
PHA0811014	2.0	2.0
PHA0811020	2.0	3.0
PHA1109002	3.0	3.0
PHA1109024	3.0	3.0
PHA1110001A	2.0	2.0
PHA1110003A	2.0	2.0
PHA1110021	2.0	2.0
PHA1111006B	1.0	2.0
VAR0909007	2.0	3.0
VAR0909009	2.0	3.0
VAR0910004	2.0	3.0
VAR0910005	1.0	2.0
VAR0910006	2.0	2.0
VAR0910007	2.0	3.0
VAR0910009	2.0	3.0
1325_1001008	5.0	4.0
1325_1001019	4.0	4.0
1325_1001022	4.0	4.0
1325_1001023	4.0	4.0
1325_1001040	4.0	4.0
1325_1001044	3.0	4.0
1325_1001045	5.0	4.0
1325_1001046	4.0	4.0
1325_1001062	4.0	4.0
1325_1001075	3.0	3.0
1325_1001081	4.0	4.0
1325_1001083	5.0	4.0
1325_1001092	4.0	4.0
1325_1001097	1.0	4.0
1325_1001108	4.0	4.0
1325_1001124	3.0	4.0
1325_1001127	4.0	4.0
1325_1001128	4.0	4.0
1325_1001131	3.0	4.0
1325_1001135	3.0	4.0
1325_1001136	3.0	4.0
1325_1001138	4.0	4.0
1325_1001139	4.0	4.0
1325_1001153	4.0	4.0
1325_1001159	5.0	4.0
1325_1001162	4.0	4.0
1325_9000059	5.0	4.0
1325_9000087	5.0	4.0
1325_9000099	5.0	4.0
1325_9000105	4.0	3.0
1325_9000138	4.0	4.0
1325_9000140	4.0	4.0
1325_9000213	4.0	4.0
1325_9000214	4.0	4.0
1325_9000240	2.0	3.0
1325_9000318	4.0	4.0
1325_9000321	4.0	4.0
1325_9000504	4.0	4.0
1325_9000612	2.0	4.0
1325_9000677	4.0	4.0
1325_9000750	4.0	4.0
1365_0100002	4.0	4.0
1365_0100006	4.0	4.0
1365_0100015	4.0	4.0
1365_0100027	4.0	4.0
1365_0100028	4.0	4.0
1365_0100051	2.0	2.0
1365_0100061	5.0	4.0
1365_0100069	4.0	4.0
1365_0100071	4.0	4.0
1365_0100073	3.0	4.0
1365_0100079	5.0	4.0
1365_0100080	4.0	4.0
1365_0100094	5.0	4.0
1365_0100096	4.0	4.0
1365_0100100	4.0	4.0
1365_0100105	5.0	4.0
1365_0100116	4.0	4.0
1365_0100117	5.0	4.0
1365_0100125	4.0	4.0
1365_0100137	5.0	4.0
1365_0100145	5.0	4.0
1365_0100166	3.0	4.0
1365_0100167	2.0	4.0
1365_0100170	4.0	4.0
1365_0100177	5.0	4.0
1365_0100180	5.0	4.0
1365_0100181	3.0	4.0
1365_0100184	4.0	4.0
1365_0100203	4.0	4.0
1365_0100213	4.0	4.0
1365_0100215	4.0	4.0
1365_0100219	4.0	4.0
1365_0100220	4.0	4.0
1365_0100252	4.0	4.0
1365_0100255	3.0	4.0
1365_0100257	3.0	4.0
1365_0100269	4.0	4.0
1365_0100274	4.0	4.0
1365_0100281	5.0	4.0
1365_0100286	3.0	4.0
1365_0100447	5.0	4.0
1365_0100448	2.0	4.0
1365_0100455	2.0	4.0
1365_0100475	4.0	4.0
1365_0100481	4.0	4.0
1365_0100482	4.0	3.0
1385_0000013	1.0	1.0
1385_0000021	1.0	2.0
1385_0000023	2.0	2.0
1385_0000034	2.0	2.0
1385_0000097	2.0	2.0
1385_0000128	1.0	2.0
1385_0000130	2.0	2.0
1385_0001109	2.0	2.0
1385_0001122	2.0	2.0
1385_0001126	1.0	1.0
1385_0001131	2.0	2.0
1385_0001135	2.0	2.0
1385_0001147	2.0	2.0
1385_0001149	2.0	2.0
1385_0001154	2.0	2.0
1385_0001160	1.0	4.0
1385_0001172	1.0	1.0
1385_0001174	1.0	2.0
1385_0001190	1.0	2.0
1385_0001191	2.0	2.0
1385_0001194	2.0	2.0
1385_0001195	2.0	4.0
1385_0001196	2.0	2.0
1385_0001503	2.0	2.0
1385_0001522	1.0	1.0
1385_0001528	2.0	2.0
1385_0001718	1.0	1.0
1385_0001719	1.0	2.0
1385_0001724	2.0	2.0
1385_0001727	1.0	2.0
1385_0001734	2.0	2.0
1385_0001736	2.0	4.0
1385_0001741	1.0	2.0
1385_0001746	1.0	2.0
1385_0001747	0.0	2.0
1385_0001748	1.0	4.0
1385_0001762	2.0	2.0
1385_0001767	1.0	2.0
1385_0001768	2.0	2.0
1385_0001789	2.0	2.0
1385_0001793	1.0	2.0
1385_0001795	1.0	1.0
1385_0001796	1.0	2.0
1385_0001799	2.0	2.0
1385_0001800	1.0	2.0
1395_0000333	2.0	2.0
1395_0000360	3.0	2.0
1395_0000365	3.0	2.0
1395_0000369	4.0	2.0
1395_0000451	2.0	2.0
1395_0000462	4.0	2.0
1395_0000504	2.0	2.0
1395_0000513	4.0	4.0
1395_0000526	2.0	2.0
1395_0000527	1.0	2.0
1395_0000582	0.0	1.0
1395_0000583	1.0	2.0
1395_0000595	0.0	1.0
1395_0000611	1.0	2.0
1395_0000630	1.0	2.0
1395_0000636	1.0	2.0
1395_0001015	2.0	2.0
1395_0001017	2.0	2.0
1395_0001020	1.0	2.0
1395_0001024	1.0	2.0
1395_0001033	2.0	4.0
1395_0001069	2.0	2.0
1395_0001074	1.0	2.0
1395_0001080	2.0	2.0
1395_0001093	2.0	2.0
1395_0001103	1.0	4.0
1395_0001109	0.0	2.0
1395_0001116	2.0	2.0
1395_0001117	1.0	2.0
1395_0001118	1.0	2.0
1395_0001164	2.0	4.0
1395_0001171	1.0	2.0
3 Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 1.36
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.07
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.39      0.24      0.29        68
         2.0       0.57      0.70      0.63       162
         3.0       0.54      0.57      0.55       105
         4.0       0.58      0.69      0.63        86
         5.0       0.00      0.00      0.00        23

    accuracy                           0.55       452
   macro avg       0.35      0.37      0.35       452
weighted avg       0.50      0.55      0.52       452

[[  0   4   4   0   0   0]
 [  0  16  50   1   1   0]
 [  0  21 113  25   3   0]
 [  0   0  24  60  21   0]
 [  0   0   4  23  59   0]
 [  0   0   2   3  18   0]]
0.5176766132603079
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 1.08
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.98
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.57      0.24      0.33        68
         2.0       0.64      0.83      0.72       162
         3.0       0.66      0.42      0.51       105
         4.0       0.52      0.88      0.66        86
         5.0       0.00      0.00      0.00        23

    accuracy                           0.60       452
   macro avg       0.40      0.40      0.37       452
weighted avg       0.57      0.60      0.55       452

[[  0   7   1   0   0   0]
 [  0  16  50   0   2   0]
 [  0   5 135  15   7   0]
 [  0   0  22  44  39   0]
 [  0   0   3   7  76   0]
 [  0   0   1   1  21   0]]
0.5529385246586801
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 48
Elapsed time 59

  Average training loss: 0.96
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.99
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.53      0.28      0.37        68
         2.0       0.64      0.80      0.71       162
         3.0       0.61      0.45      0.52       105
         4.0       0.55      0.87      0.68        86
         5.0       0.00      0.00      0.00        23

    accuracy                           0.60       452
   macro avg       0.39      0.40      0.38       452
weighted avg       0.56      0.60      0.56       452

[[  0   7   0   1   0   0]
 [  0  19  47   1   1   0]
 [  0   9 130  20   3   0]
 [  0   1  20  47  37   0]
 [  0   0   4   7  75   0]
 [  0   0   2   1  20   0]]
0.5588103793340792
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.86
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.01
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.59      0.34      0.43        68
         2.0       0.66      0.78      0.72       162
         3.0       0.60      0.40      0.48       105
         4.0       0.52      0.91      0.66        86
         5.0       0.00      0.00      0.00        23

    accuracy                           0.60       452
   macro avg       0.39      0.40      0.38       452
weighted avg       0.56      0.60      0.56       452

[[  0   7   0   1   0   0]
 [  0  23  43   1   1   0]
 [  0   8 127  21   6   0]
 [  0   1  17  42  45   0]
 [  0   0   3   5  78   0]
 [  0   0   2   0  21   0]]
0.5585808140091044
452 452 452
Filename	True Label	Prediction
1023_0001423	4.0	3.0
1023_0101693	3.0	4.0
1023_0101694	3.0	4.0
1023_0101749	4.0	4.0
1023_0101843	4.0	4.0
1023_0101844	3.0	3.0
1023_0101845	3.0	3.0
1023_0101900	3.0	4.0
1023_0102117	4.0	4.0
1023_0103823	5.0	4.0
1023_0103834	3.0	4.0
1023_0103844	4.0	4.0
1023_0104206	3.0	3.0
1023_0104209	3.0	3.0
1023_0107726	3.0	4.0
1023_0107740	3.0	4.0
1023_0107781	4.0	4.0
1023_0107783	3.0	3.0
1023_0108518	4.0	4.0
1023_0108641	5.0	4.0
1023_0108766	3.0	4.0
1023_0108934	4.0	4.0
1023_0108958	3.0	4.0
1023_0108993	4.0	4.0
1023_0109192	4.0	4.0
1023_0109248	3.0	4.0
1023_0109267	3.0	3.0
1023_0109396	3.0	4.0
1023_0109400	3.0	4.0
1023_0109518	3.0	3.0
1023_0109519	4.0	3.0
1023_0109522	3.0	4.0
1023_0109588	3.0	4.0
1023_0109591	3.0	4.0
1023_0109609	3.0	3.0
1023_0109716	5.0	4.0
1023_0109890	4.0	4.0
1023_0109954	3.0	4.0
1031_0002005	3.0	3.0
1031_0002006	4.0	4.0
1031_0002010	4.0	4.0
1031_0002040	5.0	4.0
1031_0002061	4.0	4.0
1031_0002131	3.0	4.0
1031_0002184	4.0	4.0
1031_0002185	4.0	4.0
1031_0002196	4.0	4.0
1031_0003063	4.0	4.0
1031_0003071	3.0	4.0
1031_0003077	3.0	3.0
1031_0003091	2.0	3.0
1031_0003140	4.0	4.0
1031_0003141	3.0	4.0
1031_0003149	4.0	4.0
1031_0003155	3.0	4.0
1031_0003173	3.0	4.0
1031_0003179	4.0	4.0
1031_0003205	4.0	4.0
1031_0003211	3.0	4.0
1031_0003224	3.0	4.0
1031_0003232	3.0	4.0
1031_0003240	3.0	4.0
1031_0003242	3.0	4.0
1031_0003246	3.0	3.0
1031_0003272	3.0	3.0
1031_0003314	3.0	4.0
1031_0003330	3.0	4.0
1031_0003337	3.0	4.0
1031_0003358	5.0	4.0
1031_0003359	2.0	4.0
1031_0003369	4.0	4.0
1031_0003383	3.0	3.0
1031_0003415	4.0	4.0
1031_0003419	4.0	4.0
1061_0120276	3.0	3.0
1061_0120280	2.0	2.0
1061_0120285	2.0	2.0
1061_0120295	0.0	3.0
1061_0120301	2.0	3.0
1061_0120310	2.0	2.0
1061_0120311	3.0	3.0
1061_0120330	4.0	3.0
1061_0120354	1.0	2.0
1061_0120355	2.0	2.0
1061_0120360	3.0	4.0
1061_0120366	3.0	3.0
1061_0120367	3.0	3.0
1061_0120372	3.0	3.0
1061_0120376	2.0	3.0
1061_0120388	2.0	3.0
1061_0120408	3.0	3.0
1061_0120411	4.0	4.0
1061_0120423	3.0	3.0
1061_0120426	2.0	3.0
1061_0120429	2.0	3.0
1061_0120430	2.0	3.0
1061_0120438	4.0	3.0
1061_0120453	3.0	3.0
1061_0120455	3.0	3.0
1061_0120484	3.0	4.0
1061_0120486	2.0	3.0
1061_0120495	3.0	3.0
1061_0120496	2.0	2.0
1061_0120859	3.0	3.0
1061_1029111	3.0	3.0
1061_1029117	2.0	3.0
1061_1029118	2.0	2.0
1061_1029120	1.0	2.0
1061_1202910	3.0	3.0
1061_1202911	1.0	2.0
1061_1202914	2.0	2.0
1061_1202916	2.0	2.0
1061_1202917	2.0	2.0
1071_0024682	3.0	3.0
1071_0024689	1.0	2.0
1071_0024690	2.0	3.0
1071_0024699	1.0	2.0
1071_0024705	2.0	2.0
1071_0024757	2.0	2.0
1071_0024761	2.0	2.0
1071_0024762	1.0	1.0
1071_0024763	1.0	2.0
1071_0024766	1.0	2.0
1071_0024767	2.0	2.0
1071_0024768	1.0	2.0
1071_0024772	1.0	1.0
1071_0024774	0.0	1.0
1071_0024775	0.0	1.0
1071_0024782	0.0	1.0
1071_0024797	1.0	2.0
1071_0024807	1.0	1.0
1071_0024811	1.0	2.0
1071_0024813	1.0	1.0
1071_0024818	3.0	2.0
1071_0024820	1.0	2.0
1071_0024841	1.0	1.0
1071_0024846	0.0	1.0
1071_0024856	1.0	1.0
1071_0024861	1.0	2.0
1071_0024874	2.0	2.0
1071_0024876	2.0	2.0
1071_0241832	1.0	2.0
1071_0242021	2.0	2.0
1071_0242022	1.0	2.0
1071_0243582	1.0	1.0
1071_0243622	1.0	2.0
1071_0248301	2.0	2.0
1071_0248304	1.0	1.0
1071_0248311	2.0	2.0
1071_0248321	3.0	2.0
1071_0248327	0.0	1.0
1071_0248330	2.0	2.0
1071_0248333	2.0	2.0
1071_0248336	2.0	2.0
1071_0248337	2.0	2.0
1071_0248339	2.0	2.0
1071_0248345	2.0	2.0
1071_0248346	1.0	1.0
1071_0248348	2.0	2.0
1091_0000001	2.0	2.0
1091_0000007	3.0	3.0
1091_0000008	3.0	3.0
1091_0000019	2.0	2.0
1091_0000025	1.0	2.0
1091_0000027	1.0	2.0
1091_0000028	1.0	2.0
1091_0000030	1.0	3.0
1091_0000045	2.0	3.0
1091_0000052	1.0	1.0
1091_0000053	1.0	2.0
1091_0000054	1.0	2.0
1091_0000062	3.0	2.0
1091_0000066	1.0	1.0
1091_0000078	3.0	2.0
1091_0000092	2.0	2.0
1091_0000144	2.0	1.0
1091_0000154	2.0	2.0
1091_0000162	2.0	3.0
1091_0000164	2.0	2.0
1091_0000192	2.0	3.0
1091_0000194	2.0	2.0
1091_0000195	2.0	2.0
1091_0000202	2.0	3.0
1091_0000206	1.0	2.0
1091_0000211	2.0	2.0
1091_0000212	2.0	3.0
1091_0000219	2.0	2.0
1091_0000227	1.0	2.0
1091_0000228	2.0	2.0
1091_0000233	2.0	2.0
1091_0000235	2.0	2.0
1091_0000237	2.0	2.0
1091_0000239	3.0	2.0
1091_0000241	2.0	2.0
1091_0000250	2.0	2.0
1091_0000256	2.0	2.0
1091_0000259	2.0	3.0
1091_0000261	2.0	2.0
1091_0000266	2.0	2.0
1091_0000267	2.0	2.0
1091_0000275	2.0	2.0
1091_0000276	2.0	2.0
0609	2.0	2.0
0611	2.0	2.0
0620	2.0	2.0
0623	2.0	2.0
0627	2.0	2.0
0629	2.0	2.0
0630	2.0	2.0
0631	2.0	2.0
0639	2.0	2.0
0802	2.0	2.0
0807	3.0	2.0
0823	3.0	2.0
0905	2.0	2.0
0906	3.0	2.0
0911	2.0	2.0
0914	2.0	2.0
1005	2.0	2.0
1014	3.0	2.0
1111	2.0	2.0
1116	2.0	2.0
BER0609003	2.0	2.0
BER0611005	3.0	2.0
BER0611007	1.0	2.0
KYJ0611005A	1.0	2.0
KYJ0611006B	1.0	2.0
KYJ0611009A	2.0	2.0
LIB0611002A	2.0	2.0
LIB0611004B	2.0	2.0
LON0611002B	1.0	2.0
LON0611003	3.0	3.0
MOS0611013	3.0	3.0
PAR1011009B	2.0	2.0
PAR1011018	2.0	2.0
PHA0111014	1.0	2.0
PHA0111018	2.0	2.0
PHA0112003B	1.0	2.0
PHA0112007A	2.0	2.0
PHA0112009B	2.0	2.0
PHA0112012A	2.0	2.0
PHA0209028	3.0	2.0
PHA0209034	2.0	3.0
PHA0210001	2.0	2.0
PHA0411008B	2.0	2.0
PHA0411009A	2.0	2.0
PHA0411033	2.0	2.0
PHA0411034	1.0	1.0
PHA0411038	3.0	3.0
PHA0411041	2.0	3.0
PHA0411042	3.0	3.0
PHA0411047	2.0	3.0
PHA0411051	3.0	3.0
PHA0509002	1.0	1.0
PHA0509022	3.0	3.0
PHA0509025	4.0	3.0
PHA0509031	2.0	3.0
PHA0509044	2.0	2.0
PHA0510002A	2.0	2.0
PHA0510030	3.0	3.0
PHA0510031	2.0	2.0
PHA0610007B	2.0	2.0
PHA0610015	3.0	3.0
PHA0610019A	1.0	2.0
PHA0710012	3.0	3.0
PHA0810006	2.0	2.0
PHA0810012	2.0	2.0
PHA0811013	3.0	3.0
PHA0811016	2.0	2.0
PHA0811017	3.0	3.0
PHA1109005	2.0	2.0
PHA1109007	2.0	2.0
PHA1110002A	2.0	2.0
PHA1110002B	2.0	2.0
PHA1110003B	2.0	2.0
PHA1110015	3.0	3.0
PHA1111002A	2.0	2.0
PHA1111003A	3.0	2.0
PHA1111003B	2.0	2.0
PHA1111004A	1.0	2.0
PHA1111006A	2.0	2.0
TI071122B	2.0	2.0
VAR0909003	2.0	2.0
VAR0909006	3.0	3.0
1325_1001025	4.0	4.0
1325_1001027	4.0	4.0
1325_1001033	5.0	4.0
1325_1001035	5.0	4.0
1325_1001037	5.0	4.0
1325_1001041	4.0	4.0
1325_1001048	4.0	4.0
1325_1001051	4.0	4.0
1325_1001056	4.0	4.0
1325_1001063	4.0	4.0
1325_1001082	5.0	4.0
1325_1001084	4.0	4.0
1325_1001087	4.0	4.0
1325_1001090	4.0	4.0
1325_1001099	4.0	4.0
1325_1001109	4.0	4.0
1325_1001113	4.0	4.0
1325_1001119	4.0	4.0
1325_1001120	5.0	4.0
1325_1001123	4.0	4.0
1325_1001134	4.0	4.0
1325_1001141	2.0	4.0
1325_1001156	4.0	4.0
1325_1001157	4.0	4.0
1325_1001160	4.0	4.0
1325_1001165	4.0	4.0
1325_1001167	4.0	4.0
1325_1001169	4.0	4.0
1325_9000088	4.0	4.0
1325_9000102	3.0	4.0
1325_9000136	4.0	4.0
1325_9000143	4.0	4.0
1325_9000187	4.0	4.0
1325_9000215	4.0	4.0
1325_9000303	4.0	4.0
1325_9000320	3.0	4.0
1325_9000554	3.0	4.0
1325_9000611	3.0	4.0
1325_9000674	4.0	4.0
1325_9000685	4.0	4.0
1365_0100005	3.0	4.0
1365_0100014	4.0	4.0
1365_0100022	4.0	4.0
1365_0100024	4.0	4.0
1365_0100026	2.0	4.0
1365_0100058	4.0	4.0
1365_0100072	4.0	4.0
1365_0100092	4.0	4.0
1365_0100097	4.0	4.0
1365_0100101	4.0	4.0
1365_0100106	3.0	4.0
1365_0100121	4.0	4.0
1365_0100148	4.0	4.0
1365_0100151	4.0	4.0
1365_0100163	5.0	4.0
1365_0100165	4.0	4.0
1365_0100169	5.0	4.0
1365_0100173	5.0	4.0
1365_0100176	4.0	4.0
1365_0100185	3.0	4.0
1365_0100188	5.0	4.0
1365_0100211	4.0	4.0
1365_0100217	5.0	4.0
1365_0100218	4.0	4.0
1365_0100224	3.0	4.0
1365_0100226	5.0	4.0
1365_0100228	3.0	4.0
1365_0100229	5.0	4.0
1365_0100232	4.0	4.0
1365_0100251	3.0	4.0
1365_0100261	5.0	4.0
1365_0100268	3.0	4.0
1365_0100270	3.0	4.0
1365_0100275	4.0	4.0
1365_0100276	4.0	4.0
1365_0100287	4.0	4.0
1365_0100288	4.0	4.0
1365_0100289	5.0	4.0
1365_0100299	5.0	4.0
1365_0100458	4.0	4.0
1365_0100461	2.0	4.0
1365_0100472	3.0	4.0
1365_0100479	4.0	4.0
1385_0000011	1.0	1.0
1385_0000020	2.0	2.0
1385_0000038	2.0	2.0
1385_0000042	2.0	2.0
1385_0000044	2.0	2.0
1385_0000048	2.0	2.0
1385_0000049	2.0	2.0
1385_0000050	2.0	2.0
1385_0000099	1.0	2.0
1385_0000100	2.0	1.0
1385_0000101	1.0	1.0
1385_0000103	2.0	1.0
1385_0000127	2.0	2.0
1385_0001112	2.0	2.0
1385_0001113	1.0	2.0
1385_0001121	2.0	2.0
1385_0001123	2.0	2.0
1385_0001138	2.0	2.0
1385_0001148	2.0	2.0
1385_0001151	2.0	2.0
1385_0001161	2.0	2.0
1385_0001163	1.0	2.0
1385_0001166	2.0	2.0
1385_0001189	1.0	1.0
1385_0001192	1.0	2.0
1385_0001198	2.0	2.0
1385_0001523	2.0	2.0
1385_0001525	2.0	2.0
1385_0001720	1.0	2.0
1385_0001726	1.0	2.0
1385_0001740	2.0	2.0
1385_0001749	1.0	2.0
1385_0001757	2.0	2.0
1385_0001766	2.0	3.0
1395_0000354	1.0	1.0
1395_0000356	2.0	1.0
1395_0000357	3.0	4.0
1395_0000359	3.0	2.0
1395_0000364	2.0	2.0
1395_0000366	3.0	2.0
1395_0000368	1.0	1.0
1395_0000383	5.0	2.0
1395_0000388	4.0	2.0
1395_0000389	1.0	1.0
1395_0000390	1.0	1.0
1395_0000391	3.0	4.0
1395_0000414	2.0	2.0
1395_0000438	5.0	4.0
1395_0000443	5.0	2.0
1395_0000449	4.0	2.0
1395_0000452	2.0	1.0
1395_0000455	2.0	2.0
1395_0000465	2.0	1.0
1395_0000469	2.0	1.0
1395_0000471	2.0	2.0
1395_0000515	4.0	2.0
1395_0000516	1.0	1.0
1395_0000533	4.0	4.0
1395_0000550	2.0	2.0
1395_0000551	3.0	2.0
1395_0000552	3.0	2.0
1395_0000554	2.0	2.0
1395_0000560	3.0	2.0
1395_0000564	2.0	2.0
1395_0000565	2.0	2.0
1395_0000584	0.0	1.0
1395_0000596	3.0	1.0
1395_0000610	2.0	2.0
1395_0000628	1.0	1.0
1395_0000631	1.0	2.0
1395_0000644	2.0	4.0
1395_0000646	2.0	1.0
1395_0001013	2.0	2.0
1395_0001016	2.0	2.0
1395_0001058	2.0	2.0
1395_0001066	1.0	2.0
1395_0001071	2.0	4.0
1395_0001075	1.0	2.0
1395_0001114	1.0	1.0
1395_0001122	1.0	2.0
1395_0001123	1.0	2.0
1395_0001146	0.0	1.0
1395_0001150	1.0	2.0
1395_0001158	2.0	2.0
1395_0001170	1.0	4.0
4 Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 1.36
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.12
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.47      0.29      0.36        68
         2.0       0.61      0.78      0.69       162
         3.0       0.54      0.58      0.55       106
         4.0       0.62      0.64      0.63        85
         5.0       0.00      0.00      0.00        23

    accuracy                           0.58       452
   macro avg       0.37      0.38      0.37       452
weighted avg       0.53      0.58      0.55       452

[[  0   6   2   0   0   0]
 [  0  20  46   0   2   0]
 [  0  11 127  16   8   0]
 [  0   5  28  61  12   0]
 [  0   1   2  28  54   0]
 [  0   0   3   9  11   0]]
0.5483832445141357
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 1.04
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.07
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.53      0.13      0.21        68
         2.0       0.60      0.72      0.65       162
         3.0       0.55      0.64      0.59       106
         4.0       0.60      0.84      0.70        85
         5.0       0.00      0.00      0.00        23

    accuracy                           0.58       452
   macro avg       0.38      0.39      0.36       452
weighted avg       0.54      0.58      0.54       452

[[  0   7   1   0   0   0]
 [  0   9  56   2   1   0]
 [  0   1 116  40   5   0]
 [  0   0  14  68  24   0]
 [  0   0   3  11  71   0]
 [  0   0   3   3  17   0]]
0.5362982554946775
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.94
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.49      0.46      0.47        68
         2.0       0.65      0.64      0.65       162
         3.0       0.59      0.53      0.56       106
         4.0       0.56      0.88      0.68        85
         5.0       0.00      0.00      0.00        23

    accuracy                           0.59       452
   macro avg       0.38      0.42      0.39       452
weighted avg       0.55      0.59      0.56       452

[[  0   7   1   0   0   0]
 [  0  31  34   1   2   0]
 [  0  22 104  26  10   0]
 [  0   2  18  56  30   0]
 [  0   1   1   8  75   0]
 [  0   0   2   4  17   0]]
0.5621969547320723
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.80
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.07
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.63      0.38      0.48        68
         2.0       0.66      0.70      0.68       162
         3.0       0.55      0.50      0.52       106
         4.0       0.53      0.91      0.67        85
         5.0       0.00      0.00      0.00        23

    accuracy                           0.60       452
   macro avg       0.40      0.41      0.39       452
weighted avg       0.56      0.60      0.56       452

[[  0   7   1   0   0   0]
 [  0  26  38   2   2   0]
 [  0   6 113  31  12   0]
 [  0   1  15  53  37   0]
 [  0   1   1   6  77   0]
 [  0   0   2   4  17   0]]
0.5647218611962621
452 452 452
Filename	True Label	Prediction
1023_0001418	3.0	3.0
1023_0001575	5.0	3.0
1023_0101675	4.0	3.0
1023_0101751	3.0	4.0
1023_0101753	4.0	4.0
1023_0101848	3.0	3.0
1023_0101854	2.0	3.0
1023_0101856	2.0	3.0
1023_0101895	3.0	4.0
1023_0101897	3.0	3.0
1023_0102118	3.0	3.0
1023_0103825	4.0	4.0
1023_0103826	4.0	4.0
1023_0103828	2.0	2.0
1023_0103831	4.0	4.0
1023_0103836	5.0	4.0
1023_0103837	5.0	3.0
1023_0103880	4.0	4.0
1023_0103883	3.0	4.0
1023_0106816	5.0	4.0
1023_0107075	2.0	3.0
1023_0107682	2.0	2.0
1023_0107727	5.0	4.0
1023_0107787	3.0	4.0
1023_0108307	5.0	3.0
1023_0108422	4.0	4.0
1023_0108510	4.0	4.0
1023_0108650	3.0	4.0
1023_0108753	4.0	3.0
1023_0108887	3.0	3.0
1023_0108932	3.0	4.0
1023_0109151	2.0	3.0
1023_0109391	4.0	3.0
1023_0109395	3.0	3.0
1023_0109500	4.0	4.0
1023_0109505	4.0	4.0
1023_0109520	3.0	3.0
1023_0109528	3.0	4.0
1023_0109590	3.0	3.0
1023_0109674	3.0	4.0
1023_0109878	3.0	3.0
1023_0109947	3.0	4.0
1031_0001949	4.0	4.0
1031_0001950	4.0	4.0
1031_0001951	3.0	4.0
1031_0002004	3.0	3.0
1031_0002036	5.0	4.0
1031_0002042	4.0	4.0
1031_0002088	3.0	4.0
1031_0002092	4.0	4.0
1031_0002197	3.0	4.0
1031_0003052	4.0	4.0
1031_0003065	3.0	4.0
1031_0003078	3.0	4.0
1031_0003092	2.0	3.0
1031_0003129	3.0	3.0
1031_0003135	4.0	4.0
1031_0003146	4.0	4.0
1031_0003150	4.0	4.0
1031_0003160	3.0	4.0
1031_0003161	4.0	4.0
1031_0003164	4.0	3.0
1031_0003167	3.0	4.0
1031_0003183	4.0	4.0
1031_0003186	5.0	4.0
1031_0003190	3.0	4.0
1031_0003206	3.0	4.0
1031_0003212	2.0	4.0
1031_0003217	4.0	3.0
1031_0003219	3.0	4.0
1031_0003231	3.0	4.0
1031_0003233	3.0	3.0
1031_0003243	3.0	4.0
1031_0003244	4.0	4.0
1031_0003261	3.0	3.0
1031_0003309	3.0	3.0
1031_0003336	5.0	4.0
1031_0003357	3.0	4.0
1031_0003387	3.0	3.0
1031_0003409	4.0	4.0
1061_0120271	3.0	3.0
1061_0120277	2.0	2.0
1061_0120282	1.0	1.0
1061_0120284	1.0	1.0
1061_0120288	3.0	3.0
1061_0120290	2.0	3.0
1061_0120298	2.0	3.0
1061_0120304	2.0	2.0
1061_0120308	3.0	4.0
1061_0120314	3.0	3.0
1061_0120316	2.0	3.0
1061_0120317	3.0	4.0
1061_0120320	3.0	3.0
1061_0120321	2.0	2.0
1061_0120328	2.0	2.0
1061_0120332	2.0	2.0
1061_0120337	3.0	3.0
1061_0120350	4.0	4.0
1061_0120351	2.0	3.0
1061_0120357	3.0	3.0
1061_0120358	2.0	2.0
1061_0120361	3.0	4.0
1061_0120375	2.0	1.0
1061_0120389	3.0	3.0
1061_0120390	2.0	3.0
1061_0120394	3.0	4.0
1061_0120403	4.0	3.0
1061_0120406	3.0	3.0
1061_0120413	2.0	2.0
1061_0120432	3.0	3.0
1061_0120442	2.0	4.0
1061_0120443	1.0	2.0
1061_0120449	4.0	4.0
1061_0120480	3.0	3.0
1061_0120482	2.0	3.0
1061_0120498	3.0	4.0
1061_0120858	2.0	3.0
1061_0120877	3.0	3.0
1061_0120881	3.0	4.0
1061_0120886	2.0	2.0
1061_0120887	2.0	2.0
1061_1202913	2.0	2.0
1071_0024678	2.0	1.0
1071_0024691	2.0	3.0
1071_0024693	2.0	2.0
1071_0024708	2.0	2.0
1071_0024711	1.0	1.0
1071_0024713	2.0	2.0
1071_0024756	2.0	2.0
1071_0024769	1.0	2.0
1071_0024773	1.0	1.0
1071_0024808	1.0	2.0
1071_0024817	1.0	1.0
1071_0024819	1.0	2.0
1071_0024821	1.0	1.0
1071_0024823	2.0	2.0
1071_0024827	2.0	1.0
1071_0024837	1.0	1.0
1071_0024838	1.0	1.0
1071_0024843	1.0	2.0
1071_0024850	1.0	2.0
1071_0024859	2.0	2.0
1071_0024865	2.0	3.0
1071_0024875	3.0	2.0
1071_0024877	2.0	2.0
1071_0024879	3.0	2.0
1071_0242011	2.0	3.0
1071_0242013	2.0	2.0
1071_0242091	1.0	1.0
1071_0242093	0.0	1.0
1071_0248305	0.0	1.0
1071_0248307	3.0	2.0
1071_0248312	1.0	2.0
1071_0248315	0.0	1.0
1071_0248317	1.0	1.0
1071_0248325	1.0	2.0
1071_0248329	2.0	2.0
1071_0248338	2.0	2.0
1071_0248340	1.0	1.0
1071_0248341	1.0	1.0
1071_0248349	1.0	1.0
1091_0000003	2.0	2.0
1091_0000006	1.0	1.0
1091_0000018	3.0	3.0
1091_0000023	4.0	1.0
1091_0000031	2.0	2.0
1091_0000032	2.0	4.0
1091_0000041	1.0	2.0
1091_0000044	1.0	3.0
1091_0000046	2.0	2.0
1091_0000047	3.0	2.0
1091_0000048	1.0	1.0
1091_0000050	1.0	2.0
1091_0000058	3.0	3.0
1091_0000060	3.0	3.0
1091_0000064	2.0	2.0
1091_0000071	2.0	2.0
1091_0000072	1.0	3.0
1091_0000074	2.0	3.0
1091_0000077	3.0	1.0
1091_0000087	3.0	2.0
1091_0000116	3.0	2.0
1091_0000126	3.0	3.0
1091_0000146	1.0	1.0
1091_0000151	1.0	1.0
1091_0000156	2.0	3.0
1091_0000160	3.0	3.0
1091_0000163	2.0	1.0
1091_0000167	2.0	3.0
1091_0000170	3.0	2.0
1091_0000201	3.0	3.0
1091_0000207	2.0	4.0
1091_0000224	2.0	1.0
1091_0000230	2.0	2.0
1091_0000231	3.0	3.0
1091_0000234	2.0	3.0
1091_0000245	2.0	2.0
1091_0000260	2.0	3.0
1091_0000264	2.0	2.0
1091_0000270	2.0	2.0
0601	2.0	2.0
0605	2.0	2.0
0607	3.0	3.0
0608	1.0	2.0
0610	2.0	3.0
0614	2.0	2.0
0616	2.0	2.0
0625	2.0	2.0
0636	3.0	2.0
0638	2.0	2.0
0714	2.0	2.0
0717	2.0	2.0
0719	2.0	2.0
0722	2.0	2.0
0801	2.0	2.0
0804	2.0	2.0
0806	2.0	3.0
0810	2.0	3.0
0813	2.0	2.0
0814	2.0	2.0
0824	2.0	2.0
0826	2.0	2.0
0828	2.0	2.0
0912	3.0	2.0
0918	2.0	3.0
0920	3.0	2.0
0924	2.0	2.0
0927	3.0	2.0
1017	2.0	2.0
1018	2.0	2.0
1020	2.0	2.0
1113	2.0	3.0
1115	2.0	2.0
1117	2.0	2.0
BER0611003	3.0	3.0
KYJ0611003A	2.0	2.0
KYJ0611004A	2.0	2.0
LIB0611001A	2.0	2.0
LIB0611003A	1.0	2.0
MOS0611012	3.0	3.0
PAR1011009A	2.0	3.0
PAR1011017	3.0	3.0
PHA0111001A	2.0	2.0
PHA0111002A	3.0	2.0
PHA0111003A	1.0	2.0
PHA0111004A	3.0	2.0
PHA0111011	2.0	3.0
PHA0111015	3.0	3.0
PHA0112006A	3.0	3.0
PHA0112007B	1.0	2.0
PHA0209038	3.0	3.0
PHA0210008	1.0	1.0
PHA0411035	3.0	3.0
PHA0411058	3.0	3.0
PHA0411059	3.0	3.0
PHA0411061	3.0	3.0
PHA0509017	2.0	3.0
PHA0509018	3.0	4.0
PHA0509024	2.0	2.0
PHA0509026	3.0	3.0
PHA0509038	2.0	2.0
PHA0509040	2.0	2.0
PHA0510003B	2.0	2.0
PHA0510004B	1.0	2.0
PHA0510010A	1.0	2.0
PHA0510010B	1.0	1.0
PHA0510023	3.0	3.0
PHA0510037	2.0	2.0
PHA0510038	3.0	3.0
PHA0610005B	1.0	1.0
PHA0610006B	1.0	2.0
PHA0610017	3.0	3.0
PHA0710014	3.0	3.0
PHA0810003	2.0	3.0
PHA0810010	2.0	3.0
PHA0811019	3.0	3.0
PHA1109004	2.0	3.0
PHA1109025	1.0	2.0
PHA1109027	2.0	2.0
PHA1110001B	2.0	2.0
PHA1110019	3.0	2.0
PHA1111001A	2.0	2.0
VAR0909004	2.0	2.0
1325_1001012	4.0	4.0
1325_1001014	4.0	4.0
1325_1001036	5.0	4.0
1325_1001042	5.0	4.0
1325_1001052	5.0	4.0
1325_1001053	5.0	4.0
1325_1001054	4.0	4.0
1325_1001057	4.0	4.0
1325_1001078	4.0	4.0
1325_1001079	4.0	4.0
1325_1001080	4.0	4.0
1325_1001085	4.0	4.0
1325_1001089	3.0	4.0
1325_1001093	4.0	4.0
1325_1001096	4.0	4.0
1325_1001100	4.0	4.0
1325_1001101	4.0	4.0
1325_1001107	4.0	4.0
1325_1001125	4.0	4.0
1325_1001132	4.0	4.0
1325_1001154	4.0	4.0
1325_1001164	3.0	4.0
1325_1001166	4.0	4.0
1325_9000089	4.0	4.0
1325_9000090	4.0	4.0
1325_9000106	4.0	4.0
1325_9000107	4.0	4.0
1325_9000139	4.0	4.0
1325_9000186	4.0	4.0
1325_9000188	4.0	4.0
1325_9000237	3.0	3.0
1325_9000239	3.0	4.0
1325_9000241	4.0	4.0
1325_9000296	3.0	3.0
1325_9000319	4.0	4.0
1325_9000322	4.0	4.0
1325_9000533	4.0	4.0
1325_9000534	4.0	4.0
1325_9000536	4.0	4.0
1325_9000678	4.0	4.0
1325_9000684	4.0	4.0
1365_0100003	2.0	2.0
1365_0100008	4.0	4.0
1365_0100009	3.0	4.0
1365_0100010	1.0	4.0
1365_0100031	5.0	3.0
1365_0100063	4.0	4.0
1365_0100064	4.0	4.0
1365_0100074	3.0	3.0
1365_0100093	4.0	4.0
1365_0100095	4.0	4.0
1365_0100103	5.0	4.0
1365_0100104	3.0	4.0
1365_0100118	4.0	4.0
1365_0100119	5.0	4.0
1365_0100123	4.0	4.0
1365_0100134	5.0	4.0
1365_0100138	5.0	4.0
1365_0100139	4.0	4.0
1365_0100174	3.0	4.0
1365_0100190	4.0	4.0
1365_0100194	4.0	4.0
1365_0100200	5.0	4.0
1365_0100202	3.0	4.0
1365_0100223	4.0	4.0
1365_0100231	4.0	4.0
1365_0100253	3.0	2.0
1365_0100258	5.0	4.0
1365_0100259	4.0	4.0
1365_0100260	4.0	4.0
1365_0100265	4.0	4.0
1365_0100277	4.0	4.0
1365_0100278	4.0	4.0
1365_0100280	2.0	4.0
1365_0100282	5.0	4.0
1365_0100451	2.0	4.0
1365_0100470	4.0	4.0
1365_0100473	4.0	4.0
1365_0100477	4.0	4.0
1385_0000012	2.0	2.0
1385_0000033	1.0	2.0
1385_0000039	2.0	2.0
1385_0000047	2.0	2.0
1385_0000051	2.0	2.0
1385_0000054	2.0	2.0
1385_0000114	2.0	2.0
1385_0000119	2.0	2.0
1385_0000122	2.0	2.0
1385_0000123	2.0	2.0
1385_0000126	2.0	2.0
1385_0000129	2.0	2.0
1385_0001103	2.0	1.0
1385_0001107	2.0	2.0
1385_0001110	2.0	2.0
1385_0001111	2.0	2.0
1385_0001125	2.0	2.0
1385_0001132	2.0	2.0
1385_0001137	2.0	2.0
1385_0001152	2.0	2.0
1385_0001155	2.0	2.0
1385_0001158	2.0	2.0
1385_0001164	1.0	2.0
1385_0001171	1.0	1.0
1385_0001178	1.0	1.0
1385_0001526	0.0	1.0
1385_0001716	2.0	2.0
1385_0001717	2.0	2.0
1385_0001723	0.0	1.0
1385_0001725	1.0	1.0
1385_0001729	2.0	2.0
1385_0001732	1.0	2.0
1385_0001738	0.0	1.0
1385_0001739	1.0	2.0
1385_0001744	0.0	2.0
1385_0001750	0.0	1.0
1385_0001751	1.0	2.0
1385_0001752	1.0	2.0
1385_0001753	1.0	2.0
1385_0001754	1.0	2.0
1385_0001756	2.0	2.0
1385_0001772	1.0	2.0
1385_0001774	1.0	1.0
1385_0001775	2.0	2.0
1385_0001787	1.0	2.0
1385_0001790	2.0	2.0
1385_0001792	1.0	2.0
1395_0000338	2.0	2.0
1395_0000340	2.0	4.0
1395_0000355	2.0	2.0
1395_0000380	5.0	2.0
1395_0000396	2.0	2.0
1395_0000402	2.0	2.0
1395_0000403	5.0	2.0
1395_0000404	4.0	4.0
1395_0000413	2.0	4.0
1395_0000415	2.0	2.0
1395_0000446	3.0	4.0
1395_0000454	2.0	2.0
1395_0000458	2.0	2.0
1395_0000460	2.0	2.0
1395_0000500	2.0	2.0
1395_0000528	4.0	2.0
1395_0000529	2.0	2.0
1395_0000549	2.0	2.0
1395_0000557	4.0	4.0
1395_0000559	2.0	2.0
1395_0000572	2.0	2.0
1395_0000579	1.0	1.0
1395_0000598	1.0	2.0
1395_0000608	1.0	2.0
1395_0000609	1.0	2.0
1395_0000639	1.0	4.0
1395_0001010	2.0	2.0
1395_0001019	2.0	2.0
1395_0001021	2.0	2.0
1395_0001034	2.0	2.0
1395_0001061	2.0	4.0
1395_0001064	2.0	4.0
1395_0001065	1.0	2.0
1395_0001068	1.0	2.0
1395_0001076	1.0	2.0
1395_0001078	2.0	2.0
1395_0001115	2.0	2.0
1395_0001126	1.0	1.0
1395_0001132	1.0	2.0
1395_0001160	2.0	4.0
1395_0001161	1.0	2.0
1395_0001167	1.0	2.0
1395_0001169	2.0	4.0
5 Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 1.27
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.07
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.67      0.06      0.11        68
         2.0       0.59      0.83      0.69       162
         3.0       0.52      0.57      0.54       106
         4.0       0.52      0.65      0.58        85
         5.0       0.00      0.00      0.00        23

    accuracy                           0.56       452
   macro avg       0.38      0.35      0.32       452
weighted avg       0.53      0.56      0.50       452

[[  0   2   6   0   0   0]
 [  0   4  60   1   3   0]
 [  0   0 134  21   7   0]
 [  0   0  22  60  24   0]
 [  0   0   2  28  55   0]
 [  0   0   2   5  16   0]]
0.5000339512601959
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.99
  Training epoch took: 65
Running Validation...
  Average evaluation loss: 1.04
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.68      0.34      0.45        68
         2.0       0.67      0.81      0.73       162
         3.0       0.58      0.33      0.42       106
         4.0       0.49      0.94      0.65        85
         5.0       0.00      0.00      0.00        23

    accuracy                           0.60       452
   macro avg       0.40      0.40      0.38       452
weighted avg       0.57      0.60      0.55       452

[[  0   6   2   0   0   0]
 [  0  23  41   2   2   0]
 [  0   2 131  20   9   0]
 [  0   2  19  35  50   0]
 [  0   0   2   3  80   0]
 [  0   1   1   0  21   0]]
0.550851437633199
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.86
  Training epoch took: 65
Running Validation...
  Average evaluation loss: 1.03
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.60      0.54      0.57        68
         2.0       0.69      0.73      0.71       162
         3.0       0.64      0.43      0.52       106
         4.0       0.54      0.93      0.69        85
         5.0       0.00      0.00      0.00        23

    accuracy                           0.62       452
   macro avg       0.41      0.44      0.41       452
weighted avg       0.59      0.62      0.59       452

[[  0   7   1   0   0   0]
 [  0  37  28   2   1   0]
 [  0  16 119  20   7   0]
 [  0   1  21  46  38   0]
 [  0   0   2   4  79   0]
 [  0   1   2   0  20   0]]
0.5906594044754067
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.72
  Training epoch took: 65
Running Validation...
  Average evaluation loss: 1.13
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.66      0.43      0.52        68
         2.0       0.66      0.69      0.67       162
         3.0       0.53      0.47      0.50       106
         4.0       0.51      0.88      0.65        85
         5.0       0.00      0.00      0.00        23

    accuracy                           0.59       452
   macro avg       0.39      0.41      0.39       452
weighted avg       0.56      0.59      0.56       452

[[  0   6   2   0   0   0]
 [  0  29  36   2   1   0]
 [  0   8 111  34   9   0]
 [  0   1  15  50  40   0]
 [  0   0   1   9  75   0]
 [  0   0   2   0  21   0]]
0.5585364584643047
452 452 452
Filename	True Label	Prediction
1023_0001419	4.0	4.0
1023_0001420	3.0	4.0
1023_0101688	4.0	4.0
1023_0101690	2.0	3.0
1023_0101695	4.0	3.0
1023_0101700	4.0	4.0
1023_0101851	3.0	4.0
1023_0101855	2.0	3.0
1023_0101893	3.0	4.0
1023_0101894	2.0	3.0
1023_0101901	4.0	4.0
1023_0101906	3.0	3.0
1023_0101907	4.0	4.0
1023_0101909	4.0	4.0
1023_0103821	4.0	4.0
1023_0103829	4.0	4.0
1023_0107074	3.0	4.0
1023_0108304	4.0	4.0
1023_0108426	3.0	3.0
1023_0108649	4.0	4.0
1023_0108813	4.0	4.0
1023_0108888	3.0	4.0
1023_0108889	3.0	3.0
1023_0108908	4.0	3.0
1023_0108933	4.0	4.0
1023_0109030	3.0	4.0
1023_0109399	3.0	3.0
1023_0109401	3.0	3.0
1023_0109402	3.0	3.0
1023_0109422	5.0	4.0
1023_0109495	3.0	3.0
1023_0109606	5.0	4.0
1023_0109649	3.0	3.0
1023_0109671	3.0	3.0
1023_0109721	3.0	3.0
1023_0109945	4.0	4.0
1023_0109946	2.0	3.0
1031_0001997	3.0	4.0
1031_0002084	4.0	4.0
1031_0002085	4.0	4.0
1031_0002086	3.0	4.0
1031_0002087	3.0	3.0
1031_0002089	4.0	4.0
1031_0002091	3.0	4.0
1031_0002195	3.0	3.0
1031_0003035	3.0	3.0
1031_0003042	3.0	4.0
1031_0003043	5.0	4.0
1031_0003048	4.0	4.0
1031_0003054	3.0	4.0
1031_0003072	5.0	4.0
1031_0003074	5.0	4.0
1031_0003085	4.0	4.0
1031_0003095	3.0	3.0
1031_0003121	5.0	4.0
1031_0003130	5.0	4.0
1031_0003131	3.0	4.0
1031_0003145	4.0	4.0
1031_0003156	3.0	3.0
1031_0003157	4.0	4.0
1031_0003162	4.0	4.0
1031_0003163	3.0	3.0
1031_0003172	3.0	4.0
1031_0003182	5.0	4.0
1031_0003184	4.0	4.0
1031_0003203	2.0	3.0
1031_0003207	4.0	4.0
1031_0003214	3.0	4.0
1031_0003218	4.0	4.0
1031_0003221	2.0	4.0
1031_0003225	5.0	4.0
1031_0003230	4.0	3.0
1031_0003234	3.0	3.0
1031_0003235	4.0	4.0
1031_0003237	4.0	4.0
1031_0003239	5.0	4.0
1031_0003245	3.0	4.0
1031_0003260	3.0	4.0
1031_0003262	3.0	4.0
1031_0003273	3.0	3.0
1031_0003313	3.0	4.0
1031_0003338	4.0	4.0
1031_0003352	3.0	3.0
1031_0003353	3.0	4.0
1031_0003354	3.0	4.0
1031_0003366	3.0	3.0
1031_0003367	4.0	4.0
1031_0003389	4.0	3.0
1031_0003390	5.0	4.0
1031_0003391	3.0	3.0
1031_0003393	4.0	3.0
1031_0003407	3.0	3.0
1031_0003408	2.0	3.0
1031_0003410	4.0	4.0
1061_0120273	2.0	2.0
1061_0120275	2.0	2.0
1061_0120279	2.0	2.0
1061_0120286	1.0	2.0
1061_0120287	2.0	2.0
1061_0120291	2.0	2.0
1061_0120297	3.0	3.0
1061_0120300	3.0	2.0
1061_0120302	2.0	2.0
1061_0120303	1.0	3.0
1061_0120306	4.0	3.0
1061_0120307	3.0	3.0
1061_0120315	2.0	2.0
1061_0120318	3.0	3.0
1061_0120323	2.0	2.0
1061_0120324	2.0	2.0
1061_0120341	2.0	2.0
1061_0120343	3.0	3.0
1061_0120345	3.0	3.0
1061_0120346	3.0	3.0
1061_0120347	1.0	2.0
1061_0120349	2.0	2.0
1061_0120352	1.0	1.0
1061_0120353	1.0	1.0
1061_0120383	4.0	4.0
1061_0120384	2.0	2.0
1061_0120387	2.0	3.0
1061_0120404	1.0	1.0
1061_0120405	2.0	2.0
1061_0120407	3.0	3.0
1061_0120409	3.0	3.0
1061_0120410	3.0	2.0
1061_0120414	4.0	3.0
1061_0120415	2.0	3.0
1061_0120421	3.0	4.0
1061_0120425	2.0	3.0
1061_0120428	3.0	3.0
1061_0120431	3.0	3.0
1061_0120439	2.0	2.0
1061_0120481	4.0	4.0
1061_0120491	4.0	3.0
1061_0120493	3.0	2.0
1061_0120494	2.0	2.0
1061_0120497	4.0	4.0
1061_0120499	4.0	4.0
1061_0120857	3.0	3.0
1061_0120882	4.0	4.0
1061_0120884	3.0	2.0
1061_0120890	2.0	1.0
1061_0120894	2.0	3.0
1061_1029115	3.0	3.0
1061_1202919	2.0	2.0
1071_0020001	2.0	2.0
1071_0024680	2.0	2.0
1071_0024687	1.0	1.0
1071_0024688	1.0	2.0
1071_0024694	2.0	3.0
1071_0024706	2.0	2.0
1071_0024709	3.0	3.0
1071_0024712	2.0	2.0
1071_0024770	1.0	1.0
1071_0024776	1.0	1.0
1071_0024798	1.0	2.0
1071_0024803	1.0	2.0
1071_0024804	1.0	2.0
1071_0024809	1.0	1.0
1071_0024810	2.0	2.0
1071_0024815	1.0	2.0
1071_0024825	1.0	1.0
1071_0024831	1.0	1.0
1071_0024833	2.0	2.0
1071_0024835	2.0	2.0
1071_0024844	1.0	1.0
1071_0024849	0.0	1.0
1071_0024852	1.0	1.0
1071_0024854	1.0	2.0
1071_0024855	2.0	2.0
1071_0024860	2.0	2.0
1071_0024864	1.0	1.0
1071_0024867	3.0	3.0
1071_0024878	2.0	3.0
1071_0242042	1.0	1.0
1071_0242071	0.0	1.0
1071_0242092	0.0	1.0
1071_0243591	2.0	2.0
1071_0243593	2.0	2.0
1071_0248303	1.0	1.0
1071_0248313	2.0	2.0
1071_0248323	2.0	2.0
1071_0248332	3.0	2.0
1091_0000012	2.0	2.0
1091_0000014	1.0	2.0
1091_0000024	3.0	2.0
1091_0000033	2.0	3.0
1091_0000059	2.0	3.0
1091_0000073	3.0	2.0
1091_0000075	2.0	3.0
1091_0000079	1.0	2.0
1091_0000102	2.0	2.0
1091_0000123	3.0	3.0
1091_0000125	3.0	2.0
1091_0000152	2.0	2.0
1091_0000159	2.0	2.0
1091_0000161	2.0	2.0
1091_0000165	1.0	2.0
1091_0000166	2.0	2.0
1091_0000171	2.0	2.0
1091_0000172	2.0	1.0
1091_0000197	1.0	3.0
1091_0000198	2.0	3.0
1091_0000204	2.0	3.0
1091_0000205	2.0	2.0
1091_0000208	1.0	2.0
1091_0000210	2.0	2.0
1091_0000217	3.0	2.0
1091_0000218	3.0	2.0
1091_0000222	2.0	2.0
1091_0000225	3.0	1.0
1091_0000232	2.0	2.0
1091_0000242	1.0	2.0
1091_0000244	2.0	2.0
1091_0000247	2.0	2.0
1091_0000248	2.0	2.0
1091_0000249	2.0	2.0
1091_0000251	2.0	2.0
0602	2.0	3.0
0603	2.0	2.0
0604	2.0	2.0
0617	2.0	2.0
0618	2.0	2.0
0621	2.0	3.0
0626	3.0	2.0
0635	2.0	2.0
0641	1.0	2.0
0642	2.0	3.0
0643	2.0	2.0
0715	2.0	2.0
0718	2.0	2.0
0721	3.0	2.0
0805	2.0	3.0
0809	2.0	2.0
0821	2.0	2.0
0902	3.0	3.0
0917	2.0	2.0
0922	2.0	2.0
0930	2.0	2.0
1010	2.0	2.0
1023	2.0	2.0
1114	2.0	3.0
BER0611006	3.0	4.0
KYJ0611005B	1.0	2.0
LIB0611004A	3.0	2.0
LIB0611011	1.0	2.0
LON0611004A	2.0	2.0
MOS0509001	2.0	3.0
PAR1011008A	2.0	2.0
PAR1011013	3.0	3.0
PAR1011014	2.0	3.0
PAR1011016	3.0	4.0
PHA0111003B	2.0	2.0
PHA0111005A	3.0	2.0
PHA0112002B	2.0	2.0
PHA0112003A	2.0	1.0
PHA0112006B	3.0	3.0
PHA0112009A	2.0	3.0
PHA0112012B	2.0	2.0
PHA0210004	2.0	2.0
PHA0210007	2.0	2.0
PHA0411008A	2.0	2.0
PHA0411010A	2.0	2.0
PHA0411010B	2.0	2.0
PHA0411011B	2.0	2.0
PHA0411027	2.0	3.0
PHA0411028	2.0	2.0
PHA0411032	2.0	3.0
PHA0411036	2.0	2.0
PHA0411053	3.0	3.0
PHA0411055	3.0	4.0
PHA0411056	3.0	4.0
PHA0411060	3.0	3.0
PHA0411062	2.0	4.0
PHA0509015	2.0	3.0
PHA0509027	2.0	2.0
PHA0509033	2.0	2.0
PHA0509035	2.0	3.0
PHA0509042	3.0	4.0
PHA0509043	2.0	3.0
PHA0510003A	2.0	2.0
PHA0510004A	1.0	2.0
PHA0510029	3.0	3.0
PHA0510032	3.0	4.0
PHA0510049	3.0	3.0
PHA0610005A	2.0	2.0
PHA0610018	3.0	3.0
PHA0610026	2.0	4.0
PHA0710010	3.0	3.0
PHA0710017	2.0	3.0
PHA0710018	3.0	3.0
PHA0809010	2.0	2.0
PHA0810001	3.0	3.0
PHA0810009	2.0	3.0
PHA0810011	2.0	2.0
PHA0811010	2.0	2.0
PHA1109006	3.0	2.0
PHA1109008	1.0	1.0
PHA1109023	2.0	1.0
PHA1109026	2.0	3.0
PHA1110014	2.0	3.0
PHA1110016	1.0	2.0
PHA1111004B	1.0	2.0
PHA1111008A	2.0	2.0
PHA1111008B	2.0	2.0
PHA1111009A	1.0	2.0
ST071122B	2.0	2.0
VAR0910010	2.0	2.0
VAR0910011	2.0	2.0
1325_1001011	4.0	4.0
1325_1001015	4.0	4.0
1325_1001017	4.0	4.0
1325_1001018	4.0	4.0
1325_1001020	4.0	4.0
1325_1001029	4.0	4.0
1325_1001039	5.0	4.0
1325_1001043	4.0	4.0
1325_1001047	4.0	2.0
1325_1001059	4.0	4.0
1325_1001076	5.0	4.0
1325_1001086	4.0	4.0
1325_1001088	4.0	4.0
1325_1001110	4.0	4.0
1325_1001111	4.0	4.0
1325_1001121	4.0	4.0
1325_1001126	3.0	3.0
1325_1001129	3.0	4.0
1325_1001133	5.0	4.0
1325_1001143	4.0	4.0
1325_1001152	4.0	4.0
1325_1001158	4.0	4.0
1325_1001161	4.0	4.0
1325_9000152	4.0	4.0
1325_9000210	3.0	4.0
1325_9000278	3.0	4.0
1325_9000279	3.0	4.0
1325_9000317	4.0	4.0
1325_9000503	4.0	4.0
1325_9000675	4.0	4.0
1325_9000676	3.0	4.0
1325_9000686	3.0	4.0
1365_0100007	2.0	4.0
1365_0100012	4.0	4.0
1365_0100016	4.0	4.0
1365_0100019	4.0	3.0
1365_0100020	3.0	4.0
1365_0100021	3.0	3.0
1365_0100029	1.0	2.0
1365_0100057	4.0	4.0
1365_0100065	2.0	4.0
1365_0100066	3.0	4.0
1365_0100098	2.0	4.0
1365_0100107	4.0	4.0
1365_0100120	5.0	4.0
1365_0100133	5.0	4.0
1365_0100164	3.0	4.0
1365_0100168	4.0	4.0
1365_0100171	4.0	4.0
1365_0100175	4.0	4.0
1365_0100182	4.0	4.0
1365_0100183	4.0	4.0
1365_0100187	5.0	4.0
1365_0100191	3.0	4.0
1365_0100192	4.0	4.0
1365_0100201	5.0	4.0
1365_0100204	5.0	4.0
1365_0100205	5.0	4.0
1365_0100225	4.0	4.0
1365_0100230	4.0	4.0
1365_0100256	4.0	4.0
1365_0100263	4.0	4.0
1365_0100267	4.0	4.0
1365_0100285	3.0	4.0
1365_0100456	2.0	4.0
1365_0100459	4.0	4.0
1365_0100471	3.0	4.0
1365_0100474	4.0	4.0
1365_0100476	4.0	4.0
1365_0100478	4.0	4.0
1385_0000016	1.0	1.0
1385_0000022	0.0	2.0
1385_0000036	2.0	2.0
1385_0000037	2.0	2.0
1385_0000040	1.0	1.0
1385_0000041	2.0	2.0
1385_0000045	2.0	2.0
1385_0000052	1.0	2.0
1385_0000053	1.0	2.0
1385_0000059	2.0	2.0
1385_0000095	1.0	1.0
1385_0000098	2.0	2.0
1385_0000120	1.0	1.0
1385_0000124	2.0	2.0
1385_0000125	2.0	2.0
1385_0001108	2.0	2.0
1385_0001118	2.0	1.0
1385_0001119	2.0	2.0
1385_0001127	2.0	2.0
1385_0001128	1.0	1.0
1385_0001133	2.0	2.0
1385_0001150	2.0	2.0
1385_0001153	2.0	2.0
1385_0001156	2.0	2.0
1385_0001159	1.0	2.0
1385_0001162	1.0	2.0
1385_0001165	2.0	2.0
1385_0001167	2.0	2.0
1385_0001170	1.0	1.0
1385_0001175	1.0	1.0
1385_0001193	2.0	2.0
1385_0001199	2.0	2.0
1385_0001501	1.0	2.0
1385_0001524	1.0	2.0
1385_0001527	2.0	1.0
1385_0001737	2.0	2.0
1385_0001760	0.0	2.0
1385_0001764	1.0	2.0
1385_0001765	0.0	1.0
1385_0001771	1.0	2.0
1385_0001773	1.0	1.0
1385_0001785	1.0	1.0
1385_0001788	1.0	2.0
1385_0001791	1.0	1.0
1385_0001798	1.0	2.0
1395_0000337	1.0	1.0
1395_0000398	5.0	2.0
1395_0000432	5.0	4.0
1395_0000448	2.0	1.0
1395_0000470	2.0	2.0
1395_0000514	4.0	4.0
1395_0000525	5.0	2.0
1395_0000531	2.0	1.0
1395_0000563	2.0	2.0
1395_0000581	1.0	4.0
1395_0000591	0.0	1.0
1395_0000597	1.0	2.0
1395_0000599	1.0	2.0
1395_0000607	1.0	1.0
1395_0000642	1.0	2.0
1395_0000649	2.0	2.0
1395_0001040	1.0	1.0
1395_0001045	2.0	2.0
1395_0001060	2.0	4.0
1395_0001084	2.0	2.0
1395_0001119	2.0	4.0
1395_0001121	1.0	2.0
1395_0001124	0.0	1.0
1395_0001131	1.0	1.0
1395_0001141	2.0	2.0
1395_0001145	3.0	4.0
1395_0001147	1.0	2.0
Averaged weighted F1-scores 0.5533178854349113
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
144.90552995391704 65.35717405024758
LANGUAGE: IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
1 Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 1.08
  Training epoch took: 65
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        19
         1.0       0.57      0.91      0.70       118
         2.0       0.77      0.44      0.56       163
         3.0       0.71      0.82      0.76       121
         4.0       0.61      0.67      0.63        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.66       452
   macro avg       0.44      0.47      0.44       452
weighted avg       0.66      0.66      0.63       452

[[  0  18   1   0   0   0]
 [  0 107  11   0   0   0]
 [  0  61  72  30   0   0]
 [  0   1   9  99  12   0]
 [  0   0   0  10  20   0]
 [  0   0   0   0   1   0]]
0.6320237068954063
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.75
  Training epoch took: 65
Running Validation...
  Average evaluation loss: 0.74
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        19
         1.0       0.68      0.69      0.68       118
         2.0       0.72      0.74      0.73       163
         3.0       0.75      0.83      0.79       121
         4.0       0.64      0.70      0.67        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.71       452
   macro avg       0.46      0.49      0.48       452
weighted avg       0.68      0.71      0.70       452

[[  0  18   1   0   0   0]
 [  0  81  37   0   0   0]
 [  0  19 120  24   0   0]
 [  0   1   9 100  11   0]
 [  0   0   0   9  21   0]
 [  0   0   0   0   1   0]]
0.6957506038600373
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.64
  Training epoch took: 65
Running Validation...
  Average evaluation loss: 0.73
              precision    recall  f1-score   support

         0.0       1.00      0.05      0.10        19
         1.0       0.66      0.81      0.73       118
         2.0       0.76      0.66      0.71       163
         3.0       0.75      0.80      0.77       121
         4.0       0.62      0.70      0.66        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.71       452
   macro avg       0.63      0.51      0.49       452
weighted avg       0.73      0.71      0.70       452

[[  1  17   1   0   0   0]
 [  0  96  22   0   0   0]
 [  0  31 108  24   0   0]
 [  0   1  11  97  12   0]
 [  0   0   0   9  21   0]
 [  0   0   0   0   1   0]]
0.7006415146380163
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 48
Elapsed time 60

  Average training loss: 0.54
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.73
              precision    recall  f1-score   support

         0.0       1.00      0.11      0.19        19
         1.0       0.69      0.69      0.69       118
         2.0       0.71      0.72      0.72       163
         3.0       0.74      0.81      0.77       121
         4.0       0.64      0.70      0.67        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.71       452
   macro avg       0.63      0.50      0.51       452
weighted avg       0.72      0.71      0.70       452

[[  2  16   1   0   0   0]
 [  0  81  37   0   0   0]
 [  0  20 118  25   0   0]
 [  0   1  11  98  11   0]
 [  0   0   0   9  21   0]
 [  0   0   0   0   1   0]]
0.6967430863352373
452 452 452
Filename	True Label	Prediction
1023_0101693	4.0	3.0
1023_0101694	3.0	3.0
1023_0101752	3.0	3.0
1023_0101841	3.0	3.0
1023_0101848	2.0	3.0
1023_0101855	3.0	3.0
1023_0101907	4.0	3.0
1023_0103827	3.0	3.0
1023_0103837	3.0	3.0
1023_0103841	3.0	3.0
1023_0104203	3.0	3.0
1023_0107075	3.0	3.0
1023_0107727	3.0	3.0
1023_0108306	3.0	3.0
1023_0108510	3.0	3.0
1023_0108641	4.0	3.0
1023_0108812	3.0	3.0
1023_0108815	3.0	3.0
1023_0108886	3.0	3.0
1023_0108887	2.0	3.0
1023_0108955	3.0	3.0
1023_0108958	3.0	3.0
1023_0108992	3.0	3.0
1023_0109022	3.0	3.0
1023_0109027	3.0	3.0
1023_0109250	3.0	3.0
1023_0109401	3.0	3.0
1023_0109422	3.0	3.0
1023_0109505	3.0	3.0
1023_0109590	3.0	3.0
1023_0109649	3.0	3.0
1023_0109717	3.0	3.0
1023_0109878	3.0	3.0
1023_0109880	4.0	3.0
1031_0001949	4.0	4.0
1031_0002002	3.0	4.0
1031_0002011	4.0	4.0
1031_0002042	4.0	4.0
1031_0002089	4.0	4.0
1031_0002091	4.0	4.0
1031_0002092	4.0	4.0
1031_0002197	4.0	4.0
1031_0003012	4.0	4.0
1031_0003035	4.0	3.0
1031_0003043	5.0	4.0
1031_0003048	4.0	4.0
1031_0003091	3.0	3.0
1031_0003092	3.0	4.0
1031_0003131	4.0	4.0
1031_0003132	4.0	4.0
1031_0003136	4.0	4.0
1031_0003160	3.0	4.0
1031_0003161	4.0	4.0
1031_0003165	3.0	3.0
1031_0003166	3.0	3.0
1031_0003167	3.0	4.0
1031_0003169	3.0	4.0
1031_0003170	3.0	4.0
1031_0003182	4.0	4.0
1031_0003212	3.0	4.0
1031_0003217	4.0	4.0
1031_0003218	4.0	4.0
1031_0003226	4.0	4.0
1031_0003234	3.0	3.0
1031_0003238	4.0	4.0
1031_0003240	3.0	4.0
1031_0003244	4.0	4.0
1031_0003245	4.0	4.0
1031_0003260	4.0	3.0
1031_0003331	3.0	4.0
1031_0003352	3.0	3.0
1031_0003354	3.0	4.0
1031_0003355	4.0	3.0
1031_0003386	3.0	4.0
1031_0003410	4.0	4.0
1061_0120273	1.0	2.0
1061_0120286	1.0	1.0
1061_0120289	2.0	2.0
1061_0120290	2.0	2.0
1061_0120300	2.0	2.0
1061_0120304	2.0	2.0
1061_0120315	2.0	2.0
1061_0120327	2.0	2.0
1061_0120332	2.0	2.0
1061_0120336	1.0	2.0
1061_0120337	2.0	3.0
1061_0120341	2.0	2.0
1061_0120343	2.0	2.0
1061_0120358	1.0	2.0
1061_0120370	2.0	3.0
1061_0120376	2.0	2.0
1061_0120384	2.0	2.0
1061_0120390	2.0	2.0
1061_0120391	1.0	2.0
1061_0120408	3.0	2.0
1061_0120426	2.0	2.0
1061_0120438	2.0	2.0
1061_0120441	2.0	2.0
1061_0120442	2.0	2.0
1061_0120443	0.0	1.0
1061_0120460	2.0	2.0
1061_0120479	2.0	2.0
1061_0120482	2.0	2.0
1061_0120485	2.0	2.0
1061_0120497	3.0	3.0
1061_0120498	2.0	3.0
1061_0120880	3.0	3.0
1061_0120882	3.0	3.0
1061_0120886	2.0	2.0
1061_0120889	1.0	2.0
1061_1029114	1.0	2.0
1061_1029117	2.0	2.0
1061_1029119	2.0	2.0
1061_1029120	2.0	2.0
1071_0020001	1.0	1.0
1071_0024678	2.0	1.0
1071_0024709	2.0	2.0
1071_0024710	1.0	1.0
1071_0024715	2.0	2.0
1071_0024758	2.0	2.0
1071_0024759	0.0	1.0
1071_0024773	1.0	1.0
1071_0024778	1.0	1.0
1071_0024782	0.0	1.0
1071_0024800	0.0	1.0
1071_0024802	2.0	2.0
1071_0024806	1.0	1.0
1071_0024808	1.0	1.0
1071_0024809	1.0	1.0
1071_0024814	1.0	1.0
1071_0024819	2.0	2.0
1071_0024838	0.0	0.0
1071_0024857	1.0	2.0
1071_0024860	1.0	1.0
1071_0024876	1.0	2.0
1071_0024877	1.0	1.0
1071_0024881	2.0	2.0
1071_0241833	1.0	1.0
1071_0242011	1.0	1.0
1071_0242013	1.0	1.0
1071_0242022	0.0	1.0
1071_0242043	0.0	1.0
1071_0243592	1.0	1.0
1071_0243623	1.0	1.0
1071_0248304	1.0	1.0
1071_0248310	1.0	1.0
1071_0248315	0.0	1.0
1071_0248317	0.0	0.0
1071_0248321	1.0	1.0
1071_0248322	1.0	1.0
1071_0248330	2.0	2.0
1071_0248334	2.0	2.0
1071_0248336	1.0	1.0
1071_0248338	1.0	1.0
1071_0248339	1.0	1.0
1071_0248344	2.0	1.0
1071_0248346	1.0	1.0
1071_0248349	0.0	1.0
1091_0000007	2.0	2.0
1091_0000009	0.0	1.0
1091_0000012	1.0	1.0
1091_0000014	0.0	1.0
1091_0000020	1.0	2.0
1091_0000029	2.0	1.0
1091_0000034	2.0	1.0
1091_0000038	2.0	1.0
1091_0000052	1.0	1.0
1091_0000055	1.0	2.0
1091_0000058	2.0	2.0
1091_0000061	2.0	1.0
1091_0000063	2.0	2.0
1091_0000068	1.0	2.0
1091_0000070	2.0	2.0
1091_0000077	2.0	1.0
1091_0000078	3.0	1.0
1091_0000113	1.0	2.0
1091_0000116	3.0	2.0
1091_0000125	2.0	2.0
1091_0000127	2.0	2.0
1091_0000144	2.0	1.0
1091_0000156	3.0	2.0
1091_0000163	2.0	1.0
1091_0000168	2.0	2.0
1091_0000193	2.0	1.0
1091_0000203	2.0	2.0
1091_0000217	2.0	2.0
1091_0000218	2.0	2.0
1091_0000230	2.0	2.0
1091_0000232	2.0	2.0
1091_0000236	2.0	2.0
1091_0000242	1.0	2.0
1091_0000243	1.0	2.0
1091_0000244	2.0	2.0
1091_0000247	2.0	2.0
1091_0000254	2.0	2.0
1091_0000255	0.0	2.0
1091_0000258	2.0	2.0
1091_0000263	2.0	2.0
1091_0000269	1.0	2.0
1091_0000270	2.0	2.0
1091_0000273	1.0	2.0
1091_0000276	2.0	2.0
0605	2.0	2.0
0610	2.0	2.0
0615	2.0	2.0
0618	2.0	2.0
0619	2.0	1.0
0622	1.0	1.0
0624	2.0	2.0
0625	2.0	1.0
0633	2.0	2.0
0642	2.0	2.0
0807	2.0	2.0
0808	1.0	2.0
0817	2.0	2.0
0820	1.0	1.0
0822	1.0	2.0
0823	2.0	2.0
0826	1.0	2.0
0828	2.0	2.0
0829	2.0	2.0
0923	1.0	2.0
0924	2.0	2.0
1014	2.0	2.0
1016	1.0	2.0
1019	2.0	2.0
1114	2.0	2.0
1117	1.0	2.0
KYJ0611004A	1.0	1.0
KYJ0611005A	1.0	1.0
KYJ0611009B	1.0	2.0
LIB0611002A	1.0	1.0
LON0610002B	1.0	1.0
LON0611002B	1.0	1.0
LON0611004B	1.0	1.0
MOS0611014	2.0	3.0
PAR1011008A	2.0	1.0
PHA0111015	3.0	3.0
PHA0112002B	1.0	1.0
PHA0112009A	2.0	2.0
PHA0209001	1.0	2.0
PHA0209026	3.0	3.0
PHA0209028	3.0	3.0
PHA0209031	4.0	3.0
PHA0411009A	1.0	1.0
PHA0411012A	1.0	1.0
PHA0411012B	1.0	1.0
PHA0411027	2.0	3.0
PHA0411029	3.0	3.0
PHA0411030	3.0	3.0
PHA0411031	3.0	3.0
PHA0411034	2.0	2.0
PHA0411043	3.0	3.0
PHA0411058	3.0	3.0
PHA0411060	2.0	3.0
PHA0509017	3.0	3.0
PHA0509026	3.0	3.0
PHA0509027	3.0	3.0
PHA0509030	3.0	3.0
PHA0509031	2.0	3.0
PHA0509038	2.0	2.0
PHA0509041	3.0	3.0
PHA0510002A	1.0	1.0
PHA0510002B	1.0	1.0
PHA0510003B	1.0	1.0
PHA0510035	3.0	3.0
PHA0510037	2.0	3.0
PHA0510039	3.0	3.0
PHA0510047	2.0	2.0
PHA0510048	3.0	3.0
PHA0510050	3.0	3.0
PHA0610006A	2.0	1.0
PHA0610007A	1.0	1.0
PHA0709008	3.0	3.0
PHA0810012	3.0	3.0
PHA0811010	3.0	3.0
PHA0811014	3.0	3.0
PHA0811016	3.0	3.0
PHA1109007	2.0	3.0
PHA1109028	3.0	3.0
PHA1110015	3.0	3.0
PHA1110021	3.0	3.0
PHA1111002A	1.0	1.0
PHA1111002B	1.0	1.0
PHA1111004B	1.0	1.0
VAR0909005	3.0	3.0
VAR0909006	3.0	3.0
VAR0909010	3.0	3.0
1325_1001009	3.0	3.0
1325_1001018	3.0	3.0
1325_1001027	3.0	3.0
1325_1001028	3.0	3.0
1325_1001037	2.0	3.0
1325_1001040	3.0	3.0
1325_1001042	2.0	3.0
1325_1001045	3.0	3.0
1325_1001058	2.0	3.0
1325_1001077	3.0	3.0
1325_1001079	3.0	3.0
1325_1001082	3.0	2.0
1325_1001087	2.0	3.0
1325_1001090	2.0	2.0
1325_1001095	2.0	3.0
1325_1001096	3.0	3.0
1325_1001109	2.0	3.0
1325_1001122	3.0	2.0
1325_1001130	3.0	3.0
1325_1001135	3.0	3.0
1325_1001136	3.0	3.0
1325_1001138	2.0	3.0
1325_1001139	3.0	3.0
1325_1001144	3.0	3.0
1325_1001160	3.0	3.0
1325_1001166	3.0	3.0
1325_9000059	3.0	3.0
1325_9000102	3.0	2.0
1325_9000105	2.0	3.0
1325_9000107	3.0	3.0
1325_9000143	3.0	3.0
1325_9000144	3.0	3.0
1325_9000186	3.0	3.0
1325_9000210	2.0	3.0
1325_9000237	3.0	3.0
1325_9000320	3.0	3.0
1325_9000321	3.0	3.0
1325_9000322	3.0	3.0
1325_9000503	3.0	3.0
1325_9000601	3.0	3.0
1325_9000602	4.0	3.0
1325_9000674	3.0	3.0
1325_9000677	3.0	3.0
1325_9000684	3.0	3.0
1325_9000685	3.0	3.0
1325_9000700	3.0	3.0
1325_9000750	3.0	2.0
1365_0100003	2.0	2.0
1365_0100006	2.0	3.0
1365_0100009	2.0	2.0
1365_0100026	2.0	2.0
1365_0100031	2.0	2.0
1365_0100057	2.0	3.0
1365_0100070	2.0	2.0
1365_0100071	3.0	2.0
1365_0100096	2.0	3.0
1365_0100104	2.0	2.0
1365_0100120	3.0	3.0
1365_0100121	2.0	2.0
1365_0100123	2.0	2.0
1365_0100136	2.0	2.0
1365_0100164	2.0	2.0
1365_0100167	2.0	2.0
1365_0100175	2.0	2.0
1365_0100179	2.0	2.0
1365_0100184	2.0	2.0
1365_0100185	2.0	2.0
1365_0100191	2.0	2.0
1365_0100196	2.0	3.0
1365_0100203	2.0	2.0
1365_0100211	3.0	3.0
1365_0100215	2.0	2.0
1365_0100218	2.0	2.0
1365_0100251	2.0	3.0
1365_0100252	2.0	2.0
1365_0100263	3.0	2.0
1365_0100268	2.0	2.0
1365_0100278	3.0	2.0
1365_0100288	2.0	2.0
1365_0100289	2.0	2.0
1365_0100448	2.0	2.0
1365_0100478	2.0	2.0
1385_0000021	1.0	1.0
1385_0000022	1.0	2.0
1385_0000033	1.0	1.0
1385_0000034	1.0	1.0
1385_0000040	1.0	1.0
1385_0000041	1.0	1.0
1385_0000042	1.0	1.0
1385_0000044	2.0	1.0
1385_0000047	1.0	2.0
1385_0000057	1.0	1.0
1385_0000102	2.0	1.0
1385_0000103	1.0	1.0
1385_0000119	1.0	1.0
1385_0000120	0.0	1.0
1385_0001104	1.0	1.0
1385_0001107	1.0	1.0
1385_0001121	2.0	1.0
1385_0001123	2.0	1.0
1385_0001130	1.0	1.0
1385_0001138	1.0	1.0
1385_0001152	2.0	2.0
1385_0001157	1.0	2.0
1385_0001167	1.0	1.0
1385_0001195	2.0	2.0
1385_0001522	0.0	1.0
1385_0001727	0.0	1.0
1385_0001729	1.0	2.0
1385_0001734	1.0	1.0
1385_0001746	1.0	1.0
1385_0001750	0.0	1.0
1385_0001757	1.0	1.0
1385_0001760	1.0	1.0
1385_0001761	1.0	1.0
1385_0001788	1.0	1.0
1385_0001789	1.0	1.0
1385_0001793	1.0	2.0
1385_0001798	1.0	1.0
1395_0000340	2.0	2.0
1395_0000353	1.0	1.0
1395_0000366	2.0	2.0
1395_0000398	2.0	2.0
1395_0000403	2.0	2.0
1395_0000409	2.0	2.0
1395_0000438	3.0	2.0
1395_0000447	2.0	2.0
1395_0000449	2.0	2.0
1395_0000451	2.0	2.0
1395_0000460	1.0	1.0
1395_0000500	1.0	1.0
1395_0000526	1.0	1.0
1395_0000537	2.0	2.0
1395_0000547	2.0	2.0
1395_0000555	1.0	1.0
1395_0000556	1.0	1.0
1395_0000557	2.0	2.0
1395_0000559	2.0	1.0
1395_0000563	2.0	2.0
1395_0000575	1.0	1.0
1395_0000582	0.0	1.0
1395_0000583	1.0	2.0
1395_0000587	0.0	1.0
1395_0000608	1.0	1.0
1395_0000610	2.0	2.0
1395_0000626	2.0	2.0
1395_0000631	1.0	2.0
1395_0001010	1.0	2.0
1395_0001020	1.0	1.0
1395_0001060	1.0	2.0
1395_0001061	2.0	2.0
1395_0001074	1.0	2.0
1395_0001080	2.0	2.0
1395_0001084	1.0	2.0
1395_0001093	1.0	1.0
1395_0001101	2.0	1.0
1395_0001104	1.0	1.0
1395_0001108	1.0	1.0
1395_0001109	1.0	2.0
1395_0001119	2.0	2.0
1395_0001120	1.0	1.0
1395_0001147	2.0	2.0
1395_0001158	1.0	2.0
1395_0001171	1.0	1.0
2 Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 48
Elapsed time 60

  Average training loss: 1.04
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.73
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        19
         1.0       0.60      0.73      0.66       118
         2.0       0.65      0.67      0.66       163
         3.0       0.80      0.71      0.75       121
         4.0       0.72      0.77      0.74        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.67       452
   macro avg       0.46      0.48      0.47       452
weighted avg       0.65      0.67      0.66       452

[[  0  19   0   0   0   0]
 [  0  86  32   0   0   0]
 [  0  39 109  15   0   0]
 [  0   0  27  86   8   0]
 [  0   0   0   7  23   0]
 [  0   0   0   0   1   0]]
0.6592019552238971
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 48
Elapsed time 60

  Average training loss: 0.74
  Training epoch took: 68
Running Validation...
  Average evaluation loss: 0.77
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        19
         1.0       0.65      0.71      0.68       118
         2.0       0.73      0.64      0.69       163
         3.0       0.74      0.90      0.81       121
         4.0       0.71      0.73      0.72        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.71       452
   macro avg       0.47      0.50      0.48       452
weighted avg       0.68      0.71      0.69       452

[[  0  19   0   0   0   0]
 [  0  84  34   0   0   0]
 [  0  27 105  31   0   0]
 [  0   0   4 109   8   0]
 [  0   0   0   8  22   0]
 [  0   0   0   0   1   0]]
0.6891527716486846
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 48
Elapsed time 60

  Average training loss: 0.67
  Training epoch took: 68
Running Validation...
  Average evaluation loss: 0.71
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        19
         1.0       0.62      0.81      0.70       118
         2.0       0.71      0.72      0.72       163
         3.0       0.86      0.74      0.79       121
         4.0       0.73      0.73      0.73        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.72       452
   macro avg       0.49      0.50      0.49       452
weighted avg       0.70      0.72      0.70       452

[[  0  19   0   0   0   0]
 [  0  95  23   0   0   0]
 [  0  38 118   7   0   0]
 [  0   0  25  89   7   0]
 [  0   0   0   8  22   0]
 [  0   0   0   0   1   0]]
0.7028440124768499
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 48
Elapsed time 60

  Average training loss: 0.58
  Training epoch took: 68
Running Validation...
  Average evaluation loss: 0.68
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        19
         1.0       0.66      0.71      0.69       118
         2.0       0.71      0.75      0.73       163
         3.0       0.81      0.79      0.79       121
         4.0       0.67      0.80      0.73        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.72       452
   macro avg       0.47      0.51      0.49       452
weighted avg       0.69      0.72      0.70       452

[[  0  19   0   0   0   0]
 [  0  84  34   0   0   0]
 [  0  24 122  17   0   0]
 [  0   0  15  95  11   0]
 [  0   0   0   6  24   0]
 [  0   0   0   0   1   0]]
0.7035459744758665
452 452 452
Filename	True Label	Prediction
1023_0101689	2.0	2.0
1023_0101691	3.0	3.0
1023_0101695	3.0	3.0
1023_0101700	3.0	3.0
1023_0101751	3.0	3.0
1023_0101845	3.0	3.0
1023_0101856	3.0	3.0
1023_0101894	3.0	3.0
1023_0101895	3.0	4.0
1023_0101898	4.0	4.0
1023_0102118	3.0	3.0
1023_0103821	3.0	3.0
1023_0103824	3.0	4.0
1023_0103831	3.0	3.0
1023_0103832	3.0	3.0
1023_0103838	3.0	3.0
1023_0103840	3.0	3.0
1023_0103843	3.0	3.0
1023_0103883	3.0	3.0
1023_0107740	3.0	3.0
1023_0107780	3.0	3.0
1023_0107783	3.0	2.0
1023_0107784	2.0	2.0
1023_0108518	3.0	3.0
1023_0108649	3.0	3.0
1023_0108752	3.0	3.0
1023_0108766	3.0	3.0
1023_0108810	3.0	3.0
1023_0108885	3.0	3.0
1023_0108908	3.0	3.0
1023_0108931	3.0	3.0
1023_0109033	4.0	3.0
1023_0109247	3.0	3.0
1023_0109392	3.0	3.0
1023_0109400	3.0	3.0
1023_0109515	3.0	4.0
1023_0109516	3.0	3.0
1023_0109522	3.0	3.0
1023_0109524	3.0	3.0
1023_0109591	3.0	3.0
1023_0109606	3.0	3.0
1023_0109609	3.0	3.0
1023_0109614	2.0	2.0
1023_0109951	3.0	3.0
1031_0001950	4.0	4.0
1031_0001998	4.0	4.0
1031_0002084	4.0	4.0
1031_0002184	4.0	4.0
1031_0002198	4.0	4.0
1031_0003053	4.0	4.0
1031_0003071	4.0	4.0
1031_0003073	4.0	4.0
1031_0003098	5.0	4.0
1031_0003126	4.0	4.0
1031_0003140	4.0	4.0
1031_0003155	4.0	4.0
1031_0003156	4.0	4.0
1031_0003180	4.0	4.0
1031_0003181	4.0	4.0
1031_0003185	3.0	3.0
1031_0003190	4.0	4.0
1031_0003203	2.0	3.0
1031_0003206	3.0	4.0
1031_0003207	4.0	4.0
1031_0003211	3.0	4.0
1031_0003272	3.0	3.0
1031_0003273	3.0	4.0
1031_0003274	3.0	4.0
1031_0003310	4.0	4.0
1031_0003314	4.0	4.0
1031_0003315	3.0	3.0
1031_0003337	4.0	4.0
1031_0003357	4.0	4.0
1031_0003365	4.0	3.0
1031_0003367	4.0	4.0
1031_0003368	3.0	4.0
1031_0003369	4.0	4.0
1031_0003384	3.0	4.0
1031_0003389	3.0	4.0
1031_0003393	4.0	4.0
1031_0003414	3.0	4.0
1061_0120282	0.0	1.0
1061_0120297	2.0	2.0
1061_0120302	1.0	2.0
1061_0120307	2.0	2.0
1061_0120318	2.0	2.0
1061_0120321	2.0	2.0
1061_0120323	2.0	2.0
1061_0120324	2.0	2.0
1061_0120325	2.0	2.0
1061_0120333	3.0	3.0
1061_0120345	2.0	2.0
1061_0120360	3.0	2.0
1061_0120368	2.0	2.0
1061_0120369	2.0	2.0
1061_0120382	2.0	2.0
1061_0120407	3.0	2.0
1061_0120409	2.0	2.0
1061_0120423	3.0	3.0
1061_0120431	2.0	2.0
1061_0120439	2.0	2.0
1061_0120440	1.0	2.0
1061_0120458	3.0	3.0
1061_0120480	2.0	2.0
1061_0120483	2.0	2.0
1061_0120489	2.0	2.0
1061_0120856	2.0	2.0
1061_0120878	1.0	2.0
1061_0120884	2.0	2.0
1061_0120887	2.0	2.0
1061_0120890	1.0	2.0
1061_1029111	2.0	2.0
1061_1029118	2.0	2.0
1061_1202917	2.0	2.0
1071_0024680	2.0	2.0
1071_0024682	2.0	2.0
1071_0024688	2.0	2.0
1071_0024691	2.0	2.0
1071_0024693	1.0	2.0
1071_0024699	2.0	2.0
1071_0024703	1.0	1.0
1071_0024711	2.0	2.0
1071_0024762	1.0	1.0
1071_0024763	1.0	1.0
1071_0024769	0.0	1.0
1071_0024770	1.0	1.0
1071_0024775	0.0	1.0
1071_0024776	0.0	1.0
1071_0024784	1.0	1.0
1071_0024803	1.0	1.0
1071_0024807	1.0	1.0
1071_0024810	1.0	1.0
1071_0024811	1.0	1.0
1071_0024812	1.0	1.0
1071_0024821	1.0	1.0
1071_0024823	1.0	1.0
1071_0024824	1.0	1.0
1071_0024833	2.0	1.0
1071_0024837	0.0	1.0
1071_0024841	1.0	1.0
1071_0024843	1.0	1.0
1071_0024844	1.0	1.0
1071_0024848	1.0	1.0
1071_0024851	2.0	1.0
1071_0024865	2.0	2.0
1071_0024874	1.0	1.0
1071_0024878	2.0	2.0
1071_0241832	1.0	1.0
1071_0242023	1.0	1.0
1071_0243621	2.0	2.0
1071_0248308	1.0	1.0
1071_0248311	2.0	1.0
1071_0248318	0.0	1.0
1071_0248320	0.0	1.0
1071_0248323	1.0	1.0
1071_0248335	1.0	1.0
1071_0248341	1.0	1.0
1071_0248348	1.0	1.0
1091_0000001	1.0	1.0
1091_0000006	1.0	1.0
1091_0000008	2.0	2.0
1091_0000010	2.0	2.0
1091_0000013	1.0	1.0
1091_0000022	2.0	2.0
1091_0000036	1.0	2.0
1091_0000044	1.0	1.0
1091_0000053	1.0	1.0
1091_0000057	2.0	1.0
1091_0000059	1.0	2.0
1091_0000064	1.0	2.0
1091_0000071	1.0	2.0
1091_0000074	2.0	1.0
1091_0000076	2.0	2.0
1091_0000095	2.0	1.0
1091_0000102	2.0	2.0
1091_0000114	2.0	2.0
1091_0000126	3.0	2.0
1091_0000145	1.0	1.0
1091_0000148	1.0	1.0
1091_0000153	1.0	2.0
1091_0000158	2.0	2.0
1091_0000159	2.0	2.0
1091_0000160	3.0	3.0
1091_0000161	2.0	2.0
1091_0000162	2.0	2.0
1091_0000167	1.0	2.0
1091_0000172	2.0	1.0
1091_0000190	1.0	2.0
1091_0000195	1.0	1.0
1091_0000198	2.0	2.0
1091_0000201	2.0	2.0
1091_0000205	2.0	2.0
1091_0000206	1.0	2.0
1091_0000207	2.0	2.0
1091_0000221	2.0	2.0
1091_0000223	2.0	2.0
1091_0000226	1.0	2.0
1091_0000231	2.0	2.0
1091_0000238	2.0	2.0
1091_0000245	1.0	2.0
1091_0000248	2.0	2.0
1091_0000252	2.0	2.0
1091_0000257	2.0	2.0
1091_0000261	2.0	2.0
0603	2.0	2.0
0607	2.0	2.0
0614	2.0	2.0
0627	2.0	2.0
0628	1.0	2.0
0636	2.0	2.0
0641	1.0	1.0
0645	2.0	2.0
0718	1.0	2.0
0722	2.0	2.0
0801	2.0	1.0
0805	1.0	2.0
0810	2.0	2.0
0813	2.0	1.0
0902	1.0	2.0
0906	2.0	2.0
0910	1.0	1.0
0925	1.0	2.0
0929	1.0	2.0
0930	1.0	2.0
1003	2.0	2.0
1020	2.0	2.0
1022	2.0	2.0
BER0611006	2.0	3.0
KYJ0611005B	1.0	1.0
KYJ0611006B	0.0	1.0
LIB0611004A	1.0	1.0
LIB0611011	2.0	2.0
MOS0509001	2.0	2.0
MOS0509004	3.0	2.0
MOS0611012	3.0	3.0
MOS0611015	3.0	3.0
PAR1011009B	1.0	1.0
PAR1011014	2.0	3.0
PAR1011018	4.0	3.0
PHA0111002B	2.0	2.0
PHA0111005B	1.0	1.0
PHA0111012	2.0	3.0
PHA0112006A	2.0	1.0
PHA0112006B	2.0	2.0
PHA0112012B	1.0	1.0
PHA0209013	1.0	1.0
PHA0210001	1.0	1.0
PHA0210004	1.0	1.0
PHA0411008A	1.0	1.0
PHA0411032	3.0	3.0
PHA0411037	3.0	3.0
PHA0411044	4.0	3.0
PHA0411062	3.0	3.0
PHA0509007	1.0	1.0
PHA0509013	1.0	1.0
PHA0509015	3.0	3.0
PHA0509019	3.0	3.0
PHA0509025	3.0	3.0
PHA0509028	3.0	3.0
PHA0509036	3.0	3.0
PHA0509042	3.0	3.0
PHA0509044	3.0	3.0
PHA0509045	3.0	2.0
PHA0510003A	1.0	1.0
PHA0510013A	1.0	1.0
PHA0510027	3.0	3.0
PHA0510032	3.0	3.0
PHA0510034	3.0	3.0
PHA0610015	3.0	3.0
PHA0610016	3.0	3.0
PHA0610017	3.0	3.0
PHA0710009	3.0	3.0
PHA0710011	3.0	3.0
PHA0710019	3.0	3.0
PHA0809010	2.0	2.0
PHA0810006	3.0	3.0
PHA0810010	3.0	3.0
PHA0811019	4.0	3.0
PHA1109001	1.0	1.0
PHA1109002	3.0	3.0
PHA1109027	3.0	3.0
PHA1110002B	1.0	1.0
PHA1110004A	1.0	1.0
PHA1110014	3.0	3.0
PHA1110016	3.0	3.0
PHA1111006B	1.0	1.0
PHA1111008A	1.0	1.0
PHA1111008B	1.0	1.0
VAR0209036	2.0	3.0
VAR0910004	3.0	3.0
VAR0910005	3.0	3.0
1325_1001008	3.0	3.0
1325_1001010	3.0	3.0
1325_1001019	3.0	3.0
1325_1001023	3.0	2.0
1325_1001032	3.0	3.0
1325_1001035	3.0	3.0
1325_1001048	2.0	3.0
1325_1001052	2.0	3.0
1325_1001054	3.0	2.0
1325_1001085	3.0	2.0
1325_1001086	3.0	3.0
1325_1001093	2.0	3.0
1325_1001100	2.0	3.0
1325_1001101	3.0	3.0
1325_1001111	3.0	3.0
1325_1001124	3.0	2.0
1325_1001125	3.0	3.0
1325_1001142	3.0	3.0
1325_1001157	3.0	3.0
1325_1001161	3.0	3.0
1325_1001164	3.0	3.0
1325_9000088	2.0	3.0
1325_9000106	3.0	2.0
1325_9000137	3.0	3.0
1325_9000138	4.0	3.0
1325_9000140	3.0	3.0
1325_9000185	3.0	3.0
1325_9000239	3.0	3.0
1325_9000304	3.0	3.0
1325_9000317	3.0	3.0
1325_9000534	3.0	3.0
1325_9000675	3.0	3.0
1325_9000686	3.0	3.0
1365_0100010	2.0	2.0
1365_0100012	2.0	2.0
1365_0100017	2.0	2.0
1365_0100018	2.0	2.0
1365_0100020	2.0	2.0
1365_0100021	2.0	2.0
1365_0100029	1.0	2.0
1365_0100066	2.0	2.0
1365_0100073	2.0	2.0
1365_0100080	2.0	2.0
1365_0100094	2.0	2.0
1365_0100095	2.0	2.0
1365_0100098	2.0	2.0
1365_0100103	2.0	3.0
1365_0100106	2.0	2.0
1365_0100134	2.0	2.0
1365_0100138	2.0	2.0
1365_0100147	2.0	2.0
1365_0100163	3.0	3.0
1365_0100165	3.0	2.0
1365_0100170	2.0	2.0
1365_0100172	2.0	2.0
1365_0100176	2.0	2.0
1365_0100178	2.0	2.0
1365_0100180	2.0	2.0
1365_0100188	2.0	2.0
1365_0100198	2.0	2.0
1365_0100229	2.0	3.0
1365_0100230	2.0	3.0
1365_0100253	2.0	2.0
1365_0100259	2.0	2.0
1365_0100265	2.0	2.0
1365_0100267	2.0	3.0
1365_0100270	2.0	2.0
1365_0100277	3.0	3.0
1365_0100286	2.0	2.0
1365_0100299	2.0	2.0
1365_0100447	2.0	2.0
1365_0100459	3.0	2.0
1365_0100461	2.0	3.0
1365_0100469	2.0	2.0
1365_0100470	2.0	3.0
1365_0100473	2.0	2.0
1365_0100474	2.0	3.0
1365_0100482	2.0	2.0
1385_0000036	1.0	1.0
1385_0000038	1.0	1.0
1385_0000045	2.0	1.0
1385_0000058	1.0	1.0
1385_0000059	1.0	1.0
1385_0000095	1.0	1.0
1385_0000128	1.0	1.0
1385_0001103	1.0	1.0
1385_0001110	2.0	1.0
1385_0001120	2.0	1.0
1385_0001122	2.0	1.0
1385_0001124	1.0	1.0
1385_0001132	1.0	1.0
1385_0001133	2.0	1.0
1385_0001136	1.0	1.0
1385_0001137	1.0	1.0
1385_0001147	1.0	1.0
1385_0001153	2.0	1.0
1385_0001154	1.0	1.0
1385_0001161	1.0	1.0
1385_0001166	1.0	1.0
1385_0001173	0.0	1.0
1385_0001175	0.0	1.0
1385_0001192	1.0	1.0
1385_0001193	1.0	1.0
1385_0001526	0.0	1.0
1385_0001715	1.0	1.0
1385_0001718	0.0	1.0
1385_0001728	1.0	1.0
1385_0001733	1.0	1.0
1385_0001738	0.0	1.0
1385_0001742	0.0	1.0
1385_0001744	0.0	1.0
1385_0001752	1.0	1.0
1385_0001762	1.0	1.0
1385_0001768	2.0	1.0
1385_0001772	1.0	1.0
1395_0000333	1.0	1.0
1395_0000355	2.0	1.0
1395_0000356	1.0	1.0
1395_0000357	3.0	2.0
1395_0000365	2.0	2.0
1395_0000368	0.0	1.0
1395_0000376	2.0	2.0
1395_0000388	2.0	2.0
1395_0000391	3.0	2.0
1395_0000404	2.0	2.0
1395_0000413	2.0	2.0
1395_0000432	2.0	2.0
1395_0000454	2.0	2.0
1395_0000455	2.0	2.0
1395_0000462	2.0	1.0
1395_0000529	2.0	1.0
1395_0000549	2.0	2.0
1395_0000550	2.0	2.0
1395_0000551	2.0	2.0
1395_0000552	2.0	2.0
1395_0000553	2.0	1.0
1395_0000593	1.0	2.0
1395_0000596	2.0	1.0
1395_0000606	0.0	1.0
1395_0000609	1.0	1.0
1395_0000627	1.0	2.0
1395_0000628	1.0	2.0
1395_0000639	1.0	2.0
1395_0000644	1.0	2.0
1395_0000646	1.0	1.0
1395_0000649	1.0	2.0
1395_0001015	1.0	2.0
1395_0001024	2.0	1.0
1395_0001028	1.0	2.0
1395_0001034	1.0	1.0
1395_0001040	0.0	1.0
1395_0001058	1.0	1.0
1395_0001064	2.0	2.0
1395_0001067	1.0	2.0
1395_0001070	2.0	2.0
1395_0001073	1.0	2.0
1395_0001116	2.0	1.0
1395_0001126	1.0	1.0
1395_0001146	0.0	1.0
1395_0001164	2.0	2.0
1395_0001170	1.0	2.0
3 Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 1.07
  Training epoch took: 65
Running Validation...
  Average evaluation loss: 0.74
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        19
         1.0       0.65      0.71      0.68       118
         2.0       0.72      0.69      0.70       164
         3.0       0.75      0.81      0.78       120
         4.0       0.58      0.70      0.64        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.70       452
   macro avg       0.45      0.48      0.47       452
weighted avg       0.67      0.70      0.68       452

[[  0  18   1   0   0   0]
 [  0  84  34   0   0   0]
 [  0  27 113  24   0   0]
 [  0   0   9  97  14   0]
 [  0   0   0   9  21   0]
 [  0   0   0   0   1   0]]
0.6812703381802624
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.74
  Training epoch took: 65
Running Validation...
  Average evaluation loss: 0.72
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        19
         1.0       0.63      0.82      0.72       118
         2.0       0.77      0.68      0.72       164
         3.0       0.77      0.80      0.79       120
         4.0       0.62      0.60      0.61        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.71       452
   macro avg       0.47      0.48      0.47       452
weighted avg       0.69      0.71      0.70       452

[[  0  19   0   0   0   0]
 [  0  97  21   0   0   0]
 [  0  36 112  16   0   0]
 [  0   1  13  96  10   0]
 [  0   0   0  12  18   0]
 [  0   0   0   0   1   0]]
0.6984665150080935
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.65
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.71
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        19
         1.0       0.66      0.79      0.72       118
         2.0       0.74      0.72      0.73       164
         3.0       0.78      0.78      0.78       120
         4.0       0.64      0.70      0.67        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.72       452
   macro avg       0.47      0.50      0.48       452
weighted avg       0.69      0.72      0.70       452

[[  0  19   0   0   0   0]
 [  0  93  25   0   0   0]
 [  0  28 118  18   0   0]
 [  0   0  16  93  11   0]
 [  0   0   0   9  21   0]
 [  0   0   0   0   1   0]]
0.7033106036036667
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.56
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.73
              precision    recall  f1-score   support

         0.0       1.00      0.11      0.19        19
         1.0       0.67      0.71      0.69       118
         2.0       0.73      0.73      0.73       164
         3.0       0.78      0.83      0.80       120
         4.0       0.68      0.70      0.69        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.72       452
   macro avg       0.64      0.51      0.52       452
weighted avg       0.73      0.72      0.71       452

[[  2  17   0   0   0   0]
 [  0  84  34   0   0   0]
 [  0  24 120  20   0   0]
 [  0   0  11 100   9   0]
 [  0   0   0   9  21   0]
 [  0   0   0   0   1   0]]
0.7121146742585911
452 452 452
Filename	True Label	Prediction
1023_0001422	3.0	3.0
1023_0101675	3.0	3.0
1023_0101688	3.0	3.0
1023_0101690	2.0	3.0
1023_0101701	3.0	3.0
1023_0101753	3.0	3.0
1023_0101843	3.0	3.0
1023_0101847	3.0	3.0
1023_0101849	3.0	3.0
1023_0101851	3.0	3.0
1023_0101852	3.0	3.0
1023_0101899	3.0	3.0
1023_0101900	4.0	3.0
1023_0101901	4.0	3.0
1023_0101906	3.0	3.0
1023_0101909	4.0	3.0
1023_0103825	3.0	3.0
1023_0103829	2.0	3.0
1023_0103833	4.0	4.0
1023_0103844	4.0	3.0
1023_0106816	3.0	3.0
1023_0107074	4.0	3.0
1023_0107244	3.0	3.0
1023_0107729	3.0	3.0
1023_0107787	2.0	3.0
1023_0107788	3.0	3.0
1023_0108307	3.0	3.0
1023_0108426	3.0	3.0
1023_0108648	3.0	3.0
1023_0108933	3.0	3.0
1023_0108993	3.0	3.0
1023_0109026	2.0	3.0
1023_0109030	3.0	3.0
1023_0109038	3.0	3.0
1023_0109039	3.0	3.0
1023_0109096	3.0	3.0
1023_0109248	2.0	3.0
1023_0109396	3.0	3.0
1023_0109495	3.0	3.0
1023_0109500	2.0	3.0
1023_0109518	2.0	3.0
1023_0109520	3.0	3.0
1023_0109528	3.0	3.0
1023_0109588	3.0	3.0
1023_0109651	3.0	3.0
1023_0109891	3.0	3.0
1023_0109945	4.0	3.0
1023_0109947	3.0	3.0
1031_0002004	4.0	4.0
1031_0002083	3.0	4.0
1031_0002131	3.0	4.0
1031_0002196	4.0	4.0
1031_0002199	4.0	4.0
1031_0003023	4.0	4.0
1031_0003029	4.0	4.0
1031_0003076	4.0	4.0
1031_0003078	4.0	4.0
1031_0003090	4.0	4.0
1031_0003095	3.0	3.0
1031_0003097	4.0	4.0
1031_0003099	3.0	4.0
1031_0003130	5.0	4.0
1031_0003135	4.0	4.0
1031_0003144	3.0	4.0
1031_0003154	4.0	4.0
1031_0003174	4.0	4.0
1031_0003214	3.0	4.0
1031_0003216	3.0	4.0
1031_0003220	3.0	3.0
1031_0003224	3.0	4.0
1031_0003231	4.0	4.0
1031_0003232	3.0	4.0
1031_0003233	3.0	3.0
1031_0003309	3.0	4.0
1031_0003313	4.0	4.0
1031_0003330	4.0	4.0
1031_0003339	4.0	4.0
1031_0003356	3.0	3.0
1031_0003387	4.0	4.0
1031_0003388	4.0	4.0
1031_0003392	4.0	4.0
1031_0003419	4.0	4.0
1061_0120271	2.0	2.0
1061_0120278	2.0	2.0
1061_0120281	2.0	2.0
1061_0120285	2.0	2.0
1061_0120288	2.0	2.0
1061_0120291	1.0	1.0
1061_0120298	2.0	2.0
1061_0120301	2.0	2.0
1061_0120303	1.0	2.0
1061_0120308	3.0	2.0
1061_0120309	2.0	1.0
1061_0120317	3.0	2.0
1061_0120326	2.0	2.0
1061_0120328	2.0	2.0
1061_0120329	2.0	2.0
1061_0120331	1.0	1.0
1061_0120347	2.0	2.0
1061_0120349	1.0	2.0
1061_0120353	1.0	1.0
1061_0120354	2.0	2.0
1061_0120357	3.0	3.0
1061_0120373	2.0	2.0
1061_0120383	3.0	3.0
1061_0120386	1.0	2.0
1061_0120387	2.0	2.0
1061_0120424	2.0	2.0
1061_0120427	2.0	2.0
1061_0120428	2.0	2.0
1061_0120429	3.0	2.0
1061_0120448	3.0	2.0
1061_0120453	2.0	2.0
1061_0120455	2.0	2.0
1061_0120457	2.0	2.0
1061_0120484	2.0	2.0
1061_0120493	2.0	2.0
1061_0120495	2.0	2.0
1061_0120499	2.0	2.0
1061_0120500	2.0	2.0
1061_0120877	2.0	2.0
1061_0120881	3.0	3.0
1061_0120885	2.0	2.0
1061_0120888	2.0	2.0
1061_1029113	2.0	2.0
1061_1029116	1.0	2.0
1061_1202910	2.0	2.0
1061_1202915	1.0	2.0
1071_0024683	0.0	1.0
1071_0024685	2.0	2.0
1071_0024686	2.0	2.0
1071_0024690	2.0	2.0
1071_0024694	2.0	2.0
1071_0024701	2.0	2.0
1071_0024712	1.0	1.0
1071_0024713	2.0	2.0
1071_0024714	2.0	2.0
1071_0024766	1.0	1.0
1071_0024774	0.0	0.0
1071_0024777	1.0	1.0
1071_0024781	1.0	1.0
1071_0024797	0.0	1.0
1071_0024798	0.0	1.0
1071_0024801	1.0	1.0
1071_0024804	1.0	1.0
1071_0024815	1.0	1.0
1071_0024816	1.0	1.0
1071_0024818	2.0	1.0
1071_0024820	1.0	1.0
1071_0024822	1.0	1.0
1071_0024825	1.0	1.0
1071_0024836	2.0	2.0
1071_0024840	1.0	1.0
1071_0024847	2.0	2.0
1071_0024850	1.0	1.0
1071_0024852	0.0	0.0
1071_0024859	2.0	2.0
1071_0024863	2.0	1.0
1071_0024864	0.0	1.0
1071_0024873	0.0	1.0
1071_0242012	2.0	2.0
1071_0242041	1.0	1.0
1071_0242073	1.0	1.0
1071_0242093	0.0	1.0
1071_0243581	1.0	1.0
1071_0243582	1.0	1.0
1071_0243593	1.0	2.0
1071_0248301	2.0	1.0
1071_0248307	2.0	1.0
1071_0248316	1.0	1.0
1071_0248325	0.0	1.0
1071_0248328	0.0	1.0
1071_0248332	2.0	2.0
1091_0000002	2.0	2.0
1091_0000003	2.0	1.0
1091_0000015	1.0	2.0
1091_0000019	1.0	1.0
1091_0000024	2.0	1.0
1091_0000033	1.0	2.0
1091_0000042	1.0	1.0
1091_0000047	2.0	1.0
1091_0000060	2.0	2.0
1091_0000069	2.0	1.0
1091_0000075	1.0	2.0
1091_0000086	1.0	2.0
1091_0000087	2.0	2.0
1091_0000123	2.0	2.0
1091_0000164	2.0	1.0
1091_0000171	2.0	2.0
1091_0000192	2.0	2.0
1091_0000199	2.0	2.0
1091_0000202	2.0	2.0
1091_0000210	2.0	2.0
1091_0000212	1.0	2.0
1091_0000219	1.0	2.0
1091_0000229	2.0	2.0
1091_0000233	2.0	2.0
1091_0000237	1.0	2.0
1091_0000239	2.0	2.0
1091_0000241	2.0	2.0
1091_0000251	2.0	2.0
1091_0000265	2.0	2.0
1091_0000268	2.0	2.0
1091_0000272	1.0	2.0
0602	2.0	2.0
0616	1.0	2.0
0621	2.0	2.0
0623	2.0	2.0
0626	2.0	2.0
0630	1.0	1.0
0632	1.0	1.0
0634	2.0	2.0
0635	1.0	2.0
0637	2.0	2.0
0639	1.0	2.0
0715	2.0	2.0
0719	2.0	2.0
0720	2.0	2.0
0812	2.0	1.0
0814	1.0	2.0
0818	2.0	2.0
0819	2.0	2.0
0825	1.0	2.0
0827	1.0	2.0
0901	2.0	2.0
0904	2.0	2.0
0911	2.0	2.0
0913	2.0	2.0
0914	2.0	2.0
0917	1.0	2.0
0927	1.0	2.0
0928	2.0	2.0
1005	2.0	2.0
1018	1.0	2.0
1111	2.0	2.0
1112	2.0	2.0
1116	2.0	2.0
9999	0.0	1.0
BER0609003	3.0	3.0
BER0611003	2.0	3.0
LIB0611001A	1.0	1.0
LIB0611004B	1.0	1.0
LON0611004A	1.0	1.0
PAR1011016	3.0	3.0
PHA0111001B	1.0	1.0
PHA0111003A	1.0	1.0
PHA0111004A	1.0	1.0
PHA0111016	3.0	3.0
PHA0111018	3.0	3.0
PHA0112007B	1.0	1.0
PHA0209038	4.0	3.0
PHA0210008	1.0	1.0
PHA0411035	2.0	3.0
PHA0411038	3.0	3.0
PHA0411041	3.0	3.0
PHA0411051	4.0	3.0
PHA0411059	3.0	3.0
PHA0509018	3.0	3.0
PHA0510010A	2.0	1.0
PHA0510013B	1.0	1.0
PHA0510023	3.0	3.0
PHA0510029	3.0	3.0
PHA0610005A	1.0	1.0
PHA0610007B	1.0	1.0
PHA0610019A	2.0	1.0
PHA0610019B	1.0	1.0
PHA0610026	3.0	3.0
PHA0710010	3.0	3.0
PHA0710013	4.0	3.0
PHA0710014	3.0	3.0
PHA0710016	3.0	3.0
PHA0810002	3.0	3.0
PHA0810003	3.0	3.0
PHA0810008	3.0	3.0
PHA0810011	3.0	3.0
PHA1109003	2.0	2.0
PHA1109006	2.0	3.0
PHA1109024	3.0	3.0
PHA1110001A	1.0	1.0
PHA1110002A	2.0	2.0
PHA1110013	3.0	3.0
PHA1110017	3.0	3.0
PHA1111006A	1.0	1.0
ST071122B	1.0	1.0
VAR0909003	3.0	3.0
VAR0909004	3.0	3.0
VAR0909007	3.0	3.0
VAR0910006	3.0	3.0
VAR0910009	3.0	3.0
VAR0910011	3.0	3.0
1325_1001015	3.0	3.0
1325_1001025	2.0	3.0
1325_1001036	3.0	3.0
1325_1001051	3.0	3.0
1325_1001053	2.0	2.0
1325_1001055	3.0	3.0
1325_1001084	3.0	3.0
1325_1001089	3.0	2.0
1325_1001091	3.0	3.0
1325_1001092	2.0	2.0
1325_1001094	2.0	3.0
1325_1001097	1.0	2.0
1325_1001098	3.0	3.0
1325_1001110	3.0	3.0
1325_1001120	3.0	3.0
1325_1001126	2.0	3.0
1325_1001127	3.0	3.0
1325_1001129	2.0	3.0
1325_1001132	3.0	3.0
1325_1001133	3.0	3.0
1325_1001141	2.0	3.0
1325_1001154	3.0	3.0
1325_1001155	3.0	3.0
1325_1001156	3.0	3.0
1325_1001163	2.0	3.0
1325_1001165	2.0	3.0
1325_1001167	3.0	3.0
1325_1001169	3.0	3.0
1325_9000087	2.0	3.0
1325_9000089	2.0	3.0
1325_9000104	3.0	3.0
1325_9000214	3.0	3.0
1325_9000279	3.0	3.0
1325_9000296	3.0	3.0
1325_9000302	3.0	3.0
1325_9000303	3.0	3.0
1325_9000316	3.0	3.0
1325_9000318	3.0	3.0
1325_9000319	3.0	3.0
1325_9000323	3.0	3.0
1325_9000533	3.0	3.0
1325_9000611	3.0	3.0
1365_0100004	2.0	2.0
1365_0100015	2.0	2.0
1365_0100019	2.0	2.0
1365_0100022	2.0	2.0
1365_0100024	2.0	2.0
1365_0100027	2.0	2.0
1365_0100063	3.0	3.0
1365_0100065	1.0	2.0
1365_0100069	2.0	2.0
1365_0100072	2.0	2.0
1365_0100100	2.0	3.0
1365_0100116	3.0	2.0
1365_0100119	3.0	3.0
1365_0100133	2.0	2.0
1365_0100148	2.0	2.0
1365_0100162	2.0	2.0
1365_0100168	2.0	2.0
1365_0100181	2.0	2.0
1365_0100183	2.0	2.0
1365_0100200	3.0	2.0
1365_0100221	2.0	2.0
1365_0100222	3.0	3.0
1365_0100225	2.0	2.0
1365_0100226	3.0	2.0
1365_0100227	3.0	2.0
1365_0100228	2.0	2.0
1365_0100255	2.0	2.0
1365_0100282	2.0	2.0
1365_0100287	2.0	2.0
1365_0100456	2.0	2.0
1365_0100476	2.0	2.0
1365_0100481	2.0	2.0
1385_0000016	1.0	1.0
1385_0000035	1.0	1.0
1385_0000039	1.0	1.0
1385_0000048	1.0	1.0
1385_0000050	1.0	1.0
1385_0000051	2.0	2.0
1385_0000098	2.0	1.0
1385_0000100	1.0	1.0
1385_0000114	2.0	1.0
1385_0000124	2.0	1.0
1385_0001105	1.0	1.0
1385_0001119	2.0	1.0
1385_0001134	1.0	1.0
1385_0001135	1.0	1.0
1385_0001148	2.0	1.0
1385_0001150	1.0	1.0
1385_0001151	2.0	2.0
1385_0001159	1.0	1.0
1385_0001169	1.0	1.0
1385_0001170	1.0	1.0
1385_0001189	1.0	1.0
1385_0001196	1.0	1.0
1385_0001197	1.0	1.0
1385_0001523	1.0	2.0
1385_0001527	2.0	1.0
1385_0001717	1.0	1.0
1385_0001723	0.0	1.0
1385_0001741	0.0	1.0
1385_0001747	1.0	1.0
1385_0001751	1.0	2.0
1385_0001753	1.0	1.0
1385_0001754	1.0	1.0
1385_0001756	1.0	1.0
1385_0001766	2.0	2.0
1385_0001767	1.0	1.0
1385_0001771	1.0	1.0
1385_0001774	0.0	1.0
1385_0001786	1.0	1.0
1385_0001791	1.0	1.0
1385_0001795	0.0	1.0
1385_0001796	1.0	1.0
1385_0001800	1.0	1.0
1395_0000337	1.0	1.0
1395_0000360	3.0	2.0
1395_0000361	2.0	2.0
1395_0000369	2.0	2.0
1395_0000378	2.0	2.0
1395_0000389	1.0	1.0
1395_0000390	1.0	1.0
1395_0000402	2.0	1.0
1395_0000448	2.0	1.0
1395_0000452	1.0	1.0
1395_0000499	2.0	1.0
1395_0000512	2.0	2.0
1395_0000514	3.0	2.0
1395_0000516	1.0	1.0
1395_0000518	2.0	2.0
1395_0000527	1.0	1.0
1395_0000533	2.0	2.0
1395_0000554	2.0	2.0
1395_0000560	2.0	2.0
1395_0000564	2.0	2.0
1395_0000572	1.0	1.0
1395_0000579	1.0	1.0
1395_0000584	1.0	1.0
1395_0000591	0.0	1.0
1395_0000597	1.0	1.0
1395_0000598	1.0	1.0
1395_0000630	1.0	2.0
1395_0000635	1.0	1.0
1395_0000642	1.0	1.0
1395_0001016	1.0	1.0
1395_0001021	1.0	1.0
1395_0001022	1.0	2.0
1395_0001065	1.0	2.0
1395_0001071	2.0	1.0
1395_0001075	1.0	2.0
1395_0001076	1.0	2.0
1395_0001114	0.0	1.0
1395_0001115	2.0	2.0
1395_0001122	0.0	1.0
1395_0001124	1.0	1.0
1395_0001131	0.0	1.0
1395_0001141	2.0	1.0
1395_0001145	2.0	2.0
1395_0001149	1.0	1.0
1395_0001160	1.0	2.0
1395_0001161	1.0	2.0
4 Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 1.07
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.82
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        20
         1.0       0.67      0.68      0.67       117
         2.0       0.64      0.79      0.71       164
         3.0       0.66      0.72      0.69       120
         4.0       0.00      0.00      0.00        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.65       452
   macro avg       0.33      0.37      0.35       452
weighted avg       0.58      0.65      0.62       452

[[  0  17   3   0   0   0]
 [  0  79  38   0   0   0]
 [  0  21 130  13   0   0]
 [  0   1  32  87   0   0]
 [  0   0   0  30   0   0]
 [  0   0   0   1   0   0]]
0.6151247055017527
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.76
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.93
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        20
         1.0       0.66      0.44      0.53       117
         2.0       0.59      0.72      0.65       164
         3.0       0.71      0.81      0.75       120
         4.0       0.68      0.87      0.76        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.65       452
   macro avg       0.44      0.47      0.45       452
weighted avg       0.62      0.65      0.62       452

[[  0  16   4   0   0   0]
 [  0  51  66   0   0   0]
 [  0  10 118  36   0   0]
 [  0   0  12  97  11   0]
 [  0   0   0   4  26   0]
 [  0   0   0   0   1   0]]
0.6224999298790552
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.67
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        20
         1.0       0.70      0.65      0.67       117
         2.0       0.66      0.73      0.69       164
         3.0       0.75      0.74      0.74       120
         4.0       0.68      0.93      0.79        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.69       452
   macro avg       0.46      0.51      0.48       452
weighted avg       0.66      0.69      0.68       452

[[  0  17   3   0   0   0]
 [  0  76  41   0   0   0]
 [  0  16 120  28   0   0]
 [  0   0  19  89  12   0]
 [  0   0   0   2  28   0]
 [  0   0   0   0   1   0]]
0.6751195067867923
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.56
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.77
              precision    recall  f1-score   support

         0.0       1.00      0.10      0.18        20
         1.0       0.72      0.74      0.73       117
         2.0       0.71      0.73      0.72       164
         3.0       0.75      0.76      0.76       120
         4.0       0.68      0.90      0.77        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.72       452
   macro avg       0.64      0.54      0.53       452
weighted avg       0.73      0.72      0.71       452

[[  2  16   2   0   0   0]
 [  0  87  30   0   0   0]
 [  0  18 119  27   0   0]
 [  0   0  17  91  12   0]
 [  0   0   0   3  27   0]
 [  0   0   0   0   1   0]]
0.7090834239037971
452 452 452
Filename	True Label	Prediction
1023_0001416	3.0	3.0
1023_0001423	2.0	3.0
1023_0001575	3.0	3.0
1023_0101683	3.0	3.0
1023_0101854	2.0	3.0
1023_0101896	3.0	3.0
1023_0101897	3.0	3.0
1023_0101904	2.0	3.0
1023_0102117	3.0	3.0
1023_0103822	2.0	3.0
1023_0103826	3.0	3.0
1023_0103830	3.0	3.0
1023_0103834	3.0	3.0
1023_0103839	3.0	3.0
1023_0103880	3.0	3.0
1023_0107042	3.0	3.0
1023_0107672	2.0	3.0
1023_0107773	3.0	3.0
1023_0107781	3.0	3.0
1023_0108304	3.0	3.0
1023_0108305	3.0	3.0
1023_0108422	3.0	3.0
1023_0108520	3.0	3.0
1023_0108751	3.0	3.0
1023_0108753	3.0	3.0
1023_0108811	3.0	3.0
1023_0108813	3.0	3.0
1023_0108814	3.0	3.0
1023_0108890	3.0	3.0
1023_0108932	3.0	3.0
1023_0108935	2.0	3.0
1023_0109029	2.0	2.0
1023_0109192	3.0	3.0
1023_0109249	3.0	3.0
1023_0109391	2.0	3.0
1023_0109399	2.0	3.0
1023_0109402	3.0	3.0
1023_0109496	3.0	3.0
1023_0109527	3.0	3.0
1023_0109671	3.0	3.0
1023_0109721	3.0	3.0
1023_0109915	2.0	2.0
1023_0109917	3.0	3.0
1023_0109954	3.0	3.0
1023_0111896	3.0	3.0
1031_0002005	4.0	4.0
1031_0002010	3.0	4.0
1031_0002032	3.0	4.0
1031_0002036	4.0	4.0
1031_0002043	4.0	4.0
1031_0002086	4.0	4.0
1031_0002185	4.0	4.0
1031_0003013	4.0	4.0
1031_0003054	4.0	4.0
1031_0003063	5.0	4.0
1031_0003077	4.0	4.0
1031_0003121	4.0	4.0
1031_0003128	4.0	4.0
1031_0003129	4.0	4.0
1031_0003145	4.0	3.0
1031_0003146	4.0	4.0
1031_0003149	4.0	4.0
1031_0003150	4.0	4.0
1031_0003162	4.0	4.0
1031_0003172	3.0	4.0
1031_0003173	4.0	4.0
1031_0003179	4.0	4.0
1031_0003184	4.0	4.0
1031_0003186	4.0	4.0
1031_0003187	4.0	4.0
1031_0003191	4.0	4.0
1031_0003219	3.0	4.0
1031_0003221	3.0	4.0
1031_0003225	3.0	4.0
1031_0003230	4.0	4.0
1031_0003235	4.0	4.0
1031_0003236	3.0	4.0
1031_0003239	4.0	4.0
1031_0003261	3.0	4.0
1031_0003262	3.0	4.0
1031_0003336	3.0	4.0
1031_0003338	4.0	4.0
1031_0003359	3.0	4.0
1031_0003383	4.0	4.0
1031_0003391	3.0	3.0
1031_0003408	3.0	4.0
1031_0003415	4.0	4.0
1061_0120274	1.0	2.0
1061_0120277	1.0	2.0
1061_0120287	1.0	2.0
1061_0120295	0.0	2.0
1061_0120306	2.0	2.0
1061_0120310	3.0	2.0
1061_0120312	1.0	1.0
1061_0120313	2.0	1.0
1061_0120316	2.0	2.0
1061_0120319	3.0	2.0
1061_0120335	2.0	3.0
1061_0120338	2.0	2.0
1061_0120346	2.0	2.0
1061_0120350	2.0	2.0
1061_0120352	1.0	2.0
1061_0120356	2.0	2.0
1061_0120366	3.0	2.0
1061_0120367	3.0	2.0
1061_0120372	2.0	2.0
1061_0120374	3.0	3.0
1061_0120375	2.0	2.0
1061_0120389	2.0	2.0
1061_0120394	2.0	2.0
1061_0120403	2.0	2.0
1061_0120404	2.0	2.0
1061_0120410	2.0	2.0
1061_0120411	3.0	3.0
1061_0120413	1.0	2.0
1061_0120421	2.0	2.0
1061_0120430	2.0	2.0
1061_0120432	2.0	2.0
1061_0120433	1.0	2.0
1061_0120449	2.0	2.0
1061_0120478	2.0	2.0
1061_0120481	3.0	3.0
1061_0120486	2.0	2.0
1061_0120855	2.0	2.0
1061_0120874	2.0	2.0
1061_0120894	2.0	2.0
1061_1029112	3.0	3.0
1061_1202911	1.0	2.0
1061_1202914	2.0	2.0
1061_1202918	2.0	2.0
1061_1202919	2.0	2.0
1071_0024681	2.0	2.0
1071_0024687	1.0	1.0
1071_0024692	2.0	2.0
1071_0024702	2.0	2.0
1071_0024704	1.0	1.0
1071_0024756	2.0	1.0
1071_0024757	2.0	2.0
1071_0024767	2.0	2.0
1071_0024768	1.0	1.0
1071_0024779	2.0	1.0
1071_0024783	0.0	0.0
1071_0024799	2.0	2.0
1071_0024813	0.0	1.0
1071_0024817	1.0	1.0
1071_0024826	2.0	2.0
1071_0024831	1.0	1.0
1071_0024845	0.0	1.0
1071_0024853	1.0	1.0
1071_0024854	0.0	1.0
1071_0024855	1.0	1.0
1071_0024861	0.0	1.0
1071_0024866	2.0	2.0
1071_0024867	2.0	2.0
1071_0024871	1.0	1.0
1071_0024872	1.0	2.0
1071_0241831	1.0	1.0
1071_0242071	0.0	1.0
1071_0242091	1.0	1.0
1071_0242092	0.0	0.0
1071_0243502	1.0	1.0
1071_0248303	1.0	1.0
1071_0248313	2.0	1.0
1071_0248327	1.0	1.0
1071_0248331	1.0	1.0
1071_0248333	2.0	1.0
1071_0248345	2.0	2.0
1071_0248347	1.0	1.0
1071_0248350	2.0	1.0
1091_0000005	2.0	2.0
1091_0000016	0.0	1.0
1091_0000017	2.0	2.0
1091_0000018	2.0	2.0
1091_0000023	2.0	1.0
1091_0000031	1.0	1.0
1091_0000035	2.0	1.0
1091_0000037	1.0	1.0
1091_0000045	2.0	2.0
1091_0000050	1.0	1.0
1091_0000065	1.0	1.0
1091_0000066	2.0	1.0
1091_0000079	1.0	2.0
1091_0000092	1.0	2.0
1091_0000140	2.0	1.0
1091_0000146	1.0	1.0
1091_0000152	1.0	1.0
1091_0000154	2.0	3.0
1091_0000155	2.0	3.0
1091_0000169	3.0	2.0
1091_0000170	3.0	2.0
1091_0000173	2.0	2.0
1091_0000194	1.0	2.0
1091_0000204	2.0	2.0
1091_0000209	2.0	2.0
1091_0000215	2.0	2.0
1091_0000222	2.0	2.0
1091_0000224	2.0	1.0
1091_0000234	3.0	2.0
1091_0000235	1.0	1.0
1091_0000240	2.0	1.0
1091_0000256	1.0	2.0
1091_0000260	2.0	2.0
1091_0000264	2.0	2.0
1091_0000266	2.0	2.0
1091_0000271	2.0	2.0
1091_0000274	1.0	2.0
1091_0000275	2.0	2.0
0601	2.0	2.0
0604	2.0	2.0
0606	2.0	2.0
0612	1.0	2.0
0629	1.0	2.0
0638	1.0	2.0
0640	2.0	2.0
0643	2.0	2.0
0716	2.0	2.0
0723	1.0	2.0
0724	2.0	2.0
0725	2.0	2.0
0802	1.0	1.0
0804	1.0	2.0
0809	2.0	2.0
0821	2.0	2.0
0824	2.0	2.0
0903	2.0	2.0
0907	2.0	2.0
0918	2.0	2.0
0919	1.0	1.0
0920	2.0	2.0
0922	2.0	2.0
0926	2.0	2.0
1001	2.0	2.0
1002	2.0	2.0
1004	2.0	2.0
1006	2.0	2.0
1017	1.0	2.0
1113	2.0	2.0
BER0611007	3.0	3.0
KYJ0611003A	1.0	1.0
LIB0611001B	1.0	1.0
LON0610002A	1.0	1.0
LON0611002A	1.0	1.0
MOS0611013	3.0	3.0
PAR1011013	3.0	3.0
PAR1011015	3.0	3.0
PAR1011017	3.0	3.0
PHA0111001A	1.0	1.0
PHA0111002A	1.0	1.0
PHA0111005A	1.0	1.0
PHA0112003B	1.0	1.0
PHA0112009B	1.0	1.0
PHA0209008	1.0	1.0
PHA0209039	3.0	3.0
PHA0210007	1.0	1.0
PHA0411008B	2.0	1.0
PHA0411009B	1.0	1.0
PHA0411010A	0.0	1.0
PHA0411011B	1.0	1.0
PHA0411033	3.0	3.0
PHA0411036	3.0	3.0
PHA0411053	4.0	3.0
PHA0411055	3.0	3.0
PHA0509002	1.0	1.0
PHA0509022	4.0	3.0
PHA0509024	3.0	3.0
PHA0509037	2.0	3.0
PHA0510010B	0.0	1.0
PHA0510036	3.0	3.0
PHA0510040	3.0	3.0
PHA0510049	2.0	3.0
PHA0610006B	1.0	1.0
PHA0610018	3.0	3.0
PHA0710018	3.0	3.0
PHA0810001	3.0	3.0
PHA0810004	3.0	3.0
PHA1109005	2.0	2.0
PHA1109008	1.0	1.0
PHA1109025	2.0	1.0
PHA1110001B	1.0	1.0
PHA1110019	3.0	3.0
PHA1110022	3.0	3.0
PHA1111001B	1.0	1.0
PHA1111003B	1.0	1.0
PHA1111004A	1.0	1.0
VAR0909008	3.0	3.0
VAR0909009	3.0	3.0
VAR0910007	3.0	3.0
1325_1001012	3.0	3.0
1325_1001016	2.0	3.0
1325_1001017	3.0	3.0
1325_1001020	3.0	2.0
1325_1001021	3.0	3.0
1325_1001024	3.0	3.0
1325_1001029	3.0	3.0
1325_1001033	3.0	3.0
1325_1001039	3.0	3.0
1325_1001041	3.0	3.0
1325_1001046	2.0	3.0
1325_1001047	3.0	2.0
1325_1001063	2.0	2.0
1325_1001075	2.0	3.0
1325_1001076	3.0	3.0
1325_1001078	3.0	3.0
1325_1001081	3.0	3.0
1325_1001088	2.0	3.0
1325_1001107	3.0	3.0
1325_1001119	3.0	3.0
1325_1001128	3.0	3.0
1325_1001134	2.0	3.0
1325_1001152	3.0	3.0
1325_1001153	2.0	3.0
1325_1001159	3.0	3.0
1325_1001168	3.0	3.0
1325_9000090	2.0	3.0
1325_9000099	2.0	3.0
1325_9000136	3.0	3.0
1325_9000152	3.0	3.0
1325_9000187	3.0	3.0
1325_9000209	3.0	3.0
1325_9000211	3.0	3.0
1325_9000213	3.0	2.0
1325_9000241	3.0	3.0
1325_9000278	3.0	3.0
1325_9000314	3.0	3.0
1325_9000315	2.0	2.0
1325_9000536	3.0	3.0
1325_9000676	3.0	3.0
1365_0100005	2.0	2.0
1365_0100007	1.0	2.0
1365_0100008	2.0	2.0
1365_0100011	2.0	2.0
1365_0100014	2.0	2.0
1365_0100028	2.0	2.0
1365_0100051	2.0	2.0
1365_0100061	3.0	2.0
1365_0100064	2.0	2.0
1365_0100067	2.0	2.0
1365_0100074	2.0	2.0
1365_0100079	2.0	2.0
1365_0100092	2.0	2.0
1365_0100097	2.0	2.0
1365_0100099	2.0	2.0
1365_0100101	3.0	2.0
1365_0100102	3.0	2.0
1365_0100105	3.0	2.0
1365_0100107	2.0	3.0
1365_0100137	2.0	2.0
1365_0100139	2.0	2.0
1365_0100169	2.0	2.0
1365_0100171	2.0	2.0
1365_0100173	2.0	2.0
1365_0100174	2.0	2.0
1365_0100190	2.0	3.0
1365_0100199	2.0	2.0
1365_0100204	2.0	2.0
1365_0100213	2.0	2.0
1365_0100217	3.0	3.0
1365_0100223	2.0	3.0
1365_0100224	3.0	3.0
1365_0100231	2.0	2.0
1365_0100233	2.0	2.0
1365_0100260	2.0	2.0
1365_0100262	3.0	2.0
1365_0100269	2.0	2.0
1365_0100275	3.0	2.0
1365_0100276	3.0	3.0
1365_0100285	2.0	2.0
1365_0100290	2.0	2.0
1365_0100455	2.0	3.0
1365_0100457	2.0	3.0
1365_0100458	2.0	2.0
1365_0100471	2.0	3.0
1365_0100475	2.0	2.0
1365_0100479	2.0	2.0
1385_0000011	0.0	1.0
1385_0000013	1.0	1.0
1385_0000023	1.0	1.0
1385_0000037	1.0	1.0
1385_0000049	1.0	1.0
1385_0000052	1.0	1.0
1385_0000053	1.0	1.0
1385_0000099	1.0	1.0
1385_0000104	2.0	1.0
1385_0000123	1.0	1.0
1385_0000125	2.0	2.0
1385_0000126	1.0	1.0
1385_0001113	1.0	1.0
1385_0001126	0.0	1.0
1385_0001128	1.0	1.0
1385_0001156	1.0	1.0
1385_0001163	1.0	1.0
1385_0001164	1.0	1.0
1385_0001171	0.0	1.0
1385_0001172	1.0	1.0
1385_0001174	1.0	1.0
1385_0001178	1.0	1.0
1385_0001190	1.0	1.0
1385_0001501	1.0	1.0
1385_0001503	1.0	1.0
1385_0001524	1.0	1.0
1385_0001525	1.0	1.0
1385_0001712	1.0	2.0
1385_0001714	0.0	1.0
1385_0001719	1.0	1.0
1385_0001725	1.0	1.0
1385_0001726	1.0	1.0
1385_0001730	1.0	1.0
1385_0001732	1.0	1.0
1385_0001736	1.0	2.0
1385_0001737	1.0	1.0
1385_0001739	1.0	2.0
1385_0001740	1.0	1.0
1385_0001758	1.0	1.0
1385_0001764	1.0	1.0
1385_0001773	0.0	1.0
1385_0001775	1.0	1.0
1385_0001787	0.0	1.0
1385_0001790	1.0	1.0
1385_0001792	1.0	2.0
1385_0001799	2.0	2.0
1395_0000338	2.0	2.0
1395_0000341	2.0	1.0
1395_0000359	2.0	2.0
1395_0000380	2.0	2.0
1395_0000383	2.0	2.0
1395_0000387	3.0	2.0
1395_0000392	2.0	2.0
1395_0000396	2.0	2.0
1395_0000415	2.0	1.0
1395_0000443	2.0	2.0
1395_0000450	1.0	2.0
1395_0000458	1.0	1.0
1395_0000471	2.0	2.0
1395_0000504	2.0	1.0
1395_0000513	2.0	2.0
1395_0000534	2.0	2.0
1395_0000581	1.0	2.0
1395_0000595	1.0	1.0
1395_0000599	1.0	1.0
1395_0000602	1.0	1.0
1395_0000612	0.0	2.0
1395_0001013	1.0	1.0
1395_0001017	1.0	1.0
1395_0001023	1.0	1.0
1395_0001033	1.0	2.0
1395_0001068	1.0	2.0
1395_0001078	0.0	1.0
1395_0001117	1.0	1.0
1395_0001118	1.0	1.0
1395_0001123	1.0	2.0
1395_0001150	0.0	1.0
1395_0001167	1.0	2.0
5 Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 1.06
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.74
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        20
         1.0       0.67      0.72      0.69       117
         2.0       0.72      0.71      0.72       164
         3.0       0.75      0.78      0.77       120
         4.0       0.55      0.73      0.63        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.70       452
   macro avg       0.45      0.49      0.47       452
weighted avg       0.67      0.70      0.69       452

[[  0  17   3   0   0   0]
 [  0  84  33   0   0   0]
 [  0  24 117  23   0   0]
 [  0   0   9  94  17   0]
 [  0   0   0   8  22   0]
 [  0   0   0   0   1   0]]
0.6855745739316698
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.74
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.74
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        20
         1.0       0.64      0.67      0.66       117
         2.0       0.69      0.66      0.68       164
         3.0       0.72      0.82      0.76       120
         4.0       0.57      0.70      0.63        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.68       452
   macro avg       0.44      0.47      0.45       452
weighted avg       0.65      0.68      0.66       452

[[  0  18   2   0   0   0]
 [  0  78  39   0   0   0]
 [  0  25 109  30   0   0]
 [  0   0   7  98  15   0]
 [  0   0   0   9  21   0]
 [  0   0   0   0   1   0]]
0.6601537666982981
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.63
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.72
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        20
         1.0       0.65      0.79      0.72       117
         2.0       0.76      0.66      0.71       164
         3.0       0.76      0.78      0.77       120
         4.0       0.55      0.80      0.65        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.71       452
   macro avg       0.45      0.51      0.47       452
weighted avg       0.68      0.71      0.69       452

[[  0  18   2   0   0   0]
 [  0  93  24   0   0   0]
 [  0  31 109  24   0   0]
 [  0   0   8  93  19   0]
 [  0   0   0   6  24   0]
 [  0   0   0   0   1   0]]
0.6898019986573215
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.53
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.73
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        20
         1.0       0.64      0.73      0.68       117
         2.0       0.72      0.70      0.71       164
         3.0       0.76      0.81      0.78       120
         4.0       0.59      0.67      0.62        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.70       452
   macro avg       0.45      0.48      0.47       452
weighted avg       0.67      0.70      0.68       452

[[  0  18   2   0   0   0]
 [  0  85  32   0   0   0]
 [  0  29 114  21   0   0]
 [  0   0  10  97  13   0]
 [  0   0   0  10  20   0]
 [  0   0   0   0   1   0]]
0.6827980296829848
452 452 452
Filename	True Label	Prediction
1023_0001418	3.0	3.0
1023_0001419	3.0	3.0
1023_0001420	3.0	3.0
1023_0101684	3.0	3.0
1023_0101749	4.0	3.0
1023_0101844	3.0	3.0
1023_0101846	4.0	3.0
1023_0101853	3.0	3.0
1023_0101893	3.0	3.0
1023_0103823	4.0	3.0
1023_0103828	2.0	2.0
1023_0103836	3.0	3.0
1023_0103955	4.0	3.0
1023_0104206	3.0	3.0
1023_0104207	3.0	3.0
1023_0104209	3.0	3.0
1023_0107682	2.0	3.0
1023_0107725	3.0	3.0
1023_0107726	3.0	3.0
1023_0108423	3.0	3.0
1023_0108650	3.0	3.0
1023_0108888	3.0	3.0
1023_0108889	3.0	3.0
1023_0108934	3.0	3.0
1023_0109151	4.0	3.0
1023_0109267	3.0	3.0
1023_0109395	3.0	3.0
1023_0109519	2.0	3.0
1023_0109674	3.0	3.0
1023_0109716	3.0	2.0
1023_0109890	4.0	3.0
1023_0109914	2.0	2.0
1023_0109946	3.0	3.0
1031_0001703	4.0	4.0
1031_0001951	3.0	4.0
1031_0001997	4.0	4.0
1031_0002003	3.0	3.0
1031_0002006	4.0	4.0
1031_0002040	4.0	4.0
1031_0002061	3.0	4.0
1031_0002079	5.0	4.0
1031_0002085	4.0	4.0
1031_0002087	4.0	3.0
1031_0002088	4.0	4.0
1031_0002187	4.0	4.0
1031_0002195	4.0	3.0
1031_0002200	3.0	4.0
1031_0003042	3.0	4.0
1031_0003052	4.0	4.0
1031_0003065	3.0	4.0
1031_0003072	3.0	4.0
1031_0003074	4.0	4.0
1031_0003085	3.0	4.0
1031_0003088	4.0	4.0
1031_0003106	4.0	4.0
1031_0003127	4.0	3.0
1031_0003133	4.0	4.0
1031_0003141	3.0	4.0
1031_0003157	4.0	4.0
1031_0003163	3.0	3.0
1031_0003164	4.0	4.0
1031_0003183	4.0	4.0
1031_0003189	4.0	4.0
1031_0003205	4.0	4.0
1031_0003237	3.0	4.0
1031_0003242	3.0	4.0
1031_0003243	3.0	4.0
1031_0003246	3.0	3.0
1031_0003249	3.0	4.0
1031_0003327	3.0	3.0
1031_0003353	3.0	3.0
1031_0003358	4.0	4.0
1031_0003366	3.0	3.0
1031_0003390	4.0	4.0
1031_0003407	3.0	4.0
1031_0003409	4.0	4.0
1061_0012029	3.0	2.0
1061_0120272	2.0	1.0
1061_0120275	2.0	3.0
1061_0120276	2.0	2.0
1061_0120279	2.0	2.0
1061_0120280	1.0	2.0
1061_0120283	1.0	1.0
1061_0120284	0.0	1.0
1061_0120296	2.0	2.0
1061_0120299	2.0	2.0
1061_0120311	3.0	2.0
1061_0120314	2.0	2.0
1061_0120320	3.0	3.0
1061_0120330	2.0	2.0
1061_0120334	2.0	3.0
1061_0120348	1.0	1.0
1061_0120351	2.0	2.0
1061_0120355	1.0	1.0
1061_0120359	2.0	2.0
1061_0120361	3.0	2.0
1061_0120371	3.0	3.0
1061_0120388	2.0	2.0
1061_0120405	3.0	2.0
1061_0120406	2.0	2.0
1061_0120414	2.0	2.0
1061_0120415	2.0	2.0
1061_0120425	2.0	2.0
1061_0120450	2.0	2.0
1061_0120456	2.0	2.0
1061_0120459	2.0	2.0
1061_0120487	2.0	2.0
1061_0120488	2.0	2.0
1061_0120490	2.0	2.0
1061_0120491	2.0	2.0
1061_0120492	2.0	2.0
1061_0120494	2.0	2.0
1061_0120496	2.0	2.0
1061_0120853	2.0	2.0
1061_0120857	2.0	2.0
1061_0120858	2.0	2.0
1061_0120859	2.0	3.0
1061_0120875	3.0	3.0
1061_0120876	2.0	2.0
1061_0120883	2.0	2.0
1061_1029115	2.0	2.0
1061_1202912	2.0	2.0
1061_1202913	2.0	2.0
1061_1202916	2.0	2.0
1071_0024689	1.0	1.0
1071_0024705	2.0	2.0
1071_0024706	1.0	1.0
1071_0024708	1.0	2.0
1071_0024716	1.0	1.0
1071_0024761	2.0	1.0
1071_0024765	0.0	1.0
1071_0024772	0.0	1.0
1071_0024827	1.0	1.0
1071_0024834	2.0	2.0
1071_0024835	1.0	1.0
1071_0024846	1.0	1.0
1071_0024849	0.0	1.0
1071_0024856	1.0	1.0
1071_0024862	2.0	2.0
1071_0024875	1.0	1.0
1071_0024879	1.0	1.0
1071_0242021	1.0	1.0
1071_0242042	1.0	1.0
1071_0242072	0.0	1.0
1071_0243501	1.0	1.0
1071_0243591	1.0	1.0
1071_0243622	1.0	1.0
1071_0248302	1.0	1.0
1071_0248305	0.0	1.0
1071_0248309	2.0	1.0
1071_0248312	1.0	1.0
1071_0248314	1.0	1.0
1071_0248319	1.0	1.0
1071_0248324	0.0	1.0
1071_0248326	1.0	1.0
1071_0248329	1.0	1.0
1071_0248337	1.0	2.0
1071_0248340	0.0	1.0
1071_0248342	1.0	1.0
1071_0248343	1.0	1.0
1091_0000004	1.0	1.0
1091_0000011	2.0	2.0
1091_0000021	2.0	2.0
1091_0000025	1.0	2.0
1091_0000026	1.0	1.0
1091_0000027	0.0	2.0
1091_0000028	1.0	1.0
1091_0000030	0.0	2.0
1091_0000032	1.0	2.0
1091_0000039	1.0	1.0
1091_0000041	1.0	1.0
1091_0000043	1.0	2.0
1091_0000046	1.0	2.0
1091_0000048	1.0	2.0
1091_0000049	1.0	1.0
1091_0000051	1.0	1.0
1091_0000054	0.0	1.0
1091_0000056	1.0	2.0
1091_0000062	2.0	1.0
1091_0000067	2.0	1.0
1091_0000072	1.0	2.0
1091_0000073	2.0	2.0
1091_0000101	2.0	1.0
1091_0000151	1.0	1.0
1091_0000157	3.0	2.0
1091_0000165	2.0	1.0
1091_0000166	1.0	2.0
1091_0000174	2.0	1.0
1091_0000185	2.0	1.0
1091_0000191	1.0	2.0
1091_0000196	2.0	2.0
1091_0000197	1.0	2.0
1091_0000200	2.0	2.0
1091_0000208	2.0	2.0
1091_0000211	1.0	2.0
1091_0000213	2.0	2.0
1091_0000214	2.0	2.0
1091_0000216	1.0	2.0
1091_0000220	1.0	2.0
1091_0000225	2.0	1.0
1091_0000227	1.0	2.0
1091_0000228	2.0	2.0
1091_0000246	2.0	2.0
1091_0000249	2.0	2.0
1091_0000250	2.0	2.0
1091_0000253	2.0	1.0
1091_0000259	2.0	2.0
1091_0000262	2.0	2.0
1091_0000267	2.0	2.0
0608	1.0	2.0
0609	1.0	2.0
0611	2.0	2.0
0613	1.0	2.0
0617	1.0	2.0
0620	2.0	2.0
0631	2.0	2.0
0644	2.0	1.0
0714	2.0	2.0
0717	2.0	2.0
0721	2.0	2.0
0803	1.0	2.0
0806	1.0	2.0
0811	2.0	2.0
0815	2.0	2.0
0816	2.0	2.0
0905	2.0	2.0
0912	2.0	2.0
0915	2.0	2.0
0916	1.0	2.0
0921	2.0	1.0
1007	2.0	2.0
1008	2.0	2.0
1009	2.0	2.0
1010	2.0	2.0
1015	2.0	2.0
1021	2.0	2.0
1023	2.0	2.0
1115	2.0	2.0
BER0611005	2.0	3.0
KYJ0611006A	0.0	1.0
KYJ0611009A	1.0	1.0
LIB0611002B	1.0	1.0
LIB0611003A	1.0	1.0
LON0611003	3.0	3.0
PAR1011009A	1.0	1.0
PHA0111003B	1.0	1.0
PHA0111004B	1.0	1.0
PHA0111010	2.0	3.0
PHA0111011	3.0	3.0
PHA0111014	2.0	3.0
PHA0112002A	1.0	1.0
PHA0112003A	1.0	1.0
PHA0112007A	1.0	1.0
PHA0112012A	2.0	2.0
PHA0209024	3.0	3.0
PHA0209034	3.0	3.0
PHA0411010B	0.0	1.0
PHA0411011A	1.0	1.0
PHA0411028	2.0	3.0
PHA0411039	3.0	3.0
PHA0411042	3.0	3.0
PHA0411045	3.0	3.0
PHA0411047	3.0	3.0
PHA0411054	3.0	3.0
PHA0411056	3.0	3.0
PHA0411061	3.0	3.0
PHA0509020	3.0	3.0
PHA0509021	2.0	2.0
PHA0509032	3.0	3.0
PHA0509033	2.0	3.0
PHA0509034	3.0	3.0
PHA0509035	3.0	3.0
PHA0509039	3.0	3.0
PHA0509040	3.0	3.0
PHA0509043	3.0	3.0
PHA0510004A	1.0	1.0
PHA0510004B	1.0	1.0
PHA0510030	3.0	3.0
PHA0510031	3.0	3.0
PHA0510038	3.0	3.0
PHA0510046	3.0	3.0
PHA0610005B	0.0	1.0
PHA0610025	3.0	3.0
PHA0710012	3.0	3.0
PHA0710015	3.0	3.0
PHA0710017	3.0	3.0
PHA0710021	3.0	3.0
PHA0809009	3.0	3.0
PHA0810009	3.0	3.0
PHA0810015	3.0	3.0
PHA0811012	3.0	3.0
PHA0811013	4.0	3.0
PHA0811017	3.0	3.0
PHA0811020	3.0	3.0
PHA1109004	3.0	3.0
PHA1109023	2.0	1.0
PHA1109026	3.0	3.0
PHA1110003A	1.0	1.0
PHA1110003B	0.0	1.0
PHA1111001A	2.0	1.0
PHA1111003A	1.0	1.0
PHA1111009A	1.0	1.0
TI071122B	1.0	1.0
VAR0910010	3.0	3.0
1325_1001011	3.0	3.0
1325_1001013	3.0	3.0
1325_1001014	3.0	3.0
1325_1001022	3.0	3.0
1325_1001043	3.0	3.0
1325_1001044	3.0	3.0
1325_1001050	3.0	3.0
1325_1001056	3.0	3.0
1325_1001057	2.0	3.0
1325_1001059	3.0	3.0
1325_1001062	3.0	3.0
1325_1001080	2.0	3.0
1325_1001083	3.0	2.0
1325_1001099	3.0	3.0
1325_1001108	3.0	3.0
1325_1001113	3.0	3.0
1325_1001121	2.0	2.0
1325_1001123	3.0	3.0
1325_1001131	3.0	3.0
1325_1001143	3.0	3.0
1325_1001158	3.0	3.0
1325_1001162	3.0	3.0
1325_1001170	3.0	3.0
1325_9000095	3.0	3.0
1325_9000139	3.0	3.0
1325_9000188	3.0	3.0
1325_9000215	3.0	3.0
1325_9000240	3.0	3.0
1325_9000504	3.0	3.0
1325_9000505	3.0	3.0
1325_9000554	3.0	3.0
1325_9000612	2.0	3.0
1325_9000678	3.0	3.0
1365_0100002	2.0	2.0
1365_0100013	3.0	2.0
1365_0100016	2.0	2.0
1365_0100023	2.0	2.0
1365_0100030	2.0	2.0
1365_0100056	2.0	2.0
1365_0100058	2.0	2.0
1365_0100093	2.0	2.0
1365_0100117	2.0	3.0
1365_0100118	2.0	2.0
1365_0100125	3.0	2.0
1365_0100135	2.0	2.0
1365_0100145	2.0	3.0
1365_0100146	2.0	2.0
1365_0100151	2.0	2.0
1365_0100166	2.0	2.0
1365_0100177	2.0	2.0
1365_0100182	2.0	3.0
1365_0100186	2.0	2.0
1365_0100187	2.0	2.0
1365_0100192	3.0	3.0
1365_0100194	2.0	3.0
1365_0100195	2.0	2.0
1365_0100201	2.0	2.0
1365_0100202	2.0	2.0
1365_0100205	2.0	2.0
1365_0100212	3.0	3.0
1365_0100219	2.0	3.0
1365_0100220	3.0	2.0
1365_0100232	2.0	2.0
1365_0100256	2.0	2.0
1365_0100257	2.0	2.0
1365_0100258	2.0	2.0
1365_0100261	2.0	2.0
1365_0100266	2.0	3.0
1365_0100274	2.0	3.0
1365_0100279	2.0	2.0
1365_0100280	1.0	2.0
1365_0100281	2.0	2.0
1365_0100451	2.0	3.0
1365_0100472	2.0	2.0
1365_0100477	2.0	2.0
1365_0100480	2.0	2.0
1385_0000012	1.0	1.0
1385_0000017	1.0	1.0
1385_0000020	1.0	1.0
1385_0000043	1.0	1.0
1385_0000054	2.0	1.0
1385_0000097	2.0	1.0
1385_0000101	1.0	1.0
1385_0000122	1.0	1.0
1385_0000127	2.0	1.0
1385_0000129	1.0	1.0
1385_0000130	1.0	1.0
1385_0001108	1.0	1.0
1385_0001109	1.0	1.0
1385_0001111	2.0	1.0
1385_0001112	2.0	1.0
1385_0001118	2.0	1.0
1385_0001125	1.0	1.0
1385_0001127	2.0	1.0
1385_0001129	1.0	1.0
1385_0001131	1.0	1.0
1385_0001149	2.0	1.0
1385_0001155	1.0	1.0
1385_0001158	1.0	1.0
1385_0001160	1.0	2.0
1385_0001162	1.0	1.0
1385_0001165	1.0	1.0
1385_0001188	1.0	1.0
1385_0001191	1.0	1.0
1385_0001194	1.0	1.0
1385_0001198	1.0	2.0
1385_0001199	1.0	1.0
1385_0001528	1.0	2.0
1385_0001716	1.0	1.0
1385_0001720	0.0	1.0
1385_0001724	1.0	1.0
1385_0001748	1.0	2.0
1385_0001749	1.0	1.0
1385_0001759	1.0	1.0
1385_0001765	0.0	1.0
1385_0001785	1.0	1.0
1385_0001794	1.0	1.0
1395_0000354	1.0	1.0
1395_0000364	2.0	2.0
1395_0000379	2.0	1.0
1395_0000399	2.0	2.0
1395_0000414	2.0	2.0
1395_0000446	2.0	2.0
1395_0000465	1.0	1.0
1395_0000469	2.0	1.0
1395_0000470	2.0	1.0
1395_0000515	2.0	2.0
1395_0000525	2.0	1.0
1395_0000528	2.0	2.0
1395_0000531	2.0	1.0
1395_0000535	1.0	1.0
1395_0000548	2.0	2.0
1395_0000565	1.0	1.0
1395_0000585	1.0	1.0
1395_0000604	0.0	1.0
1395_0000607	0.0	1.0
1395_0000611	1.0	2.0
1395_0000636	1.0	1.0
1395_0001019	1.0	1.0
1395_0001045	2.0	1.0
1395_0001066	1.0	2.0
1395_0001069	2.0	2.0
1395_0001090	1.0	1.0
1395_0001103	1.0	2.0
1395_0001121	0.0	1.0
1395_0001132	2.0	2.0
1395_0001133	1.0	1.0
1395_0001169	2.0	2.0
Averaged weighted F1-scores 0.7008570377312953
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
144.90552995391704 65.35717405024758
LANGUAGE: IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
1 Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 1.20
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.03
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        30
         1.0       0.55      0.65      0.59       119
         2.0       0.58      0.52      0.55       165
         3.0       0.60      0.87      0.71       115
         4.0       0.00      0.00      0.00        20
         5.0       0.00      0.00      0.00         3

    accuracy                           0.58       452
   macro avg       0.29      0.34      0.31       452
weighted avg       0.51      0.58      0.54       452

[[  0  24   6   0   0   0]
 [  0  77  42   0   0   0]
 [  0  37  85  43   0   0]
 [  0   2  13 100   0   0]
 [  0   0   0  20   0   0]
 [  0   0   0   3   0   0]]
0.5371686208177036
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.95
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.93
              precision    recall  f1-score   support

         0.0       1.00      0.07      0.12        30
         1.0       0.56      0.79      0.65       119
         2.0       0.61      0.45      0.52       165
         3.0       0.59      0.83      0.69       115
         4.0       0.00      0.00      0.00        20
         5.0       0.00      0.00      0.00         3

    accuracy                           0.59       452
   macro avg       0.46      0.36      0.33       452
weighted avg       0.59      0.59      0.54       452

[[ 2 24  4  0  0  0]
 [ 0 94 25  0  0  0]
 [ 0 49 74 42  0  0]
 [ 0  2 18 95  0  0]
 [ 0  0  0 20  0  0]
 [ 0  0  0  3  0  0]]
0.5448445052980451
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.81
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.96
              precision    recall  f1-score   support

         0.0       0.80      0.13      0.23        30
         1.0       0.54      0.61      0.57       119
         2.0       0.56      0.50      0.53       165
         3.0       0.60      0.76      0.67       115
         4.0       0.45      0.45      0.45        20
         5.0       0.00      0.00      0.00         3

    accuracy                           0.56       452
   macro avg       0.49      0.41      0.41       452
weighted avg       0.57      0.56      0.55       452

[[ 4 22  4  0  0  0]
 [ 1 73 45  0  0  0]
 [ 0 39 82 44  0  0]
 [ 0  1 16 87 11  0]
 [ 0  0  0 11  9  0]
 [ 0  0  0  3  0  0]]
0.5485642153818587
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.67
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.98
              precision    recall  f1-score   support

         0.0       0.55      0.20      0.29        30
         1.0       0.56      0.64      0.60       119
         2.0       0.59      0.51      0.55       165
         3.0       0.61      0.76      0.67       115
         4.0       0.40      0.40      0.40        20
         5.0       0.00      0.00      0.00         3

    accuracy                           0.58       452
   macro avg       0.45      0.42      0.42       452
weighted avg       0.57      0.58      0.57       452

[[ 6 20  4  0  0  0]
 [ 3 76 39  1  0  0]
 [ 2 38 84 41  0  0]
 [ 0  1 16 87 11  0]
 [ 0  0  0 12  8  0]
 [ 0  0  0  2  1  0]]
0.5653788180640678
452 452 452
Filename	True Label	Prediction
1023_0101683	3.0	3.0
1023_0101689	2.0	2.0
1023_0101693	3.0	3.0
1023_0101751	3.0	3.0
1023_0101847	3.0	3.0
1023_0101851	3.0	3.0
1023_0101852	5.0	3.0
1023_0101854	2.0	2.0
1023_0101855	2.0	2.0
1023_0101894	2.0	3.0
1023_0101899	2.0	2.0
1023_0103844	5.0	4.0
1023_0107727	5.0	3.0
1023_0107773	3.0	3.0
1023_0107781	3.0	3.0
1023_0107784	2.0	2.0
1023_0108649	3.0	3.0
1023_0108753	2.0	2.0
1023_0108766	2.0	3.0
1023_0108810	3.0	3.0
1023_0108812	3.0	3.0
1023_0108815	2.0	3.0
1023_0108887	2.0	2.0
1023_0108934	3.0	3.0
1023_0108935	2.0	3.0
1023_0109500	2.0	3.0
1023_0109520	2.0	3.0
1023_0109915	2.0	2.0
1031_0002005	4.0	3.0
1031_0002011	4.0	4.0
1031_0002083	3.0	3.0
1031_0002087	4.0	3.0
1031_0002187	3.0	4.0
1031_0002196	4.0	3.0
1031_0002199	4.0	4.0
1031_0003023	4.0	3.0
1031_0003029	4.0	3.0
1031_0003035	3.0	3.0
1031_0003054	4.0	3.0
1031_0003071	3.0	3.0
1031_0003077	3.0	3.0
1031_0003085	3.0	3.0
1031_0003099	3.0	3.0
1031_0003128	4.0	4.0
1031_0003136	3.0	3.0
1031_0003140	3.0	4.0
1031_0003154	4.0	3.0
1031_0003163	3.0	3.0
1031_0003174	3.0	3.0
1031_0003180	4.0	4.0
1031_0003182	4.0	4.0
1031_0003183	3.0	4.0
1031_0003185	3.0	3.0
1031_0003189	3.0	4.0
1031_0003205	3.0	4.0
1031_0003207	4.0	4.0
1031_0003218	3.0	4.0
1031_0003220	3.0	2.0
1031_0003221	2.0	3.0
1031_0003225	3.0	4.0
1031_0003226	3.0	3.0
1031_0003235	3.0	3.0
1031_0003244	3.0	4.0
1031_0003245	3.0	4.0
1031_0003249	3.0	4.0
1031_0003260	4.0	3.0
1031_0003272	3.0	2.0
1031_0003330	4.0	3.0
1031_0003336	3.0	3.0
1031_0003354	3.0	4.0
1031_0003356	3.0	3.0
1031_0003358	4.0	4.0
1031_0003366	3.0	3.0
1031_0003367	4.0	4.0
1031_0003389	3.0	3.0
1031_0003391	2.0	3.0
1031_0003392	4.0	3.0
1061_0120272	2.0	2.0
1061_0120279	1.0	2.0
1061_0120280	1.0	1.0
1061_0120282	0.0	1.0
1061_0120284	0.0	1.0
1061_0120286	1.0	2.0
1061_0120295	0.0	2.0
1061_0120312	1.0	1.0
1061_0120323	1.0	2.0
1061_0120331	1.0	1.0
1061_0120333	2.0	3.0
1061_0120338	2.0	2.0
1061_0120341	2.0	1.0
1061_0120346	2.0	2.0
1061_0120350	2.0	3.0
1061_0120360	3.0	3.0
1061_0120366	3.0	3.0
1061_0120369	2.0	2.0
1061_0120373	2.0	2.0
1061_0120384	1.0	1.0
1061_0120409	3.0	2.0
1061_0120411	3.0	3.0
1061_0120443	0.0	1.0
1061_0120455	2.0	2.0
1061_0120456	2.0	2.0
1061_0120460	2.0	2.0
1061_0120481	3.0	3.0
1061_0120493	2.0	2.0
1061_0120497	2.0	3.0
1061_0120499	2.0	2.0
1061_0120855	2.0	2.0
1061_0120858	2.0	2.0
1061_0120874	1.0	2.0
1061_0120877	2.0	2.0
1061_0120886	2.0	2.0
1061_1029119	1.0	2.0
1061_1202917	2.0	1.0
1071_0020001	1.0	1.0
1071_0024687	1.0	1.0
1071_0024689	1.0	1.0
1071_0024690	1.0	2.0
1071_0024701	2.0	2.0
1071_0024705	1.0	2.0
1071_0024708	1.0	1.0
1071_0024715	1.0	1.0
1071_0024766	1.0	1.0
1071_0024769	0.0	1.0
1071_0024772	0.0	0.0
1071_0024781	0.0	1.0
1071_0024797	0.0	1.0
1071_0024817	1.0	1.0
1071_0024827	1.0	1.0
1071_0024838	0.0	0.0
1071_0024845	0.0	1.0
1071_0024848	1.0	1.0
1071_0024851	2.0	1.0
1071_0024857	0.0	1.0
1071_0024861	0.0	1.0
1071_0024878	2.0	2.0
1071_0242072	0.0	1.0
1071_0242091	1.0	1.0
1071_0243502	1.0	0.0
1071_0243581	1.0	1.0
1071_0243623	1.0	1.0
1071_0248319	0.0	1.0
1071_0248327	0.0	1.0
1071_0248334	2.0	1.0
1071_0248336	1.0	1.0
1071_0248338	2.0	1.0
1071_0248339	2.0	1.0
1071_0248340	0.0	0.0
1071_0248344	1.0	1.0
1071_0248345	1.0	2.0
1071_0248350	2.0	0.0
1091_0000001	1.0	2.0
1091_0000008	2.0	2.0
1091_0000010	3.0	2.0
1091_0000012	1.0	1.0
1091_0000013	1.0	1.0
1091_0000014	0.0	1.0
1091_0000015	1.0	2.0
1091_0000025	1.0	1.0
1091_0000027	0.0	1.0
1091_0000029	2.0	1.0
1091_0000031	1.0	1.0
1091_0000033	1.0	1.0
1091_0000034	2.0	1.0
1091_0000035	1.0	1.0
1091_0000043	1.0	2.0
1091_0000053	0.0	1.0
1091_0000056	1.0	2.0
1091_0000067	1.0	1.0
1091_0000078	3.0	1.0
1091_0000125	2.0	1.0
1091_0000127	2.0	2.0
1091_0000152	1.0	1.0
1091_0000154	1.0	2.0
1091_0000155	3.0	3.0
1091_0000161	2.0	2.0
1091_0000164	2.0	1.0
1091_0000169	3.0	2.0
1091_0000203	2.0	1.0
1091_0000210	2.0	1.0
1091_0000214	2.0	1.0
1091_0000219	1.0	2.0
1091_0000224	1.0	1.0
1091_0000225	2.0	1.0
1091_0000230	1.0	2.0
1091_0000231	2.0	2.0
1091_0000236	2.0	2.0
1091_0000239	2.0	1.0
1091_0000243	1.0	1.0
1091_0000245	1.0	2.0
1091_0000247	1.0	1.0
1091_0000258	2.0	2.0
1091_0000260	2.0	2.0
1091_0000265	2.0	2.0
1091_0000270	2.0	1.0
1091_0000273	1.0	2.0
0603	2.0	2.0
0618	1.0	2.0
0619	1.0	2.0
0628	2.0	2.0
0631	2.0	2.0
0635	2.0	1.0
0715	2.0	2.0
0722	2.0	2.0
0805	2.0	2.0
0809	2.0	2.0
0812	1.0	2.0
0813	1.0	2.0
0819	3.0	2.0
0829	1.0	2.0
0901	2.0	2.0
0904	1.0	1.0
0911	1.0	2.0
0914	1.0	2.0
0915	3.0	2.0
0924	1.0	1.0
0925	2.0	2.0
0927	2.0	2.0
0928	1.0	2.0
0930	2.0	2.0
1015	1.0	2.0
1020	2.0	2.0
1022	2.0	2.0
1112	1.0	2.0
1113	2.0	2.0
1114	3.0	2.0
BER0611005	3.0	3.0
KYJ0611005A	0.0	1.0
KYJ0611006A	1.0	1.0
LIB0611003A	1.0	1.0
LON0610002B	2.0	1.0
LON0611004B	1.0	1.0
MOS0509004	2.0	2.0
PAR1011015	2.0	2.0
PHA0111002A	2.0	1.0
PHA0111004B	1.0	1.0
PHA0111010	4.0	3.0
PHA0111015	4.0	3.0
PHA0112006A	3.0	2.0
PHA0112007A	2.0	2.0
PHA0209039	3.0	3.0
PHA0210008	2.0	1.0
PHA0411008A	2.0	2.0
PHA0411031	3.0	3.0
PHA0411034	2.0	2.0
PHA0411041	3.0	3.0
PHA0509007	1.0	2.0
PHA0509015	3.0	3.0
PHA0509018	3.0	3.0
PHA0509019	2.0	3.0
PHA0509028	3.0	3.0
PHA0509034	2.0	3.0
PHA0509037	3.0	3.0
PHA0509040	2.0	3.0
PHA0509042	3.0	3.0
PHA0509044	2.0	2.0
PHA0510002A	2.0	2.0
PHA0510002B	1.0	1.0
PHA0510027	2.0	2.0
PHA0510029	3.0	3.0
PHA0510030	2.0	2.0
PHA0510035	2.0	3.0
PHA0510038	3.0	3.0
PHA0510040	2.0	3.0
PHA0510046	2.0	3.0
PHA0510048	3.0	2.0
PHA0610005B	1.0	1.0
PHA0610019A	2.0	2.0
PHA0710010	2.0	3.0
PHA0710011	3.0	3.0
PHA0809010	3.0	3.0
PHA0810002	2.0	2.0
PHA0810010	3.0	3.0
PHA0811013	3.0	3.0
PHA0811016	3.0	2.0
PHA1109007	3.0	3.0
PHA1109008	1.0	1.0
PHA1109025	1.0	1.0
PHA1109026	3.0	3.0
PHA1109028	2.0	3.0
PHA1110022	3.0	3.0
PHA1111003A	1.0	2.0
PHA1111003B	1.0	1.0
PHA1111006A	1.0	1.0
PHA1111008A	2.0	1.0
PHA1111009A	1.0	1.0
VAR0209036	3.0	3.0
VAR0909003	3.0	3.0
VAR0910011	2.0	2.0
1325_1001010	3.0	3.0
1325_1001013	3.0	3.0
1325_1001014	3.0	3.0
1325_1001016	2.0	3.0
1325_1001018	2.0	3.0
1325_1001022	3.0	2.0
1325_1001035	3.0	3.0
1325_1001042	3.0	3.0
1325_1001050	3.0	3.0
1325_1001051	3.0	2.0
1325_1001056	2.0	3.0
1325_1001058	3.0	2.0
1325_1001076	2.0	3.0
1325_1001079	3.0	3.0
1325_1001087	3.0	3.0
1325_1001094	2.0	3.0
1325_1001096	2.0	3.0
1325_1001125	3.0	3.0
1325_1001131	3.0	2.0
1325_1001139	3.0	3.0
1325_1001141	2.0	2.0
1325_1001153	3.0	3.0
1325_1001154	3.0	3.0
1325_1001162	2.0	3.0
1325_1001169	3.0	3.0
1325_9000059	2.0	3.0
1325_9000104	3.0	3.0
1325_9000209	3.0	3.0
1325_9000210	2.0	3.0
1325_9000237	3.0	3.0
1325_9000240	2.0	2.0
1325_9000278	3.0	3.0
1325_9000279	3.0	3.0
1325_9000304	3.0	3.0
1325_9000317	3.0	3.0
1325_9000320	3.0	3.0
1325_9000322	3.0	3.0
1325_9000533	3.0	3.0
1325_9000611	2.0	3.0
1325_9000674	3.0	3.0
1325_9000678	3.0	3.0
1365_0100004	2.0	3.0
1365_0100008	2.0	2.0
1365_0100011	2.0	2.0
1365_0100013	2.0	3.0
1365_0100014	2.0	2.0
1365_0100015	1.0	2.0
1365_0100020	2.0	2.0
1365_0100057	2.0	2.0
1365_0100058	3.0	3.0
1365_0100061	3.0	3.0
1365_0100066	2.0	2.0
1365_0100071	3.0	3.0
1365_0100080	2.0	2.0
1365_0100094	2.0	2.0
1365_0100096	2.0	3.0
1365_0100116	3.0	3.0
1365_0100121	3.0	3.0
1365_0100163	3.0	3.0
1365_0100166	2.0	2.0
1365_0100171	2.0	2.0
1365_0100174	2.0	2.0
1365_0100182	2.0	3.0
1365_0100190	3.0	3.0
1365_0100199	2.0	3.0
1365_0100200	3.0	3.0
1365_0100215	2.0	3.0
1365_0100228	2.0	2.0
1365_0100229	2.0	3.0
1365_0100255	2.0	2.0
1365_0100257	2.0	2.0
1365_0100260	2.0	3.0
1365_0100263	3.0	3.0
1365_0100265	3.0	3.0
1365_0100270	2.0	3.0
1365_0100282	2.0	3.0
1365_0100471	2.0	3.0
1365_0100478	2.0	2.0
1385_0000012	1.0	1.0
1385_0000016	1.0	1.0
1385_0000020	1.0	1.0
1385_0000023	1.0	1.0
1385_0000034	1.0	1.0
1385_0000035	1.0	2.0
1385_0000037	1.0	1.0
1385_0000038	1.0	1.0
1385_0000044	2.0	1.0
1385_0000045	2.0	1.0
1385_0000050	1.0	1.0
1385_0000098	2.0	1.0
1385_0000101	1.0	1.0
1385_0000102	2.0	1.0
1385_0000103	2.0	1.0
1385_0000122	1.0	1.0
1385_0000124	1.0	2.0
1385_0001103	2.0	0.0
1385_0001104	1.0	0.0
1385_0001110	2.0	1.0
1385_0001111	2.0	1.0
1385_0001132	1.0	1.0
1385_0001133	2.0	1.0
1385_0001137	2.0	1.0
1385_0001151	2.0	2.0
1385_0001163	1.0	1.0
1385_0001169	1.0	1.0
1385_0001190	0.0	1.0
1385_0001191	1.0	1.0
1385_0001193	1.0	1.0
1385_0001527	2.0	1.0
1385_0001730	2.0	1.0
1385_0001739	1.0	1.0
1385_0001750	0.0	0.0
1385_0001753	1.0	1.0
1385_0001754	1.0	1.0
1385_0001757	1.0	1.0
1385_0001760	1.0	1.0
1385_0001765	0.0	0.0
1385_0001772	1.0	1.0
1385_0001786	1.0	1.0
1385_0001787	1.0	1.0
1385_0001789	1.0	1.0
1385_0001795	0.0	0.0
1385_0001796	1.0	1.0
1395_0000340	2.0	2.0
1395_0000353	1.0	1.0
1395_0000360	3.0	2.0
1395_0000364	2.0	2.0
1395_0000379	2.0	1.0
1395_0000380	2.0	2.0
1395_0000390	1.0	1.0
1395_0000398	2.0	2.0
1395_0000415	1.0	1.0
1395_0000458	2.0	1.0
1395_0000469	1.0	1.0
1395_0000504	2.0	1.0
1395_0000528	2.0	2.0
1395_0000550	2.0	2.0
1395_0000559	2.0	1.0
1395_0000563	2.0	2.0
1395_0000575	1.0	1.0
1395_0000579	1.0	0.0
1395_0000593	0.0	2.0
1395_0000597	1.0	2.0
1395_0000602	1.0	1.0
1395_0000612	0.0	2.0
1395_0000642	1.0	2.0
1395_0000649	1.0	2.0
1395_0001013	1.0	2.0
1395_0001021	1.0	2.0
1395_0001024	1.0	2.0
1395_0001058	1.0	1.0
1395_0001061	2.0	3.0
1395_0001065	1.0	2.0
1395_0001070	2.0	2.0
1395_0001075	0.0	1.0
1395_0001076	0.0	2.0
1395_0001104	0.0	1.0
1395_0001116	2.0	1.0
1395_0001124	0.0	1.0
1395_0001141	2.0	1.0
1395_0001145	1.0	3.0
1395_0001150	1.0	1.0
1395_0001171	1.0	1.0
2 Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 1.21
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.21
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        30
         1.0       0.60      0.30      0.40       119
         2.0       0.49      0.60      0.54       166
         3.0       0.55      0.90      0.68       114
         4.0       0.00      0.00      0.00        21
         5.0       0.00      0.00      0.00         2

    accuracy                           0.53       452
   macro avg       0.27      0.30      0.27       452
weighted avg       0.48      0.53      0.48       452

[[  0  18  12   0   0   0]
 [  0  36  83   0   0   0]
 [  0   5 100  61   0   0]
 [  0   1  10 103   0   0]
 [  0   0   0  21   0   0]
 [  0   0   0   2   0   0]]
0.47649054416476294
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.95
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.92
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        30
         1.0       0.58      0.70      0.63       119
         2.0       0.61      0.68      0.64       166
         3.0       0.63      0.68      0.65       114
         4.0       0.00      0.00      0.00        21
         5.0       0.00      0.00      0.00         2

    accuracy                           0.60       452
   macro avg       0.30      0.34      0.32       452
weighted avg       0.53      0.60      0.57       452

[[  0  28   2   0   0   0]
 [  0  83  36   0   0   0]
 [  0  30 113  23   0   0]
 [  0   3  34  77   0   0]
 [  0   0   0  21   0   0]
 [  0   0   0   2   0   0]]
0.5665252354202993
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.79
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.98
              precision    recall  f1-score   support

         0.0       0.40      0.27      0.32        30
         1.0       0.59      0.55      0.57       119
         2.0       0.61      0.54      0.57       166
         3.0       0.57      0.88      0.69       114
         4.0       0.00      0.00      0.00        21
         5.0       0.00      0.00      0.00         2

    accuracy                           0.58       452
   macro avg       0.36      0.37      0.36       452
weighted avg       0.55      0.58      0.56       452

[[  8  18   4   0   0   0]
 [ 11  66  41   1   0   0]
 [  1  24  89  52   0   0]
 [  0   3  11 100   0   0]
 [  0   0   0  21   0   0]
 [  0   0   0   2   0   0]]
0.556473425069328
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.64
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.99
              precision    recall  f1-score   support

         0.0       0.55      0.20      0.29        30
         1.0       0.61      0.61      0.61       119
         2.0       0.61      0.57      0.59       166
         3.0       0.58      0.84      0.69       114
         4.0       0.00      0.00      0.00        21
         5.0       0.00      0.00      0.00         2

    accuracy                           0.60       452
   macro avg       0.39      0.37      0.36       452
weighted avg       0.57      0.60      0.57       452

[[ 6 19  5  0  0  0]
 [ 5 73 40  1  0  0]
 [ 0 25 95 46  0  0]
 [ 0  2 16 96  0  0]
 [ 0  0  0 21  0  0]
 [ 0  0  0  2  0  0]]
0.5705801480869753
452 452 452
Filename	True Label	Prediction
1023_0101848	2.0	2.0
1023_0101849	3.0	3.0
1023_0101893	3.0	3.0
1023_0101895	3.0	3.0
1023_0101898	4.0	3.0
1023_0101907	4.0	3.0
1023_0101909	4.0	3.0
1023_0103821	3.0	3.0
1023_0103824	3.0	3.0
1023_0103825	3.0	3.0
1023_0103829	2.0	3.0
1023_0103836	4.0	3.0
1023_0103838	3.0	3.0
1023_0107074	4.0	3.0
1023_0107244	3.0	3.0
1023_0107780	3.0	3.0
1023_0107788	3.0	3.0
1023_0108307	3.0	3.0
1023_0108423	3.0	2.0
1023_0108648	2.0	3.0
1023_0108650	3.0	3.0
1023_0108752	3.0	3.0
1023_0108811	3.0	3.0
1023_0108813	3.0	3.0
1023_0108933	3.0	3.0
1023_0108955	3.0	3.0
1023_0108992	3.0	3.0
1023_0109022	3.0	3.0
1023_0109026	2.0	2.0
1023_0109027	3.0	2.0
1023_0109029	1.0	2.0
1023_0109030	2.0	3.0
1023_0109096	3.0	3.0
1023_0109248	2.0	3.0
1023_0109250	2.0	3.0
1023_0109267	2.0	3.0
1023_0109399	2.0	3.0
1023_0109402	3.0	2.0
1023_0109516	3.0	3.0
1023_0109588	3.0	3.0
1023_0109590	2.0	2.0
1023_0109591	4.0	3.0
1023_0109671	2.0	3.0
1023_0109717	3.0	3.0
1023_0109721	2.0	2.0
1023_0109945	5.0	3.0
1023_0109946	3.0	3.0
1023_0109947	2.0	3.0
1023_0109951	3.0	3.0
1031_0002003	3.0	3.0
1031_0002040	4.0	3.0
1031_0002043	4.0	3.0
1031_0002061	3.0	3.0
1031_0002088	4.0	3.0
1031_0002089	4.0	3.0
1031_0002092	4.0	3.0
1031_0002198	3.0	3.0
1031_0003012	3.0	3.0
1031_0003052	4.0	3.0
1031_0003065	3.0	3.0
1031_0003072	3.0	3.0
1031_0003074	3.0	3.0
1031_0003106	3.0	3.0
1031_0003127	4.0	3.0
1031_0003144	3.0	3.0
1031_0003146	4.0	3.0
1031_0003165	2.0	3.0
1031_0003186	4.0	3.0
1031_0003214	2.0	3.0
1031_0003217	4.0	3.0
1031_0003219	3.0	3.0
1031_0003230	3.0	3.0
1031_0003237	3.0	3.0
1031_0003238	3.0	3.0
1031_0003274	3.0	3.0
1031_0003309	3.0	3.0
1031_0003310	3.0	3.0
1031_0003339	3.0	3.0
1031_0003353	3.0	3.0
1031_0003408	3.0	3.0
1031_0003409	5.0	3.0
1031_0003419	3.0	3.0
1061_0120273	1.0	1.0
1061_0120274	1.0	1.0
1061_0120289	1.0	2.0
1061_0120291	1.0	1.0
1061_0120304	1.0	2.0
1061_0120306	3.0	2.0
1061_0120311	3.0	3.0
1061_0120313	2.0	1.0
1061_0120314	2.0	2.0
1061_0120325	2.0	2.0
1061_0120336	1.0	2.0
1061_0120345	2.0	2.0
1061_0120349	1.0	1.0
1061_0120356	2.0	2.0
1061_0120358	1.0	2.0
1061_0120372	1.0	2.0
1061_0120375	2.0	1.0
1061_0120386	1.0	2.0
1061_0120389	2.0	2.0
1061_0120391	1.0	1.0
1061_0120403	3.0	3.0
1061_0120404	2.0	1.0
1061_0120406	2.0	2.0
1061_0120413	1.0	2.0
1061_0120426	2.0	2.0
1061_0120430	2.0	2.0
1061_0120432	2.0	2.0
1061_0120441	2.0	2.0
1061_0120450	2.0	2.0
1061_0120458	3.0	2.0
1061_0120480	2.0	2.0
1061_0120484	2.0	2.0
1061_0120496	2.0	2.0
1061_0120498	2.0	3.0
1061_0120856	2.0	2.0
1061_0120857	2.0	2.0
1061_0120881	2.0	2.0
1061_0120885	2.0	3.0
1061_0120887	1.0	2.0
1061_1029112	3.0	3.0
1061_1029120	2.0	2.0
1061_1202915	1.0	2.0
1061_1202918	1.0	2.0
1071_0024699	1.0	1.0
1071_0024710	0.0	1.0
1071_0024713	1.0	1.0
1071_0024716	1.0	1.0
1071_0024756	1.0	1.0
1071_0024758	2.0	2.0
1071_0024761	2.0	1.0
1071_0024763	1.0	1.0
1071_0024799	2.0	2.0
1071_0024802	1.0	1.0
1071_0024804	0.0	1.0
1071_0024811	1.0	1.0
1071_0024815	1.0	1.0
1071_0024820	0.0	0.0
1071_0024837	0.0	0.0
1071_0024840	1.0	1.0
1071_0024843	1.0	1.0
1071_0024844	1.0	1.0
1071_0024846	0.0	0.0
1071_0024855	1.0	0.0
1071_0024859	1.0	1.0
1071_0024873	0.0	1.0
1071_0024875	1.0	1.0
1071_0024879	1.0	1.0
1071_0242013	1.0	1.0
1071_0242021	1.0	1.0
1071_0242041	1.0	2.0
1071_0242043	0.0	1.0
1071_0243592	1.0	1.0
1071_0248301	2.0	1.0
1071_0248303	1.0	0.0
1071_0248304	1.0	1.0
1071_0248307	2.0	1.0
1071_0248311	2.0	1.0
1071_0248312	1.0	1.0
1071_0248316	1.0	0.0
1071_0248325	0.0	1.0
1071_0248326	1.0	1.0
1071_0248346	1.0	1.0
1071_0248348	1.0	1.0
1091_0000005	2.0	2.0
1091_0000016	1.0	1.0
1091_0000020	1.0	2.0
1091_0000030	0.0	2.0
1091_0000032	1.0	2.0
1091_0000036	1.0	2.0
1091_0000039	1.0	1.0
1091_0000044	0.0	2.0
1091_0000048	1.0	2.0
1091_0000050	1.0	1.0
1091_0000059	1.0	3.0
1091_0000062	2.0	2.0
1091_0000065	1.0	2.0
1091_0000072	0.0	2.0
1091_0000075	2.0	2.0
1091_0000101	2.0	1.0
1091_0000145	1.0	2.0
1091_0000153	1.0	2.0
1091_0000160	2.0	2.0
1091_0000168	2.0	2.0
1091_0000171	2.0	2.0
1091_0000191	1.0	2.0
1091_0000195	1.0	1.0
1091_0000197	1.0	2.0
1091_0000199	2.0	2.0
1091_0000217	3.0	2.0
1091_0000221	2.0	1.0
1091_0000226	1.0	1.0
1091_0000228	2.0	2.0
1091_0000229	2.0	2.0
1091_0000232	2.0	2.0
1091_0000235	1.0	1.0
1091_0000238	1.0	1.0
1091_0000240	1.0	1.0
1091_0000241	2.0	1.0
1091_0000244	2.0	2.0
1091_0000250	1.0	2.0
1091_0000251	1.0	1.0
1091_0000252	1.0	2.0
1091_0000254	3.0	1.0
1091_0000262	2.0	1.0
1091_0000266	1.0	2.0
1091_0000275	2.0	2.0
0602	1.0	2.0
0604	2.0	2.0
0605	2.0	2.0
0611	2.0	2.0
0613	1.0	1.0
0616	1.0	2.0
0625	1.0	1.0
0626	2.0	2.0
0630	1.0	1.0
0638	2.0	2.0
0640	2.0	2.0
0643	2.0	2.0
0644	1.0	2.0
0714	2.0	2.0
0801	2.0	2.0
0807	2.0	2.0
0814	2.0	1.0
0822	1.0	2.0
0824	1.0	2.0
0826	2.0	2.0
0902	2.0	2.0
0905	2.0	2.0
0907	2.0	2.0
0916	2.0	1.0
0917	1.0	1.0
0921	1.0	1.0
1002	2.0	2.0
1003	1.0	2.0
1006	2.0	2.0
1007	2.0	2.0
1009	2.0	2.0
1019	2.0	2.0
1021	2.0	2.0
1115	2.0	2.0
LIB0611002A	1.0	1.0
LIB0611002B	1.0	1.0
LIB0611004A	1.0	1.0
LON0610002A	1.0	2.0
LON0611002B	0.0	1.0
MOS0611012	2.0	3.0
PAR1011009A	2.0	2.0
PAR1011016	2.0	3.0
PHA0111001B	1.0	1.0
PHA0111002B	3.0	2.0
PHA0111004A	1.0	1.0
PHA0111012	2.0	3.0
PHA0112002A	2.0	2.0
PHA0112009B	1.0	1.0
PHA0209026	3.0	3.0
PHA0209031	4.0	3.0
PHA0209038	4.0	3.0
PHA0411009B	1.0	1.0
PHA0411029	3.0	2.0
PHA0411030	3.0	3.0
PHA0411036	3.0	3.0
PHA0411039	3.0	3.0
PHA0411047	3.0	3.0
PHA0411053	3.0	3.0
PHA0411056	4.0	3.0
PHA0411059	2.0	3.0
PHA0509002	1.0	1.0
PHA0509017	3.0	3.0
PHA0509021	3.0	2.0
PHA0509031	2.0	3.0
PHA0509036	2.0	3.0
PHA0509038	2.0	3.0
PHA0510034	3.0	3.0
PHA0510036	3.0	3.0
PHA0510039	2.0	3.0
PHA0510049	2.0	3.0
PHA0610007A	3.0	1.0
PHA0710009	2.0	3.0
PHA0710014	3.0	3.0
PHA0710015	3.0	3.0
PHA0710018	3.0	3.0
PHA0809009	2.0	3.0
PHA0811020	2.0	2.0
PHA1109023	2.0	1.0
PHA1110002B	2.0	1.0
PHA1110004A	2.0	2.0
PHA1110014	3.0	3.0
PHA1110016	2.0	2.0
PHA1111001B	1.0	1.0
VAR0909004	3.0	3.0
VAR0909010	3.0	2.0
1325_1001012	3.0	3.0
1325_1001028	3.0	3.0
1325_1001033	3.0	3.0
1325_1001040	3.0	3.0
1325_1001043	3.0	3.0
1325_1001047	2.0	2.0
1325_1001054	3.0	3.0
1325_1001063	2.0	3.0
1325_1001082	2.0	2.0
1325_1001084	3.0	3.0
1325_1001090	3.0	2.0
1325_1001093	2.0	3.0
1325_1001097	1.0	2.0
1325_1001098	2.0	3.0
1325_1001100	3.0	3.0
1325_1001107	3.0	3.0
1325_1001109	2.0	2.0
1325_1001111	3.0	3.0
1325_1001121	3.0	3.0
1325_1001123	3.0	3.0
1325_1001129	2.0	3.0
1325_1001136	2.0	2.0
1325_1001138	2.0	3.0
1325_1001142	2.0	3.0
1325_1001155	3.0	3.0
1325_1001156	2.0	3.0
1325_1001157	2.0	3.0
1325_1001158	3.0	3.0
1325_1001159	3.0	3.0
1325_1001166	3.0	3.0
1325_1001170	3.0	3.0
1325_9000090	3.0	3.0
1325_9000107	3.0	3.0
1325_9000139	3.0	3.0
1325_9000143	3.0	3.0
1325_9000187	3.0	3.0
1325_9000213	3.0	3.0
1325_9000296	2.0	3.0
1325_9000321	3.0	3.0
1325_9000505	3.0	3.0
1325_9000601	3.0	3.0
1325_9000602	4.0	3.0
1325_9000612	2.0	3.0
1325_9000675	3.0	3.0
1325_9000685	4.0	3.0
1325_9000686	2.0	3.0
1365_0100028	2.0	2.0
1365_0100029	1.0	2.0
1365_0100098	2.0	2.0
1365_0100100	3.0	3.0
1365_0100101	2.0	2.0
1365_0100118	2.0	3.0
1365_0100134	2.0	2.0
1365_0100135	2.0	2.0
1365_0100136	2.0	2.0
1365_0100137	2.0	2.0
1365_0100145	3.0	3.0
1365_0100147	3.0	2.0
1365_0100165	3.0	3.0
1365_0100178	2.0	2.0
1365_0100184	2.0	2.0
1365_0100188	2.0	2.0
1365_0100196	2.0	2.0
1365_0100198	2.0	2.0
1365_0100202	2.0	2.0
1365_0100213	2.0	2.0
1365_0100219	2.0	3.0
1365_0100224	3.0	3.0
1365_0100232	2.0	3.0
1365_0100252	3.0	3.0
1365_0100266	2.0	3.0
1365_0100276	3.0	3.0
1365_0100277	3.0	3.0
1365_0100290	2.0	3.0
1365_0100447	3.0	2.0
1365_0100458	2.0	3.0
1365_0100472	2.0	3.0
1365_0100477	2.0	3.0
1365_0100479	2.0	3.0
1385_0000017	1.0	0.0
1385_0000054	2.0	1.0
1385_0000095	1.0	1.0
1385_0000104	2.0	1.0
1385_0001122	2.0	1.0
1385_0001123	2.0	1.0
1385_0001128	1.0	1.0
1385_0001130	1.0	0.0
1385_0001134	1.0	1.0
1385_0001136	1.0	1.0
1385_0001138	1.0	1.0
1385_0001147	1.0	1.0
1385_0001148	2.0	2.0
1385_0001156	1.0	1.0
1385_0001157	1.0	1.0
1385_0001160	1.0	2.0
1385_0001161	1.0	1.0
1385_0001162	1.0	1.0
1385_0001164	1.0	2.0
1385_0001172	0.0	1.0
1385_0001173	0.0	0.0
1385_0001175	0.0	0.0
1385_0001198	1.0	2.0
1385_0001522	1.0	1.0
1385_0001714	1.0	1.0
1385_0001720	0.0	1.0
1385_0001727	0.0	1.0
1385_0001728	1.0	2.0
1385_0001729	1.0	1.0
1385_0001737	2.0	1.0
1385_0001744	0.0	1.0
1385_0001748	1.0	2.0
1385_0001758	0.0	1.0
1385_0001762	2.0	1.0
1385_0001764	1.0	1.0
1385_0001766	2.0	2.0
1385_0001767	0.0	1.0
1385_0001773	0.0	1.0
1385_0001774	0.0	1.0
1385_0001775	0.0	1.0
1395_0000333	1.0	2.0
1395_0000338	2.0	1.0
1395_0000355	2.0	2.0
1395_0000365	3.0	2.0
1395_0000376	2.0	2.0
1395_0000387	3.0	2.0
1395_0000388	2.0	2.0
1395_0000392	2.0	2.0
1395_0000414	2.0	1.0
1395_0000432	2.0	2.0
1395_0000438	2.0	3.0
1395_0000446	2.0	2.0
1395_0000447	2.0	2.0
1395_0000448	2.0	1.0
1395_0000499	1.0	1.0
1395_0000500	2.0	1.0
1395_0000512	2.0	2.0
1395_0000533	3.0	2.0
1395_0000548	2.0	2.0
1395_0000557	2.0	3.0
1395_0000560	2.0	2.0
1395_0000581	2.0	2.0
1395_0000584	0.0	0.0
1395_0000585	0.0	2.0
1395_0000599	1.0	1.0
1395_0000631	0.0	2.0
1395_0000636	0.0	1.0
1395_0001020	1.0	2.0
1395_0001034	1.0	1.0
1395_0001040	0.0	1.0
1395_0001064	2.0	2.0
1395_0001078	1.0	1.0
1395_0001093	1.0	1.0
1395_0001109	0.0	1.0
1395_0001120	1.0	1.0
1395_0001121	0.0	1.0
1395_0001123	1.0	1.0
1395_0001126	1.0	1.0
1395_0001149	1.0	1.0
1395_0001169	2.0	2.0
3 Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 1.22
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.12
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        30
         1.0       0.59      0.61      0.60       119
         2.0       0.52      0.36      0.43       166
         3.0       0.50      0.93      0.65       115
         4.0       0.00      0.00      0.00        20
         5.0       0.00      0.00      0.00         2

    accuracy                           0.53       452
   macro avg       0.27      0.32      0.28       452
weighted avg       0.48      0.53      0.48       452

[[  0  26   4   0   0   0]
 [  0  73  43   3   0   0]
 [  0  24  60  82   0   0]
 [  0   0   8 107   0   0]
 [  0   0   0  20   0   0]
 [  0   0   0   2   0   0]]
0.48116261007963895
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 1.00
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.88
              precision    recall  f1-score   support

         0.0       1.00      0.07      0.12        30
         1.0       0.59      0.76      0.67       119
         2.0       0.63      0.53      0.58       166
         3.0       0.59      0.80      0.68       115
         4.0       0.00      0.00      0.00        20
         5.0       0.00      0.00      0.00         2

    accuracy                           0.60       452
   macro avg       0.47      0.36      0.34       452
weighted avg       0.60      0.60      0.57       452

[[ 2 27  1  0  0  0]
 [ 0 91 28  0  0  0]
 [ 0 36 88 42  0  0]
 [ 0  0 23 92  0  0]
 [ 0  0  0 20  0  0]
 [ 0  0  0  2  0  0]]
0.5677912352392656
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.87
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.89
              precision    recall  f1-score   support

         0.0       0.83      0.17      0.28        30
         1.0       0.59      0.73      0.65       119
         2.0       0.60      0.56      0.58       166
         3.0       0.59      0.74      0.66       115
         4.0       0.00      0.00      0.00        20
         5.0       0.00      0.00      0.00         2

    accuracy                           0.60       452
   macro avg       0.44      0.37      0.36       452
weighted avg       0.58      0.60      0.57       452

[[ 5 23  2  0  0  0]
 [ 1 87 31  0  0  0]
 [ 0 36 93 37  0  0]
 [ 0  1 29 85  0  0]
 [ 0  0  0 20  0  0]
 [ 0  0  0  2  0  0]]
0.5704534955021398
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.78
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.90
              precision    recall  f1-score   support

         0.0       0.86      0.20      0.32        30
         1.0       0.60      0.64      0.62       119
         2.0       0.57      0.58      0.58       166
         3.0       0.60      0.78      0.68       115
         4.0       0.00      0.00      0.00        20
         5.0       0.00      0.00      0.00         2

    accuracy                           0.59       452
   macro avg       0.44      0.37      0.37       452
weighted avg       0.58      0.59      0.57       452

[[ 6 20  4  0  0  0]
 [ 1 76 42  0  0  0]
 [ 0 31 96 39  0  0]
 [ 0  0 25 90  0  0]
 [ 0  0  0 20  0  0]
 [ 0  0  0  2  0  0]]
0.5681182424738379
452 452 452
Filename	True Label	Prediction
1023_0001419	3.0	3.0
1023_0001423	2.0	3.0
1023_0101675	3.0	3.0
1023_0101694	2.0	3.0
1023_0101700	2.0	3.0
1023_0101753	3.0	3.0
1023_0101841	3.0	3.0
1023_0101843	3.0	3.0
1023_0101844	2.0	3.0
1023_0101846	4.0	3.0
1023_0101853	2.0	3.0
1023_0101897	3.0	3.0
1023_0101900	3.0	3.0
1023_0103827	3.0	3.0
1023_0103843	3.0	2.0
1023_0103955	4.0	3.0
1023_0104206	3.0	3.0
1023_0106816	3.0	3.0
1023_0107682	2.0	3.0
1023_0107729	3.0	3.0
1023_0108306	3.0	3.0
1023_0108422	3.0	3.0
1023_0108510	3.0	3.0
1023_0108518	3.0	3.0
1023_0108641	3.0	3.0
1023_0108814	3.0	3.0
1023_0108885	2.0	3.0
1023_0108886	3.0	3.0
1023_0108889	3.0	3.0
1023_0108931	3.0	3.0
1023_0108958	2.0	3.0
1023_0109039	3.0	3.0
1023_0109192	3.0	3.0
1023_0109247	3.0	3.0
1023_0109391	2.0	3.0
1023_0109392	3.0	3.0
1023_0109395	2.0	3.0
1023_0109496	3.0	3.0
1023_0109518	2.0	2.0
1023_0109524	2.0	3.0
1023_0109651	3.0	3.0
1023_0109674	3.0	3.0
1023_0109880	3.0	3.0
1023_0111896	2.0	3.0
1031_0001950	4.0	3.0
1031_0001951	2.0	3.0
1031_0001998	4.0	3.0
1031_0002004	4.0	3.0
1031_0002006	4.0	3.0
1031_0002032	3.0	3.0
1031_0002036	4.0	3.0
1031_0002079	5.0	3.0
1031_0002085	3.0	3.0
1031_0002185	4.0	3.0
1031_0002195	3.0	3.0
1031_0002200	3.0	3.0
1031_0003013	4.0	3.0
1031_0003043	5.0	3.0
1031_0003073	3.0	3.0
1031_0003126	3.0	3.0
1031_0003129	4.0	3.0
1031_0003156	3.0	3.0
1031_0003157	4.0	3.0
1031_0003161	3.0	3.0
1031_0003164	4.0	3.0
1031_0003166	3.0	3.0
1031_0003170	3.0	3.0
1031_0003172	3.0	3.0
1031_0003181	4.0	3.0
1031_0003184	4.0	3.0
1031_0003203	2.0	3.0
1031_0003211	2.0	3.0
1031_0003212	3.0	3.0
1031_0003231	4.0	3.0
1031_0003234	3.0	3.0
1031_0003239	4.0	3.0
1031_0003242	3.0	3.0
1031_0003327	3.0	3.0
1031_0003338	3.0	3.0
1031_0003365	3.0	3.0
1031_0003368	3.0	3.0
1031_0003369	3.0	3.0
1031_0003383	3.0	3.0
1031_0003386	3.0	3.0
1031_0003387	3.0	3.0
1031_0003388	3.0	3.0
1031_0003390	3.0	3.0
1061_0120276	2.0	2.0
1061_0120277	1.0	2.0
1061_0120278	2.0	2.0
1061_0120290	1.0	2.0
1061_0120297	1.0	2.0
1061_0120328	1.0	1.0
1061_0120329	2.0	2.0
1061_0120332	2.0	1.0
1061_0120337	2.0	2.0
1061_0120347	2.0	2.0
1061_0120351	2.0	2.0
1061_0120353	1.0	1.0
1061_0120354	1.0	1.0
1061_0120361	3.0	2.0
1061_0120367	3.0	2.0
1061_0120371	3.0	3.0
1061_0120374	2.0	2.0
1061_0120383	2.0	3.0
1061_0120394	2.0	2.0
1061_0120405	2.0	2.0
1061_0120407	3.0	2.0
1061_0120408	2.0	2.0
1061_0120424	2.0	2.0
1061_0120425	2.0	2.0
1061_0120428	2.0	2.0
1061_0120429	3.0	2.0
1061_0120433	1.0	1.0
1061_0120439	1.0	1.0
1061_0120448	3.0	2.0
1061_0120459	2.0	2.0
1061_0120479	2.0	2.0
1061_0120482	2.0	2.0
1061_0120485	3.0	2.0
1061_0120489	2.0	2.0
1061_0120491	2.0	2.0
1061_0120492	3.0	2.0
1061_0120494	2.0	2.0
1061_0120495	2.0	2.0
1061_0120876	2.0	2.0
1061_0120878	2.0	2.0
1061_0120894	2.0	2.0
1061_1029111	2.0	2.0
1061_1029118	1.0	1.0
1061_1202913	2.0	2.0
1061_1202919	2.0	1.0
1071_0024678	2.0	1.0
1071_0024680	2.0	2.0
1071_0024683	0.0	1.0
1071_0024686	3.0	2.0
1071_0024688	1.0	1.0
1071_0024702	1.0	1.0
1071_0024706	1.0	1.0
1071_0024711	1.0	1.0
1071_0024759	0.0	1.0
1071_0024765	0.0	0.0
1071_0024773	1.0	1.0
1071_0024782	0.0	0.0
1071_0024783	0.0	1.0
1071_0024803	1.0	1.0
1071_0024814	1.0	1.0
1071_0024826	1.0	1.0
1071_0024831	0.0	1.0
1071_0024850	0.0	1.0
1071_0024863	1.0	1.0
1071_0024865	2.0	2.0
1071_0024871	1.0	1.0
1071_0024872	1.0	1.0
1071_0024874	0.0	1.0
1071_0024876	1.0	1.0
1071_0024881	1.0	1.0
1071_0241832	1.0	1.0
1071_0241833	1.0	1.0
1071_0242012	1.0	2.0
1071_0242023	1.0	1.0
1071_0242071	0.0	1.0
1071_0242093	0.0	0.0
1071_0248305	0.0	1.0
1071_0248309	2.0	1.0
1071_0248313	1.0	1.0
1071_0248315	0.0	0.0
1071_0248318	0.0	0.0
1071_0248323	0.0	1.0
1071_0248330	2.0	1.0
1071_0248333	2.0	1.0
1071_0248335	1.0	1.0
1071_0248337	1.0	2.0
1071_0248342	0.0	1.0
1091_0000011	2.0	1.0
1091_0000018	2.0	2.0
1091_0000021	1.0	2.0
1091_0000022	2.0	2.0
1091_0000038	2.0	1.0
1091_0000041	1.0	1.0
1091_0000045	1.0	2.0
1091_0000046	1.0	1.0
1091_0000047	2.0	1.0
1091_0000055	1.0	2.0
1091_0000071	1.0	2.0
1091_0000074	2.0	2.0
1091_0000086	1.0	2.0
1091_0000092	1.0	1.0
1091_0000095	1.0	1.0
1091_0000113	1.0	2.0
1091_0000146	1.0	1.0
1091_0000151	0.0	1.0
1091_0000156	3.0	2.0
1091_0000157	2.0	2.0
1091_0000165	1.0	1.0
1091_0000172	2.0	1.0
1091_0000174	2.0	1.0
1091_0000185	2.0	1.0
1091_0000192	1.0	2.0
1091_0000193	2.0	1.0
1091_0000194	1.0	2.0
1091_0000200	2.0	2.0
1091_0000202	1.0	2.0
1091_0000204	3.0	2.0
1091_0000207	2.0	2.0
1091_0000209	3.0	2.0
1091_0000216	1.0	2.0
1091_0000222	2.0	1.0
1091_0000234	2.0	2.0
1091_0000249	2.0	2.0
1091_0000259	2.0	2.0
1091_0000272	1.0	2.0
1091_0000274	1.0	1.0
1091_0000276	2.0	1.0
0601	1.0	1.0
0607	3.0	2.0
0610	1.0	2.0
0614	2.0	2.0
0617	1.0	2.0
0621	1.0	2.0
0633	2.0	2.0
0639	1.0	2.0
0717	1.0	1.0
0719	2.0	2.0
0723	2.0	2.0
0810	1.0	2.0
0811	2.0	2.0
0815	2.0	2.0
0821	1.0	2.0
0827	2.0	2.0
0906	2.0	2.0
0919	2.0	1.0
0920	2.0	2.0
0923	2.0	2.0
1001	1.0	2.0
1010	2.0	2.0
1111	1.0	2.0
KYJ0611005B	1.0	1.0
KYJ0611009A	1.0	1.0
LIB0611004B	1.0	2.0
LIB0611011	2.0	2.0
LON0611004A	0.0	1.0
MOS0611015	2.0	3.0
PAR1011009B	1.0	1.0
PHA0111005A	1.0	1.0
PHA0112002B	1.0	2.0
PHA0112003B	1.0	1.0
PHA0112009A	2.0	2.0
PHA0112012A	2.0	2.0
PHA0209013	1.0	1.0
PHA0210001	1.0	1.0
PHA0210004	2.0	1.0
PHA0210007	2.0	2.0
PHA0411009A	1.0	1.0
PHA0411010B	1.0	1.0
PHA0411027	2.0	2.0
PHA0411033	2.0	2.0
PHA0411035	3.0	3.0
PHA0411045	2.0	3.0
PHA0411054	3.0	2.0
PHA0411061	2.0	3.0
PHA0509025	4.0	3.0
PHA0509026	4.0	3.0
PHA0509032	2.0	3.0
PHA0509033	2.0	2.0
PHA0509043	3.0	3.0
PHA0509045	2.0	2.0
PHA0510003B	1.0	1.0
PHA0510004B	1.0	1.0
PHA0510013A	1.0	2.0
PHA0510013B	1.0	1.0
PHA0510032	2.0	3.0
PHA0510047	2.0	2.0
PHA0610005A	1.0	2.0
PHA0610015	2.0	3.0
PHA0610019B	2.0	1.0
PHA0709008	3.0	3.0
PHA0710012	3.0	3.0
PHA0810003	2.0	3.0
PHA0810008	2.0	3.0
PHA0810011	2.0	3.0
PHA0810015	2.0	3.0
PHA0811012	4.0	3.0
PHA0811019	3.0	3.0
PHA1109001	2.0	2.0
PHA1109002	3.0	3.0
PHA1109004	3.0	3.0
PHA1109005	3.0	2.0
PHA1110002A	2.0	2.0
PHA1110003B	1.0	2.0
PHA1111002B	1.0	1.0
PHA1111004A	2.0	1.0
PHA1111008B	1.0	1.0
ST071122B	1.0	2.0
VAR0909006	2.0	3.0
VAR0910004	3.0	3.0
VAR0910005	3.0	2.0
VAR0910007	2.0	3.0
1325_1001020	2.0	3.0
1325_1001023	3.0	2.0
1325_1001025	2.0	3.0
1325_1001029	3.0	3.0
1325_1001036	3.0	3.0
1325_1001080	3.0	3.0
1325_1001081	2.0	3.0
1325_1001085	3.0	3.0
1325_1001086	3.0	2.0
1325_1001088	2.0	2.0
1325_1001110	3.0	3.0
1325_1001113	3.0	3.0
1325_1001119	3.0	3.0
1325_1001120	3.0	3.0
1325_1001122	2.0	2.0
1325_1001152	3.0	3.0
1325_1001164	3.0	3.0
1325_1001165	2.0	2.0
1325_1001168	2.0	3.0
1325_9000089	3.0	3.0
1325_9000105	2.0	2.0
1325_9000106	3.0	3.0
1325_9000138	4.0	3.0
1325_9000140	3.0	3.0
1325_9000152	3.0	3.0
1325_9000188	3.0	3.0
1325_9000215	3.0	3.0
1325_9000302	2.0	3.0
1325_9000314	2.0	3.0
1325_9000319	2.0	3.0
1325_9000323	2.0	3.0
1325_9000503	3.0	3.0
1325_9000536	3.0	3.0
1325_9000554	2.0	3.0
1325_9000676	3.0	3.0
1365_0100005	1.0	2.0
1365_0100012	2.0	2.0
1365_0100017	2.0	3.0
1365_0100018	2.0	2.0
1365_0100021	2.0	2.0
1365_0100023	2.0	2.0
1365_0100024	2.0	2.0
1365_0100026	2.0	1.0
1365_0100027	3.0	2.0
1365_0100030	2.0	2.0
1365_0100063	3.0	3.0
1365_0100065	1.0	2.0
1365_0100072	2.0	2.0
1365_0100092	2.0	2.0
1365_0100102	2.0	2.0
1365_0100103	3.0	3.0
1365_0100104	2.0	2.0
1365_0100105	3.0	2.0
1365_0100107	3.0	3.0
1365_0100138	2.0	2.0
1365_0100148	3.0	3.0
1365_0100151	2.0	2.0
1365_0100167	2.0	2.0
1365_0100168	3.0	2.0
1365_0100173	2.0	2.0
1365_0100180	2.0	2.0
1365_0100183	2.0	2.0
1365_0100204	2.0	2.0
1365_0100211	3.0	3.0
1365_0100217	3.0	3.0
1365_0100220	3.0	3.0
1365_0100225	3.0	2.0
1365_0100226	3.0	2.0
1365_0100231	2.0	2.0
1365_0100233	3.0	3.0
1365_0100251	3.0	3.0
1365_0100261	2.0	2.0
1365_0100267	2.0	3.0
1365_0100269	2.0	2.0
1365_0100278	3.0	2.0
1365_0100285	2.0	2.0
1365_0100299	3.0	2.0
1365_0100459	3.0	3.0
1365_0100480	2.0	2.0
1385_0000011	1.0	1.0
1385_0000021	1.0	1.0
1385_0000033	2.0	1.0
1385_0000036	1.0	1.0
1385_0000042	1.0	1.0
1385_0000047	1.0	1.0
1385_0000049	1.0	1.0
1385_0000119	1.0	1.0
1385_0000129	2.0	1.0
1385_0001108	2.0	1.0
1385_0001118	2.0	1.0
1385_0001120	2.0	1.0
1385_0001121	2.0	1.0
1385_0001126	0.0	1.0
1385_0001135	1.0	1.0
1385_0001149	2.0	1.0
1385_0001154	1.0	1.0
1385_0001159	1.0	1.0
1385_0001170	0.0	1.0
1385_0001174	0.0	1.0
1385_0001192	1.0	1.0
1385_0001194	1.0	1.0
1385_0001199	1.0	1.0
1385_0001717	1.0	2.0
1385_0001718	0.0	1.0
1385_0001723	0.0	1.0
1385_0001733	1.0	1.0
1385_0001742	0.0	1.0
1385_0001747	1.0	1.0
1385_0001749	1.0	1.0
1385_0001751	0.0	2.0
1385_0001752	1.0	1.0
1385_0001761	0.0	1.0
1385_0001771	1.0	1.0
1385_0001800	1.0	1.0
1395_0000337	1.0	0.0
1395_0000354	1.0	1.0
1395_0000356	1.0	1.0
1395_0000366	2.0	2.0
1395_0000378	2.0	2.0
1395_0000396	1.0	2.0
1395_0000402	1.0	1.0
1395_0000404	2.0	2.0
1395_0000450	1.0	2.0
1395_0000452	1.0	1.0
1395_0000454	2.0	2.0
1395_0000471	2.0	1.0
1395_0000513	2.0	2.0
1395_0000525	2.0	1.0
1395_0000526	1.0	1.0
1395_0000527	1.0	1.0
1395_0000531	2.0	1.0
1395_0000534	2.0	2.0
1395_0000535	1.0	1.0
1395_0000554	2.0	1.0
1395_0000583	1.0	2.0
1395_0000607	0.0	0.0
1395_0000609	1.0	1.0
1395_0000610	2.0	2.0
1395_0000628	0.0	2.0
1395_0000630	0.0	2.0
1395_0000639	0.0	2.0
1395_0000644	1.0	2.0
1395_0001015	1.0	2.0
1395_0001019	1.0	1.0
1395_0001060	2.0	2.0
1395_0001066	1.0	2.0
1395_0001071	2.0	2.0
1395_0001074	1.0	1.0
1395_0001080	2.0	2.0
1395_0001131	0.0	1.0
1395_0001132	2.0	2.0
1395_0001133	1.0	2.0
1395_0001147	1.0	2.0
1395_0001170	1.0	2.0
4 Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 1.22
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        31
         1.0       0.52      0.75      0.61       118
         2.0       0.58      0.35      0.44       166
         3.0       0.58      0.91      0.71       115
         4.0       0.00      0.00      0.00        20
         5.0       0.00      0.00      0.00         2

    accuracy                           0.56       452
   macro avg       0.28      0.33      0.29       452
weighted avg       0.49      0.56      0.50       452

[[  0  27   4   0   0   0]
 [  0  88  30   0   0   0]
 [  0  52  58  56   0   0]
 [  0   2   8 105   0   0]
 [  0   1   0  19   0   0]
 [  0   0   0   2   0   0]]
0.4995911943979014
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.99
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.94
              precision    recall  f1-score   support

         0.0       0.56      0.16      0.25        31
         1.0       0.56      0.86      0.68       118
         2.0       0.73      0.39      0.51       166
         3.0       0.60      0.90      0.72       115
         4.0       0.50      0.05      0.09        20
         5.0       0.00      0.00      0.00         2

    accuracy                           0.61       452
   macro avg       0.49      0.39      0.37       452
weighted avg       0.63      0.61      0.57       452

[[  5  25   1   0   0   0]
 [  4 101  13   0   0   0]
 [  0  53  65  48   0   0]
 [  0   2   9 103   1   0]
 [  0   0   1  18   1   0]
 [  0   0   0   2   0   0]]
0.5680240357873724
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.90
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.90
              precision    recall  f1-score   support

         0.0       0.83      0.16      0.27        31
         1.0       0.60      0.66      0.63       118
         2.0       0.62      0.61      0.62       166
         3.0       0.62      0.83      0.71       115
         4.0       0.00      0.00      0.00        20
         5.0       0.00      0.00      0.00         2

    accuracy                           0.62       452
   macro avg       0.45      0.38      0.37       452
weighted avg       0.60      0.62      0.59       452

[[  5  22   4   0   0   0]
 [  1  78  39   0   0   0]
 [  0  28 101  37   0   0]
 [  0   1  17  96   1   0]
 [  0   0   1  19   0   0]
 [  0   0   0   2   0   0]]
0.5911906767069521
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.82
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.88
              precision    recall  f1-score   support

         0.0       0.55      0.19      0.29        31
         1.0       0.56      0.76      0.65       118
         2.0       0.65      0.54      0.59       166
         3.0       0.63      0.73      0.67       115
         4.0       0.22      0.10      0.14        20
         5.0       0.00      0.00      0.00         2

    accuracy                           0.60       452
   macro avg       0.43      0.39      0.39       452
weighted avg       0.59      0.60      0.58       452

[[ 6 24  1  0  0  0]
 [ 5 90 23  0  0  0]
 [ 0 45 90 31  0  0]
 [ 0  1 23 84  7  0]
 [ 0  0  1 17  2  0]
 [ 0  0  0  2  0  0]]
0.5838461866695429
452 452 452
Filename	True Label	Prediction
1023_0001416	5.0	3.0
1023_0001420	4.0	3.0
1023_0001422	3.0	2.0
1023_0101684	2.0	2.0
1023_0101691	4.0	3.0
1023_0101695	2.0	3.0
1023_0101701	2.0	3.0
1023_0101749	4.0	3.0
1023_0101845	3.0	3.0
1023_0101856	3.0	3.0
1023_0101904	2.0	3.0
1023_0101906	2.0	3.0
1023_0102117	3.0	3.0
1023_0102118	3.0	3.0
1023_0103822	2.0	2.0
1023_0103823	4.0	3.0
1023_0103830	3.0	3.0
1023_0103832	3.0	3.0
1023_0103834	3.0	3.0
1023_0103839	3.0	3.0
1023_0103840	3.0	3.0
1023_0103880	3.0	3.0
1023_0104207	2.0	3.0
1023_0104209	3.0	3.0
1023_0107075	2.0	3.0
1023_0107672	2.0	3.0
1023_0107740	3.0	3.0
1023_0107783	3.0	2.0
1023_0108426	2.0	3.0
1023_0108751	3.0	3.0
1023_0108888	2.0	3.0
1023_0108890	3.0	3.0
1023_0108908	2.0	3.0
1023_0108932	2.0	3.0
1023_0108993	3.0	3.0
1023_0109038	3.0	3.0
1023_0109396	2.0	3.0
1023_0109400	3.0	3.0
1023_0109505	3.0	3.0
1023_0109528	3.0	3.0
1023_0109614	2.0	2.0
1023_0109891	3.0	3.0
1023_0109917	5.0	3.0
1023_0109954	3.0	3.0
1031_0001949	3.0	4.0
1031_0001997	3.0	3.0
1031_0002010	3.0	3.0
1031_0002042	4.0	3.0
1031_0002086	3.0	3.0
1031_0002091	4.0	4.0
1031_0002184	3.0	4.0
1031_0002197	4.0	3.0
1031_0003042	3.0	3.0
1031_0003048	4.0	3.0
1031_0003053	3.0	3.0
1031_0003076	4.0	3.0
1031_0003078	3.0	3.0
1031_0003097	3.0	3.0
1031_0003131	3.0	4.0
1031_0003132	4.0	3.0
1031_0003145	4.0	3.0
1031_0003149	4.0	3.0
1031_0003150	3.0	4.0
1031_0003155	3.0	3.0
1031_0003160	3.0	3.0
1031_0003190	3.0	4.0
1031_0003206	3.0	3.0
1031_0003216	3.0	4.0
1031_0003224	3.0	3.0
1031_0003232	2.0	3.0
1031_0003236	3.0	3.0
1031_0003243	3.0	3.0
1031_0003246	4.0	3.0
1031_0003313	3.0	4.0
1031_0003314	4.0	4.0
1031_0003315	3.0	3.0
1031_0003331	3.0	3.0
1031_0003337	3.0	3.0
1031_0003355	3.0	3.0
1031_0003359	3.0	3.0
1031_0003407	3.0	3.0
1061_0012029	4.0	2.0
1061_0120271	2.0	2.0
1061_0120287	1.0	2.0
1061_0120298	2.0	2.0
1061_0120299	2.0	2.0
1061_0120307	2.0	2.0
1061_0120309	1.0	1.0
1061_0120310	2.0	2.0
1061_0120317	3.0	2.0
1061_0120319	2.0	2.0
1061_0120320	3.0	3.0
1061_0120321	2.0	2.0
1061_0120326	2.0	2.0
1061_0120334	2.0	2.0
1061_0120355	1.0	1.0
1061_0120359	2.0	2.0
1061_0120370	2.0	2.0
1061_0120376	2.0	2.0
1061_0120382	1.0	2.0
1061_0120387	1.0	2.0
1061_0120390	2.0	2.0
1061_0120414	2.0	2.0
1061_0120431	2.0	2.0
1061_0120438	3.0	2.0
1061_0120440	1.0	1.0
1061_0120442	3.0	2.0
1061_0120449	2.0	2.0
1061_0120483	2.0	2.0
1061_0120490	2.0	2.0
1061_0120859	2.0	2.0
1061_0120882	3.0	3.0
1061_1029115	1.0	2.0
1061_1029116	1.0	2.0
1061_1202911	0.0	2.0
1061_1202914	1.0	1.0
1061_1202916	2.0	2.0
1071_0024681	2.0	2.0
1071_0024692	2.0	2.0
1071_0024704	1.0	1.0
1071_0024712	1.0	1.0
1071_0024762	0.0	1.0
1071_0024768	1.0	1.0
1071_0024777	1.0	1.0
1071_0024778	0.0	0.0
1071_0024779	1.0	1.0
1071_0024784	1.0	1.0
1071_0024798	0.0	1.0
1071_0024800	0.0	1.0
1071_0024810	1.0	1.0
1071_0024816	1.0	1.0
1071_0024822	0.0	1.0
1071_0024824	1.0	1.0
1071_0024833	1.0	1.0
1071_0024834	2.0	2.0
1071_0024835	1.0	0.0
1071_0024853	0.0	0.0
1071_0024856	1.0	1.0
1071_0024860	1.0	0.0
1071_0024862	2.0	1.0
1071_0024864	0.0	0.0
1071_0024867	1.0	1.0
1071_0242011	1.0	1.0
1071_0242022	1.0	0.0
1071_0242042	0.0	1.0
1071_0243501	1.0	1.0
1071_0243582	1.0	1.0
1071_0243593	1.0	1.0
1071_0243621	2.0	1.0
1071_0243622	1.0	0.0
1071_0248310	1.0	1.0
1071_0248314	1.0	1.0
1071_0248317	0.0	0.0
1071_0248320	0.0	0.0
1071_0248322	1.0	1.0
1091_0000002	2.0	1.0
1091_0000004	1.0	1.0
1091_0000017	2.0	1.0
1091_0000019	1.0	1.0
1091_0000026	1.0	1.0
1091_0000028	1.0	1.0
1091_0000037	1.0	1.0
1091_0000049	1.0	1.0
1091_0000051	1.0	1.0
1091_0000052	1.0	1.0
1091_0000054	0.0	1.0
1091_0000060	2.0	2.0
1091_0000063	2.0	1.0
1091_0000064	2.0	1.0
1091_0000066	1.0	1.0
1091_0000068	1.0	1.0
1091_0000069	2.0	1.0
1091_0000073	3.0	1.0
1091_0000076	2.0	1.0
1091_0000079	1.0	2.0
1091_0000087	2.0	1.0
1091_0000102	2.0	1.0
1091_0000114	2.0	2.0
1091_0000116	2.0	2.0
1091_0000123	2.0	2.0
1091_0000140	2.0	1.0
1091_0000163	2.0	1.0
1091_0000166	2.0	1.0
1091_0000167	1.0	2.0
1091_0000198	2.0	2.0
1091_0000201	2.0	1.0
1091_0000205	1.0	2.0
1091_0000206	1.0	1.0
1091_0000211	1.0	2.0
1091_0000213	1.0	2.0
1091_0000220	1.0	2.0
1091_0000223	2.0	2.0
1091_0000237	2.0	2.0
1091_0000248	2.0	1.0
1091_0000253	1.0	1.0
1091_0000257	1.0	2.0
1091_0000261	2.0	1.0
1091_0000263	3.0	2.0
1091_0000264	2.0	1.0
1091_0000271	1.0	1.0
0606	2.0	2.0
0612	1.0	1.0
0620	1.0	2.0
0622	1.0	1.0
0627	2.0	2.0
0642	1.0	2.0
0645	2.0	2.0
0716	2.0	2.0
0718	2.0	2.0
0720	1.0	1.0
0721	2.0	2.0
0725	1.0	2.0
0802	2.0	1.0
0803	2.0	2.0
0808	2.0	1.0
0825	2.0	2.0
0828	2.0	1.0
0910	1.0	1.0
0918	1.0	2.0
0922	1.0	2.0
0926	2.0	2.0
0929	1.0	2.0
1004	1.0	1.0
1005	1.0	2.0
1014	2.0	1.0
1016	2.0	1.0
1018	2.0	2.0
1023	2.0	2.0
1116	2.0	2.0
1117	1.0	2.0
9999	1.0	1.0
BER0609003	3.0	2.0
BER0611003	2.0	2.0
BER0611007	3.0	2.0
KYJ0611003A	1.0	1.0
KYJ0611004A	1.0	1.0
KYJ0611006B	0.0	1.0
LIB0611001B	1.0	1.0
LON0611002A	1.0	1.0
LON0611003	3.0	3.0
MOS0611014	2.0	2.0
PAR1011008A	1.0	1.0
PAR1011013	2.0	2.0
PAR1011014	3.0	3.0
PHA0111003A	1.0	1.0
PHA0111016	4.0	3.0
PHA0111018	2.0	2.0
PHA0112003A	2.0	1.0
PHA0112007B	1.0	1.0
PHA0112012B	1.0	2.0
PHA0209008	2.0	1.0
PHA0209024	3.0	2.0
PHA0209028	2.0	3.0
PHA0209034	2.0	3.0
PHA0411008B	1.0	1.0
PHA0411011A	1.0	1.0
PHA0411011B	1.0	1.0
PHA0411028	2.0	2.0
PHA0411037	2.0	3.0
PHA0411042	2.0	3.0
PHA0411051	3.0	3.0
PHA0411055	3.0	3.0
PHA0411058	3.0	3.0
PHA0411060	3.0	3.0
PHA0509022	4.0	3.0
PHA0509035	3.0	3.0
PHA0509041	3.0	3.0
PHA0510010A	2.0	1.0
PHA0510023	4.0	3.0
PHA0510031	3.0	2.0
PHA0510037	2.0	2.0
PHA0510050	2.0	3.0
PHA0610006B	1.0	1.0
PHA0610016	3.0	3.0
PHA0610017	3.0	3.0
PHA0610025	3.0	3.0
PHA0710013	4.0	3.0
PHA0710016	3.0	2.0
PHA0710017	3.0	3.0
PHA0710019	4.0	3.0
PHA0810009	3.0	3.0
PHA0811017	3.0	3.0
PHA1109003	2.0	2.0
PHA1109006	2.0	2.0
PHA1110001A	2.0	1.0
PHA1110013	2.0	3.0
PHA1110015	3.0	3.0
PHA1110017	2.0	2.0
PHA1111006B	1.0	1.0
VAR0909007	3.0	2.0
VAR0909009	3.0	3.0
1325_1001009	3.0	3.0
1325_1001021	3.0	3.0
1325_1001032	2.0	3.0
1325_1001039	3.0	3.0
1325_1001044	3.0	3.0
1325_1001052	2.0	2.0
1325_1001055	3.0	3.0
1325_1001057	2.0	3.0
1325_1001062	3.0	3.0
1325_1001092	2.0	2.0
1325_1001095	2.0	2.0
1325_1001101	3.0	3.0
1325_1001108	3.0	3.0
1325_1001124	3.0	2.0
1325_1001127	3.0	3.0
1325_1001132	3.0	3.0
1325_1001133	3.0	3.0
1325_1001134	3.0	3.0
1325_1001135	3.0	3.0
1325_1001144	3.0	3.0
1325_1001167	3.0	3.0
1325_9000088	2.0	3.0
1325_9000095	2.0	3.0
1325_9000185	3.0	3.0
1325_9000211	2.0	3.0
1325_9000239	3.0	3.0
1325_9000303	2.0	3.0
1325_9000315	2.0	2.0
1325_9000534	3.0	3.0
1325_9000677	3.0	3.0
1325_9000700	3.0	3.0
1325_9000750	3.0	2.0
1365_0100002	3.0	2.0
1365_0100007	2.0	1.0
1365_0100009	2.0	1.0
1365_0100016	2.0	3.0
1365_0100019	2.0	2.0
1365_0100064	3.0	2.0
1365_0100069	3.0	2.0
1365_0100070	2.0	2.0
1365_0100073	3.0	2.0
1365_0100074	2.0	2.0
1365_0100095	2.0	2.0
1365_0100097	2.0	2.0
1365_0100117	3.0	2.0
1365_0100119	3.0	3.0
1365_0100123	2.0	3.0
1365_0100146	3.0	2.0
1365_0100162	3.0	2.0
1365_0100164	3.0	3.0
1365_0100169	2.0	2.0
1365_0100179	2.0	2.0
1365_0100181	2.0	2.0
1365_0100185	2.0	2.0
1365_0100186	2.0	2.0
1365_0100191	2.0	3.0
1365_0100192	3.0	3.0
1365_0100201	2.0	2.0
1365_0100205	2.0	2.0
1365_0100222	2.0	3.0
1365_0100253	2.0	2.0
1365_0100256	2.0	2.0
1365_0100258	2.0	2.0
1365_0100259	2.0	2.0
1365_0100262	3.0	3.0
1365_0100279	2.0	3.0
1365_0100286	2.0	2.0
1365_0100287	2.0	2.0
1365_0100288	2.0	2.0
1365_0100289	2.0	2.0
1365_0100448	2.0	2.0
1365_0100451	3.0	3.0
1365_0100457	3.0	3.0
1365_0100461	3.0	2.0
1365_0100469	2.0	2.0
1365_0100470	2.0	3.0
1385_0000013	1.0	1.0
1385_0000022	1.0	2.0
1385_0000039	1.0	1.0
1385_0000040	1.0	1.0
1385_0000041	1.0	1.0
1385_0000043	1.0	1.0
1385_0000052	1.0	1.0
1385_0000097	2.0	1.0
1385_0000114	2.0	1.0
1385_0000120	0.0	0.0
1385_0000123	1.0	1.0
1385_0000126	1.0	1.0
1385_0000127	2.0	1.0
1385_0000128	1.0	1.0
1385_0001105	1.0	1.0
1385_0001107	2.0	1.0
1385_0001109	2.0	1.0
1385_0001113	1.0	1.0
1385_0001119	2.0	1.0
1385_0001124	2.0	1.0
1385_0001127	2.0	1.0
1385_0001129	1.0	1.0
1385_0001131	1.0	1.0
1385_0001150	1.0	1.0
1385_0001152	2.0	2.0
1385_0001155	1.0	1.0
1385_0001158	1.0	1.0
1385_0001167	1.0	1.0
1385_0001188	0.0	1.0
1385_0001197	1.0	1.0
1385_0001501	1.0	1.0
1385_0001526	0.0	1.0
1385_0001716	1.0	1.0
1385_0001719	1.0	1.0
1385_0001734	1.0	1.0
1385_0001738	0.0	1.0
1385_0001756	2.0	1.0
1385_0001759	0.0	1.0
1385_0001768	2.0	1.0
1385_0001788	1.0	1.0
1385_0001790	1.0	1.0
1385_0001791	0.0	1.0
1385_0001792	1.0	1.0
1385_0001793	0.0	1.0
1385_0001794	1.0	1.0
1385_0001798	1.0	1.0
1385_0001799	1.0	2.0
1395_0000341	2.0	1.0
1395_0000359	2.0	1.0
1395_0000368	1.0	0.0
1395_0000369	2.0	2.0
1395_0000383	2.0	1.0
1395_0000391	3.0	2.0
1395_0000403	2.0	2.0
1395_0000409	2.0	1.0
1395_0000413	2.0	2.0
1395_0000443	2.0	2.0
1395_0000449	2.0	2.0
1395_0000455	2.0	1.0
1395_0000462	2.0	1.0
1395_0000518	2.0	2.0
1395_0000529	2.0	1.0
1395_0000537	2.0	2.0
1395_0000555	1.0	1.0
1395_0000556	1.0	1.0
1395_0000572	1.0	1.0
1395_0000582	0.0	1.0
1395_0000595	0.0	1.0
1395_0000598	0.0	1.0
1395_0000604	0.0	1.0
1395_0000606	0.0	1.0
1395_0000611	0.0	1.0
1395_0000635	0.0	1.0
1395_0001016	1.0	1.0
1395_0001022	1.0	1.0
1395_0001023	1.0	1.0
1395_0001067	0.0	1.0
1395_0001068	0.0	1.0
1395_0001084	1.0	1.0
1395_0001114	1.0	1.0
1395_0001117	1.0	1.0
1395_0001118	0.0	1.0
1395_0001119	2.0	2.0
1395_0001146	0.0	1.0
1395_0001164	2.0	2.0
5 Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 1.25
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.05
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        31
         1.0       0.55      0.40      0.46       119
         2.0       0.52      0.66      0.58       165
         3.0       0.58      0.78      0.67       115
         4.0       0.00      0.00      0.00        20
         5.0       0.00      0.00      0.00         2

    accuracy                           0.55       452
   macro avg       0.27      0.31      0.29       452
weighted avg       0.48      0.55      0.50       452

[[  0  25   6   0   0   0]
 [  0  48  71   0   0   0]
 [  0  14 109  42   0   0]
 [  0   1  24  90   0   0]
 [  0   0   0  20   0   0]
 [  0   0   0   2   0   0]]
0.5045576961494579
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.99
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.98
              precision    recall  f1-score   support

         0.0       1.00      0.10      0.18        31
         1.0       0.52      0.57      0.55       119
         2.0       0.55      0.53      0.54       165
         3.0       0.58      0.82      0.68       115
         4.0       0.00      0.00      0.00        20
         5.0       0.00      0.00      0.00         2

    accuracy                           0.56       452
   macro avg       0.44      0.34      0.32       452
weighted avg       0.56      0.56      0.53       452

[[ 3 27  1  0  0  0]
 [ 0 68 51  0  0  0]
 [ 0 32 87 46  0  0]
 [ 0  3 18 94  0  0]
 [ 0  0  0 20  0  0]
 [ 0  0  0  2  0  0]]
0.5258376202887443
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.89
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.97
              precision    recall  f1-score   support

         0.0       0.75      0.19      0.31        31
         1.0       0.55      0.64      0.59       119
         2.0       0.56      0.50      0.53       165
         3.0       0.57      0.79      0.66       115
         4.0       0.00      0.00      0.00        20
         5.0       0.00      0.00      0.00         2

    accuracy                           0.57       452
   macro avg       0.41      0.35      0.35       452
weighted avg       0.55      0.57      0.54       452

[[ 6 24  1  0  0  0]
 [ 1 76 42  0  0  0]
 [ 1 34 83 47  0  0]
 [ 0  3 21 91  0  0]
 [ 0  0  0 20  0  0]
 [ 0  0  0  2  0  0]]
0.5400271327124203
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.82
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.94
              precision    recall  f1-score   support

         0.0       0.71      0.39      0.50        31
         1.0       0.61      0.59      0.60       119
         2.0       0.58      0.61      0.59       165
         3.0       0.59      0.77      0.67       115
         4.0       0.00      0.00      0.00        20
         5.0       0.00      0.00      0.00         2

    accuracy                           0.60       452
   macro avg       0.42      0.39      0.39       452
weighted avg       0.57      0.60      0.58       452

[[ 12  17   2   0   0   0]
 [  4  70  45   0   0   0]
 [  1  26 100  38   0   0]
 [  0   2  25  88   0   0]
 [  0   0   0  20   0   0]
 [  0   0   0   2   0   0]]
0.578711514969497
452 452 452
Filename	True Label	Prediction
1023_0001418	4.0	3.0
1023_0001575	3.0	3.0
1023_0101688	4.0	3.0
1023_0101690	2.0	3.0
1023_0101752	3.0	3.0
1023_0101896	3.0	2.0
1023_0101901	3.0	3.0
1023_0103826	3.0	3.0
1023_0103828	1.0	2.0
1023_0103831	3.0	3.0
1023_0103833	5.0	3.0
1023_0103837	3.0	3.0
1023_0103841	5.0	3.0
1023_0103883	3.0	3.0
1023_0104203	2.0	3.0
1023_0107042	4.0	3.0
1023_0107725	3.0	3.0
1023_0107726	3.0	3.0
1023_0107787	2.0	3.0
1023_0108304	3.0	3.0
1023_0108305	3.0	3.0
1023_0108520	3.0	3.0
1023_0109033	4.0	3.0
1023_0109151	3.0	3.0
1023_0109249	2.0	3.0
1023_0109401	3.0	3.0
1023_0109422	4.0	3.0
1023_0109495	3.0	3.0
1023_0109515	4.0	3.0
1023_0109519	2.0	2.0
1023_0109522	3.0	3.0
1023_0109527	4.0	3.0
1023_0109606	3.0	3.0
1023_0109609	3.0	2.0
1023_0109649	3.0	3.0
1023_0109716	3.0	3.0
1023_0109878	2.0	3.0
1023_0109890	3.0	3.0
1023_0109914	2.0	2.0
1031_0001703	4.0	3.0
1031_0002002	2.0	3.0
1031_0002084	4.0	3.0
1031_0002131	3.0	3.0
1031_0003063	4.0	3.0
1031_0003088	4.0	3.0
1031_0003090	4.0	3.0
1031_0003091	2.0	3.0
1031_0003092	3.0	3.0
1031_0003095	3.0	3.0
1031_0003098	4.0	3.0
1031_0003121	3.0	3.0
1031_0003130	4.0	3.0
1031_0003133	4.0	3.0
1031_0003135	3.0	3.0
1031_0003141	3.0	3.0
1031_0003162	3.0	3.0
1031_0003167	3.0	3.0
1031_0003169	3.0	3.0
1031_0003173	3.0	3.0
1031_0003179	3.0	3.0
1031_0003187	4.0	3.0
1031_0003191	3.0	3.0
1031_0003233	3.0	3.0
1031_0003240	2.0	3.0
1031_0003261	3.0	3.0
1031_0003262	3.0	3.0
1031_0003273	3.0	3.0
1031_0003352	2.0	3.0
1031_0003357	3.0	3.0
1031_0003384	2.0	2.0
1031_0003393	3.0	3.0
1031_0003410	3.0	3.0
1031_0003414	3.0	3.0
1031_0003415	4.0	3.0
1061_0120275	2.0	2.0
1061_0120281	1.0	2.0
1061_0120283	1.0	1.0
1061_0120285	2.0	2.0
1061_0120288	2.0	2.0
1061_0120296	2.0	2.0
1061_0120300	3.0	2.0
1061_0120301	2.0	2.0
1061_0120302	1.0	2.0
1061_0120303	1.0	1.0
1061_0120308	2.0	2.0
1061_0120315	2.0	2.0
1061_0120316	2.0	2.0
1061_0120318	1.0	2.0
1061_0120324	2.0	2.0
1061_0120327	2.0	2.0
1061_0120330	2.0	2.0
1061_0120335	3.0	3.0
1061_0120343	2.0	2.0
1061_0120348	1.0	1.0
1061_0120352	1.0	1.0
1061_0120357	3.0	3.0
1061_0120368	2.0	2.0
1061_0120388	2.0	2.0
1061_0120410	2.0	2.0
1061_0120415	2.0	2.0
1061_0120421	2.0	2.0
1061_0120423	2.0	3.0
1061_0120427	2.0	2.0
1061_0120453	2.0	2.0
1061_0120457	3.0	2.0
1061_0120478	2.0	2.0
1061_0120486	2.0	2.0
1061_0120487	2.0	2.0
1061_0120488	3.0	2.0
1061_0120500	2.0	2.0
1061_0120853	2.0	2.0
1061_0120875	3.0	3.0
1061_0120880	3.0	3.0
1061_0120883	2.0	2.0
1061_0120884	2.0	2.0
1061_0120888	1.0	2.0
1061_0120889	1.0	1.0
1061_0120890	1.0	1.0
1061_1029113	2.0	2.0
1061_1029114	1.0	1.0
1061_1029117	2.0	2.0
1061_1202910	3.0	2.0
1061_1202912	2.0	2.0
1071_0024682	2.0	2.0
1071_0024685	2.0	2.0
1071_0024691	1.0	2.0
1071_0024693	1.0	1.0
1071_0024694	1.0	2.0
1071_0024703	1.0	1.0
1071_0024709	2.0	2.0
1071_0024714	2.0	1.0
1071_0024757	2.0	2.0
1071_0024767	2.0	1.0
1071_0024770	1.0	1.0
1071_0024774	0.0	0.0
1071_0024775	0.0	0.0
1071_0024776	0.0	0.0
1071_0024801	1.0	1.0
1071_0024806	1.0	1.0
1071_0024807	1.0	1.0
1071_0024808	1.0	1.0
1071_0024809	0.0	0.0
1071_0024812	0.0	0.0
1071_0024813	0.0	1.0
1071_0024818	2.0	1.0
1071_0024819	1.0	1.0
1071_0024821	0.0	1.0
1071_0024823	1.0	1.0
1071_0024825	1.0	1.0
1071_0024836	2.0	2.0
1071_0024841	0.0	1.0
1071_0024847	1.0	2.0
1071_0024849	0.0	0.0
1071_0024852	0.0	0.0
1071_0024854	0.0	0.0
1071_0024866	2.0	2.0
1071_0024877	1.0	1.0
1071_0241831	1.0	1.0
1071_0242073	1.0	1.0
1071_0242092	0.0	0.0
1071_0243591	1.0	1.0
1071_0248302	1.0	0.0
1071_0248308	1.0	1.0
1071_0248321	1.0	1.0
1071_0248324	0.0	1.0
1071_0248328	0.0	0.0
1071_0248329	1.0	1.0
1071_0248331	1.0	1.0
1071_0248332	2.0	2.0
1071_0248341	1.0	1.0
1071_0248343	1.0	1.0
1071_0248347	1.0	0.0
1071_0248349	0.0	1.0
1091_0000003	2.0	1.0
1091_0000006	0.0	1.0
1091_0000007	3.0	2.0
1091_0000009	0.0	1.0
1091_0000023	2.0	1.0
1091_0000024	3.0	1.0
1091_0000042	0.0	1.0
1091_0000057	2.0	1.0
1091_0000058	2.0	2.0
1091_0000061	2.0	0.0
1091_0000070	2.0	1.0
1091_0000077	2.0	1.0
1091_0000126	2.0	2.0
1091_0000144	1.0	1.0
1091_0000148	1.0	1.0
1091_0000158	2.0	2.0
1091_0000159	2.0	2.0
1091_0000162	1.0	2.0
1091_0000170	3.0	1.0
1091_0000173	2.0	1.0
1091_0000190	1.0	1.0
1091_0000196	2.0	1.0
1091_0000208	1.0	2.0
1091_0000212	1.0	2.0
1091_0000215	1.0	2.0
1091_0000218	2.0	1.0
1091_0000227	1.0	1.0
1091_0000233	2.0	2.0
1091_0000242	1.0	1.0
1091_0000246	2.0	2.0
1091_0000255	0.0	2.0
1091_0000256	1.0	2.0
1091_0000267	1.0	2.0
1091_0000268	2.0	2.0
1091_0000269	1.0	2.0
0608	1.0	2.0
0609	2.0	2.0
0615	1.0	1.0
0623	1.0	2.0
0624	2.0	2.0
0629	2.0	2.0
0632	1.0	1.0
0634	3.0	2.0
0636	2.0	2.0
0637	2.0	2.0
0641	1.0	1.0
0724	3.0	2.0
0804	1.0	2.0
0806	2.0	2.0
0816	3.0	2.0
0817	1.0	2.0
0818	1.0	2.0
0820	2.0	1.0
0823	1.0	2.0
0903	1.0	2.0
0912	1.0	2.0
0913	2.0	2.0
1008	1.0	2.0
1017	2.0	2.0
BER0611006	3.0	3.0
KYJ0611009B	0.0	1.0
LIB0611001A	1.0	1.0
MOS0509001	2.0	2.0
MOS0611013	2.0	3.0
PAR1011017	3.0	3.0
PAR1011018	3.0	2.0
PHA0111001A	1.0	1.0
PHA0111003B	1.0	1.0
PHA0111005B	1.0	1.0
PHA0111011	3.0	3.0
PHA0111014	1.0	2.0
PHA0112006B	3.0	2.0
PHA0209001	2.0	2.0
PHA0411010A	1.0	1.0
PHA0411012A	1.0	1.0
PHA0411012B	1.0	1.0
PHA0411032	2.0	3.0
PHA0411038	3.0	3.0
PHA0411043	3.0	2.0
PHA0411044	3.0	3.0
PHA0411062	2.0	3.0
PHA0509013	1.0	1.0
PHA0509020	3.0	3.0
PHA0509024	3.0	3.0
PHA0509027	2.0	2.0
PHA0509030	3.0	3.0
PHA0509039	2.0	3.0
PHA0510003A	2.0	2.0
PHA0510004A	1.0	1.0
PHA0510010B	1.0	1.0
PHA0610006A	2.0	1.0
PHA0610007B	1.0	1.0
PHA0610018	3.0	3.0
PHA0610026	3.0	3.0
PHA0710021	4.0	3.0
PHA0810001	2.0	3.0
PHA0810004	3.0	2.0
PHA0810006	2.0	2.0
PHA0810012	2.0	2.0
PHA0811010	2.0	2.0
PHA0811014	2.0	2.0
PHA1109024	4.0	3.0
PHA1109027	4.0	3.0
PHA1110001B	1.0	1.0
PHA1110003A	1.0	2.0
PHA1110019	2.0	2.0
PHA1110021	3.0	3.0
PHA1111001A	2.0	1.0
PHA1111002A	1.0	1.0
PHA1111004B	1.0	1.0
TI071122B	2.0	1.0
VAR0909005	2.0	2.0
VAR0909008	3.0	2.0
VAR0910006	2.0	3.0
VAR0910009	3.0	3.0
VAR0910010	3.0	2.0
1325_1001008	3.0	3.0
1325_1001011	3.0	3.0
1325_1001015	2.0	3.0
1325_1001017	3.0	2.0
1325_1001019	3.0	3.0
1325_1001024	2.0	3.0
1325_1001027	3.0	3.0
1325_1001037	3.0	2.0
1325_1001041	3.0	3.0
1325_1001045	3.0	3.0
1325_1001046	2.0	3.0
1325_1001048	2.0	3.0
1325_1001053	2.0	2.0
1325_1001059	2.0	3.0
1325_1001075	2.0	3.0
1325_1001077	3.0	3.0
1325_1001078	3.0	3.0
1325_1001083	2.0	2.0
1325_1001089	2.0	3.0
1325_1001091	2.0	3.0
1325_1001099	3.0	3.0
1325_1001126	2.0	3.0
1325_1001128	3.0	3.0
1325_1001130	3.0	2.0
1325_1001143	3.0	3.0
1325_1001160	3.0	3.0
1325_1001161	2.0	3.0
1325_1001163	2.0	3.0
1325_9000087	2.0	3.0
1325_9000099	3.0	3.0
1325_9000102	3.0	3.0
1325_9000136	3.0	3.0
1325_9000137	3.0	3.0
1325_9000144	3.0	3.0
1325_9000186	3.0	3.0
1325_9000214	3.0	3.0
1325_9000241	3.0	3.0
1325_9000316	3.0	3.0
1325_9000318	3.0	3.0
1325_9000504	3.0	3.0
1325_9000684	3.0	3.0
1365_0100003	2.0	1.0
1365_0100006	2.0	3.0
1365_0100010	2.0	2.0
1365_0100022	2.0	2.0
1365_0100031	2.0	2.0
1365_0100051	2.0	2.0
1365_0100056	2.0	2.0
1365_0100067	2.0	2.0
1365_0100079	2.0	2.0
1365_0100093	2.0	2.0
1365_0100099	2.0	2.0
1365_0100106	2.0	3.0
1365_0100120	3.0	3.0
1365_0100125	3.0	3.0
1365_0100133	2.0	2.0
1365_0100139	2.0	2.0
1365_0100170	2.0	3.0
1365_0100172	2.0	2.0
1365_0100175	2.0	2.0
1365_0100176	2.0	2.0
1365_0100177	3.0	2.0
1365_0100187	3.0	2.0
1365_0100194	3.0	3.0
1365_0100195	2.0	2.0
1365_0100203	2.0	2.0
1365_0100212	3.0	3.0
1365_0100218	3.0	2.0
1365_0100221	2.0	2.0
1365_0100223	2.0	3.0
1365_0100227	3.0	3.0
1365_0100230	2.0	3.0
1365_0100268	2.0	2.0
1365_0100274	3.0	3.0
1365_0100275	3.0	2.0
1365_0100280	1.0	2.0
1365_0100281	2.0	2.0
1365_0100455	3.0	3.0
1365_0100456	3.0	3.0
1365_0100473	2.0	3.0
1365_0100474	2.0	3.0
1365_0100475	2.0	3.0
1365_0100476	2.0	3.0
1365_0100481	2.0	3.0
1365_0100482	2.0	2.0
1385_0000048	1.0	1.0
1385_0000051	2.0	2.0
1385_0000053	2.0	1.0
1385_0000057	1.0	1.0
1385_0000058	1.0	1.0
1385_0000059	2.0	1.0
1385_0000099	1.0	1.0
1385_0000100	2.0	1.0
1385_0000125	2.0	1.0
1385_0000130	2.0	1.0
1385_0001112	2.0	2.0
1385_0001125	2.0	1.0
1385_0001153	2.0	2.0
1385_0001165	1.0	2.0
1385_0001166	1.0	1.0
1385_0001171	0.0	0.0
1385_0001178	0.0	1.0
1385_0001189	0.0	1.0
1385_0001195	2.0	2.0
1385_0001196	1.0	1.0
1385_0001503	1.0	1.0
1385_0001523	1.0	2.0
1385_0001524	1.0	1.0
1385_0001525	1.0	2.0
1385_0001528	1.0	2.0
1385_0001712	1.0	2.0
1385_0001715	0.0	2.0
1385_0001724	1.0	2.0
1385_0001725	1.0	1.0
1385_0001726	1.0	2.0
1385_0001732	0.0	1.0
1385_0001736	1.0	2.0
1385_0001740	1.0	1.0
1385_0001741	0.0	1.0
1385_0001746	1.0	1.0
1385_0001785	1.0	1.0
1395_0000357	3.0	2.0
1395_0000361	2.0	2.0
1395_0000389	1.0	0.0
1395_0000399	2.0	2.0
1395_0000451	1.0	2.0
1395_0000460	1.0	1.0
1395_0000465	1.0	1.0
1395_0000470	2.0	1.0
1395_0000514	3.0	2.0
1395_0000515	2.0	2.0
1395_0000516	1.0	0.0
1395_0000547	2.0	2.0
1395_0000549	2.0	2.0
1395_0000551	2.0	2.0
1395_0000552	2.0	2.0
1395_0000553	2.0	1.0
1395_0000564	1.0	1.0
1395_0000565	2.0	2.0
1395_0000587	0.0	1.0
1395_0000591	0.0	0.0
1395_0000596	2.0	1.0
1395_0000608	0.0	1.0
1395_0000626	1.0	2.0
1395_0000627	1.0	1.0
1395_0000646	1.0	1.0
1395_0001010	1.0	1.0
1395_0001017	1.0	1.0
1395_0001028	1.0	2.0
1395_0001033	1.0	2.0
1395_0001045	2.0	1.0
1395_0001069	2.0	2.0
1395_0001073	2.0	2.0
1395_0001090	1.0	2.0
1395_0001101	1.0	1.0
1395_0001103	1.0	1.0
1395_0001108	0.0	1.0
1395_0001115	1.0	2.0
1395_0001122	0.0	1.0
1395_0001158	1.0	2.0
1395_0001160	1.0	2.0
1395_0001161	1.0	2.0
1395_0001167	1.0	2.0
Averaged weighted F1-scores 0.5733269820527842
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
144.90552995391704 65.35717405024758
LANGUAGE: IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
1 Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 1.11
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.91
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        38
         1.0       0.62      0.62      0.62       133
         2.0       0.66      0.75      0.70       165
         3.0       0.65      0.89      0.75        97
         4.0       0.00      0.00      0.00        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.65       452
   macro avg       0.32      0.38      0.35       452
weighted avg       0.56      0.65      0.60       452

[[  0  34   4   0   0   0]
 [  0  83  49   1   0   0]
 [  0  16 123  26   0   0]
 [  0   0  11  86   0   0]
 [  0   0   0  18   0   0]
 [  0   0   0   1   0   0]]
0.5999300778683773
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.82
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.84
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        38
         1.0       0.62      0.68      0.65       133
         2.0       0.70      0.76      0.73       165
         3.0       0.68      0.89      0.77        97
         4.0       0.00      0.00      0.00        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.67       452
   macro avg       0.33      0.39      0.36       452
weighted avg       0.58      0.67      0.62       452

[[  0  36   2   0   0   0]
 [  0  91  41   1   0   0]
 [  0  20 125  20   0   0]
 [  0   0  11  86   0   0]
 [  0   0   0  18   0   0]
 [  0   0   0   1   0   0]]
0.6220772880429917
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.71
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.86
              precision    recall  f1-score   support

         0.0       0.53      0.21      0.30        38
         1.0       0.66      0.65      0.66       133
         2.0       0.69      0.71      0.70       165
         3.0       0.64      0.89      0.74        97
         4.0       0.00      0.00      0.00        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.66       452
   macro avg       0.42      0.41      0.40       452
weighted avg       0.63      0.66      0.63       452

[[  8  27   3   0   0   0]
 [  6  87  39   1   0   0]
 [  1  18 117  29   0   0]
 [  0   0  11  86   0   0]
 [  0   0   0  18   0   0]
 [  0   0   0   1   0   0]]
0.6326715142626812
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.62
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.90
              precision    recall  f1-score   support

         0.0       0.50      0.32      0.39        38
         1.0       0.67      0.62      0.64       133
         2.0       0.68      0.69      0.69       165
         3.0       0.62      0.89      0.73        97
         4.0       0.00      0.00      0.00        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.65       452
   macro avg       0.41      0.42      0.41       452
weighted avg       0.62      0.65      0.63       452

[[ 12  24   2   0   0   0]
 [ 10  82  40   1   0   0]
 [  2  17 114  32   0   0]
 [  0   0  11  86   0   0]
 [  0   0   0  18   0   0]
 [  0   0   0   1   0   0]]
0.6288092922837479
452 452 452
Filename	True Label	Prediction
1023_0101694	3.0	3.0
1023_0101751	3.0	3.0
1023_0101752	3.0	3.0
1023_0101846	4.0	3.0
1023_0101851	2.0	3.0
1023_0101852	2.0	3.0
1023_0101853	2.0	3.0
1023_0101855	2.0	2.0
1023_0101893	3.0	3.0
1023_0101904	2.0	2.0
1023_0102117	2.0	3.0
1023_0103823	3.0	3.0
1023_0103824	3.0	3.0
1023_0103825	2.0	3.0
1023_0103827	3.0	3.0
1023_0103828	1.0	2.0
1023_0103841	3.0	3.0
1023_0103843	2.0	2.0
1023_0103955	4.0	3.0
1023_0107074	3.0	3.0
1023_0107672	3.0	3.0
1023_0108518	3.0	3.0
1023_0108648	3.0	3.0
1023_0108649	3.0	3.0
1023_0108885	3.0	3.0
1023_0108886	3.0	3.0
1023_0108888	3.0	3.0
1023_0108889	3.0	3.0
1023_0108931	2.0	3.0
1023_0108934	2.0	3.0
1023_0108958	3.0	3.0
1023_0108992	2.0	3.0
1023_0109096	3.0	3.0
1023_0109402	2.0	3.0
1023_0109519	2.0	2.0
1023_0109520	3.0	3.0
1023_0109524	3.0	3.0
1023_0109527	3.0	3.0
1023_0109590	3.0	3.0
1023_0109591	2.0	3.0
1023_0109721	2.0	3.0
1023_0109878	2.0	3.0
1023_0109914	2.0	3.0
1023_0109945	3.0	3.0
1031_0002002	2.0	3.0
1031_0002010	3.0	3.0
1031_0002032	3.0	3.0
1031_0002086	3.0	3.0
1031_0002087	4.0	3.0
1031_0002196	3.0	3.0
1031_0003043	4.0	3.0
1031_0003052	4.0	3.0
1031_0003074	4.0	3.0
1031_0003078	3.0	3.0
1031_0003091	3.0	3.0
1031_0003098	4.0	3.0
1031_0003121	3.0	3.0
1031_0003129	3.0	3.0
1031_0003130	4.0	3.0
1031_0003132	4.0	3.0
1031_0003174	4.0	3.0
1031_0003182	4.0	3.0
1031_0003203	3.0	3.0
1031_0003206	3.0	3.0
1031_0003214	3.0	3.0
1031_0003221	3.0	3.0
1031_0003238	3.0	3.0
1031_0003245	3.0	3.0
1031_0003330	3.0	3.0
1031_0003331	3.0	3.0
1031_0003337	4.0	3.0
1031_0003352	3.0	3.0
1031_0003356	3.0	3.0
1031_0003358	4.0	3.0
1031_0003368	3.0	3.0
1031_0003369	4.0	3.0
1031_0003387	4.0	3.0
1031_0003388	3.0	3.0
1031_0003392	4.0	3.0
1031_0003393	4.0	3.0
1031_0003409	5.0	3.0
1031_0003414	4.0	3.0
1061_0120283	0.0	1.0
1061_0120284	0.0	1.0
1061_0120287	1.0	2.0
1061_0120296	2.0	2.0
1061_0120299	2.0	2.0
1061_0120301	2.0	2.0
1061_0120308	3.0	2.0
1061_0120320	3.0	3.0
1061_0120321	2.0	2.0
1061_0120324	2.0	2.0
1061_0120325	2.0	2.0
1061_0120329	2.0	3.0
1061_0120335	2.0	3.0
1061_0120347	1.0	2.0
1061_0120354	1.0	2.0
1061_0120376	2.0	2.0
1061_0120384	2.0	1.0
1061_0120389	2.0	2.0
1061_0120390	2.0	2.0
1061_0120413	2.0	1.0
1061_0120428	2.0	2.0
1061_0120429	3.0	2.0
1061_0120440	1.0	1.0
1061_0120441	2.0	2.0
1061_0120459	2.0	2.0
1061_0120481	3.0	3.0
1061_0120486	1.0	2.0
1061_0120492	2.0	3.0
1061_0120875	3.0	3.0
1061_0120877	3.0	2.0
1061_0120886	2.0	2.0
1061_1029117	1.0	2.0
1061_1202914	1.0	1.0
1071_0024680	2.0	2.0
1071_0024683	1.0	1.0
1071_0024772	0.0	0.0
1071_0024774	0.0	0.0
1071_0024803	0.0	1.0
1071_0024821	0.0	0.0
1071_0024825	0.0	0.0
1071_0024827	1.0	1.0
1071_0024852	0.0	0.0
1071_0024854	0.0	0.0
1071_0024861	0.0	1.0
1071_0024863	1.0	1.0
1071_0024867	2.0	1.0
1071_0024871	1.0	1.0
1071_0024873	1.0	0.0
1071_0241833	1.0	1.0
1071_0242093	0.0	0.0
1071_0248302	1.0	0.0
1071_0248307	2.0	1.0
1071_0248308	1.0	1.0
1071_0248311	1.0	1.0
1071_0248313	1.0	2.0
1071_0248316	1.0	0.0
1071_0248325	0.0	1.0
1071_0248329	1.0	1.0
1071_0248333	2.0	1.0
1071_0248335	1.0	1.0
1071_0248338	2.0	1.0
1071_0248339	1.0	1.0
1071_0248340	0.0	0.0
1071_0248341	1.0	1.0
1071_0248346	1.0	1.0
1091_0000002	2.0	2.0
1091_0000006	0.0	1.0
1091_0000014	0.0	1.0
1091_0000019	2.0	2.0
1091_0000025	1.0	1.0
1091_0000027	1.0	1.0
1091_0000043	1.0	2.0
1091_0000047	2.0	1.0
1091_0000049	1.0	1.0
1091_0000053	0.0	1.0
1091_0000054	0.0	1.0
1091_0000060	2.0	2.0
1091_0000066	2.0	1.0
1091_0000067	2.0	2.0
1091_0000127	1.0	2.0
1091_0000154	1.0	3.0
1091_0000160	2.0	3.0
1091_0000162	2.0	2.0
1091_0000164	1.0	1.0
1091_0000166	1.0	2.0
1091_0000167	1.0	2.0
1091_0000171	1.0	2.0
1091_0000173	2.0	2.0
1091_0000191	1.0	2.0
1091_0000207	2.0	2.0
1091_0000214	2.0	1.0
1091_0000218	2.0	1.0
1091_0000219	1.0	2.0
1091_0000223	2.0	2.0
1091_0000224	2.0	0.0
1091_0000232	2.0	2.0
1091_0000236	2.0	2.0
1091_0000237	2.0	2.0
1091_0000241	1.0	1.0
1091_0000243	0.0	2.0
1091_0000246	2.0	2.0
1091_0000250	2.0	2.0
1091_0000253	2.0	0.0
1091_0000257	2.0	2.0
1091_0000260	3.0	2.0
1091_0000269	1.0	2.0
1091_0000273	1.0	2.0
1091_0000274	1.0	2.0
1091_0000276	2.0	2.0
0602	2.0	2.0
0604	2.0	2.0
0611	2.0	2.0
0612	2.0	2.0
0615	2.0	2.0
0619	2.0	2.0
0621	2.0	2.0
0622	2.0	2.0
0634	2.0	2.0
0642	2.0	2.0
0643	2.0	2.0
0714	2.0	2.0
0723	1.0	2.0
0725	2.0	2.0
0804	2.0	2.0
0806	2.0	2.0
0808	2.0	2.0
0823	2.0	2.0
0825	2.0	2.0
0901	3.0	2.0
0919	2.0	2.0
0927	2.0	2.0
0930	2.0	2.0
1002	2.0	2.0
1005	2.0	2.0
1015	2.0	2.0
1018	2.0	2.0
1019	2.0	2.0
1020	2.0	2.0
1021	2.0	2.0
1116	2.0	2.0
1117	2.0	2.0
BER0611003	3.0	3.0
BER0611005	3.0	2.0
KYJ0611003A	1.0	1.0
KYJ0611005B	1.0	1.0
KYJ0611009B	1.0	2.0
LIB0611003A	2.0	2.0
LON0611002B	1.0	1.0
MOS0509001	2.0	2.0
MOS0611015	3.0	3.0
PAR1011009B	1.0	1.0
PAR1011018	3.0	3.0
PHA0111001B	1.0	1.0
PHA0111004B	1.0	2.0
PHA0111005B	2.0	1.0
PHA0112002A	1.0	1.0
PHA0112002B	2.0	2.0
PHA0112006A	3.0	2.0
PHA0112009A	2.0	2.0
PHA0112012A	1.0	2.0
PHA0112012B	1.0	2.0
PHA0209024	3.0	3.0
PHA0209038	3.0	3.0
PHA0411009B	2.0	1.0
PHA0411011A	1.0	1.0
PHA0411012A	2.0	1.0
PHA0411012B	1.0	1.0
PHA0411027	3.0	3.0
PHA0411035	3.0	3.0
PHA0411036	3.0	3.0
PHA0411037	2.0	3.0
PHA0411051	3.0	3.0
PHA0411060	3.0	3.0
PHA0509007	1.0	1.0
PHA0509017	3.0	3.0
PHA0509022	3.0	3.0
PHA0509024	3.0	3.0
PHA0509027	2.0	3.0
PHA0509037	3.0	3.0
PHA0509041	3.0	3.0
PHA0509044	3.0	3.0
PHA0510010A	1.0	2.0
PHA0510023	3.0	3.0
PHA0510032	3.0	3.0
PHA0510046	2.0	3.0
PHA0510048	3.0	3.0
PHA0510049	3.0	3.0
PHA0610025	3.0	3.0
PHA0810002	3.0	3.0
PHA0810012	3.0	3.0
PHA0810015	3.0	3.0
PHA0811012	3.0	3.0
PHA0811013	3.0	3.0
PHA1109002	3.0	3.0
PHA1109007	2.0	3.0
PHA1110015	3.0	3.0
PHA1110016	2.0	2.0
PHA1110017	3.0	3.0
PHA1111001B	1.0	1.0
PHA1111002A	1.0	1.0
PHA1111006A	1.0	1.0
PHA1111008A	2.0	1.0
ST071122B	1.0	1.0
VAR0909003	3.0	3.0
VAR0909007	3.0	3.0
VAR0909008	3.0	2.0
VAR0909009	3.0	3.0
VAR0909010	3.0	3.0
VAR0910004	3.0	3.0
VAR0910011	3.0	3.0
1325_1001017	2.0	2.0
1325_1001018	2.0	3.0
1325_1001035	3.0	3.0
1325_1001045	2.0	2.0
1325_1001046	2.0	2.0
1325_1001051	2.0	2.0
1325_1001078	2.0	2.0
1325_1001083	2.0	2.0
1325_1001088	2.0	2.0
1325_1001097	0.0	2.0
1325_1001107	2.0	3.0
1325_1001108	3.0	3.0
1325_1001110	2.0	3.0
1325_1001122	2.0	2.0
1325_1001124	2.0	2.0
1325_1001129	1.0	2.0
1325_1001130	2.0	2.0
1325_1001132	2.0	3.0
1325_1001158	2.0	2.0
1325_1001160	2.0	3.0
1325_1001162	2.0	2.0
1325_1001169	2.0	2.0
1325_9000059	2.0	3.0
1325_9000087	2.0	2.0
1325_9000099	3.0	2.0
1325_9000152	2.0	3.0
1325_9000187	2.0	3.0
1325_9000210	1.0	2.0
1325_9000241	3.0	3.0
1325_9000296	1.0	2.0
1325_9000303	2.0	2.0
1325_9000315	2.0	2.0
1325_9000317	3.0	3.0
1325_9000323	2.0	2.0
1325_9000504	2.0	3.0
1325_9000534	2.0	2.0
1325_9000612	2.0	2.0
1325_9000675	2.0	2.0
1325_9000700	2.0	2.0
1365_0100006	2.0	2.0
1365_0100019	1.0	1.0
1365_0100020	2.0	2.0
1365_0100021	2.0	2.0
1365_0100022	2.0	2.0
1365_0100057	2.0	2.0
1365_0100065	1.0	1.0
1365_0100067	1.0	2.0
1365_0100070	2.0	2.0
1365_0100079	2.0	2.0
1365_0100093	1.0	2.0
1365_0100099	1.0	2.0
1365_0100101	2.0	2.0
1365_0100104	2.0	2.0
1365_0100105	3.0	2.0
1365_0100117	2.0	2.0
1365_0100134	2.0	2.0
1365_0100137	1.0	2.0
1365_0100139	1.0	2.0
1365_0100165	2.0	2.0
1365_0100168	2.0	2.0
1365_0100174	1.0	1.0
1365_0100181	1.0	1.0
1365_0100185	1.0	1.0
1365_0100186	1.0	2.0
1365_0100188	2.0	2.0
1365_0100190	2.0	3.0
1365_0100191	2.0	2.0
1365_0100196	1.0	2.0
1365_0100199	2.0	2.0
1365_0100223	2.0	2.0
1365_0100228	1.0	2.0
1365_0100230	2.0	2.0
1365_0100255	1.0	2.0
1365_0100256	2.0	2.0
1365_0100260	2.0	2.0
1365_0100276	3.0	2.0
1365_0100280	1.0	1.0
1365_0100455	2.0	2.0
1365_0100469	2.0	2.0
1365_0100473	2.0	2.0
1365_0100480	1.0	2.0
1385_0000016	1.0	0.0
1385_0000017	1.0	0.0
1385_0000021	1.0	1.0
1385_0000053	1.0	1.0
1385_0000098	1.0	1.0
1385_0000099	1.0	1.0
1385_0000100	1.0	0.0
1385_0000103	1.0	1.0
1385_0000104	1.0	1.0
1385_0000122	1.0	1.0
1385_0000126	1.0	1.0
1385_0000130	1.0	0.0
1385_0001107	1.0	1.0
1385_0001118	1.0	1.0
1385_0001122	1.0	1.0
1385_0001123	1.0	1.0
1385_0001129	0.0	1.0
1385_0001133	1.0	1.0
1385_0001134	1.0	1.0
1385_0001148	1.0	1.0
1385_0001156	1.0	1.0
1385_0001159	1.0	1.0
1385_0001160	1.0	2.0
1385_0001165	1.0	1.0
1385_0001166	1.0	1.0
1385_0001172	0.0	1.0
1385_0001193	1.0	1.0
1385_0001195	1.0	1.0
1385_0001524	0.0	1.0
1385_0001527	1.0	1.0
1385_0001528	1.0	1.0
1385_0001712	1.0	1.0
1385_0001714	1.0	1.0
1385_0001718	0.0	1.0
1385_0001733	0.0	1.0
1385_0001736	1.0	2.0
1385_0001740	1.0	1.0
1385_0001744	0.0	1.0
1385_0001750	0.0	0.0
1385_0001790	0.0	1.0
1385_0001792	1.0	1.0
1385_0001798	1.0	1.0
1395_0000333	1.0	1.0
1395_0000353	1.0	1.0
1395_0000355	1.0	2.0
1395_0000357	2.0	2.0
1395_0000390	1.0	0.0
1395_0000399	1.0	1.0
1395_0000413	1.0	1.0
1395_0000451	1.0	1.0
1395_0000469	1.0	0.0
1395_0000504	1.0	1.0
1395_0000515	2.0	1.0
1395_0000516	1.0	0.0
1395_0000533	2.0	2.0
1395_0000547	1.0	1.0
1395_0000551	2.0	1.0
1395_0000572	1.0	1.0
1395_0000581	1.0	1.0
1395_0000582	0.0	0.0
1395_0000587	0.0	1.0
1395_0000593	0.0	1.0
1395_0000595	0.0	0.0
1395_0000608	0.0	1.0
1395_0000610	1.0	1.0
1395_0000628	0.0	1.0
1395_0000635	0.0	1.0
1395_0001010	1.0	1.0
1395_0001017	0.0	1.0
1395_0001022	1.0	1.0
1395_0001045	1.0	1.0
1395_0001069	2.0	1.0
1395_0001084	0.0	1.0
1395_0001090	1.0	1.0
1395_0001126	1.0	1.0
1395_0001131	0.0	0.0
1395_0001149	0.0	1.0
1395_0001164	1.0	2.0
1395_0001169	1.0	1.0
2 Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 1.11
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.84
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        39
         1.0       0.62      0.81      0.70       132
         2.0       0.72      0.79      0.76       165
         3.0       0.73      0.73      0.73        97
         4.0       0.00      0.00      0.00        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.68       452
   macro avg       0.35      0.39      0.36       452
weighted avg       0.60      0.68      0.64       452

[[  0  38   1   0   0   0]
 [  0 107  25   0   0   0]
 [  0  26 131   8   0   0]
 [  0   2  24  71   0   0]
 [  0   0   1  17   0   0]
 [  0   0   0   1   0   0]]
0.6376073584235938
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.82
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.80
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        39
         1.0       0.62      0.77      0.69       132
         2.0       0.73      0.79      0.76       165
         3.0       0.71      0.72      0.72        97
         4.0       0.36      0.22      0.28        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.68       452
   macro avg       0.41      0.42      0.41       452
weighted avg       0.62      0.68      0.64       452

[[  0  39   0   0   0   0]
 [  0 102  30   0   0   0]
 [  0  20 131  14   0   0]
 [  0   3  17  70   7   0]
 [  0   0   1  13   4   0]
 [  0   0   0   1   0   0]]
0.644354248908276
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.73
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.89
              precision    recall  f1-score   support

         0.0       1.00      0.03      0.05        39
         1.0       0.64      0.75      0.69       132
         2.0       0.72      0.69      0.70       165
         3.0       0.58      0.61      0.60        97
         4.0       0.30      0.61      0.40        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.63       452
   macro avg       0.54      0.45      0.41       452
weighted avg       0.67      0.63      0.61       452

[[  1  37   1   0   0   0]
 [  0  99  32   1   0   0]
 [  0  17 114  34   0   0]
 [  0   1  12  59  25   0]
 [  0   0   0   7  11   0]
 [  0   0   0   0   1   0]]
0.6071986887252374
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.65
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.80
              precision    recall  f1-score   support

         0.0       1.00      0.05      0.10        39
         1.0       0.62      0.70      0.66       132
         2.0       0.70      0.79      0.74       165
         3.0       0.70      0.64      0.67        97
         4.0       0.31      0.44      0.36        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.65       452
   macro avg       0.55      0.44      0.42       452
weighted avg       0.69      0.65      0.63       452

[[  2  36   1   0   0   0]
 [  0  93  39   0   0   0]
 [  0  18 130  17   0   0]
 [  0   2  16  62  17   0]
 [  0   0   0  10   8   0]
 [  0   0   0   0   1   0]]
0.6296745286804225
452 452 452
Filename	True Label	Prediction
1023_0001423	2.0	2.0
1023_0101688	3.0	3.0
1023_0101689	2.0	2.0
1023_0101691	4.0	3.0
1023_0101701	2.0	2.0
1023_0101753	3.0	3.0
1023_0101841	2.0	3.0
1023_0101845	2.0	2.0
1023_0101847	3.0	3.0
1023_0101849	2.0	2.0
1023_0101898	3.0	3.0
1023_0103822	2.0	2.0
1023_0103826	3.0	3.0
1023_0103834	3.0	3.0
1023_0103836	3.0	3.0
1023_0103839	3.0	3.0
1023_0103840	3.0	3.0
1023_0103880	3.0	3.0
1023_0103883	3.0	3.0
1023_0104207	2.0	3.0
1023_0107075	2.0	3.0
1023_0107244	3.0	3.0
1023_0108641	4.0	3.0
1023_0108887	2.0	3.0
1023_0108993	3.0	3.0
1023_0109249	3.0	3.0
1023_0109250	2.0	2.0
1023_0109392	3.0	3.0
1023_0109515	3.0	4.0
1023_0109516	3.0	3.0
1023_0109528	3.0	3.0
1023_0109606	2.0	2.0
1023_0109609	2.0	2.0
1023_0109649	3.0	3.0
1023_0109671	3.0	3.0
1023_0109946	2.0	3.0
1023_0111896	2.0	3.0
1031_0002006	5.0	4.0
1031_0002088	3.0	4.0
1031_0002195	3.0	3.0
1031_0002197	3.0	4.0
1031_0002200	3.0	3.0
1031_0003012	3.0	4.0
1031_0003029	3.0	3.0
1031_0003048	4.0	4.0
1031_0003054	3.0	4.0
1031_0003071	3.0	4.0
1031_0003072	3.0	4.0
1031_0003085	3.0	3.0
1031_0003088	4.0	4.0
1031_0003126	4.0	3.0
1031_0003127	4.0	3.0
1031_0003157	4.0	4.0
1031_0003160	3.0	3.0
1031_0003161	4.0	4.0
1031_0003166	2.0	3.0
1031_0003167	3.0	4.0
1031_0003181	4.0	4.0
1031_0003217	4.0	4.0
1031_0003219	3.0	4.0
1031_0003220	3.0	3.0
1031_0003226	3.0	4.0
1031_0003230	3.0	4.0
1031_0003231	3.0	3.0
1031_0003232	3.0	4.0
1031_0003233	3.0	3.0
1031_0003236	3.0	3.0
1031_0003237	3.0	4.0
1031_0003242	3.0	4.0
1031_0003273	3.0	3.0
1031_0003274	4.0	4.0
1031_0003313	4.0	4.0
1031_0003327	3.0	3.0
1031_0003355	4.0	3.0
1031_0003357	3.0	4.0
1031_0003359	3.0	4.0
1031_0003386	3.0	3.0
1031_0003389	3.0	4.0
1061_0120276	2.0	2.0
1061_0120279	2.0	1.0
1061_0120290	1.0	2.0
1061_0120291	1.0	1.0
1061_0120298	2.0	2.0
1061_0120307	2.0	2.0
1061_0120311	3.0	2.0
1061_0120312	1.0	1.0
1061_0120313	2.0	1.0
1061_0120315	2.0	1.0
1061_0120328	1.0	2.0
1061_0120332	2.0	2.0
1061_0120346	2.0	2.0
1061_0120351	2.0	2.0
1061_0120356	2.0	2.0
1061_0120366	3.0	2.0
1061_0120369	2.0	2.0
1061_0120373	2.0	2.0
1061_0120382	2.0	2.0
1061_0120388	2.0	2.0
1061_0120407	3.0	2.0
1061_0120408	2.0	2.0
1061_0120409	2.0	2.0
1061_0120414	3.0	2.0
1061_0120421	2.0	2.0
1061_0120425	2.0	2.0
1061_0120430	2.0	2.0
1061_0120431	3.0	2.0
1061_0120432	2.0	2.0
1061_0120478	2.0	2.0
1061_0120490	2.0	2.0
1061_0120494	2.0	2.0
1061_0120498	2.0	2.0
1061_0120500	2.0	2.0
1061_0120858	2.0	2.0
1061_0120874	1.0	2.0
1061_0120883	2.0	2.0
1061_0120885	2.0	2.0
1061_1029112	3.0	3.0
1061_1029116	1.0	2.0
1061_1029119	2.0	2.0
1061_1202913	2.0	2.0
1071_0024682	2.0	2.0
1071_0024693	1.0	1.0
1071_0024702	1.0	2.0
1071_0024703	1.0	1.0
1071_0024708	1.0	1.0
1071_0024713	1.0	1.0
1071_0024714	2.0	1.0
1071_0024757	2.0	2.0
1071_0024765	0.0	1.0
1071_0024775	0.0	1.0
1071_0024802	1.0	1.0
1071_0024804	0.0	1.0
1071_0024806	1.0	1.0
1071_0024815	0.0	1.0
1071_0024817	1.0	1.0
1071_0024833	1.0	1.0
1071_0024836	1.0	2.0
1071_0024848	1.0	1.0
1071_0024850	0.0	1.0
1071_0024851	1.0	1.0
1071_0024862	2.0	1.0
1071_0024865	1.0	1.0
1071_0024866	2.0	2.0
1071_0024874	1.0	1.0
1071_0024875	1.0	1.0
1071_0024876	1.0	1.0
1071_0024877	1.0	1.0
1071_0242021	1.0	1.0
1071_0242022	0.0	1.0
1071_0242023	0.0	1.0
1071_0242041	1.0	1.0
1071_0242043	0.0	1.0
1071_0242072	0.0	1.0
1071_0243582	1.0	1.0
1071_0243591	1.0	1.0
1071_0248301	1.0	1.0
1071_0248310	1.0	1.0
1071_0248315	0.0	0.0
1071_0248317	0.0	1.0
1071_0248319	0.0	1.0
1071_0248321	1.0	1.0
1071_0248334	1.0	2.0
1071_0248337	2.0	1.0
1071_0248347	1.0	1.0
1071_0248348	1.0	1.0
1091_0000011	2.0	2.0
1091_0000015	2.0	2.0
1091_0000016	1.0	1.0
1091_0000020	2.0	2.0
1091_0000022	2.0	2.0
1091_0000023	2.0	1.0
1091_0000030	0.0	1.0
1091_0000036	1.0	2.0
1091_0000038	1.0	1.0
1091_0000039	1.0	1.0
1091_0000048	1.0	1.0
1091_0000055	2.0	2.0
1091_0000056	1.0	2.0
1091_0000064	1.0	1.0
1091_0000068	1.0	1.0
1091_0000072	1.0	2.0
1091_0000073	2.0	1.0
1091_0000076	2.0	2.0
1091_0000086	1.0	2.0
1091_0000092	1.0	2.0
1091_0000125	3.0	1.0
1091_0000144	2.0	1.0
1091_0000146	0.0	1.0
1091_0000152	1.0	1.0
1091_0000163	2.0	1.0
1091_0000168	2.0	2.0
1091_0000170	3.0	1.0
1091_0000192	2.0	2.0
1091_0000197	1.0	2.0
1091_0000200	2.0	2.0
1091_0000204	2.0	2.0
1091_0000208	1.0	2.0
1091_0000226	1.0	2.0
1091_0000234	2.0	2.0
1091_0000238	2.0	2.0
1091_0000245	1.0	1.0
1091_0000259	2.0	2.0
1091_0000267	2.0	2.0
1091_0000268	2.0	2.0
0603	2.0	2.0
0610	2.0	2.0
0617	2.0	2.0
0625	1.0	2.0
0628	2.0	2.0
0636	2.0	2.0
0638	2.0	2.0
0640	2.0	2.0
0724	3.0	2.0
0801	1.0	2.0
0802	1.0	2.0
0803	1.0	2.0
0807	2.0	2.0
0810	2.0	2.0
0816	2.0	2.0
0826	2.0	2.0
0827	2.0	2.0
0828	2.0	2.0
0903	2.0	2.0
0904	1.0	2.0
0910	1.0	2.0
0914	2.0	2.0
0918	2.0	2.0
0920	2.0	2.0
0921	2.0	2.0
0922	1.0	2.0
0925	2.0	2.0
0929	1.0	2.0
1001	2.0	2.0
1014	2.0	2.0
1111	2.0	2.0
9999	1.0	2.0
LON0610002A	2.0	1.0
LON0611002A	1.0	1.0
PHA0111003B	1.0	1.0
PHA0111005A	2.0	1.0
PHA0111010	3.0	3.0
PHA0111014	2.0	3.0
PHA0111015	4.0	3.0
PHA0111016	3.0	3.0
PHA0111018	2.0	3.0
PHA0112006B	2.0	2.0
PHA0209001	2.0	2.0
PHA0209028	3.0	3.0
PHA0411008B	1.0	1.0
PHA0411038	3.0	3.0
PHA0411042	3.0	3.0
PHA0411043	3.0	3.0
PHA0411044	4.0	3.0
PHA0411054	3.0	3.0
PHA0411056	3.0	3.0
PHA0509015	3.0	2.0
PHA0509021	2.0	3.0
PHA0509028	3.0	3.0
PHA0509034	2.0	3.0
PHA0509039	3.0	3.0
PHA0509040	3.0	3.0
PHA0510010B	1.0	1.0
PHA0510029	3.0	3.0
PHA0510036	3.0	3.0
PHA0510038	3.0	3.0
PHA0510039	3.0	3.0
PHA0610006A	1.0	1.0
PHA0610007B	1.0	1.0
PHA0610018	3.0	3.0
PHA0610019A	1.0	1.0
PHA0610026	3.0	3.0
PHA0710010	3.0	3.0
PHA0710017	3.0	3.0
PHA0710019	3.0	3.0
PHA0710021	4.0	3.0
PHA0810001	3.0	3.0
PHA0810004	3.0	3.0
PHA0810011	3.0	3.0
PHA0811014	3.0	3.0
PHA0811017	4.0	3.0
PHA0811020	2.0	3.0
PHA1109026	3.0	3.0
PHA1110001A	2.0	2.0
PHA1110004A	1.0	1.0
PHA1110014	3.0	3.0
PHA1110022	4.0	3.0
VAR0910006	3.0	3.0
VAR0910007	3.0	3.0
1325_1001009	3.0	2.0
1325_1001024	2.0	2.0
1325_1001029	2.0	2.0
1325_1001032	2.0	2.0
1325_1001036	2.0	2.0
1325_1001043	2.0	2.0
1325_1001048	2.0	2.0
1325_1001053	1.0	2.0
1325_1001056	2.0	3.0
1325_1001058	2.0	2.0
1325_1001076	2.0	2.0
1325_1001082	2.0	2.0
1325_1001084	2.0	2.0
1325_1001093	2.0	2.0
1325_1001100	2.0	2.0
1325_1001109	2.0	2.0
1325_1001111	3.0	3.0
1325_1001119	2.0	2.0
1325_1001123	2.0	2.0
1325_1001134	2.0	2.0
1325_1001138	2.0	2.0
1325_1001142	2.0	2.0
1325_1001155	2.0	2.0
1325_1001161	2.0	2.0
1325_1001163	2.0	2.0
1325_1001166	2.0	2.0
1325_9000088	2.0	2.0
1325_9000089	2.0	2.0
1325_9000090	2.0	2.0
1325_9000136	2.0	2.0
1325_9000144	3.0	3.0
1325_9000185	3.0	2.0
1325_9000188	2.0	3.0
1325_9000209	2.0	2.0
1325_9000211	2.0	3.0
1325_9000237	2.0	3.0
1325_9000239	2.0	2.0
1325_9000533	2.0	2.0
1325_9000554	2.0	2.0
1325_9000601	2.0	3.0
1325_9000602	3.0	2.0
1325_9000686	2.0	2.0
1325_9000750	3.0	2.0
1365_0100008	2.0	2.0
1365_0100010	1.0	1.0
1365_0100011	2.0	1.0
1365_0100015	2.0	1.0
1365_0100016	2.0	2.0
1365_0100023	1.0	2.0
1365_0100028	1.0	2.0
1365_0100061	3.0	2.0
1365_0100066	1.0	1.0
1365_0100071	2.0	2.0
1365_0100092	2.0	2.0
1365_0100119	3.0	2.0
1365_0100162	2.0	2.0
1365_0100179	1.0	2.0
1365_0100194	2.0	2.0
1365_0100201	1.0	2.0
1365_0100204	1.0	2.0
1365_0100211	3.0	2.0
1365_0100212	3.0	2.0
1365_0100217	3.0	2.0
1365_0100218	2.0	2.0
1365_0100233	2.0	2.0
1365_0100251	2.0	2.0
1365_0100259	2.0	2.0
1365_0100261	2.0	2.0
1365_0100262	2.0	2.0
1365_0100274	2.0	2.0
1365_0100287	1.0	2.0
1365_0100290	1.0	2.0
1365_0100299	2.0	2.0
1365_0100448	1.0	2.0
1365_0100451	2.0	2.0
1365_0100458	2.0	2.0
1365_0100470	1.0	2.0
1365_0100471	1.0	2.0
1365_0100475	2.0	2.0
1365_0100476	2.0	2.0
1365_0100478	1.0	2.0
1365_0100479	2.0	2.0
1365_0100481	1.0	2.0
1365_0100482	2.0	2.0
1385_0000020	1.0	1.0
1385_0000022	1.0	1.0
1385_0000033	1.0	1.0
1385_0000034	1.0	1.0
1385_0000036	1.0	1.0
1385_0000042	1.0	1.0
1385_0000049	1.0	1.0
1385_0000058	1.0	1.0
1385_0000101	1.0	1.0
1385_0000120	0.0	0.0
1385_0000128	1.0	1.0
1385_0000129	1.0	1.0
1385_0001104	0.0	1.0
1385_0001105	1.0	1.0
1385_0001120	1.0	1.0
1385_0001131	1.0	1.0
1385_0001136	1.0	1.0
1385_0001149	1.0	1.0
1385_0001150	1.0	1.0
1385_0001154	1.0	1.0
1385_0001164	1.0	1.0
1385_0001170	0.0	1.0
1385_0001175	0.0	1.0
1385_0001189	0.0	1.0
1385_0001190	0.0	1.0
1385_0001194	1.0	1.0
1385_0001199	1.0	1.0
1385_0001501	0.0	1.0
1385_0001727	0.0	1.0
1385_0001728	0.0	2.0
1385_0001729	1.0	1.0
1385_0001730	1.0	1.0
1385_0001747	0.0	1.0
1385_0001748	2.0	1.0
1385_0001751	0.0	1.0
1385_0001752	1.0	1.0
1385_0001754	0.0	1.0
1385_0001760	1.0	1.0
1385_0001761	0.0	1.0
1385_0001766	1.0	1.0
1385_0001771	0.0	1.0
1385_0001791	1.0	1.0
1385_0001794	1.0	1.0
1385_0001795	0.0	1.0
1385_0001800	1.0	1.0
1395_0000337	0.0	1.0
1395_0000341	1.0	1.0
1395_0000380	1.0	1.0
1395_0000383	1.0	1.0
1395_0000388	2.0	1.0
1395_0000389	0.0	1.0
1395_0000404	1.0	1.0
1395_0000409	2.0	1.0
1395_0000414	1.0	1.0
1395_0000443	2.0	1.0
1395_0000454	1.0	1.0
1395_0000518	1.0	2.0
1395_0000528	1.0	1.0
1395_0000548	1.0	1.0
1395_0000554	1.0	1.0
1395_0000575	1.0	1.0
1395_0000583	1.0	1.0
1395_0000606	0.0	1.0
1395_0000626	1.0	1.0
1395_0000636	0.0	1.0
1395_0000642	0.0	1.0
1395_0001023	1.0	1.0
1395_0001024	1.0	1.0
1395_0001040	0.0	1.0
1395_0001068	0.0	1.0
1395_0001070	1.0	1.0
1395_0001071	1.0	1.0
1395_0001078	0.0	1.0
1395_0001104	0.0	1.0
1395_0001116	1.0	1.0
1395_0001119	1.0	2.0
1395_0001121	0.0	1.0
1395_0001124	1.0	1.0
1395_0001161	1.0	1.0
1395_0001171	1.0	1.0
3 Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 1.11
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.89
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        39
         1.0       0.59      0.86      0.70       132
         2.0       0.72      0.68      0.70       166
         3.0       0.66      0.70      0.68        96
         4.0       0.00      0.00      0.00        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.65       452
   macro avg       0.33      0.37      0.35       452
weighted avg       0.58      0.65      0.61       452

[[  0  38   1   0   0   0]
 [  0 114  18   0   0   0]
 [  0  38 113  15   0   0]
 [  0   3  26  67   0   0]
 [  0   0   0  18   0   0]
 [  0   0   0   1   0   0]]
0.605514807003659
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 48
Elapsed time 60

  Average training loss: 0.82
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.86
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        39
         1.0       0.58      0.89      0.70       132
         2.0       0.75      0.64      0.69       166
         3.0       0.67      0.77      0.71        96
         4.0       0.00      0.00      0.00        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.66       452
   macro avg       0.33      0.38      0.35       452
weighted avg       0.59      0.66      0.61       452

[[  0  38   1   0   0   0]
 [  0 117  15   0   0   0]
 [  0  42 106  18   0   0]
 [  0   3  19  74   0   0]
 [  0   0   0  18   0   0]
 [  0   0   0   1   0   0]]
0.6112959016854818
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.72
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.85
              precision    recall  f1-score   support

         0.0       0.88      0.18      0.30        39
         1.0       0.61      0.81      0.70       132
         2.0       0.71      0.68      0.70       166
         3.0       0.66      0.76      0.71        96
         4.0       0.00      0.00      0.00        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.66       452
   macro avg       0.48      0.41      0.40       452
weighted avg       0.66      0.66      0.64       452

[[  7  30   2   0   0   0]
 [  1 107  24   0   0   0]
 [  0  34 113  19   0   0]
 [  0   3  20  73   0   0]
 [  0   0   0  18   0   0]
 [  0   0   0   1   0   0]]
0.6351211014340138
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 48
Elapsed time 59

  Average training loss: 0.63
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.87
              precision    recall  f1-score   support

         0.0       0.83      0.26      0.39        39
         1.0       0.62      0.73      0.67       132
         2.0       0.68      0.72      0.70       166
         3.0       0.70      0.68      0.69        96
         4.0       0.40      0.33      0.36        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.65       452
   macro avg       0.54      0.45      0.47       452
weighted avg       0.66      0.65      0.64       452

[[ 10  27   2   0   0   0]
 [  2  96  34   0   0   0]
 [  0  30 119  16   1   0]
 [  0   3  21  65   7   0]
 [  0   0   0  12   6   0]
 [  0   0   0   0   1   0]]
0.6446720930542406
452 452 452
Filename	True Label	Prediction
1023_0001416	3.0	3.0
1023_0001420	3.0	3.0
1023_0001575	3.0	3.0
1023_0101693	4.0	3.0
1023_0101700	3.0	3.0
1023_0101843	2.0	2.0
1023_0101854	2.0	2.0
1023_0101896	3.0	2.0
1023_0101907	3.0	3.0
1023_0102118	2.0	3.0
1023_0103821	3.0	3.0
1023_0103829	2.0	3.0
1023_0103831	3.0	3.0
1023_0103833	4.0	3.0
1023_0104206	3.0	2.0
1023_0107042	3.0	2.0
1023_0107682	3.0	2.0
1023_0107725	2.0	3.0
1023_0107727	3.0	3.0
1023_0107729	3.0	3.0
1023_0107784	1.0	2.0
1023_0108307	3.0	3.0
1023_0108422	3.0	3.0
1023_0108751	3.0	3.0
1023_0108811	3.0	3.0
1023_0108812	3.0	3.0
1023_0108933	3.0	3.0
1023_0109027	3.0	2.0
1023_0109033	4.0	3.0
1023_0109151	4.0	3.0
1023_0109247	4.0	3.0
1023_0109248	2.0	3.0
1023_0109399	2.0	3.0
1023_0109496	3.0	3.0
1023_0109505	3.0	3.0
1023_0109518	2.0	2.0
1023_0109588	3.0	3.0
1023_0109651	3.0	3.0
1023_0109716	3.0	2.0
1023_0109890	4.0	3.0
1023_0109917	2.0	3.0
1023_0109951	2.0	3.0
1031_0001951	3.0	3.0
1031_0001997	4.0	3.0
1031_0002011	4.0	4.0
1031_0002040	5.0	4.0
1031_0002091	3.0	4.0
1031_0002092	4.0	4.0
1031_0002184	3.0	3.0
1031_0002187	3.0	4.0
1031_0002199	3.0	4.0
1031_0003092	2.0	4.0
1031_0003095	3.0	3.0
1031_0003128	3.0	4.0
1031_0003131	3.0	4.0
1031_0003135	3.0	4.0
1031_0003149	3.0	3.0
1031_0003154	3.0	3.0
1031_0003172	3.0	4.0
1031_0003173	3.0	3.0
1031_0003180	4.0	3.0
1031_0003183	4.0	3.0
1031_0003184	4.0	3.0
1031_0003185	3.0	3.0
1031_0003187	3.0	3.0
1031_0003191	3.0	3.0
1031_0003218	4.0	4.0
1031_0003239	4.0	4.0
1031_0003243	3.0	3.0
1031_0003244	4.0	4.0
1031_0003249	4.0	3.0
1031_0003310	3.0	3.0
1031_0003314	4.0	4.0
1031_0003315	4.0	3.0
1031_0003353	2.0	3.0
1031_0003365	3.0	3.0
1031_0003407	2.0	3.0
1031_0003408	2.0	3.0
1031_0003410	3.0	3.0
1061_0120271	2.0	2.0
1061_0120272	1.0	2.0
1061_0120274	2.0	2.0
1061_0120275	2.0	2.0
1061_0120278	2.0	2.0
1061_0120280	1.0	2.0
1061_0120288	2.0	2.0
1061_0120295	0.0	2.0
1061_0120300	2.0	2.0
1061_0120302	1.0	2.0
1061_0120304	2.0	2.0
1061_0120309	1.0	1.0
1061_0120319	3.0	2.0
1061_0120334	3.0	2.0
1061_0120337	2.0	2.0
1061_0120345	2.0	2.0
1061_0120349	1.0	2.0
1061_0120352	1.0	1.0
1061_0120355	1.0	1.0
1061_0120357	2.0	3.0
1061_0120367	3.0	2.0
1061_0120368	2.0	2.0
1061_0120371	3.0	3.0
1061_0120375	2.0	1.0
1061_0120386	0.0	2.0
1061_0120403	2.0	2.0
1061_0120406	2.0	2.0
1061_0120415	2.0	2.0
1061_0120423	3.0	3.0
1061_0120426	2.0	2.0
1061_0120460	2.0	2.0
1061_0120479	2.0	2.0
1061_0120480	2.0	2.0
1061_0120484	2.0	2.0
1061_0120485	2.0	2.0
1061_0120487	2.0	2.0
1061_0120493	2.0	2.0
1061_0120881	2.0	2.0
1061_0120889	1.0	1.0
1061_0120894	2.0	2.0
1061_1029111	2.0	2.0
1061_1029115	2.0	2.0
1061_1029120	2.0	2.0
1061_1202910	2.0	2.0
1061_1202915	1.0	1.0
1061_1202919	1.0	2.0
1071_0020001	2.0	1.0
1071_0024678	2.0	1.0
1071_0024689	1.0	1.0
1071_0024694	2.0	2.0
1071_0024706	1.0	1.0
1071_0024715	2.0	1.0
1071_0024758	1.0	2.0
1071_0024768	0.0	1.0
1071_0024778	0.0	1.0
1071_0024779	1.0	1.0
1071_0024784	1.0	0.0
1071_0024798	0.0	1.0
1071_0024799	2.0	2.0
1071_0024800	0.0	1.0
1071_0024801	1.0	1.0
1071_0024808	1.0	1.0
1071_0024812	0.0	1.0
1071_0024814	1.0	1.0
1071_0024816	1.0	1.0
1071_0024820	1.0	1.0
1071_0024835	1.0	1.0
1071_0024841	0.0	1.0
1071_0024844	1.0	1.0
1071_0024845	0.0	1.0
1071_0024846	1.0	1.0
1071_0024847	1.0	1.0
1071_0024849	0.0	0.0
1071_0024859	1.0	1.0
1071_0024872	1.0	1.0
1071_0024881	2.0	1.0
1071_0241831	1.0	1.0
1071_0242013	1.0	1.0
1071_0242042	1.0	1.0
1071_0242092	0.0	0.0
1071_0243502	0.0	0.0
1071_0243622	0.0	0.0
1071_0248312	1.0	1.0
1071_0248320	0.0	0.0
1071_0248328	0.0	1.0
1071_0248349	1.0	1.0
1071_0248350	2.0	1.0
1091_0000003	2.0	2.0
1091_0000005	2.0	2.0
1091_0000018	2.0	2.0
1091_0000021	2.0	2.0
1091_0000024	3.0	1.0
1091_0000035	2.0	1.0
1091_0000037	1.0	1.0
1091_0000046	2.0	1.0
1091_0000051	1.0	1.0
1091_0000057	2.0	1.0
1091_0000061	2.0	1.0
1091_0000069	2.0	1.0
1091_0000070	2.0	1.0
1091_0000071	2.0	2.0
1091_0000074	2.0	1.0
1091_0000078	3.0	1.0
1091_0000101	2.0	1.0
1091_0000102	2.0	2.0
1091_0000116	2.0	2.0
1091_0000123	2.0	2.0
1091_0000126	2.0	2.0
1091_0000140	2.0	1.0
1091_0000145	1.0	1.0
1091_0000148	1.0	1.0
1091_0000151	0.0	0.0
1091_0000155	2.0	3.0
1091_0000156	2.0	2.0
1091_0000157	2.0	2.0
1091_0000172	2.0	1.0
1091_0000185	2.0	1.0
1091_0000193	2.0	1.0
1091_0000196	2.0	1.0
1091_0000198	2.0	2.0
1091_0000201	2.0	1.0
1091_0000202	1.0	2.0
1091_0000206	1.0	1.0
1091_0000209	2.0	2.0
1091_0000212	1.0	2.0
1091_0000215	2.0	2.0
1091_0000216	1.0	2.0
1091_0000228	1.0	2.0
1091_0000231	2.0	2.0
1091_0000233	2.0	2.0
1091_0000235	2.0	1.0
1091_0000239	2.0	2.0
1091_0000240	2.0	1.0
1091_0000242	1.0	2.0
1091_0000248	2.0	1.0
1091_0000251	2.0	2.0
1091_0000252	2.0	2.0
1091_0000254	2.0	1.0
1091_0000258	1.0	2.0
1091_0000263	2.0	2.0
1091_0000264	1.0	1.0
0605	2.0	2.0
0606	2.0	2.0
0609	2.0	2.0
0614	2.0	2.0
0618	2.0	2.0
0620	2.0	2.0
0624	2.0	2.0
0641	2.0	2.0
0644	2.0	2.0
0716	2.0	2.0
0717	2.0	2.0
0719	2.0	2.0
0813	2.0	2.0
0814	1.0	2.0
0819	3.0	2.0
0824	2.0	2.0
0829	2.0	2.0
0907	2.0	2.0
0912	2.0	2.0
0913	2.0	2.0
0916	1.0	2.0
1003	2.0	2.0
1006	2.0	2.0
1007	2.0	2.0
1008	2.0	2.0
1009	2.0	2.0
1010	1.0	2.0
1115	2.0	2.0
BER0611007	3.0	3.0
KYJ0611004A	1.0	1.0
KYJ0611005A	1.0	1.0
KYJ0611006A	1.0	2.0
LIB0611001A	1.0	1.0
LIB0611001B	1.0	1.0
LIB0611004B	2.0	1.0
MOS0611013	3.0	3.0
PAR1011013	3.0	3.0
PHA0111001A	1.0	2.0
PHA0111004A	1.0	1.0
PHA0112007A	1.0	2.0
PHA0209008	1.0	1.0
PHA0209013	1.0	1.0
PHA0209031	3.0	3.0
PHA0210004	1.0	1.0
PHA0411010A	0.0	1.0
PHA0411011B	1.0	1.0
PHA0411031	3.0	3.0
PHA0411032	3.0	3.0
PHA0411045	3.0	3.0
PHA0411047	3.0	3.0
PHA0509018	3.0	3.0
PHA0509019	3.0	3.0
PHA0509025	3.0	3.0
PHA0509032	3.0	3.0
PHA0509033	2.0	3.0
PHA0509036	3.0	3.0
PHA0509038	2.0	2.0
PHA0510002A	1.0	2.0
PHA0510027	3.0	3.0
PHA0510035	3.0	3.0
PHA0610005A	1.0	1.0
PHA0610006B	1.0	1.0
PHA0610007A	1.0	1.0
PHA0610015	3.0	3.0
PHA0610016	3.0	3.0
PHA0610017	3.0	3.0
PHA0710009	3.0	3.0
PHA0710011	3.0	3.0
PHA0710012	3.0	3.0
PHA0710018	3.0	3.0
PHA0810008	3.0	3.0
PHA0811010	3.0	3.0
PHA0811019	3.0	3.0
PHA1109004	3.0	3.0
PHA1109023	1.0	1.0
PHA1109025	1.0	1.0
PHA1110003A	1.0	1.0
PHA1111003B	1.0	1.0
PHA1111004A	1.0	2.0
PHA1111004B	1.0	1.0
PHA1111008B	1.0	1.0
VAR0910005	3.0	3.0
VAR0910009	3.0	3.0
1325_1001008	2.0	2.0
1325_1001010	2.0	2.0
1325_1001011	2.0	2.0
1325_1001014	3.0	2.0
1325_1001020	2.0	2.0
1325_1001040	2.0	2.0
1325_1001044	2.0	2.0
1325_1001047	1.0	2.0
1325_1001054	2.0	2.0
1325_1001081	2.0	2.0
1325_1001085	2.0	2.0
1325_1001090	2.0	2.0
1325_1001098	2.0	2.0
1325_1001101	3.0	2.0
1325_1001120	2.0	3.0
1325_1001121	2.0	2.0
1325_1001127	3.0	2.0
1325_1001128	2.0	2.0
1325_1001135	2.0	2.0
1325_1001141	2.0	2.0
1325_1001143	2.0	3.0
1325_1001154	3.0	3.0
1325_1001159	2.0	2.0
1325_1001170	2.0	2.0
1325_9000102	2.0	2.0
1325_9000105	2.0	2.0
1325_9000107	2.0	2.0
1325_9000137	2.0	2.0
1325_9000139	2.0	2.0
1325_9000186	3.0	2.0
1325_9000213	3.0	2.0
1325_9000278	3.0	2.0
1325_9000279	3.0	2.0
1325_9000302	2.0	2.0
1325_9000316	2.0	2.0
1325_9000320	3.0	2.0
1325_9000611	2.0	3.0
1325_9000677	2.0	2.0
1325_9000678	3.0	3.0
1325_9000685	3.0	3.0
1365_0100004	2.0	2.0
1365_0100012	1.0	1.0
1365_0100024	1.0	2.0
1365_0100030	1.0	2.0
1365_0100064	2.0	2.0
1365_0100069	1.0	2.0
1365_0100095	2.0	2.0
1365_0100100	2.0	2.0
1365_0100116	2.0	2.0
1365_0100120	3.0	2.0
1365_0100121	2.0	2.0
1365_0100123	2.0	2.0
1365_0100136	1.0	2.0
1365_0100145	2.0	2.0
1365_0100151	1.0	1.0
1365_0100163	2.0	2.0
1365_0100166	1.0	2.0
1365_0100167	1.0	1.0
1365_0100169	1.0	2.0
1365_0100171	1.0	2.0
1365_0100184	1.0	2.0
1365_0100192	3.0	2.0
1365_0100195	1.0	1.0
1365_0100198	1.0	2.0
1365_0100200	2.0	2.0
1365_0100220	2.0	2.0
1365_0100224	2.0	2.0
1365_0100225	2.0	1.0
1365_0100253	1.0	2.0
1365_0100263	3.0	2.0
1365_0100275	2.0	2.0
1365_0100278	2.0	2.0
1365_0100281	2.0	2.0
1365_0100456	2.0	2.0
1385_0000011	0.0	1.0
1385_0000043	1.0	1.0
1385_0000044	1.0	1.0
1385_0000051	1.0	1.0
1385_0000095	1.0	1.0
1385_0000102	1.0	1.0
1385_0000123	1.0	1.0
1385_0000127	1.0	1.0
1385_0001111	1.0	1.0
1385_0001112	1.0	1.0
1385_0001113	1.0	1.0
1385_0001124	1.0	1.0
1385_0001125	1.0	1.0
1385_0001130	1.0	0.0
1385_0001132	1.0	1.0
1385_0001137	1.0	1.0
1385_0001151	1.0	1.0
1385_0001155	1.0	1.0
1385_0001171	0.0	0.0
1385_0001196	0.0	1.0
1385_0001197	0.0	1.0
1385_0001198	2.0	1.0
1385_0001503	1.0	1.0
1385_0001526	0.0	0.0
1385_0001715	1.0	1.0
1385_0001720	0.0	1.0
1385_0001732	0.0	1.0
1385_0001741	0.0	1.0
1385_0001757	1.0	1.0
1385_0001759	1.0	1.0
1385_0001765	0.0	0.0
1385_0001775	1.0	1.0
1385_0001785	0.0	1.0
1385_0001796	1.0	1.0
1385_0001799	1.0	1.0
1395_0000354	1.0	1.0
1395_0000361	1.0	1.0
1395_0000368	0.0	0.0
1395_0000387	3.0	1.0
1395_0000396	1.0	1.0
1395_0000438	2.0	2.0
1395_0000449	2.0	1.0
1395_0000452	1.0	1.0
1395_0000455	1.0	1.0
1395_0000458	1.0	1.0
1395_0000462	1.0	1.0
1395_0000470	1.0	1.0
1395_0000499	1.0	1.0
1395_0000513	2.0	1.0
1395_0000526	1.0	1.0
1395_0000550	1.0	1.0
1395_0000552	1.0	1.0
1395_0000555	1.0	1.0
1395_0000564	1.0	1.0
1395_0000584	0.0	1.0
1395_0000585	0.0	1.0
1395_0000598	1.0	1.0
1395_0000602	1.0	1.0
1395_0000609	1.0	1.0
1395_0000627	1.0	1.0
1395_0000639	0.0	1.0
1395_0001019	0.0	1.0
1395_0001058	1.0	1.0
1395_0001061	1.0	2.0
1395_0001065	0.0	1.0
1395_0001075	0.0	1.0
1395_0001076	1.0	1.0
1395_0001093	0.0	1.0
1395_0001114	0.0	1.0
1395_0001120	0.0	1.0
1395_0001122	0.0	1.0
1395_0001146	0.0	1.0
1395_0001158	2.0	1.0
1395_0001160	1.0	1.0
1395_0001170	1.0	2.0
4 Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 1.12
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.05
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        39
         1.0       0.54      0.95      0.69       132
         2.0       0.66      0.61      0.63       166
         3.0       0.71      0.48      0.57        96
         4.0       0.00      0.00      0.00        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.60       452
   macro avg       0.32      0.34      0.32       452
weighted avg       0.55      0.60      0.55       452

[[  0  39   0   0   0   0]
 [  0 126   5   1   0   0]
 [  0  65 101   0   0   0]
 [  0   4  46  46   0   0]
 [  0   0   1  17   0   0]
 [  0   0   0   1   0   0]]
0.5549964075654806
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 48
Elapsed time 59

  Average training loss: 0.85
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.75
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        39
         1.0       0.60      0.89      0.72       132
         2.0       0.75      0.75      0.75       166
         3.0       0.73      0.69      0.71        96
         4.0       0.00      0.00      0.00        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.68       452
   macro avg       0.35      0.39      0.36       452
weighted avg       0.61      0.68      0.64       452

[[  0  39   0   0   0   0]
 [  0 118  13   1   0   0]
 [  0  37 124   5   0   0]
 [  0   2  28  66   0   0]
 [  0   0   1  17   0   0]
 [  0   0   0   1   0   0]]
0.6351872610932789
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 48
Elapsed time 59

  Average training loss: 0.76
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.67
              precision    recall  f1-score   support

         0.0       1.00      0.13      0.23        39
         1.0       0.68      0.80      0.74       132
         2.0       0.75      0.85      0.80       166
         3.0       0.70      0.77      0.74        96
         4.0       0.00      0.00      0.00        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.72       452
   macro avg       0.52      0.43      0.42       452
weighted avg       0.71      0.72      0.69       452

[[  5  34   0   0   0   0]
 [  0 106  25   1   0   0]
 [  0  14 141  11   0   0]
 [  0   1  21  74   0   0]
 [  0   0   0  18   0   0]
 [  0   0   0   1   0   0]]
0.685104708021347
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.67
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.67
              precision    recall  f1-score   support

         0.0       0.75      0.31      0.44        39
         1.0       0.70      0.75      0.73       132
         2.0       0.74      0.82      0.78       166
         3.0       0.68      0.79      0.73        96
         4.0       0.00      0.00      0.00        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.71       452
   macro avg       0.48      0.44      0.45       452
weighted avg       0.69      0.71      0.69       452

[[ 12  27   0   0   0   0]
 [  4  99  28   1   0   0]
 [  0  14 136  16   0   0]
 [  0   1  19  76   0   0]
 [  0   0   0  18   0   0]
 [  0   0   0   1   0   0]]
0.690893030695754
452 452 452
Filename	True Label	Prediction
1023_0001419	3.0	3.0
1023_0001422	3.0	3.0
1023_0101675	3.0	3.0
1023_0101690	2.0	3.0
1023_0101749	3.0	3.0
1023_0101844	2.0	2.0
1023_0101848	2.0	2.0
1023_0101856	2.0	3.0
1023_0101894	3.0	3.0
1023_0101897	2.0	2.0
1023_0101899	2.0	2.0
1023_0101900	3.0	3.0
1023_0104209	3.0	3.0
1023_0106816	3.0	3.0
1023_0107726	3.0	3.0
1023_0107773	2.0	3.0
1023_0107780	3.0	3.0
1023_0107781	3.0	3.0
1023_0107787	2.0	3.0
1023_0107788	3.0	3.0
1023_0108304	3.0	3.0
1023_0108305	3.0	3.0
1023_0108510	3.0	3.0
1023_0108650	3.0	3.0
1023_0108753	3.0	3.0
1023_0108815	3.0	3.0
1023_0108935	2.0	3.0
1023_0108955	4.0	3.0
1023_0109022	3.0	3.0
1023_0109026	2.0	3.0
1023_0109039	3.0	3.0
1023_0109267	2.0	3.0
1023_0109391	2.0	3.0
1023_0109422	3.0	3.0
1023_0109500	2.0	3.0
1023_0109614	2.0	2.0
1023_0109880	3.0	3.0
1023_0109915	2.0	2.0
1023_0109954	3.0	3.0
1031_0001703	4.0	3.0
1031_0001950	4.0	3.0
1031_0001998	4.0	3.0
1031_0002003	3.0	3.0
1031_0002004	4.0	3.0
1031_0002005	4.0	3.0
1031_0002042	4.0	3.0
1031_0002084	3.0	3.0
1031_0002085	3.0	3.0
1031_0003013	4.0	3.0
1031_0003023	3.0	3.0
1031_0003053	4.0	3.0
1031_0003063	5.0	3.0
1031_0003065	3.0	3.0
1031_0003073	4.0	3.0
1031_0003076	4.0	3.0
1031_0003097	4.0	3.0
1031_0003136	4.0	3.0
1031_0003145	4.0	3.0
1031_0003146	4.0	3.0
1031_0003155	3.0	3.0
1031_0003169	3.0	3.0
1031_0003170	3.0	3.0
1031_0003179	4.0	3.0
1031_0003207	4.0	3.0
1031_0003216	3.0	3.0
1031_0003234	3.0	3.0
1031_0003246	3.0	3.0
1031_0003336	3.0	3.0
1031_0003339	3.0	3.0
1031_0003366	3.0	3.0
1031_0003367	3.0	3.0
1031_0003384	3.0	3.0
1061_0120273	2.0	2.0
1061_0120282	0.0	1.0
1061_0120285	1.0	2.0
1061_0120297	2.0	2.0
1061_0120306	2.0	2.0
1061_0120316	2.0	2.0
1061_0120317	3.0	2.0
1061_0120318	2.0	2.0
1061_0120323	1.0	2.0
1061_0120326	2.0	2.0
1061_0120330	3.0	2.0
1061_0120341	1.0	1.0
1061_0120359	2.0	2.0
1061_0120360	3.0	3.0
1061_0120361	2.0	2.0
1061_0120370	2.0	2.0
1061_0120374	3.0	3.0
1061_0120387	2.0	2.0
1061_0120391	1.0	1.0
1061_0120394	2.0	2.0
1061_0120404	2.0	1.0
1061_0120410	2.0	2.0
1061_0120411	4.0	3.0
1061_0120424	2.0	2.0
1061_0120427	2.0	2.0
1061_0120442	2.0	2.0
1061_0120448	2.0	2.0
1061_0120453	2.0	2.0
1061_0120456	2.0	2.0
1061_0120458	3.0	2.0
1061_0120482	2.0	2.0
1061_0120488	2.0	2.0
1061_0120489	2.0	2.0
1061_0120495	2.0	2.0
1061_0120497	3.0	3.0
1061_0120853	2.0	3.0
1061_0120855	2.0	2.0
1061_0120856	2.0	2.0
1061_0120857	2.0	2.0
1061_0120859	2.0	2.0
1061_0120880	3.0	2.0
1061_0120882	3.0	2.0
1061_0120884	2.0	2.0
1061_0120888	1.0	2.0
1061_0120890	2.0	1.0
1061_1029113	1.0	2.0
1061_1202918	2.0	2.0
1071_0024681	2.0	2.0
1071_0024688	1.0	1.0
1071_0024692	2.0	2.0
1071_0024699	1.0	2.0
1071_0024710	0.0	1.0
1071_0024711	1.0	1.0
1071_0024716	1.0	1.0
1071_0024759	0.0	1.0
1071_0024761	1.0	1.0
1071_0024762	0.0	1.0
1071_0024763	1.0	1.0
1071_0024766	0.0	1.0
1071_0024767	2.0	1.0
1071_0024769	1.0	1.0
1071_0024773	1.0	1.0
1071_0024776	0.0	0.0
1071_0024777	1.0	1.0
1071_0024781	1.0	1.0
1071_0024783	0.0	0.0
1071_0024809	1.0	1.0
1071_0024810	1.0	1.0
1071_0024813	1.0	1.0
1071_0024818	1.0	1.0
1071_0024819	1.0	1.0
1071_0024822	0.0	1.0
1071_0024824	1.0	1.0
1071_0024834	2.0	2.0
1071_0024838	0.0	0.0
1071_0024856	1.0	1.0
1071_0024860	1.0	1.0
1071_0024879	1.0	0.0
1071_0241832	0.0	1.0
1071_0242012	1.0	1.0
1071_0242071	0.0	0.0
1071_0242073	1.0	1.0
1071_0242091	0.0	1.0
1071_0243501	1.0	1.0
1071_0243581	0.0	1.0
1071_0243592	1.0	1.0
1071_0248309	2.0	1.0
1071_0248318	0.0	0.0
1071_0248322	0.0	1.0
1071_0248324	0.0	1.0
1071_0248326	1.0	1.0
1071_0248332	2.0	2.0
1071_0248336	0.0	1.0
1071_0248343	1.0	1.0
1071_0248345	2.0	2.0
1091_0000007	2.0	2.0
1091_0000008	2.0	2.0
1091_0000012	1.0	1.0
1091_0000013	1.0	1.0
1091_0000031	1.0	1.0
1091_0000032	1.0	2.0
1091_0000033	1.0	1.0
1091_0000042	1.0	0.0
1091_0000044	0.0	1.0
1091_0000045	2.0	2.0
1091_0000058	2.0	2.0
1091_0000062	3.0	1.0
1091_0000063	1.0	1.0
1091_0000095	2.0	1.0
1091_0000114	2.0	2.0
1091_0000153	1.0	2.0
1091_0000161	2.0	2.0
1091_0000169	2.0	2.0
1091_0000190	1.0	2.0
1091_0000199	2.0	2.0
1091_0000203	1.0	2.0
1091_0000213	2.0	2.0
1091_0000221	2.0	1.0
1091_0000225	2.0	1.0
1091_0000227	1.0	2.0
1091_0000230	2.0	2.0
1091_0000247	2.0	2.0
1091_0000265	2.0	2.0
1091_0000266	2.0	2.0
1091_0000270	2.0	1.0
1091_0000271	2.0	2.0
0601	2.0	2.0
0607	2.0	2.0
0608	1.0	2.0
0613	2.0	2.0
0616	2.0	2.0
0629	2.0	2.0
0630	1.0	2.0
0632	1.0	2.0
0637	2.0	2.0
0639	2.0	2.0
0715	2.0	2.0
0720	2.0	2.0
0721	2.0	2.0
0722	2.0	2.0
0811	2.0	2.0
0812	1.0	2.0
0820	1.0	2.0
0905	2.0	2.0
0911	2.0	2.0
0924	1.0	2.0
0926	2.0	2.0
1016	2.0	2.0
1022	2.0	2.0
1023	2.0	2.0
1114	2.0	2.0
BER0609003	3.0	3.0
KYJ0611006B	1.0	1.0
KYJ0611009A	2.0	2.0
LIB0611002A	1.0	2.0
LIB0611011	2.0	3.0
LON0611003	3.0	3.0
LON0611004A	1.0	1.0
MOS0611012	3.0	3.0
MOS0611014	1.0	3.0
PAR1011014	3.0	3.0
PAR1011016	3.0	3.0
PHA0111003A	1.0	1.0
PHA0111011	3.0	3.0
PHA0111012	3.0	3.0
PHA0112003A	1.0	1.0
PHA0112003B	1.0	2.0
PHA0112007B	1.0	1.0
PHA0209026	3.0	3.0
PHA0209034	3.0	3.0
PHA0210001	1.0	1.0
PHA0210007	1.0	2.0
PHA0210008	1.0	1.0
PHA0411008A	2.0	2.0
PHA0411009A	1.0	2.0
PHA0411028	2.0	2.0
PHA0411029	3.0	3.0
PHA0411034	3.0	2.0
PHA0411041	3.0	3.0
PHA0411055	3.0	3.0
PHA0411058	3.0	3.0
PHA0411059	3.0	3.0
PHA0509002	1.0	0.0
PHA0509020	3.0	3.0
PHA0509030	3.0	3.0
PHA0509035	3.0	3.0
PHA0509042	3.0	3.0
PHA0509043	3.0	3.0
PHA0510002B	2.0	2.0
PHA0510003B	1.0	1.0
PHA0510004A	1.0	1.0
PHA0510040	3.0	3.0
PHA0610005B	1.0	1.0
PHA0709008	3.0	3.0
PHA0710013	3.0	3.0
PHA0710014	3.0	3.0
PHA0710015	3.0	3.0
PHA0710016	3.0	3.0
PHA0809009	3.0	3.0
PHA0810006	3.0	3.0
PHA0810009	3.0	3.0
PHA0810010	3.0	3.0
PHA1109006	2.0	3.0
PHA1109028	3.0	3.0
PHA1110002B	2.0	2.0
PHA1110003B	1.0	2.0
PHA1111001A	2.0	2.0
PHA1111002B	1.0	1.0
PHA1111009A	1.0	1.0
VAR0909004	3.0	3.0
VAR0909006	3.0	3.0
1325_1001015	2.0	2.0
1325_1001016	2.0	2.0
1325_1001021	2.0	2.0
1325_1001022	2.0	2.0
1325_1001027	3.0	2.0
1325_1001028	2.0	3.0
1325_1001042	2.0	2.0
1325_1001052	2.0	2.0
1325_1001055	2.0	2.0
1325_1001062	2.0	3.0
1325_1001075	1.0	2.0
1325_1001079	2.0	2.0
1325_1001080	2.0	2.0
1325_1001086	2.0	2.0
1325_1001089	2.0	2.0
1325_1001092	2.0	2.0
1325_1001094	2.0	2.0
1325_1001095	2.0	2.0
1325_1001099	3.0	2.0
1325_1001113	3.0	2.0
1325_1001131	2.0	2.0
1325_1001133	2.0	2.0
1325_1001136	2.0	2.0
1325_1001144	2.0	2.0
1325_1001152	2.0	2.0
1325_1001153	2.0	2.0
1325_1001156	2.0	2.0
1325_1001157	2.0	2.0
1325_1001164	2.0	2.0
1325_1001165	2.0	2.0
1325_1001167	2.0	2.0
1325_1001168	2.0	2.0
1325_9000095	2.0	3.0
1325_9000104	2.0	2.0
1325_9000140	3.0	2.0
1325_9000215	3.0	2.0
1325_9000240	3.0	2.0
1325_9000321	3.0	2.0
1325_9000503	3.0	3.0
1325_9000505	3.0	2.0
1325_9000536	3.0	3.0
1325_9000676	3.0	2.0
1325_9000684	3.0	2.0
1365_0100003	1.0	1.0
1365_0100018	2.0	2.0
1365_0100027	2.0	2.0
1365_0100051	0.0	1.0
1365_0100056	2.0	2.0
1365_0100063	2.0	2.0
1365_0100072	2.0	2.0
1365_0100073	2.0	2.0
1365_0100080	2.0	2.0
1365_0100096	2.0	2.0
1365_0100097	2.0	2.0
1365_0100098	1.0	2.0
1365_0100102	2.0	2.0
1365_0100103	2.0	2.0
1365_0100106	2.0	2.0
1365_0100107	2.0	3.0
1365_0100118	2.0	2.0
1365_0100133	2.0	2.0
1365_0100135	2.0	2.0
1365_0100146	2.0	2.0
1365_0100148	2.0	2.0
1365_0100173	1.0	1.0
1365_0100175	1.0	2.0
1365_0100178	1.0	2.0
1365_0100180	1.0	1.0
1365_0100202	1.0	2.0
1365_0100221	3.0	2.0
1365_0100222	2.0	2.0
1365_0100229	2.0	2.0
1365_0100231	2.0	2.0
1365_0100232	1.0	2.0
1365_0100252	2.0	2.0
1365_0100257	2.0	2.0
1365_0100258	2.0	2.0
1365_0100269	2.0	2.0
1365_0100270	2.0	2.0
1365_0100277	3.0	2.0
1365_0100279	2.0	2.0
1365_0100282	2.0	2.0
1365_0100288	1.0	1.0
1365_0100447	2.0	2.0
1365_0100457	2.0	2.0
1365_0100459	2.0	2.0
1385_0000012	1.0	1.0
1385_0000013	1.0	1.0
1385_0000023	1.0	1.0
1385_0000035	1.0	1.0
1385_0000038	1.0	1.0
1385_0000048	1.0	1.0
1385_0000052	1.0	1.0
1385_0000097	1.0	1.0
1385_0000119	1.0	1.0
1385_0000125	1.0	1.0
1385_0001103	1.0	0.0
1385_0001109	1.0	1.0
1385_0001110	1.0	1.0
1385_0001121	1.0	1.0
1385_0001126	0.0	0.0
1385_0001138	1.0	1.0
1385_0001152	1.0	1.0
1385_0001153	2.0	1.0
1385_0001157	1.0	1.0
1385_0001163	1.0	1.0
1385_0001167	0.0	1.0
1385_0001173	0.0	0.0
1385_0001174	0.0	1.0
1385_0001178	0.0	0.0
1385_0001191	1.0	1.0
1385_0001192	1.0	1.0
1385_0001525	1.0	1.0
1385_0001723	0.0	1.0
1385_0001724	2.0	1.0
1385_0001725	0.0	1.0
1385_0001726	1.0	1.0
1385_0001737	1.0	1.0
1385_0001738	0.0	0.0
1385_0001739	1.0	1.0
1385_0001742	0.0	0.0
1385_0001746	1.0	1.0
1385_0001758	0.0	0.0
1385_0001762	1.0	1.0
1385_0001773	0.0	1.0
1385_0001786	1.0	1.0
1385_0001787	0.0	1.0
1385_0001789	1.0	1.0
1395_0000340	1.0	1.0
1395_0000356	1.0	1.0
1395_0000364	1.0	1.0
1395_0000365	2.0	1.0
1395_0000366	2.0	1.0
1395_0000391	3.0	2.0
1395_0000392	1.0	1.0
1395_0000402	1.0	1.0
1395_0000403	1.0	1.0
1395_0000415	1.0	1.0
1395_0000446	2.0	1.0
1395_0000460	1.0	1.0
1395_0000500	1.0	1.0
1395_0000514	2.0	2.0
1395_0000525	1.0	1.0
1395_0000531	1.0	1.0
1395_0000537	1.0	1.0
1395_0000556	1.0	1.0
1395_0000565	1.0	1.0
1395_0000591	0.0	0.0
1395_0000599	1.0	1.0
1395_0000611	0.0	1.0
1395_0000631	0.0	1.0
1395_0000644	1.0	1.0
1395_0001013	1.0	1.0
1395_0001015	0.0	1.0
1395_0001021	1.0	1.0
1395_0001028	1.0	1.0
1395_0001033	1.0	1.0
1395_0001060	1.0	1.0
1395_0001064	1.0	1.0
1395_0001073	1.0	1.0
1395_0001074	1.0	1.0
1395_0001101	1.0	1.0
1395_0001108	0.0	1.0
1395_0001117	0.0	1.0
1395_0001123	0.0	1.0
1395_0001141	1.0	1.0
1395_0001145	1.0	2.0
1395_0001150	0.0	1.0
1395_0001167	2.0	1.0
5 Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 1.09
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.90
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        39
         1.0       0.56      0.88      0.68       133
         2.0       0.66      0.67      0.66       165
         3.0       0.74      0.58      0.65        96
         4.0       0.00      0.00      0.00        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.63       452
   macro avg       0.33      0.35      0.33       452
weighted avg       0.56      0.63      0.58       452

[[  0  39   0   0   0   0]
 [  0 117  16   0   0   0]
 [  0  52 110   3   0   0]
 [  0   1  39  56   0   0]
 [  0   0   2  16   0   0]
 [  0   0   0   1   0   0]]
0.5815242860720511
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.80
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.78
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        39
         1.0       0.59      0.79      0.68       133
         2.0       0.74      0.72      0.73       165
         3.0       0.73      0.88      0.80        96
         4.0       0.00      0.00      0.00        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.68       452
   macro avg       0.34      0.40      0.37       452
weighted avg       0.60      0.68      0.63       452

[[  0  38   1   0   0   0]
 [  0 105  28   0   0   0]
 [  0  35 118  12   0   0]
 [  0   0  12  84   0   0]
 [  0   0   0  18   0   0]
 [  0   0   0   1   0   0]]
0.6336908807467148
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.72
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.75
              precision    recall  f1-score   support

         0.0       1.00      0.08      0.14        39
         1.0       0.60      0.80      0.69       133
         2.0       0.75      0.73      0.74       165
         3.0       0.74      0.88      0.80        96
         4.0       0.00      0.00      0.00        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.69       452
   macro avg       0.52      0.41      0.40       452
weighted avg       0.70      0.69      0.65       452

[[  3  35   1   0   0   0]
 [  0 106  27   0   0   0]
 [  0  35 120  10   0   0]
 [  0   0  12  84   0   0]
 [  0   0   0  18   0   0]
 [  0   0   0   1   0   0]]
0.6545005253781555
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.58
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.77
              precision    recall  f1-score   support

         0.0       0.69      0.28      0.40        39
         1.0       0.62      0.74      0.68       133
         2.0       0.74      0.72      0.73       165
         3.0       0.78      0.78      0.78        96
         4.0       0.50      0.61      0.55        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.69       452
   macro avg       0.56      0.52      0.52       452
weighted avg       0.70      0.69      0.69       452

[[ 11  27   1   0   0   0]
 [  5  99  29   0   0   0]
 [  0  33 118  14   0   0]
 [  0   0  11  75  10   0]
 [  0   0   0   7  11   0]
 [  0   0   0   0   1   0]]
0.6877657448938815
452 452 452
Filename	True Label	Prediction
1023_0001418	3.0	3.0
1023_0101683	2.0	2.0
1023_0101684	2.0	2.0
1023_0101695	2.0	2.0
1023_0101895	4.0	3.0
1023_0101901	3.0	3.0
1023_0101906	2.0	2.0
1023_0101909	4.0	3.0
1023_0103830	3.0	3.0
1023_0103832	2.0	2.0
1023_0103837	3.0	3.0
1023_0103838	3.0	3.0
1023_0103844	4.0	3.0
1023_0104203	3.0	3.0
1023_0107740	3.0	3.0
1023_0107783	3.0	2.0
1023_0108306	4.0	3.0
1023_0108423	3.0	2.0
1023_0108426	2.0	2.0
1023_0108520	3.0	3.0
1023_0108752	3.0	3.0
1023_0108766	2.0	3.0
1023_0108810	3.0	3.0
1023_0108813	3.0	3.0
1023_0108814	3.0	3.0
1023_0108890	3.0	3.0
1023_0108908	3.0	3.0
1023_0108932	3.0	2.0
1023_0109029	2.0	2.0
1023_0109030	3.0	3.0
1023_0109038	3.0	3.0
1023_0109192	3.0	2.0
1023_0109395	2.0	2.0
1023_0109396	2.0	3.0
1023_0109400	3.0	3.0
1023_0109401	3.0	3.0
1023_0109495	3.0	3.0
1023_0109522	3.0	3.0
1023_0109674	3.0	3.0
1023_0109717	3.0	3.0
1023_0109891	2.0	3.0
1023_0109947	3.0	3.0
1031_0001949	4.0	4.0
1031_0002036	4.0	4.0
1031_0002043	4.0	4.0
1031_0002061	3.0	3.0
1031_0002079	4.0	4.0
1031_0002083	3.0	3.0
1031_0002089	4.0	4.0
1031_0002131	3.0	3.0
1031_0002185	4.0	4.0
1031_0002198	4.0	4.0
1031_0003035	4.0	3.0
1031_0003042	3.0	3.0
1031_0003077	3.0	3.0
1031_0003090	3.0	3.0
1031_0003099	4.0	3.0
1031_0003106	3.0	3.0
1031_0003133	4.0	4.0
1031_0003140	3.0	3.0
1031_0003141	3.0	4.0
1031_0003144	3.0	3.0
1031_0003150	3.0	4.0
1031_0003156	3.0	4.0
1031_0003162	3.0	3.0
1031_0003163	3.0	3.0
1031_0003164	3.0	3.0
1031_0003165	3.0	3.0
1031_0003186	3.0	3.0
1031_0003189	4.0	4.0
1031_0003190	3.0	4.0
1031_0003205	3.0	4.0
1031_0003211	3.0	3.0
1031_0003212	3.0	3.0
1031_0003224	3.0	3.0
1031_0003225	3.0	4.0
1031_0003235	4.0	4.0
1031_0003240	3.0	4.0
1031_0003260	3.0	3.0
1031_0003261	3.0	3.0
1031_0003262	3.0	3.0
1031_0003272	3.0	3.0
1031_0003309	3.0	4.0
1031_0003338	4.0	4.0
1031_0003354	3.0	4.0
1031_0003383	4.0	3.0
1031_0003390	3.0	4.0
1031_0003391	2.0	3.0
1031_0003415	5.0	4.0
1031_0003419	3.0	3.0
1061_0012029	2.0	3.0
1061_0120277	1.0	2.0
1061_0120281	1.0	2.0
1061_0120286	1.0	1.0
1061_0120289	2.0	2.0
1061_0120303	0.0	1.0
1061_0120310	3.0	2.0
1061_0120314	2.0	2.0
1061_0120327	2.0	2.0
1061_0120331	1.0	1.0
1061_0120333	2.0	3.0
1061_0120336	1.0	2.0
1061_0120338	1.0	2.0
1061_0120343	2.0	2.0
1061_0120348	1.0	0.0
1061_0120350	2.0	2.0
1061_0120353	1.0	1.0
1061_0120358	1.0	2.0
1061_0120372	2.0	2.0
1061_0120383	3.0	3.0
1061_0120405	2.0	2.0
1061_0120433	1.0	2.0
1061_0120438	2.0	2.0
1061_0120439	2.0	1.0
1061_0120443	1.0	1.0
1061_0120449	2.0	2.0
1061_0120450	2.0	2.0
1061_0120455	2.0	2.0
1061_0120457	2.0	2.0
1061_0120483	2.0	2.0
1061_0120491	2.0	2.0
1061_0120496	2.0	2.0
1061_0120499	2.0	2.0
1061_0120876	2.0	2.0
1061_0120878	1.0	2.0
1061_0120887	2.0	2.0
1061_1029114	2.0	2.0
1061_1029118	1.0	2.0
1061_1202911	1.0	2.0
1061_1202912	2.0	2.0
1061_1202916	1.0	2.0
1061_1202917	1.0	2.0
1071_0024685	2.0	2.0
1071_0024686	2.0	2.0
1071_0024687	0.0	1.0
1071_0024690	2.0	2.0
1071_0024691	1.0	2.0
1071_0024701	2.0	2.0
1071_0024704	1.0	1.0
1071_0024705	2.0	2.0
1071_0024709	2.0	2.0
1071_0024712	1.0	1.0
1071_0024756	1.0	1.0
1071_0024770	1.0	1.0
1071_0024782	0.0	0.0
1071_0024797	0.0	0.0
1071_0024807	0.0	1.0
1071_0024811	1.0	1.0
1071_0024823	1.0	1.0
1071_0024826	2.0	1.0
1071_0024831	0.0	1.0
1071_0024837	0.0	0.0
1071_0024840	1.0	0.0
1071_0024843	0.0	0.0
1071_0024853	0.0	0.0
1071_0024855	1.0	0.0
1071_0024857	1.0	1.0
1071_0024864	0.0	0.0
1071_0024878	2.0	2.0
1071_0242011	1.0	1.0
1071_0243593	0.0	1.0
1071_0243621	1.0	1.0
1071_0243623	1.0	1.0
1071_0248303	0.0	0.0
1071_0248304	0.0	1.0
1071_0248305	0.0	1.0
1071_0248314	1.0	1.0
1071_0248323	1.0	1.0
1071_0248327	0.0	0.0
1071_0248330	2.0	1.0
1071_0248331	1.0	1.0
1071_0248342	1.0	1.0
1071_0248344	2.0	1.0
1091_0000001	1.0	2.0
1091_0000004	1.0	1.0
1091_0000009	1.0	1.0
1091_0000010	3.0	2.0
1091_0000017	2.0	2.0
1091_0000026	1.0	1.0
1091_0000028	1.0	1.0
1091_0000029	2.0	1.0
1091_0000034	2.0	1.0
1091_0000041	1.0	1.0
1091_0000050	0.0	1.0
1091_0000052	0.0	1.0
1091_0000059	1.0	2.0
1091_0000065	1.0	2.0
1091_0000075	1.0	2.0
1091_0000077	2.0	1.0
1091_0000079	1.0	2.0
1091_0000087	2.0	2.0
1091_0000113	1.0	2.0
1091_0000158	2.0	2.0
1091_0000159	2.0	2.0
1091_0000165	1.0	1.0
1091_0000174	2.0	1.0
1091_0000194	1.0	2.0
1091_0000195	1.0	1.0
1091_0000205	1.0	2.0
1091_0000210	2.0	1.0
1091_0000211	1.0	2.0
1091_0000217	2.0	1.0
1091_0000220	1.0	2.0
1091_0000222	2.0	1.0
1091_0000229	2.0	2.0
1091_0000244	2.0	2.0
1091_0000249	2.0	2.0
1091_0000255	0.0	2.0
1091_0000256	1.0	1.0
1091_0000261	2.0	2.0
1091_0000262	2.0	2.0
1091_0000272	1.0	2.0
1091_0000275	2.0	2.0
0623	2.0	2.0
0626	2.0	2.0
0627	2.0	2.0
0631	2.0	2.0
0633	2.0	2.0
0635	2.0	2.0
0645	2.0	2.0
0718	2.0	2.0
0805	2.0	2.0
0809	2.0	2.0
0815	2.0	2.0
0817	2.0	2.0
0818	1.0	2.0
0821	2.0	2.0
0822	2.0	2.0
0902	2.0	2.0
0906	2.0	2.0
0915	2.0	2.0
0917	2.0	2.0
0923	2.0	2.0
0928	2.0	2.0
1004	2.0	2.0
1017	2.0	2.0
1112	2.0	2.0
1113	2.0	2.0
BER0611006	3.0	3.0
LIB0611002B	1.0	1.0
LIB0611004A	1.0	1.0
LON0610002B	1.0	1.0
LON0611004B	1.0	1.0
MOS0509004	2.0	2.0
PAR1011008A	2.0	1.0
PAR1011009A	2.0	2.0
PAR1011015	2.0	3.0
PAR1011017	3.0	3.0
PHA0111002A	1.0	1.0
PHA0111002B	2.0	1.0
PHA0112009B	2.0	1.0
PHA0209039	3.0	3.0
PHA0411010B	1.0	1.0
PHA0411030	3.0	3.0
PHA0411033	3.0	3.0
PHA0411039	3.0	3.0
PHA0411053	3.0	3.0
PHA0411061	3.0	3.0
PHA0411062	3.0	3.0
PHA0509013	1.0	1.0
PHA0509026	3.0	3.0
PHA0509031	2.0	3.0
PHA0509045	3.0	2.0
PHA0510003A	1.0	1.0
PHA0510004B	1.0	1.0
PHA0510013A	2.0	1.0
PHA0510013B	1.0	1.0
PHA0510030	3.0	3.0
PHA0510031	3.0	3.0
PHA0510034	3.0	3.0
PHA0510037	2.0	3.0
PHA0510047	2.0	3.0
PHA0510050	3.0	3.0
PHA0610019B	2.0	2.0
PHA0809010	2.0	3.0
PHA0810003	3.0	3.0
PHA0811016	3.0	3.0
PHA1109001	2.0	1.0
PHA1109003	2.0	2.0
PHA1109005	3.0	3.0
PHA1109008	1.0	1.0
PHA1109024	3.0	3.0
PHA1109027	3.0	3.0
PHA1110001B	2.0	1.0
PHA1110002A	2.0	2.0
PHA1110013	3.0	3.0
PHA1110019	3.0	3.0
PHA1110021	3.0	3.0
PHA1111003A	1.0	1.0
PHA1111006B	2.0	1.0
TI071122B	1.0	1.0
VAR0209036	2.0	3.0
VAR0909005	3.0	3.0
VAR0910010	3.0	3.0
1325_1001012	2.0	3.0
1325_1001013	2.0	2.0
1325_1001019	2.0	2.0
1325_1001023	2.0	2.0
1325_1001025	2.0	2.0
1325_1001033	2.0	3.0
1325_1001037	2.0	1.0
1325_1001039	3.0	3.0
1325_1001041	3.0	2.0
1325_1001050	2.0	2.0
1325_1001057	1.0	2.0
1325_1001059	2.0	2.0
1325_1001063	2.0	2.0
1325_1001077	2.0	2.0
1325_1001087	2.0	2.0
1325_1001091	2.0	2.0
1325_1001096	2.0	2.0
1325_1001125	3.0	2.0
1325_1001126	2.0	2.0
1325_1001139	2.0	2.0
1325_9000106	2.0	2.0
1325_9000138	3.0	3.0
1325_9000143	3.0	3.0
1325_9000214	3.0	2.0
1325_9000304	2.0	2.0
1325_9000314	2.0	2.0
1325_9000318	3.0	2.0
1325_9000319	2.0	2.0
1325_9000322	3.0	3.0
1325_9000674	3.0	3.0
1365_0100002	2.0	2.0
1365_0100005	1.0	2.0
1365_0100007	1.0	1.0
1365_0100009	1.0	1.0
1365_0100013	2.0	2.0
1365_0100014	2.0	2.0
1365_0100017	2.0	2.0
1365_0100026	1.0	1.0
1365_0100029	1.0	1.0
1365_0100031	2.0	2.0
1365_0100058	2.0	2.0
1365_0100074	2.0	2.0
1365_0100094	2.0	2.0
1365_0100125	2.0	2.0
1365_0100138	2.0	1.0
1365_0100147	2.0	2.0
1365_0100164	2.0	2.0
1365_0100170	2.0	2.0
1365_0100172	1.0	2.0
1365_0100176	2.0	2.0
1365_0100177	2.0	1.0
1365_0100182	2.0	2.0
1365_0100183	2.0	2.0
1365_0100187	2.0	2.0
1365_0100203	2.0	1.0
1365_0100205	2.0	1.0
1365_0100213	1.0	2.0
1365_0100215	2.0	2.0
1365_0100219	2.0	2.0
1365_0100226	2.0	2.0
1365_0100227	2.0	2.0
1365_0100265	2.0	2.0
1365_0100266	2.0	2.0
1365_0100267	2.0	2.0
1365_0100268	1.0	1.0
1365_0100285	2.0	1.0
1365_0100286	1.0	1.0
1365_0100289	2.0	1.0
1365_0100461	2.0	2.0
1365_0100472	2.0	2.0
1365_0100474	2.0	2.0
1365_0100477	1.0	2.0
1385_0000037	1.0	1.0
1385_0000039	1.0	1.0
1385_0000040	1.0	1.0
1385_0000041	1.0	1.0
1385_0000045	1.0	1.0
1385_0000047	1.0	1.0
1385_0000050	1.0	1.0
1385_0000054	1.0	1.0
1385_0000057	1.0	1.0
1385_0000059	1.0	1.0
1385_0000114	1.0	1.0
1385_0000124	1.0	1.0
1385_0001108	1.0	1.0
1385_0001119	1.0	1.0
1385_0001127	1.0	1.0
1385_0001128	0.0	1.0
1385_0001135	1.0	1.0
1385_0001147	1.0	1.0
1385_0001158	1.0	1.0
1385_0001161	1.0	1.0
1385_0001162	1.0	1.0
1385_0001169	1.0	0.0
1385_0001188	1.0	1.0
1385_0001522	0.0	0.0
1385_0001523	1.0	1.0
1385_0001716	1.0	1.0
1385_0001717	1.0	1.0
1385_0001719	1.0	1.0
1385_0001734	0.0	1.0
1385_0001749	0.0	1.0
1385_0001753	1.0	1.0
1385_0001756	1.0	1.0
1385_0001764	0.0	1.0
1385_0001767	0.0	1.0
1385_0001768	1.0	1.0
1385_0001772	1.0	1.0
1385_0001774	0.0	0.0
1385_0001788	0.0	1.0
1385_0001793	0.0	1.0
1395_0000338	1.0	1.0
1395_0000359	1.0	1.0
1395_0000360	2.0	1.0
1395_0000369	2.0	1.0
1395_0000376	2.0	1.0
1395_0000378	1.0	1.0
1395_0000379	1.0	1.0
1395_0000398	2.0	1.0
1395_0000432	1.0	1.0
1395_0000447	1.0	1.0
1395_0000448	1.0	0.0
1395_0000450	1.0	1.0
1395_0000465	1.0	1.0
1395_0000471	1.0	1.0
1395_0000512	1.0	1.0
1395_0000527	0.0	1.0
1395_0000529	1.0	1.0
1395_0000534	1.0	1.0
1395_0000535	1.0	1.0
1395_0000549	1.0	1.0
1395_0000553	1.0	1.0
1395_0000557	2.0	2.0
1395_0000559	1.0	1.0
1395_0000560	2.0	1.0
1395_0000563	1.0	1.0
1395_0000579	1.0	1.0
1395_0000596	2.0	1.0
1395_0000597	1.0	1.0
1395_0000604	0.0	1.0
1395_0000607	0.0	0.0
1395_0000612	1.0	1.0
1395_0000630	0.0	1.0
1395_0000646	0.0	1.0
1395_0000649	2.0	1.0
1395_0001016	1.0	1.0
1395_0001020	1.0	1.0
1395_0001034	0.0	1.0
1395_0001066	0.0	1.0
1395_0001067	0.0	1.0
1395_0001080	1.0	1.0
1395_0001103	0.0	1.0
1395_0001109	0.0	1.0
1395_0001115	1.0	1.0
1395_0001118	0.0	1.0
1395_0001132	2.0	1.0
1395_0001133	1.0	1.0
1395_0001147	0.0	1.0
Averaged weighted F1-scores 0.6563629379216093
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
144.90552995391704 65.35717405024758
LANGUAGE: IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']
1 Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 1.11
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.89
              precision    recall  f1-score   support

         0.0       0.96      0.64      0.76        74
         1.0       0.60      0.75      0.66       140
         2.0       0.70      0.52      0.60       146
         3.0       0.57      0.86      0.68        78
         4.0       0.00      0.00      0.00        14

    accuracy                           0.65       452
   macro avg       0.56      0.55      0.54       452
weighted avg       0.67      0.65      0.64       452

[[ 47  26   1   0   0]
 [  2 105  25   8   0]
 [  0  41  76  29   0]
 [  0   4   7  67   0]
 [  0   0   0  14   0]]
0.6414708004527359
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.80
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.77
              precision    recall  f1-score   support

         0.0       1.00      0.64      0.78        74
         1.0       0.64      0.76      0.70       140
         2.0       0.69      0.68      0.69       146
         3.0       0.67      0.83      0.74        78
         4.0       0.00      0.00      0.00        14

    accuracy                           0.70       452
   macro avg       0.60      0.58      0.58       452
weighted avg       0.70      0.70      0.69       452

[[ 47  25   2   0   0]
 [  0 106  33   1   0]
 [  0  30  99  17   0]
 [  0   4   9  65   0]
 [  0   0   0  14   0]]
0.6919681803526061
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.70
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.76
              precision    recall  f1-score   support

         0.0       1.00      0.66      0.80        74
         1.0       0.67      0.74      0.71       140
         2.0       0.69      0.73      0.71       146
         3.0       0.70      0.82      0.75        78
         4.0       1.00      0.07      0.13        14

    accuracy                           0.72       452
   macro avg       0.81      0.61      0.62       452
weighted avg       0.75      0.72      0.71       452

[[ 49  22   3   0   0]
 [  0 104  34   2   0]
 [  0  26 107  13   0]
 [  0   3  11  64   0]
 [  0   0   0  13   1]]
0.7125397276183084
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.62
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       0.98      0.70      0.82        74
         1.0       0.71      0.65      0.68       140
         2.0       0.66      0.80      0.72       146
         3.0       0.69      0.82      0.75        78
         4.0       0.00      0.00      0.00        14

    accuracy                           0.72       452
   macro avg       0.61      0.59      0.59       452
weighted avg       0.71      0.72      0.71       452

[[ 52  19   3   0   0]
 [  1  91  46   2   0]
 [  0  16 117  13   0]
 [  0   3  11  64   0]
 [  0   0   0  14   0]]
0.70680636622351
452 452 452
Filename	True Label	Prediction
1023_0001420	3.0	3.0
1023_0001423	2.0	2.0
1023_0001575	3.0	3.0
1023_0101675	3.0	3.0
1023_0101691	3.0	3.0
1023_0101695	2.0	3.0
1023_0101749	3.0	3.0
1023_0101847	3.0	3.0
1023_0101848	2.0	2.0
1023_0101854	2.0	2.0
1023_0101895	3.0	3.0
1023_0101897	2.0	3.0
1023_0101904	2.0	3.0
1023_0103821	3.0	3.0
1023_0103823	3.0	3.0
1023_0103837	3.0	3.0
1023_0103880	3.0	3.0
1023_0104207	3.0	3.0
1023_0107787	3.0	3.0
1023_0108304	4.0	3.0
1023_0108423	3.0	2.0
1023_0108641	4.0	3.0
1023_0108650	3.0	3.0
1023_0108752	4.0	3.0
1023_0108753	2.0	3.0
1023_0108815	3.0	3.0
1023_0108888	3.0	3.0
1023_0108890	3.0	3.0
1023_0108993	4.0	3.0
1023_0109030	3.0	3.0
1023_0109039	3.0	3.0
1023_0109396	2.0	3.0
1023_0109651	3.0	3.0
1023_0109671	3.0	3.0
1023_0109717	3.0	3.0
1023_0109890	4.0	3.0
1023_0109891	3.0	3.0
1023_0109951	3.0	3.0
1023_0109954	3.0	3.0
1031_0001949	3.0	3.0
1031_0001951	2.0	3.0
1031_0002003	3.0	3.0
1031_0002006	4.0	3.0
1031_0002040	4.0	3.0
1031_0002083	3.0	3.0
1031_0002088	3.0	3.0
1031_0002131	3.0	3.0
1031_0002195	3.0	3.0
1031_0003023	3.0	3.0
1031_0003048	4.0	3.0
1031_0003071	3.0	3.0
1031_0003074	3.0	3.0
1031_0003077	3.0	3.0
1031_0003088	4.0	3.0
1031_0003098	4.0	3.0
1031_0003128	3.0	3.0
1031_0003144	3.0	3.0
1031_0003149	3.0	3.0
1031_0003156	3.0	3.0
1031_0003165	2.0	3.0
1031_0003170	3.0	3.0
1031_0003173	3.0	3.0
1031_0003179	3.0	3.0
1031_0003189	4.0	3.0
1031_0003207	4.0	3.0
1031_0003211	2.0	3.0
1031_0003218	3.0	3.0
1031_0003262	3.0	3.0
1031_0003309	3.0	3.0
1031_0003315	4.0	3.0
1031_0003330	3.0	3.0
1031_0003331	3.0	3.0
1031_0003354	3.0	3.0
1031_0003357	3.0	3.0
1031_0003358	4.0	3.0
1031_0003367	3.0	3.0
1031_0003386	2.0	3.0
1031_0003389	3.0	3.0
1031_0003393	3.0	3.0
1061_0120274	2.0	1.0
1061_0120289	2.0	2.0
1061_0120295	0.0	2.0
1061_0120302	1.0	2.0
1061_0120303	1.0	2.0
1061_0120304	2.0	2.0
1061_0120306	3.0	2.0
1061_0120310	2.0	2.0
1061_0120320	3.0	3.0
1061_0120321	2.0	2.0
1061_0120325	3.0	2.0
1061_0120326	3.0	3.0
1061_0120331	1.0	1.0
1061_0120335	3.0	3.0
1061_0120336	1.0	2.0
1061_0120348	1.0	1.0
1061_0120350	3.0	3.0
1061_0120356	2.0	2.0
1061_0120358	1.0	2.0
1061_0120368	2.0	2.0
1061_0120374	3.0	3.0
1061_0120383	2.0	3.0
1061_0120409	3.0	2.0
1061_0120411	3.0	3.0
1061_0120426	2.0	3.0
1061_0120427	2.0	2.0
1061_0120442	3.0	3.0
1061_0120455	2.0	2.0
1061_0120489	3.0	3.0
1061_0120856	2.0	2.0
1061_0120857	2.0	2.0
1061_0120875	3.0	3.0
1061_0120880	3.0	3.0
1061_0120881	2.0	3.0
1061_0120882	3.0	3.0
1061_1029112	3.0	3.0
1061_1029115	2.0	2.0
1061_1029119	1.0	2.0
1061_1202911	1.0	2.0
1061_1202918	2.0	2.0
1071_0024680	2.0	2.0
1071_0024686	2.0	2.0
1071_0024687	0.0	1.0
1071_0024690	2.0	2.0
1071_0024699	1.0	1.0
1071_0024704	1.0	1.0
1071_0024706	1.0	1.0
1071_0024712	1.0	2.0
1071_0024713	2.0	2.0
1071_0024757	2.0	2.0
1071_0024758	2.0	2.0
1071_0024778	0.0	1.0
1071_0024784	1.0	1.0
1071_0024808	1.0	1.0
1071_0024812	1.0	1.0
1071_0024813	0.0	0.0
1071_0024818	2.0	1.0
1071_0024819	1.0	1.0
1071_0024821	1.0	1.0
1071_0024824	1.0	1.0
1071_0024826	1.0	2.0
1071_0024837	1.0	1.0
1071_0024843	1.0	1.0
1071_0024844	1.0	1.0
1071_0024847	2.0	2.0
1071_0024849	0.0	0.0
1071_0024864	0.0	1.0
1071_0024873	1.0	1.0
1071_0024879	1.0	1.0
1071_0242012	2.0	2.0
1071_0242013	1.0	2.0
1071_0242022	1.0	1.0
1071_0242023	1.0	1.0
1071_0242071	0.0	1.0
1071_0242093	0.0	0.0
1071_0243621	2.0	2.0
1071_0243623	1.0	1.0
1071_0248301	1.0	1.0
1071_0248303	1.0	1.0
1071_0248309	2.0	1.0
1071_0248315	0.0	0.0
1071_0248317	0.0	0.0
1071_0248329	1.0	1.0
1071_0248334	2.0	2.0
1071_0248347	1.0	0.0
1091_0000010	3.0	2.0
1091_0000013	1.0	1.0
1091_0000016	1.0	2.0
1091_0000021	1.0	2.0
1091_0000031	1.0	1.0
1091_0000039	1.0	1.0
1091_0000045	2.0	2.0
1091_0000054	0.0	1.0
1091_0000062	3.0	1.0
1091_0000066	2.0	1.0
1091_0000067	2.0	2.0
1091_0000068	1.0	2.0
1091_0000069	1.0	1.0
1091_0000072	1.0	2.0
1091_0000073	3.0	1.0
1091_0000074	2.0	2.0
1091_0000076	2.0	2.0
1091_0000078	3.0	1.0
1091_0000079	1.0	2.0
1091_0000140	1.0	1.0
1091_0000144	1.0	1.0
1091_0000152	1.0	1.0
1091_0000154	1.0	3.0
1091_0000155	2.0	3.0
1091_0000159	2.0	2.0
1091_0000166	1.0	2.0
1091_0000167	1.0	3.0
1091_0000169	2.0	2.0
1091_0000185	2.0	1.0
1091_0000193	2.0	1.0
1091_0000196	2.0	1.0
1091_0000197	1.0	2.0
1091_0000198	2.0	2.0
1091_0000210	2.0	2.0
1091_0000215	1.0	2.0
1091_0000227	0.0	2.0
1091_0000235	1.0	1.0
1091_0000236	2.0	2.0
1091_0000238	1.0	2.0
1091_0000241	2.0	1.0
1091_0000244	2.0	2.0
1091_0000245	1.0	2.0
1091_0000257	1.0	2.0
0621	2.0	2.0
0622	1.0	1.0
0627	2.0	2.0
0631	2.0	2.0
0635	2.0	2.0
0645	2.0	2.0
0714	2.0	2.0
0715	2.0	2.0
0721	2.0	2.0
0805	2.0	2.0
0811	2.0	2.0
0814	1.0	2.0
0815	2.0	2.0
0816	3.0	2.0
0823	1.0	2.0
0824	1.0	2.0
0829	1.0	2.0
0904	1.0	2.0
0905	1.0	2.0
0911	2.0	2.0
0919	2.0	2.0
0930	1.0	2.0
1006	1.0	2.0
1020	1.0	2.0
1114	1.0	2.0
BER0609003	0.0	0.0
BER0611005	0.0	0.0
KYJ0611004A	1.0	1.0
KYJ0611009A	1.0	1.0
LIB0611001A	0.0	1.0
LIB0611001B	0.0	0.0
LIB0611002A	0.0	1.0
LIB0611004A	1.0	1.0
LIB0611011	0.0	0.0
LON0611002B	0.0	0.0
MOS0611013	0.0	0.0
PHA0111001A	1.0	1.0
PHA0111003A	1.0	1.0
PHA0111003B	0.0	0.0
PHA0111004A	1.0	1.0
PHA0111005A	1.0	1.0
PHA0112007B	0.0	0.0
PHA0112012A	1.0	1.0
PHA0112012B	0.0	0.0
PHA0209024	0.0	0.0
PHA0210007	0.0	0.0
PHA0411008B	0.0	0.0
PHA0411012A	1.0	1.0
PHA0411028	0.0	0.0
PHA0411031	0.0	0.0
PHA0411035	0.0	0.0
PHA0411038	0.0	0.0
PHA0411039	0.0	0.0
PHA0411041	0.0	0.0
PHA0411043	0.0	0.0
PHA0411056	0.0	0.0
PHA0411061	0.0	0.0
PHA0509022	0.0	0.0
PHA0509037	0.0	0.0
PHA0509040	0.0	0.0
PHA0510002B	0.0	0.0
PHA0510010A	1.0	1.0
PHA0510032	0.0	0.0
PHA0510038	0.0	0.0
PHA0510040	0.0	0.0
PHA0510046	0.0	0.0
PHA0510049	0.0	0.0
PHA0610005A	1.0	1.0
PHA0610005B	0.0	0.0
PHA0610006A	0.0	1.0
PHA0610006B	0.0	0.0
PHA0610019A	0.0	1.0
PHA0610019B	0.0	0.0
PHA0810004	0.0	0.0
PHA0810015	0.0	0.0
PHA0811012	0.0	0.0
PHA0811019	0.0	0.0
PHA1109003	0.0	0.0
PHA1109005	0.0	0.0
PHA1109007	0.0	0.0
PHA1109024	0.0	0.0
PHA1110003B	0.0	0.0
PHA1110004A	1.0	1.0
PHA1111002B	0.0	0.0
PHA1111004B	0.0	0.0
PHA1111006A	0.0	1.0
VAR0909009	0.0	0.0
VAR0910005	0.0	0.0
VAR0910011	0.0	0.0
1325_1001008	2.0	2.0
1325_1001012	2.0	2.0
1325_1001013	3.0	2.0
1325_1001020	2.0	2.0
1325_1001021	2.0	2.0
1325_1001027	2.0	2.0
1325_1001047	2.0	2.0
1325_1001062	2.0	2.0
1325_1001077	2.0	2.0
1325_1001088	2.0	2.0
1325_1001089	2.0	2.0
1325_1001093	2.0	2.0
1325_1001095	2.0	2.0
1325_1001096	2.0	2.0
1325_1001123	2.0	2.0
1325_1001126	2.0	2.0
1325_1001127	2.0	2.0
1325_1001144	2.0	2.0
1325_1001152	2.0	2.0
1325_1001163	1.0	2.0
1325_9000088	2.0	2.0
1325_9000107	3.0	2.0
1325_9000136	2.0	2.0
1325_9000139	2.0	2.0
1325_9000144	3.0	2.0
1325_9000215	2.0	2.0
1325_9000240	2.0	2.0
1325_9000296	2.0	2.0
1325_9000315	1.0	2.0
1325_9000503	3.0	2.0
1325_9000533	2.0	2.0
1325_9000601	2.0	2.0
1325_9000676	2.0	2.0
1325_9000686	2.0	2.0
1365_0100009	2.0	1.0
1365_0100012	2.0	2.0
1365_0100018	2.0	2.0
1365_0100021	2.0	2.0
1365_0100026	2.0	1.0
1365_0100073	2.0	2.0
1365_0100080	2.0	2.0
1365_0100101	2.0	2.0
1365_0100107	2.0	2.0
1365_0100118	2.0	2.0
1365_0100125	2.0	2.0
1365_0100133	2.0	2.0
1365_0100137	2.0	2.0
1365_0100145	2.0	2.0
1365_0100164	1.0	2.0
1365_0100166	2.0	2.0
1365_0100168	2.0	2.0
1365_0100169	2.0	2.0
1365_0100171	1.0	2.0
1365_0100175	2.0	2.0
1365_0100177	2.0	2.0
1365_0100190	2.0	2.0
1365_0100192	3.0	2.0
1365_0100195	1.0	1.0
1365_0100200	2.0	2.0
1365_0100203	2.0	2.0
1365_0100215	2.0	2.0
1365_0100225	2.0	2.0
1365_0100227	2.0	2.0
1365_0100228	2.0	2.0
1365_0100229	2.0	2.0
1365_0100231	2.0	2.0
1365_0100255	1.0	2.0
1365_0100260	1.0	2.0
1365_0100261	2.0	2.0
1365_0100274	2.0	2.0
1365_0100286	1.0	2.0
1365_0100448	2.0	2.0
1365_0100457	2.0	2.0
1365_0100475	2.0	2.0
1385_0000012	1.0	1.0
1385_0000033	1.0	1.0
1385_0000042	2.0	2.0
1385_0000044	2.0	2.0
1385_0000045	2.0	2.0
1385_0000052	2.0	2.0
1385_0000095	1.0	1.0
1385_0000119	2.0	2.0
1385_0000127	1.0	1.0
1385_0001107	2.0	1.0
1385_0001108	1.0	1.0
1385_0001121	1.0	1.0
1385_0001123	1.0	1.0
1385_0001130	1.0	1.0
1385_0001131	1.0	1.0
1385_0001133	1.0	1.0
1385_0001156	2.0	2.0
1385_0001161	2.0	2.0
1385_0001171	1.0	1.0
1385_0001173	0.0	1.0
1385_0001193	1.0	1.0
1385_0001196	1.0	1.0
1385_0001198	1.0	1.0
1385_0001503	1.0	2.0
1385_0001527	1.0	1.0
1385_0001715	0.0	1.0
1385_0001718	1.0	1.0
1385_0001720	0.0	1.0
1385_0001724	1.0	1.0
1385_0001737	2.0	2.0
1385_0001738	0.0	1.0
1385_0001748	1.0	2.0
1385_0001760	0.0	1.0
1385_0001765	0.0	1.0
1385_0001767	1.0	1.0
1385_0001772	0.0	1.0
1385_0001775	1.0	1.0
1385_0001785	0.0	1.0
1385_0001794	1.0	2.0
1385_0001795	1.0	1.0
1395_0000355	1.0	2.0
1395_0000357	2.0	2.0
1395_0000383	1.0	1.0
1395_0000389	1.0	1.0
1395_0000398	1.0	1.0
1395_0000402	1.0	1.0
1395_0000415	1.0	1.0
1395_0000448	1.0	1.0
1395_0000450	1.0	1.0
1395_0000454	2.0	1.0
1395_0000470	1.0	1.0
1395_0000471	1.0	1.0
1395_0000513	1.0	2.0
1395_0000518	2.0	2.0
1395_0000529	2.0	1.0
1395_0000535	1.0	1.0
1395_0000555	2.0	1.0
1395_0000559	1.0	1.0
1395_0000565	1.0	1.0
1395_0000610	2.0	1.0
1395_0000612	1.0	1.0
1395_0000639	0.0	2.0
1395_0000642	1.0	1.0
1395_0001015	1.0	1.0
1395_0001016	1.0	1.0
1395_0001021	1.0	1.0
1395_0001045	2.0	1.0
1395_0001061	1.0	2.0
1395_0001067	1.0	1.0
1395_0001069	1.0	1.0
1395_0001070	1.0	2.0
1395_0001071	1.0	1.0
1395_0001076	1.0	1.0
1395_0001104	1.0	1.0
1395_0001114	1.0	1.0
1395_0001118	1.0	1.0
1395_0001119	1.0	2.0
1395_0001120	1.0	1.0
1395_0001124	0.0	1.0
1395_0001132	1.0	1.0
1395_0001145	1.0	2.0
1395_0001150	1.0	1.0
2 Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 1.09
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.89
              precision    recall  f1-score   support

         0.0       0.96      0.61      0.74        74
         1.0       0.55      0.91      0.68       140
         2.0       0.70      0.50      0.58       147
         3.0       0.66      0.60      0.63        77
         4.0       0.00      0.00      0.00        14

    accuracy                           0.64       452
   macro avg       0.57      0.52      0.53       452
weighted avg       0.67      0.64      0.63       452

[[ 45  29   0   0   0]
 [  1 127  12   0   0]
 [  1  63  73  10   0]
 [  0  12  19  46   0]
 [  0   0   0  14   0]]
0.6296168831945064
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.82
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.81
              precision    recall  f1-score   support

         0.0       1.00      0.61      0.76        74
         1.0       0.61      0.78      0.68       140
         2.0       0.66      0.63      0.64       147
         3.0       0.62      0.71      0.67        77
         4.0       0.00      0.00      0.00        14

    accuracy                           0.67       452
   macro avg       0.58      0.55      0.55       452
weighted avg       0.67      0.67      0.66       452

[[ 45  28   1   0   0]
 [  0 109  30   1   0]
 [  0  37  92  18   0]
 [  0   5  17  55   0]
 [  0   0   0  14   0]]
0.6575610697305356
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.72
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       1.00      0.61      0.76        74
         1.0       0.65      0.70      0.68       140
         2.0       0.63      0.74      0.68       147
         3.0       0.63      0.69      0.66        77
         4.0       0.00      0.00      0.00        14

    accuracy                           0.67       452
   macro avg       0.58      0.55      0.55       452
weighted avg       0.68      0.67      0.67       452

[[ 45  28   1   0   0]
 [  0  98  41   1   0]
 [  0  22 109  16   0]
 [  0   2  22  53   0]
 [  0   0   0  14   0]]
0.666872739786963
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.64
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       1.00      0.61      0.76        74
         1.0       0.65      0.74      0.69       140
         2.0       0.66      0.73      0.69       147
         3.0       0.64      0.70      0.67        77
         4.0       0.00      0.00      0.00        14

    accuracy                           0.69       452
   macro avg       0.59      0.56      0.56       452
weighted avg       0.69      0.69      0.68       452

[[ 45  28   1   0   0]
 [  0 103  36   1   0]
 [  0  23 108  16   0]
 [  0   4  19  54   0]
 [  0   0   0  14   0]]
0.677377669027425
452 452 452
Filename	True Label	Prediction
1023_0001416	3.0	3.0
1023_0101753	3.0	3.0
1023_0101843	2.0	3.0
1023_0101849	3.0	3.0
1023_0101893	3.0	3.0
1023_0101894	2.0	3.0
1023_0101896	2.0	2.0
1023_0101900	3.0	3.0
1023_0101901	3.0	3.0
1023_0101907	3.0	3.0
1023_0101909	4.0	3.0
1023_0102117	3.0	3.0
1023_0102118	3.0	3.0
1023_0103825	2.0	3.0
1023_0103826	3.0	3.0
1023_0103827	3.0	3.0
1023_0103828	1.0	2.0
1023_0103832	2.0	3.0
1023_0103838	3.0	3.0
1023_0104203	3.0	3.0
1023_0104209	3.0	3.0
1023_0107074	4.0	3.0
1023_0107682	2.0	2.0
1023_0107725	3.0	3.0
1023_0107727	3.0	3.0
1023_0108426	2.0	3.0
1023_0108510	3.0	3.0
1023_0108810	3.0	3.0
1023_0108932	3.0	3.0
1023_0108934	3.0	3.0
1023_0109029	2.0	2.0
1023_0109033	4.0	3.0
1023_0109248	3.0	3.0
1023_0109267	3.0	3.0
1023_0109399	2.0	3.0
1023_0109518	2.0	3.0
1023_0109520	3.0	3.0
1023_0109528	3.0	3.0
1023_0109588	2.0	3.0
1023_0109614	2.0	3.0
1023_0109721	2.0	3.0
1023_0109947	3.0	3.0
1031_0002004	4.0	3.0
1031_0002005	4.0	3.0
1031_0002042	3.0	3.0
1031_0002061	3.0	3.0
1031_0002085	3.0	3.0
1031_0002091	3.0	3.0
1031_0002197	3.0	3.0
1031_0003013	4.0	3.0
1031_0003042	3.0	3.0
1031_0003072	3.0	3.0
1031_0003073	4.0	3.0
1031_0003097	3.0	3.0
1031_0003126	4.0	3.0
1031_0003127	4.0	3.0
1031_0003131	3.0	3.0
1031_0003132	3.0	3.0
1031_0003133	4.0	3.0
1031_0003135	3.0	3.0
1031_0003141	3.0	3.0
1031_0003154	3.0	3.0
1031_0003157	4.0	3.0
1031_0003169	3.0	3.0
1031_0003184	4.0	3.0
1031_0003206	3.0	3.0
1031_0003224	2.0	3.0
1031_0003225	3.0	3.0
1031_0003234	2.0	3.0
1031_0003242	2.0	3.0
1031_0003243	3.0	3.0
1031_0003244	4.0	3.0
1031_0003336	3.0	3.0
1031_0003365	3.0	3.0
1031_0003366	3.0	3.0
1031_0003383	3.0	3.0
1031_0003415	4.0	3.0
1061_0120277	1.0	2.0
1061_0120278	2.0	2.0
1061_0120282	0.0	1.0
1061_0120286	1.0	1.0
1061_0120288	2.0	2.0
1061_0120290	1.0	2.0
1061_0120297	2.0	2.0
1061_0120309	1.0	1.0
1061_0120312	1.0	1.0
1061_0120316	2.0	2.0
1061_0120319	3.0	2.0
1061_0120333	3.0	3.0
1061_0120345	3.0	2.0
1061_0120346	2.0	2.0
1061_0120352	1.0	1.0
1061_0120359	1.0	2.0
1061_0120360	3.0	3.0
1061_0120367	3.0	2.0
1061_0120369	2.0	2.0
1061_0120376	2.0	2.0
1061_0120382	2.0	2.0
1061_0120388	2.0	2.0
1061_0120394	2.0	3.0
1061_0120404	2.0	1.0
1061_0120406	2.0	3.0
1061_0120407	3.0	3.0
1061_0120408	3.0	2.0
1061_0120413	1.0	2.0
1061_0120431	3.0	2.0
1061_0120440	1.0	1.0
1061_0120448	3.0	3.0
1061_0120457	3.0	2.0
1061_0120481	3.0	3.0
1061_0120482	2.0	2.0
1061_0120486	2.0	2.0
1061_0120487	3.0	2.0
1061_0120491	2.0	2.0
1061_0120492	3.0	3.0
1061_0120495	2.0	2.0
1061_0120498	3.0	3.0
1061_0120855	1.0	2.0
1061_0120859	3.0	3.0
1061_0120887	2.0	2.0
1061_0120894	2.0	3.0
1061_1029113	1.0	2.0
1061_1029118	2.0	2.0
1061_1029120	2.0	2.0
1061_1202914	1.0	1.0
1061_1202917	2.0	2.0
1071_0024691	2.0	2.0
1071_0024692	3.0	2.0
1071_0024703	1.0	2.0
1071_0024711	2.0	1.0
1071_0024714	2.0	2.0
1071_0024769	1.0	1.0
1071_0024772	0.0	1.0
1071_0024774	0.0	1.0
1071_0024775	0.0	1.0
1071_0024777	1.0	1.0
1071_0024798	1.0	1.0
1071_0024802	1.0	2.0
1071_0024814	0.0	1.0
1071_0024815	1.0	1.0
1071_0024816	1.0	1.0
1071_0024827	1.0	1.0
1071_0024833	2.0	2.0
1071_0024846	1.0	1.0
1071_0024853	0.0	1.0
1071_0024854	1.0	1.0
1071_0024855	1.0	1.0
1071_0024861	0.0	1.0
1071_0024862	1.0	2.0
1071_0024876	1.0	2.0
1071_0241833	1.0	1.0
1071_0242092	0.0	1.0
1071_0243582	1.0	1.0
1071_0243593	1.0	1.0
1071_0248304	1.0	1.0
1071_0248310	1.0	1.0
1071_0248312	1.0	1.0
1071_0248318	0.0	1.0
1071_0248319	0.0	1.0
1071_0248323	1.0	1.0
1071_0248324	0.0	1.0
1071_0248325	1.0	1.0
1071_0248327	0.0	1.0
1071_0248330	1.0	1.0
1071_0248332	2.0	2.0
1071_0248342	1.0	1.0
1071_0248343	1.0	1.0
1071_0248346	0.0	1.0
1091_0000007	2.0	2.0
1091_0000008	3.0	2.0
1091_0000009	0.0	1.0
1091_0000012	1.0	1.0
1091_0000014	1.0	1.0
1091_0000023	3.0	1.0
1091_0000033	1.0	2.0
1091_0000037	1.0	1.0
1091_0000038	2.0	1.0
1091_0000042	1.0	1.0
1091_0000044	0.0	2.0
1091_0000046	2.0	1.0
1091_0000050	1.0	1.0
1091_0000052	1.0	1.0
1091_0000059	1.0	2.0
1091_0000092	1.0	2.0
1091_0000125	3.0	2.0
1091_0000145	1.0	1.0
1091_0000156	3.0	2.0
1091_0000160	1.0	3.0
1091_0000161	2.0	2.0
1091_0000163	2.0	1.0
1091_0000194	2.0	2.0
1091_0000204	2.0	2.0
1091_0000206	1.0	1.0
1091_0000211	1.0	2.0
1091_0000213	2.0	2.0
1091_0000224	2.0	1.0
1091_0000225	3.0	1.0
1091_0000226	1.0	2.0
1091_0000239	3.0	1.0
1091_0000248	3.0	1.0
1091_0000259	2.0	2.0
1091_0000260	2.0	2.0
1091_0000263	3.0	2.0
1091_0000264	1.0	1.0
1091_0000266	1.0	2.0
1091_0000274	2.0	2.0
0601	1.0	1.0
0602	1.0	2.0
0605	2.0	2.0
0606	2.0	2.0
0609	1.0	2.0
0611	1.0	2.0
0612	2.0	1.0
0613	0.0	1.0
0618	2.0	1.0
0625	1.0	1.0
0626	2.0	2.0
0630	0.0	1.0
0633	2.0	2.0
0801	1.0	1.0
0804	2.0	2.0
0806	2.0	2.0
0807	2.0	2.0
0809	1.0	1.0
0813	2.0	2.0
0818	1.0	2.0
0819	3.0	2.0
0825	1.0	2.0
0826	2.0	2.0
0902	2.0	2.0
0914	1.0	2.0
0916	1.0	1.0
0923	2.0	2.0
0924	1.0	1.0
0928	2.0	2.0
1004	1.0	1.0
1010	1.0	1.0
1014	1.0	1.0
1017	1.0	1.0
1021	1.0	1.0
1022	1.0	1.0
1111	1.0	1.0
KYJ0611003A	1.0	1.0
LIB0611003A	1.0	1.0
LON0610002A	1.0	1.0
LON0611004B	0.0	0.0
MOS0611014	0.0	0.0
PHA0111002A	0.0	1.0
PHA0111004B	0.0	0.0
PHA0111012	0.0	0.0
PHA0111016	0.0	0.0
PHA0112009A	2.0	1.0
PHA0209034	0.0	0.0
PHA0210008	0.0	0.0
PHA0411011B	0.0	0.0
PHA0411030	0.0	0.0
PHA0411037	0.0	0.0
PHA0411042	0.0	0.0
PHA0411047	0.0	0.0
PHA0411054	0.0	0.0
PHA0411062	0.0	0.0
PHA0509007	0.0	0.0
PHA0509027	0.0	0.0
PHA0509030	0.0	0.0
PHA0509032	0.0	0.0
PHA0509035	0.0	0.0
PHA0509038	0.0	0.0
PHA0509039	0.0	0.0
PHA0510013A	1.0	1.0
PHA0510023	0.0	0.0
PHA0510027	0.0	0.0
PHA0510031	0.0	0.0
PHA0510034	0.0	0.0
PHA0510037	0.0	0.0
PHA0510047	0.0	0.0
PHA0510048	0.0	0.0
PHA0709008	0.0	0.0
PHA0710012	0.0	0.0
PHA0710014	0.0	0.0
PHA0710018	0.0	0.0
PHA0810001	0.0	0.0
PHA0810006	0.0	0.0
PHA0810008	0.0	0.0
PHA0810011	0.0	0.0
PHA0810012	0.0	0.0
PHA0811010	0.0	0.0
PHA0811013	0.0	0.0
PHA0811014	0.0	0.0
PHA1110002A	1.0	1.0
PHA1111009A	0.0	1.0
ST071122B	0.0	0.0
TI071122B	0.0	0.0
VAR0209036	0.0	0.0
VAR0910006	0.0	0.0
VAR0910009	0.0	0.0
1325_1001009	2.0	2.0
1325_1001014	2.0	2.0
1325_1001039	2.0	2.0
1325_1001044	2.0	2.0
1325_1001045	2.0	2.0
1325_1001059	2.0	2.0
1325_1001078	2.0	2.0
1325_1001086	2.0	2.0
1325_1001091	2.0	2.0
1325_1001098	2.0	2.0
1325_1001099	3.0	2.0
1325_1001100	2.0	2.0
1325_1001101	3.0	2.0
1325_1001122	2.0	2.0
1325_1001131	2.0	2.0
1325_1001135	2.0	2.0
1325_1001141	1.0	2.0
1325_1001157	2.0	2.0
1325_1001164	2.0	2.0
1325_1001166	2.0	2.0
1325_1001169	2.0	2.0
1325_9000089	2.0	2.0
1325_9000099	2.0	2.0
1325_9000105	2.0	2.0
1325_9000138	2.0	2.0
1325_9000214	2.0	2.0
1325_9000239	3.0	2.0
1325_9000304	2.0	2.0
1325_9000323	1.0	2.0
1325_9000536	3.0	2.0
1325_9000611	2.0	2.0
1325_9000700	2.0	2.0
1325_9000750	3.0	2.0
1365_0100004	1.0	2.0
1365_0100006	2.0	2.0
1365_0100008	1.0	2.0
1365_0100011	2.0	2.0
1365_0100013	2.0	2.0
1365_0100015	2.0	2.0
1365_0100024	1.0	2.0
1365_0100027	2.0	2.0
1365_0100028	2.0	2.0
1365_0100029	2.0	1.0
1365_0100051	1.0	2.0
1365_0100057	1.0	2.0
1365_0100061	3.0	2.0
1365_0100067	1.0	2.0
1365_0100070	2.0	2.0
1365_0100092	2.0	2.0
1365_0100093	2.0	2.0
1365_0100094	2.0	2.0
1365_0100096	2.0	2.0
1365_0100098	2.0	2.0
1365_0100116	2.0	2.0
1365_0100147	2.0	1.0
1365_0100162	2.0	2.0
1365_0100163	2.0	2.0
1365_0100174	1.0	2.0
1365_0100181	1.0	2.0
1365_0100182	2.0	2.0
1365_0100184	2.0	2.0
1365_0100188	1.0	2.0
1365_0100191	1.0	2.0
1365_0100230	2.0	2.0
1365_0100233	2.0	2.0
1365_0100256	2.0	2.0
1365_0100267	2.0	2.0
1365_0100270	2.0	2.0
1365_0100278	2.0	2.0
1365_0100289	2.0	2.0
1365_0100458	2.0	2.0
1365_0100459	2.0	2.0
1365_0100469	2.0	2.0
1365_0100470	2.0	2.0
1365_0100474	2.0	2.0
1365_0100479	2.0	2.0
1365_0100480	2.0	2.0
1365_0100482	2.0	2.0
1385_0000011	1.0	1.0
1385_0000016	1.0	1.0
1385_0000023	1.0	1.0
1385_0000037	0.0	1.0
1385_0000058	2.0	2.0
1385_0000059	2.0	2.0
1385_0000099	1.0	1.0
1385_0000100	1.0	1.0
1385_0000120	0.0	1.0
1385_0000123	1.0	2.0
1385_0000129	2.0	1.0
1385_0000130	2.0	1.0
1385_0001109	1.0	1.0
1385_0001113	1.0	1.0
1385_0001124	1.0	1.0
1385_0001127	1.0	1.0
1385_0001136	1.0	1.0
1385_0001138	1.0	1.0
1385_0001152	2.0	2.0
1385_0001153	2.0	1.0
1385_0001155	2.0	2.0
1385_0001157	2.0	1.0
1385_0001158	2.0	2.0
1385_0001159	2.0	1.0
1385_0001164	1.0	1.0
1385_0001174	1.0	1.0
1385_0001189	0.0	1.0
1385_0001191	1.0	1.0
1385_0001501	0.0	1.0
1385_0001522	1.0	1.0
1385_0001741	0.0	1.0
1385_0001744	0.0	1.0
1385_0001758	1.0	1.0
1385_0001759	1.0	1.0
1385_0001773	1.0	1.0
1385_0001793	1.0	1.0
1395_0000340	1.0	1.0
1395_0000360	2.0	1.0
1395_0000364	1.0	1.0
1395_0000369	2.0	1.0
1395_0000376	2.0	1.0
1395_0000380	1.0	1.0
1395_0000390	1.0	1.0
1395_0000403	1.0	1.0
1395_0000446	2.0	1.0
1395_0000452	1.0	1.0
1395_0000455	1.0	1.0
1395_0000460	1.0	1.0
1395_0000462	2.0	1.0
1395_0000514	2.0	1.0
1395_0000525	2.0	1.0
1395_0000526	1.0	1.0
1395_0000533	2.0	2.0
1395_0000549	1.0	1.0
1395_0000553	1.0	1.0
1395_0000556	1.0	1.0
1395_0000564	1.0	1.0
1395_0000598	1.0	1.0
1395_0000599	1.0	1.0
1395_0000602	1.0	1.0
1395_0000626	0.0	1.0
1395_0000644	1.0	1.0
1395_0000646	1.0	1.0
1395_0001022	1.0	1.0
1395_0001023	0.0	1.0
1395_0001024	0.0	1.0
1395_0001058	1.0	1.0
1395_0001064	1.0	1.0
1395_0001066	1.0	1.0
1395_0001068	0.0	1.0
1395_0001074	1.0	1.0
1395_0001090	1.0	1.0
1395_0001101	1.0	1.0
1395_0001103	1.0	1.0
1395_0001115	1.0	1.0
1395_0001133	1.0	1.0
1395_0001141	1.0	1.0
1395_0001160	1.0	1.0
1395_0001167	1.0	1.0
3 Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 1.13
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.90
              precision    recall  f1-score   support

         0.0       1.00      0.68      0.81        74
         1.0       0.67      0.54      0.60       140
         2.0       0.56      0.72      0.63       147
         3.0       0.63      0.83      0.72        77
         4.0       0.00      0.00      0.00        14

    accuracy                           0.65       452
   macro avg       0.57      0.55      0.55       452
weighted avg       0.66      0.65      0.64       452

[[ 50  19   5   0   0]
 [  0  75  65   0   0]
 [  0  17 106  24   0]
 [  0   1  12  64   0]
 [  0   0   0  14   0]]
0.6440244930835848
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 48
Elapsed time 59

  Average training loss: 0.80
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.80
              precision    recall  f1-score   support

         0.0       1.00      0.73      0.84        74
         1.0       0.69      0.69      0.69       140
         2.0       0.63      0.65      0.64       147
         3.0       0.62      0.87      0.72        77
         4.0       0.00      0.00      0.00        14

    accuracy                           0.69       452
   macro avg       0.59      0.59      0.58       452
weighted avg       0.69      0.69      0.68       452

[[54 18  2  0  0]
 [ 0 96 44  0  0]
 [ 0 25 95 27  0]
 [ 0  1  9 67  0]
 [ 0  0  0 14  0]]
0.6819709668437545
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 48
Elapsed time 60

  Average training loss: 0.71
  Training epoch took: 68
Running Validation...
  Average evaluation loss: 0.90
              precision    recall  f1-score   support

         0.0       1.00      0.73      0.84        74
         1.0       0.71      0.48      0.57       140
         2.0       0.55      0.75      0.64       147
         3.0       0.63      0.86      0.73        77
         4.0       0.00      0.00      0.00        14

    accuracy                           0.66       452
   macro avg       0.58      0.56      0.56       452
weighted avg       0.67      0.66      0.65       452

[[ 54  15   5   0   0]
 [  0  67  73   0   0]
 [  0  12 110  25   0]
 [  0   0  11  66   0]
 [  0   0   0  14   0]]
0.6458469073220786
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.64
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.81
              precision    recall  f1-score   support

         0.0       0.98      0.74      0.85        74
         1.0       0.71      0.64      0.68       140
         2.0       0.62      0.72      0.67       147
         3.0       0.65      0.83      0.73        77
         4.0       0.00      0.00      0.00        14

    accuracy                           0.70       452
   macro avg       0.59      0.59      0.58       452
weighted avg       0.69      0.70      0.69       452

[[ 55  16   3   0   0]
 [  1  90  49   0   0]
 [  0  20 106  21   0]
 [  0   0  13  64   0]
 [  0   0   0  14   0]]
0.6888323600014331
452 452 452
Filename	True Label	Prediction
1023_0001422	2.0	3.0
1023_0101683	3.0	3.0
1023_0101684	2.0	2.0
1023_0101688	3.0	3.0
1023_0101689	2.0	2.0
1023_0101700	3.0	3.0
1023_0101701	2.0	3.0
1023_0101751	3.0	3.0
1023_0101752	3.0	3.0
1023_0101851	3.0	3.0
1023_0101898	3.0	3.0
1023_0103822	2.0	2.0
1023_0103829	2.0	3.0
1023_0103831	3.0	3.0
1023_0103833	3.0	3.0
1023_0107075	3.0	3.0
1023_0107244	3.0	3.0
1023_0107740	3.0	3.0
1023_0107780	3.0	3.0
1023_0107783	3.0	3.0
1023_0107788	3.0	3.0
1023_0108305	3.0	3.0
1023_0108307	2.0	3.0
1023_0108648	3.0	3.0
1023_0108649	4.0	3.0
1023_0108751	3.0	3.0
1023_0108813	2.0	3.0
1023_0108886	3.0	3.0
1023_0108889	3.0	3.0
1023_0108908	3.0	3.0
1023_0108933	3.0	3.0
1023_0108955	4.0	3.0
1023_0108958	3.0	3.0
1023_0109026	2.0	3.0
1023_0109192	3.0	3.0
1023_0109249	3.0	3.0
1023_0109250	2.0	3.0
1023_0109400	3.0	3.0
1023_0109402	3.0	3.0
1023_0109515	3.0	3.0
1023_0109524	3.0	3.0
1023_0109527	3.0	3.0
1023_0109590	2.0	3.0
1023_0109649	3.0	3.0
1023_0109674	3.0	3.0
1023_0109878	3.0	3.0
1023_0109914	3.0	3.0
1023_0109946	2.0	3.0
1023_0111896	3.0	3.0
1031_0001703	3.0	3.0
1031_0002002	2.0	3.0
1031_0002010	3.0	3.0
1031_0002011	3.0	3.0
1031_0002036	4.0	3.0
1031_0002087	4.0	3.0
1031_0002184	3.0	3.0
1031_0002198	3.0	3.0
1031_0002200	2.0	3.0
1031_0003012	4.0	3.0
1031_0003043	4.0	3.0
1031_0003065	3.0	3.0
1031_0003090	4.0	3.0
1031_0003092	2.0	3.0
1031_0003099	3.0	3.0
1031_0003129	3.0	3.0
1031_0003160	3.0	3.0
1031_0003161	3.0	3.0
1031_0003163	3.0	3.0
1031_0003164	4.0	3.0
1031_0003166	2.0	3.0
1031_0003174	4.0	3.0
1031_0003180	4.0	3.0
1031_0003186	4.0	3.0
1031_0003212	2.0	3.0
1031_0003230	3.0	3.0
1031_0003231	3.0	3.0
1031_0003232	2.0	3.0
1031_0003233	3.0	3.0
1031_0003240	3.0	3.0
1031_0003245	3.0	3.0
1031_0003246	3.0	3.0
1031_0003260	3.0	3.0
1031_0003310	3.0	3.0
1031_0003314	4.0	3.0
1031_0003353	2.0	3.0
1031_0003369	4.0	3.0
1031_0003384	3.0	3.0
1031_0003391	2.0	3.0
1031_0003409	4.0	3.0
1031_0003410	3.0	3.0
1031_0003414	3.0	3.0
1061_0012029	2.0	3.0
1061_0120273	2.0	2.0
1061_0120275	3.0	2.0
1061_0120284	0.0	1.0
1061_0120285	2.0	2.0
1061_0120298	2.0	2.0
1061_0120300	2.0	1.0
1061_0120314	2.0	2.0
1061_0120315	2.0	1.0
1061_0120318	2.0	2.0
1061_0120327	3.0	3.0
1061_0120328	1.0	2.0
1061_0120334	3.0	3.0
1061_0120337	3.0	2.0
1061_0120338	1.0	2.0
1061_0120341	1.0	2.0
1061_0120343	3.0	3.0
1061_0120347	2.0	2.0
1061_0120361	2.0	3.0
1061_0120373	2.0	2.0
1061_0120384	2.0	2.0
1061_0120387	2.0	2.0
1061_0120405	3.0	2.0
1061_0120415	1.0	2.0
1061_0120423	3.0	3.0
1061_0120424	2.0	3.0
1061_0120458	3.0	3.0
1061_0120479	2.0	3.0
1061_0120488	3.0	3.0
1061_0120493	2.0	2.0
1061_0120494	2.0	2.0
1061_0120878	2.0	2.0
1061_0120883	2.0	2.0
1061_0120884	2.0	2.0
1061_0120885	3.0	2.0
1061_0120890	1.0	1.0
1061_1029114	1.0	2.0
1061_1029117	2.0	2.0
1061_1202913	1.0	2.0
1061_1202916	2.0	2.0
1071_0024678	2.0	1.0
1071_0024688	2.0	2.0
1071_0024693	1.0	2.0
1071_0024705	1.0	2.0
1071_0024708	2.0	1.0
1071_0024756	1.0	2.0
1071_0024761	1.0	1.0
1071_0024773	1.0	1.0
1071_0024776	0.0	0.0
1071_0024779	1.0	1.0
1071_0024783	0.0	0.0
1071_0024797	0.0	1.0
1071_0024800	1.0	1.0
1071_0024803	1.0	1.0
1071_0024806	1.0	1.0
1071_0024807	1.0	1.0
1071_0024809	1.0	1.0
1071_0024810	1.0	1.0
1071_0024817	0.0	1.0
1071_0024831	0.0	0.0
1071_0024836	2.0	2.0
1071_0024840	1.0	1.0
1071_0024845	1.0	1.0
1071_0024851	2.0	1.0
1071_0024857	1.0	1.0
1071_0024860	0.0	1.0
1071_0024872	1.0	2.0
1071_0024874	1.0	1.0
1071_0024881	2.0	2.0
1071_0242011	2.0	2.0
1071_0242042	1.0	1.0
1071_0242043	0.0	1.0
1071_0243501	1.0	1.0
1071_0248302	1.0	0.0
1071_0248311	2.0	1.0
1071_0248320	0.0	0.0
1071_0248337	1.0	1.0
1071_0248338	2.0	1.0
1071_0248341	1.0	1.0
1071_0248344	1.0	1.0
1071_0248345	2.0	2.0
1071_0248348	1.0	1.0
1071_0248349	1.0	1.0
1091_0000001	1.0	1.0
1091_0000005	3.0	2.0
1091_0000011	1.0	2.0
1091_0000019	1.0	1.0
1091_0000026	1.0	1.0
1091_0000056	1.0	2.0
1091_0000061	2.0	1.0
1091_0000070	2.0	1.0
1091_0000077	2.0	1.0
1091_0000095	2.0	1.0
1091_0000101	1.0	2.0
1091_0000114	1.0	2.0
1091_0000123	2.0	2.0
1091_0000126	2.0	2.0
1091_0000127	2.0	2.0
1091_0000158	1.0	2.0
1091_0000164	1.0	1.0
1091_0000192	1.0	2.0
1091_0000202	2.0	2.0
1091_0000212	2.0	2.0
1091_0000214	2.0	1.0
1091_0000218	3.0	2.0
1091_0000220	1.0	2.0
1091_0000222	2.0	2.0
1091_0000223	1.0	2.0
1091_0000229	1.0	2.0
1091_0000232	2.0	2.0
1091_0000234	3.0	2.0
1091_0000240	1.0	1.0
1091_0000251	2.0	2.0
1091_0000252	2.0	2.0
1091_0000255	0.0	2.0
1091_0000256	0.0	1.0
1091_0000258	2.0	2.0
1091_0000261	2.0	2.0
1091_0000268	2.0	2.0
1091_0000275	2.0	2.0
0614	2.0	2.0
0615	1.0	1.0
0617	0.0	1.0
0624	2.0	2.0
0628	2.0	2.0
0643	1.0	2.0
0718	1.0	2.0
0725	1.0	2.0
0802	1.0	1.0
0803	1.0	2.0
0810	1.0	2.0
0812	1.0	2.0
0907	2.0	2.0
0910	2.0	2.0
0915	2.0	2.0
0918	1.0	2.0
0921	2.0	1.0
0922	1.0	1.0
0925	2.0	2.0
0926	2.0	2.0
1001	1.0	1.0
1005	1.0	1.0
1008	1.0	1.0
1009	2.0	1.0
1019	1.0	1.0
1023	1.0	1.0
1112	1.0	2.0
1117	1.0	1.0
9999	0.0	1.0
BER0611003	0.0	0.0
BER0611007	0.0	0.0
KYJ0611005B	0.0	0.0
KYJ0611006A	1.0	1.0
KYJ0611009B	0.0	0.0
LIB0611002B	0.0	0.0
LON0610002B	0.0	0.0
LON0611002A	1.0	1.0
LON0611003	0.0	0.0
MOS0509001	0.0	0.0
MOS0509004	0.0	0.0
MOS0611015	0.0	0.0
PAR1011008A	2.0	1.0
PHA0111005B	0.0	0.0
PHA0111010	0.0	0.0
PHA0111014	0.0	0.0
PHA0112002B	0.0	0.0
PHA0112006B	0.0	0.0
PHA0112007A	1.0	1.0
PHA0209031	0.0	0.0
PHA0209039	0.0	0.0
PHA0210004	0.0	0.0
PHA0411010B	0.0	0.0
PHA0411029	0.0	0.0
PHA0411034	0.0	0.0
PHA0411044	0.0	0.0
PHA0411060	0.0	0.0
PHA0509013	0.0	0.0
PHA0509015	0.0	0.0
PHA0509025	0.0	0.0
PHA0509034	0.0	0.0
PHA0509041	0.0	0.0
PHA0509042	0.0	0.0
PHA0510002A	0.0	0.0
PHA0510004A	0.0	0.0
PHA0510010B	0.0	0.0
PHA0510013B	0.0	0.0
PHA0510036	0.0	0.0
PHA0510039	0.0	0.0
PHA0610007B	0.0	0.0
PHA0610016	0.0	0.0
PHA0610017	0.0	0.0
PHA0710009	0.0	0.0
PHA0710015	0.0	0.0
PHA0710017	0.0	0.0
PHA0710021	0.0	0.0
PHA0809009	0.0	0.0
PHA0810009	0.0	0.0
PHA1110002B	0.0	0.0
PHA1110016	0.0	0.0
PHA1111001A	1.0	1.0
PHA1111001B	0.0	0.0
PHA1111002A	1.0	1.0
PHA1111003A	1.0	1.0
PHA1111003B	0.0	0.0
PHA1111004A	1.0	1.0
PHA1111008A	1.0	1.0
PHA1111008B	0.0	0.0
VAR0909003	0.0	0.0
VAR0909010	0.0	0.0
1325_1001017	1.0	2.0
1325_1001019	2.0	2.0
1325_1001024	2.0	2.0
1325_1001025	2.0	2.0
1325_1001033	2.0	2.0
1325_1001050	2.0	2.0
1325_1001063	1.0	2.0
1325_1001076	2.0	2.0
1325_1001085	2.0	2.0
1325_1001087	2.0	2.0
1325_1001097	1.0	2.0
1325_1001111	3.0	2.0
1325_1001113	3.0	2.0
1325_1001121	2.0	2.0
1325_1001124	1.0	2.0
1325_1001125	2.0	2.0
1325_1001129	1.0	2.0
1325_1001153	2.0	2.0
1325_1001160	2.0	2.0
1325_9000087	2.0	2.0
1325_9000106	2.0	2.0
1325_9000137	2.0	2.0
1325_9000152	2.0	2.0
1325_9000187	2.0	2.0
1325_9000209	2.0	2.0
1325_9000211	2.0	2.0
1325_9000279	2.0	2.0
1325_9000302	2.0	2.0
1325_9000316	2.0	2.0
1325_9000318	2.0	2.0
1325_9000319	2.0	2.0
1325_9000320	3.0	2.0
1325_9000321	3.0	2.0
1325_9000505	2.0	2.0
1325_9000554	2.0	2.0
1325_9000685	3.0	3.0
1365_0100002	1.0	2.0
1365_0100003	2.0	2.0
1365_0100005	2.0	2.0
1365_0100010	1.0	2.0
1365_0100016	1.0	2.0
1365_0100020	2.0	2.0
1365_0100022	1.0	2.0
1365_0100031	1.0	2.0
1365_0100065	1.0	2.0
1365_0100071	2.0	2.0
1365_0100079	1.0	2.0
1365_0100095	2.0	2.0
1365_0100097	2.0	2.0
1365_0100102	2.0	2.0
1365_0100117	2.0	2.0
1365_0100119	3.0	2.0
1365_0100121	2.0	2.0
1365_0100134	2.0	2.0
1365_0100167	1.0	2.0
1365_0100170	1.0	2.0
1365_0100176	2.0	2.0
1365_0100178	2.0	2.0
1365_0100180	1.0	2.0
1365_0100185	2.0	2.0
1365_0100198	1.0	2.0
1365_0100202	2.0	2.0
1365_0100211	2.0	2.0
1365_0100213	2.0	2.0
1365_0100218	2.0	2.0
1365_0100222	2.0	2.0
1365_0100252	2.0	2.0
1365_0100253	1.0	2.0
1365_0100257	2.0	2.0
1365_0100258	2.0	2.0
1365_0100263	3.0	2.0
1365_0100265	2.0	2.0
1365_0100281	2.0	2.0
1365_0100285	2.0	2.0
1365_0100299	1.0	2.0
1365_0100451	2.0	2.0
1365_0100456	2.0	2.0
1365_0100461	2.0	2.0
1365_0100472	2.0	2.0
1365_0100481	2.0	2.0
1385_0000022	0.0	1.0
1385_0000036	1.0	1.0
1385_0000038	2.0	2.0
1385_0000047	2.0	2.0
1385_0000057	2.0	1.0
1385_0000097	1.0	1.0
1385_0000101	1.0	1.0
1385_0000102	1.0	1.0
1385_0000114	2.0	2.0
1385_0000125	1.0	1.0
1385_0000128	1.0	1.0
1385_0001110	1.0	1.0
1385_0001111	2.0	1.0
1385_0001118	1.0	1.0
1385_0001134	1.0	1.0
1385_0001163	1.0	1.0
1385_0001166	1.0	1.0
1385_0001190	0.0	1.0
1385_0001195	2.0	1.0
1385_0001199	0.0	1.0
1385_0001524	1.0	1.0
1385_0001528	1.0	1.0
1385_0001712	1.0	2.0
1385_0001716	1.0	1.0
1385_0001717	1.0	1.0
1385_0001725	1.0	1.0
1385_0001726	0.0	1.0
1385_0001730	1.0	1.0
1385_0001750	0.0	1.0
1385_0001751	1.0	1.0
1385_0001754	0.0	1.0
1385_0001761	1.0	1.0
1385_0001764	1.0	1.0
1385_0001774	0.0	1.0
1385_0001788	0.0	2.0
1385_0001792	1.0	1.0
1385_0001798	1.0	2.0
1385_0001800	1.0	1.0
1395_0000333	1.0	1.0
1395_0000337	1.0	1.0
1395_0000361	1.0	1.0
1395_0000365	2.0	2.0
1395_0000388	1.0	1.0
1395_0000399	1.0	1.0
1395_0000404	1.0	1.0
1395_0000413	1.0	1.0
1395_0000443	1.0	1.0
1395_0000449	1.0	1.0
1395_0000499	1.0	1.0
1395_0000500	1.0	1.0
1395_0000504	1.0	1.0
1395_0000550	1.0	1.0
1395_0000551	2.0	1.0
1395_0000552	2.0	2.0
1395_0000557	2.0	2.0
1395_0000581	1.0	2.0
1395_0000582	1.0	1.0
1395_0000585	1.0	1.0
1395_0000596	2.0	1.0
1395_0000604	0.0	1.0
1395_0000611	1.0	1.0
1395_0000628	1.0	1.0
1395_0000630	1.0	1.0
1395_0000631	1.0	2.0
1395_0001017	1.0	1.0
1395_0001019	1.0	1.0
1395_0001078	1.0	1.0
1395_0001108	1.0	1.0
1395_0001109	1.0	1.0
1395_0001147	0.0	2.0
1395_0001158	1.0	1.0
1395_0001170	1.0	1.0
4 Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 1.06
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.82
              precision    recall  f1-score   support

         0.0       1.00      0.66      0.80        74
         1.0       0.63      0.79      0.70       140
         2.0       0.64      0.69      0.67       147
         3.0       0.69      0.61      0.65        77
         4.0       0.00      0.00      0.00        14

    accuracy                           0.68       452
   macro avg       0.59      0.55      0.56       452
weighted avg       0.69      0.68      0.68       452

[[ 49  24   1   0   0]
 [  0 111  29   0   0]
 [  0  38 102   7   0]
 [  0   3  27  47   0]
 [  0   0   0  14   0]]
0.6752898702490113
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.79
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       0.94      0.69      0.80        74
         1.0       0.65      0.78      0.71       140
         2.0       0.68      0.63      0.65       147
         3.0       0.62      0.75      0.68        77
         4.0       0.00      0.00      0.00        14

    accuracy                           0.69       452
   macro avg       0.58      0.57      0.57       452
weighted avg       0.68      0.69      0.68       452

[[ 51  23   0   0   0]
 [  3 109  28   0   0]
 [  0  33  93  21   0]
 [  0   3  16  58   0]
 [  0   0   0  14   0]]
0.6789280607274807
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.69
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.80
              precision    recall  f1-score   support

         0.0       0.94      0.69      0.80        74
         1.0       0.68      0.74      0.71       140
         2.0       0.68      0.68      0.68       147
         3.0       0.62      0.78      0.69        77
         4.0       0.00      0.00      0.00        14

    accuracy                           0.70       452
   macro avg       0.58      0.58      0.57       452
weighted avg       0.69      0.70      0.69       452

[[ 51  21   2   0   0]
 [  3 104  32   1   0]
 [  0  25 100  22   0]
 [  0   3  14  60   0]
 [  0   0   0  14   0]]
0.6883161078628869
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.60
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.78
              precision    recall  f1-score   support

         0.0       0.94      0.69      0.80        74
         1.0       0.68      0.74      0.71       140
         2.0       0.66      0.73      0.69       147
         3.0       0.65      0.73      0.69        77
         4.0       0.00      0.00      0.00        14

    accuracy                           0.70       452
   macro avg       0.59      0.58      0.58       452
weighted avg       0.69      0.70      0.69       452

[[ 51  21   2   0   0]
 [  3 103  34   0   0]
 [  0  24 107  16   0]
 [  0   3  18  56   0]
 [  0   0   0  14   0]]
0.6927425183763458
452 452 452
Filename	True Label	Prediction
1023_0001419	3.0	3.0
1023_0101693	4.0	3.0
1023_0101844	2.0	3.0
1023_0101853	2.0	3.0
1023_0103824	3.0	3.0
1023_0103830	3.0	3.0
1023_0103834	3.0	3.0
1023_0103836	2.0	3.0
1023_0103839	3.0	3.0
1023_0103844	4.0	3.0
1023_0103955	3.0	3.0
1023_0104206	2.0	3.0
1023_0107729	3.0	3.0
1023_0107773	3.0	3.0
1023_0108306	4.0	3.0
1023_0108518	3.0	3.0
1023_0108812	2.0	3.0
1023_0108885	3.0	3.0
1023_0109038	4.0	3.0
1023_0109391	3.0	3.0
1023_0109422	3.0	3.0
1023_0109495	3.0	3.0
1023_0109500	2.0	3.0
1023_0109505	3.0	3.0
1023_0109519	2.0	3.0
1023_0109522	3.0	3.0
1023_0109591	2.0	3.0
1023_0109606	3.0	3.0
1023_0109609	2.0	3.0
1023_0109880	3.0	3.0
1023_0109915	2.0	2.0
1023_0109917	3.0	3.0
1031_0001997	3.0	3.0
1031_0001998	4.0	3.0
1031_0002043	4.0	3.0
1031_0002086	3.0	3.0
1031_0002089	3.0	3.0
1031_0002185	3.0	3.0
1031_0002187	3.0	3.0
1031_0002199	4.0	3.0
1031_0003053	3.0	3.0
1031_0003076	3.0	3.0
1031_0003085	3.0	3.0
1031_0003095	3.0	3.0
1031_0003121	3.0	3.0
1031_0003136	3.0	3.0
1031_0003140	3.0	3.0
1031_0003145	3.0	3.0
1031_0003150	3.0	3.0
1031_0003155	3.0	3.0
1031_0003162	4.0	3.0
1031_0003181	4.0	3.0
1031_0003182	4.0	3.0
1031_0003183	4.0	3.0
1031_0003185	3.0	3.0
1031_0003187	3.0	3.0
1031_0003203	2.0	3.0
1031_0003205	3.0	3.0
1031_0003214	3.0	3.0
1031_0003216	2.0	3.0
1031_0003217	4.0	3.0
1031_0003219	3.0	3.0
1031_0003226	3.0	3.0
1031_0003236	3.0	3.0
1031_0003237	3.0	3.0
1031_0003261	3.0	3.0
1031_0003272	3.0	3.0
1031_0003313	3.0	3.0
1031_0003338	3.0	3.0
1031_0003388	4.0	3.0
1031_0003392	4.0	3.0
1031_0003407	3.0	3.0
1031_0003419	3.0	3.0
1061_0120271	2.0	2.0
1061_0120272	1.0	1.0
1061_0120276	3.0	2.0
1061_0120280	1.0	1.0
1061_0120287	1.0	2.0
1061_0120299	2.0	2.0
1061_0120301	2.0	2.0
1061_0120308	2.0	3.0
1061_0120311	2.0	3.0
1061_0120313	2.0	2.0
1061_0120324	3.0	2.0
1061_0120349	1.0	1.0
1061_0120366	3.0	3.0
1061_0120370	2.0	2.0
1061_0120372	2.0	2.0
1061_0120375	2.0	1.0
1061_0120386	1.0	2.0
1061_0120403	3.0	3.0
1061_0120410	2.0	2.0
1061_0120421	2.0	3.0
1061_0120425	3.0	3.0
1061_0120429	3.0	3.0
1061_0120430	2.0	2.0
1061_0120432	2.0	2.0
1061_0120433	1.0	1.0
1061_0120438	2.0	3.0
1061_0120439	1.0	1.0
1061_0120449	3.0	2.0
1061_0120459	3.0	3.0
1061_0120478	3.0	3.0
1061_0120480	2.0	2.0
1061_0120485	3.0	2.0
1061_0120497	3.0	3.0
1061_0120500	2.0	2.0
1061_0120853	2.0	3.0
1061_0120874	2.0	2.0
1061_0120876	3.0	3.0
1061_0120877	2.0	2.0
1061_0120888	2.0	2.0
1061_1029111	3.0	3.0
1061_1202910	3.0	2.0
1061_1202919	2.0	2.0
1071_0024681	1.0	2.0
1071_0024682	2.0	2.0
1071_0024683	0.0	1.0
1071_0024689	2.0	1.0
1071_0024710	1.0	1.0
1071_0024715	2.0	2.0
1071_0024763	1.0	1.0
1071_0024765	1.0	1.0
1071_0024766	1.0	1.0
1071_0024767	2.0	2.0
1071_0024770	1.0	1.0
1071_0024781	1.0	1.0
1071_0024782	0.0	0.0
1071_0024801	1.0	1.0
1071_0024804	1.0	1.0
1071_0024811	1.0	1.0
1071_0024822	0.0	1.0
1071_0024835	1.0	1.0
1071_0024848	1.0	2.0
1071_0024856	1.0	1.0
1071_0024863	2.0	1.0
1071_0024865	2.0	2.0
1071_0024867	2.0	2.0
1071_0024875	1.0	1.0
1071_0024877	1.0	1.0
1071_0241831	1.0	2.0
1071_0241832	1.0	1.0
1071_0242091	1.0	1.0
1071_0243592	1.0	1.0
1071_0248308	1.0	1.0
1071_0248313	1.0	2.0
1071_0248314	1.0	1.0
1071_0248316	1.0	1.0
1071_0248321	2.0	1.0
1071_0248322	1.0	1.0
1071_0248333	2.0	2.0
1071_0248336	1.0	1.0
1071_0248339	1.0	1.0
1071_0248340	0.0	0.0
1071_0248350	1.0	0.0
1091_0000002	2.0	2.0
1091_0000006	0.0	1.0
1091_0000025	1.0	1.0
1091_0000034	2.0	1.0
1091_0000047	2.0	1.0
1091_0000048	1.0	1.0
1091_0000051	1.0	1.0
1091_0000065	1.0	2.0
1091_0000113	1.0	2.0
1091_0000116	2.0	2.0
1091_0000151	1.0	1.0
1091_0000157	3.0	2.0
1091_0000162	2.0	2.0
1091_0000165	2.0	1.0
1091_0000168	1.0	2.0
1091_0000171	1.0	2.0
1091_0000172	2.0	1.0
1091_0000173	2.0	1.0
1091_0000174	1.0	0.0
1091_0000190	0.0	1.0
1091_0000200	2.0	2.0
1091_0000209	2.0	2.0
1091_0000216	1.0	2.0
1091_0000219	2.0	2.0
1091_0000230	2.0	2.0
1091_0000231	2.0	2.0
1091_0000233	2.0	2.0
1091_0000242	1.0	2.0
1091_0000243	1.0	1.0
1091_0000246	2.0	2.0
1091_0000249	2.0	2.0
1091_0000250	1.0	2.0
1091_0000253	2.0	1.0
1091_0000254	3.0	1.0
1091_0000267	2.0	2.0
1091_0000271	2.0	2.0
1091_0000273	1.0	2.0
0603	2.0	2.0
0607	2.0	2.0
0608	0.0	1.0
0616	2.0	2.0
0623	0.0	2.0
0629	2.0	2.0
0634	3.0	2.0
0636	2.0	2.0
0637	2.0	2.0
0639	1.0	2.0
0641	1.0	1.0
0642	2.0	2.0
0716	2.0	2.0
0717	1.0	1.0
0719	1.0	1.0
0720	2.0	1.0
0808	1.0	2.0
0820	1.0	1.0
0827	2.0	1.0
0901	2.0	2.0
0906	2.0	2.0
0913	2.0	2.0
0920	2.0	2.0
1003	1.0	1.0
1007	1.0	2.0
1018	1.0	1.0
1116	1.0	2.0
BER0611006	0.0	0.0
KYJ0611005A	1.0	1.0
LON0611004A	1.0	1.0
PAR1011009A	2.0	1.0
PAR1011013	0.0	0.0
PAR1011014	0.0	0.0
PAR1011015	0.0	0.0
PHA0112003A	1.0	0.0
PHA0112003B	0.0	0.0
PHA0112006A	3.0	1.0
PHA0209001	0.0	0.0
PHA0209008	0.0	0.0
PHA0209038	0.0	0.0
PHA0411009A	1.0	1.0
PHA0411009B	0.0	0.0
PHA0411010A	1.0	1.0
PHA0411012B	0.0	0.0
PHA0411027	0.0	0.0
PHA0411032	0.0	0.0
PHA0411033	0.0	0.0
PHA0411055	0.0	0.0
PHA0411059	0.0	0.0
PHA0509002	0.0	0.0
PHA0509017	0.0	0.0
PHA0509018	0.0	0.0
PHA0509020	0.0	0.0
PHA0509024	0.0	0.0
PHA0509026	0.0	0.0
PHA0509028	0.0	0.0
PHA0509031	0.0	0.0
PHA0509036	0.0	0.0
PHA0509045	0.0	0.0
PHA0510003B	0.0	0.0
PHA0510030	0.0	0.0
PHA0510035	0.0	0.0
PHA0710010	0.0	0.0
PHA0710011	0.0	0.0
PHA0810003	0.0	0.0
PHA0810010	0.0	0.0
PHA0811016	0.0	0.0
PHA0811020	0.0	0.0
PHA1109001	0.0	0.0
PHA1109002	0.0	0.0
PHA1109004	0.0	0.0
PHA1109006	0.0	0.0
PHA1109008	0.0	0.0
PHA1109028	0.0	0.0
PHA1110001A	1.0	1.0
PHA1110001B	0.0	0.0
PHA1110017	0.0	0.0
PHA1110019	0.0	0.0
PHA1110022	0.0	0.0
VAR0909005	0.0	0.0
VAR0909006	0.0	0.0
VAR0909008	0.0	0.0
VAR0910004	0.0	0.0
VAR0910007	0.0	0.0
1325_1001010	2.0	2.0
1325_1001011	3.0	2.0
1325_1001022	2.0	2.0
1325_1001028	1.0	2.0
1325_1001036	2.0	2.0
1325_1001042	2.0	2.0
1325_1001046	1.0	2.0
1325_1001052	2.0	2.0
1325_1001056	2.0	2.0
1325_1001075	2.0	2.0
1325_1001079	2.0	2.0
1325_1001082	2.0	2.0
1325_1001083	2.0	2.0
1325_1001084	2.0	2.0
1325_1001107	2.0	2.0
1325_1001108	2.0	2.0
1325_1001110	2.0	2.0
1325_1001120	2.0	2.0
1325_1001128	2.0	2.0
1325_1001133	2.0	2.0
1325_1001134	2.0	2.0
1325_1001136	2.0	2.0
1325_1001154	3.0	2.0
1325_1001159	2.0	2.0
1325_1001161	2.0	2.0
1325_1001162	2.0	2.0
1325_1001168	1.0	2.0
1325_9000059	3.0	2.0
1325_9000104	2.0	2.0
1325_9000140	3.0	2.0
1325_9000143	3.0	2.0
1325_9000185	3.0	2.0
1325_9000210	1.0	2.0
1325_9000241	3.0	2.0
1325_9000278	3.0	2.0
1325_9000303	2.0	2.0
1325_9000322	2.0	2.0
1325_9000504	2.0	2.0
1325_9000534	1.0	2.0
1325_9000602	2.0	2.0
1325_9000612	1.0	2.0
1325_9000674	3.0	2.0
1325_9000675	2.0	2.0
1325_9000677	2.0	2.0
1325_9000678	3.0	2.0
1365_0100014	2.0	2.0
1365_0100017	2.0	2.0
1365_0100023	1.0	2.0
1365_0100058	2.0	2.0
1365_0100063	2.0	2.0
1365_0100064	2.0	2.0
1365_0100066	1.0	2.0
1365_0100069	2.0	2.0
1365_0100072	2.0	2.0
1365_0100103	2.0	2.0
1365_0100104	1.0	2.0
1365_0100135	1.0	2.0
1365_0100173	1.0	2.0
1365_0100187	2.0	2.0
1365_0100194	2.0	2.0
1365_0100196	1.0	2.0
1365_0100199	2.0	2.0
1365_0100201	2.0	2.0
1365_0100205	2.0	2.0
1365_0100212	3.0	2.0
1365_0100219	2.0	2.0
1365_0100232	2.0	2.0
1365_0100262	2.0	2.0
1365_0100266	1.0	2.0
1365_0100269	2.0	2.0
1365_0100277	2.0	2.0
1365_0100279	2.0	2.0
1365_0100280	1.0	2.0
1365_0100282	2.0	2.0
1365_0100455	2.0	2.0
1365_0100473	2.0	2.0
1385_0000020	2.0	1.0
1385_0000034	1.0	1.0
1385_0000035	1.0	1.0
1385_0000039	1.0	1.0
1385_0000040	1.0	1.0
1385_0000041	2.0	2.0
1385_0000048	2.0	2.0
1385_0000049	2.0	2.0
1385_0000051	2.0	2.0
1385_0000053	1.0	1.0
1385_0000054	2.0	1.0
1385_0000098	1.0	1.0
1385_0000122	1.0	2.0
1385_0000124	2.0	2.0
1385_0001105	1.0	1.0
1385_0001112	1.0	1.0
1385_0001119	1.0	1.0
1385_0001125	1.0	1.0
1385_0001126	1.0	1.0
1385_0001129	1.0	1.0
1385_0001132	1.0	1.0
1385_0001135	1.0	1.0
1385_0001137	1.0	1.0
1385_0001148	1.0	1.0
1385_0001149	2.0	2.0
1385_0001154	2.0	2.0
1385_0001162	1.0	1.0
1385_0001165	1.0	1.0
1385_0001169	1.0	1.0
1385_0001170	0.0	1.0
1385_0001178	0.0	1.0
1385_0001188	1.0	1.0
1385_0001192	0.0	1.0
1385_0001523	1.0	1.0
1385_0001714	0.0	1.0
1385_0001719	1.0	1.0
1385_0001727	0.0	1.0
1385_0001728	1.0	1.0
1385_0001732	1.0	1.0
1385_0001740	1.0	1.0
1385_0001746	1.0	1.0
1385_0001747	0.0	1.0
1385_0001749	0.0	1.0
1385_0001752	0.0	1.0
1385_0001753	1.0	1.0
1385_0001757	1.0	1.0
1385_0001762	1.0	1.0
1385_0001766	2.0	1.0
1385_0001787	0.0	2.0
1385_0001790	1.0	1.0
1385_0001791	1.0	1.0
1395_0000341	1.0	1.0
1395_0000353	1.0	1.0
1395_0000354	0.0	1.0
1395_0000356	1.0	1.0
1395_0000366	2.0	1.0
1395_0000368	0.0	1.0
1395_0000378	1.0	1.0
1395_0000387	3.0	1.0
1395_0000391	2.0	2.0
1395_0000392	1.0	1.0
1395_0000396	1.0	1.0
1395_0000414	1.0	1.0
1395_0000438	2.0	2.0
1395_0000469	1.0	1.0
1395_0000512	2.0	1.0
1395_0000515	1.0	1.0
1395_0000527	1.0	1.0
1395_0000528	2.0	1.0
1395_0000531	2.0	1.0
1395_0000534	1.0	1.0
1395_0000537	1.0	1.0
1395_0000548	1.0	1.0
1395_0000563	2.0	1.0
1395_0000575	1.0	1.0
1395_0000579	1.0	1.0
1395_0000584	0.0	1.0
1395_0000591	0.0	1.0
1395_0000595	0.0	1.0
1395_0000606	1.0	1.0
1395_0000607	0.0	1.0
1395_0000608	1.0	1.0
1395_0000609	0.0	1.0
1395_0000627	1.0	1.0
1395_0000649	1.0	1.0
1395_0001010	2.0	1.0
1395_0001013	1.0	1.0
1395_0001028	1.0	2.0
1395_0001033	1.0	1.0
1395_0001040	1.0	1.0
1395_0001060	1.0	1.0
1395_0001065	1.0	1.0
1395_0001073	1.0	1.0
1395_0001116	2.0	1.0
1395_0001117	1.0	1.0
1395_0001126	1.0	1.0
1395_0001131	1.0	1.0
1395_0001146	0.0	1.0
1395_0001149	1.0	1.0
1395_0001164	2.0	1.0
5 Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 1.07
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.88
              precision    recall  f1-score   support

         0.0       1.00      0.69      0.82        75
         1.0       0.68      0.45      0.54       140
         2.0       0.53      0.90      0.67       147
         3.0       0.71      0.53      0.61        77
         4.0       0.00      0.00      0.00        13

    accuracy                           0.64       452
   macro avg       0.58      0.52      0.53       452
weighted avg       0.67      0.64      0.63       452

[[ 52  19   4   0   0]
 [  0  63  77   0   0]
 [  0   9 133   5   0]
 [  0   2  34  41   0]
 [  0   0   1  12   0]]
0.625305911402385
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.81
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.74
              precision    recall  f1-score   support

         0.0       1.00      0.69      0.82        75
         1.0       0.67      0.70      0.69       140
         2.0       0.64      0.77      0.70       147
         3.0       0.69      0.70      0.70        77
         4.0       0.00      0.00      0.00        13

    accuracy                           0.70       452
   macro avg       0.60      0.57      0.58       452
weighted avg       0.70      0.70      0.69       452

[[ 52  22   1   0   0]
 [  0  98  42   0   0]
 [  0  23 113  11   0]
 [  0   3  20  54   0]
 [  0   0   0  13   0]]
0.6943970815177303
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 48
Elapsed time 59

  Average training loss: 0.74
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.74
              precision    recall  f1-score   support

         0.0       1.00      0.72      0.84        75
         1.0       0.69      0.69      0.69       140
         2.0       0.63      0.71      0.67       147
         3.0       0.64      0.75      0.69        77
         4.0       0.00      0.00      0.00        13

    accuracy                           0.69       452
   macro avg       0.59      0.57      0.58       452
weighted avg       0.69      0.69      0.69       452

[[ 54  20   1   0   0]
 [  0  96  43   1   0]
 [  0  23 105  19   0]
 [  0   1  18  58   0]
 [  0   0   0  13   0]]
0.6864368722387462
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.64
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.78
              precision    recall  f1-score   support

         0.0       0.96      0.72      0.82        75
         1.0       0.69      0.66      0.68       140
         2.0       0.64      0.70      0.67       147
         3.0       0.61      0.79      0.69        77
         4.0       0.00      0.00      0.00        13

    accuracy                           0.69       452
   macro avg       0.58      0.58      0.57       452
weighted avg       0.69      0.69      0.68       452

[[ 54  20   1   0   0]
 [  2  93  44   1   0]
 [  0  19 103  25   0]
 [  0   2  14  61   0]
 [  0   0   0  13   0]]
0.6812876189087408
452 452 452
Filename	True Label	Prediction
1023_0001418	3.0	3.0
1023_0101690	2.0	3.0
1023_0101694	3.0	3.0
1023_0101841	3.0	3.0
1023_0101845	2.0	3.0
1023_0101846	4.0	3.0
1023_0101852	3.0	3.0
1023_0101855	2.0	3.0
1023_0101856	2.0	3.0
1023_0101899	3.0	2.0
1023_0101906	2.0	3.0
1023_0103840	4.0	3.0
1023_0103841	3.0	3.0
1023_0103843	3.0	2.0
1023_0103883	3.0	3.0
1023_0106816	3.0	3.0
1023_0107042	3.0	3.0
1023_0107672	3.0	3.0
1023_0107726	2.0	3.0
1023_0107781	3.0	3.0
1023_0107784	2.0	2.0
1023_0108422	4.0	3.0
1023_0108520	3.0	3.0
1023_0108766	2.0	3.0
1023_0108811	4.0	3.0
1023_0108814	3.0	3.0
1023_0108887	2.0	3.0
1023_0108931	3.0	3.0
1023_0108935	3.0	3.0
1023_0108992	3.0	3.0
1023_0109022	3.0	3.0
1023_0109027	3.0	3.0
1023_0109096	3.0	3.0
1023_0109151	4.0	3.0
1023_0109247	4.0	3.0
1023_0109392	3.0	3.0
1023_0109395	2.0	3.0
1023_0109401	3.0	3.0
1023_0109496	3.0	3.0
1023_0109516	3.0	3.0
1023_0109716	3.0	3.0
1023_0109945	3.0	3.0
1031_0001950	3.0	3.0
1031_0002032	3.0	3.0
1031_0002079	4.0	3.0
1031_0002084	3.0	3.0
1031_0002092	3.0	3.0
1031_0002196	3.0	3.0
1031_0003029	3.0	3.0
1031_0003035	3.0	3.0
1031_0003052	3.0	3.0
1031_0003054	3.0	3.0
1031_0003063	4.0	3.0
1031_0003078	3.0	3.0
1031_0003091	2.0	3.0
1031_0003106	3.0	3.0
1031_0003130	4.0	3.0
1031_0003146	4.0	3.0
1031_0003167	3.0	3.0
1031_0003172	3.0	3.0
1031_0003190	3.0	3.0
1031_0003191	3.0	3.0
1031_0003220	3.0	3.0
1031_0003221	2.0	3.0
1031_0003235	3.0	3.0
1031_0003238	3.0	3.0
1031_0003239	4.0	3.0
1031_0003249	4.0	3.0
1031_0003273	3.0	3.0
1031_0003274	3.0	3.0
1031_0003327	2.0	3.0
1031_0003337	3.0	3.0
1031_0003339	3.0	3.0
1031_0003352	3.0	3.0
1031_0003355	3.0	3.0
1031_0003356	3.0	3.0
1031_0003359	2.0	3.0
1031_0003368	3.0	3.0
1031_0003387	4.0	3.0
1031_0003390	3.0	3.0
1031_0003408	3.0	3.0
1061_0120279	1.0	2.0
1061_0120281	1.0	2.0
1061_0120283	1.0	1.0
1061_0120291	1.0	1.0
1061_0120296	2.0	2.0
1061_0120307	3.0	3.0
1061_0120317	2.0	3.0
1061_0120323	1.0	2.0
1061_0120329	2.0	3.0
1061_0120330	3.0	3.0
1061_0120332	1.0	1.0
1061_0120351	2.0	3.0
1061_0120353	1.0	1.0
1061_0120354	2.0	1.0
1061_0120355	1.0	1.0
1061_0120357	3.0	3.0
1061_0120371	3.0	3.0
1061_0120389	2.0	2.0
1061_0120390	3.0	3.0
1061_0120391	1.0	1.0
1061_0120414	3.0	3.0
1061_0120428	2.0	3.0
1061_0120441	2.0	2.0
1061_0120443	0.0	1.0
1061_0120450	1.0	2.0
1061_0120453	2.0	3.0
1061_0120456	1.0	3.0
1061_0120460	2.0	2.0
1061_0120483	2.0	3.0
1061_0120484	2.0	3.0
1061_0120490	3.0	3.0
1061_0120496	2.0	2.0
1061_0120499	2.0	3.0
1061_0120858	2.0	2.0
1061_0120886	2.0	2.0
1061_0120889	1.0	1.0
1061_1029116	1.0	2.0
1061_1202912	3.0	2.0
1061_1202915	1.0	2.0
1071_0020001	2.0	1.0
1071_0024685	2.0	2.0
1071_0024694	2.0	2.0
1071_0024701	2.0	2.0
1071_0024702	2.0	2.0
1071_0024709	3.0	2.0
1071_0024716	1.0	1.0
1071_0024759	0.0	1.0
1071_0024762	1.0	1.0
1071_0024768	1.0	1.0
1071_0024799	3.0	2.0
1071_0024820	0.0	1.0
1071_0024823	1.0	1.0
1071_0024825	0.0	1.0
1071_0024834	2.0	2.0
1071_0024838	1.0	0.0
1071_0024841	1.0	1.0
1071_0024850	1.0	1.0
1071_0024852	0.0	0.0
1071_0024859	1.0	2.0
1071_0024866	2.0	2.0
1071_0024871	1.0	1.0
1071_0024878	2.0	2.0
1071_0242021	1.0	1.0
1071_0242041	1.0	1.0
1071_0242072	0.0	1.0
1071_0242073	1.0	1.0
1071_0243502	1.0	1.0
1071_0243581	1.0	1.0
1071_0243591	1.0	1.0
1071_0243622	1.0	1.0
1071_0248305	0.0	1.0
1071_0248307	2.0	1.0
1071_0248326	1.0	1.0
1071_0248328	0.0	1.0
1071_0248331	1.0	1.0
1071_0248335	1.0	1.0
1091_0000003	2.0	1.0
1091_0000004	1.0	1.0
1091_0000015	2.0	2.0
1091_0000017	3.0	2.0
1091_0000018	3.0	2.0
1091_0000020	1.0	1.0
1091_0000022	2.0	2.0
1091_0000024	3.0	1.0
1091_0000027	1.0	1.0
1091_0000028	1.0	1.0
1091_0000029	3.0	2.0
1091_0000030	1.0	2.0
1091_0000032	1.0	2.0
1091_0000035	2.0	1.0
1091_0000036	1.0	2.0
1091_0000041	0.0	1.0
1091_0000043	1.0	1.0
1091_0000049	1.0	1.0
1091_0000053	1.0	1.0
1091_0000055	2.0	2.0
1091_0000057	2.0	1.0
1091_0000058	2.0	2.0
1091_0000060	3.0	2.0
1091_0000063	1.0	1.0
1091_0000064	1.0	1.0
1091_0000071	2.0	2.0
1091_0000075	1.0	2.0
1091_0000086	1.0	1.0
1091_0000087	2.0	2.0
1091_0000102	2.0	1.0
1091_0000146	0.0	1.0
1091_0000148	1.0	1.0
1091_0000153	1.0	2.0
1091_0000170	2.0	2.0
1091_0000191	1.0	2.0
1091_0000195	1.0	1.0
1091_0000199	2.0	2.0
1091_0000201	2.0	2.0
1091_0000203	1.0	2.0
1091_0000205	1.0	2.0
1091_0000207	2.0	2.0
1091_0000208	1.0	2.0
1091_0000217	2.0	2.0
1091_0000221	2.0	1.0
1091_0000228	1.0	1.0
1091_0000237	1.0	2.0
1091_0000247	2.0	2.0
1091_0000262	2.0	2.0
1091_0000265	3.0	2.0
1091_0000269	1.0	2.0
1091_0000270	3.0	1.0
1091_0000272	1.0	2.0
1091_0000276	2.0	1.0
0604	1.0	2.0
0610	1.0	2.0
0619	2.0	1.0
0620	1.0	2.0
0632	1.0	1.0
0638	1.0	2.0
0640	2.0	2.0
0644	1.0	1.0
0722	2.0	2.0
0723	2.0	2.0
0724	2.0	2.0
0817	1.0	1.0
0821	2.0	1.0
0822	1.0	2.0
0828	1.0	2.0
0903	1.0	2.0
0912	2.0	2.0
0917	2.0	2.0
0927	2.0	2.0
0929	0.0	1.0
1002	1.0	1.0
1015	1.0	1.0
1016	1.0	1.0
1113	1.0	1.0
1115	1.0	1.0
KYJ0611006B	0.0	0.0
LIB0611004B	0.0	0.0
MOS0611012	0.0	0.0
PAR1011009B	0.0	0.0
PAR1011016	0.0	0.0
PAR1011017	0.0	0.0
PAR1011018	0.0	0.0
PHA0111001B	0.0	0.0
PHA0111002B	0.0	0.0
PHA0111011	0.0	0.0
PHA0111015	0.0	0.0
PHA0111018	0.0	0.0
PHA0112002A	1.0	1.0
PHA0112009B	0.0	0.0
PHA0209013	0.0	0.0
PHA0209026	0.0	0.0
PHA0209028	0.0	0.0
PHA0210001	0.0	0.0
PHA0411008A	0.0	1.0
PHA0411011A	1.0	1.0
PHA0411036	0.0	0.0
PHA0411045	0.0	0.0
PHA0411051	0.0	0.0
PHA0411053	0.0	0.0
PHA0411058	0.0	0.0
PHA0509019	0.0	0.0
PHA0509021	0.0	0.0
PHA0509033	0.0	0.0
PHA0509043	0.0	0.0
PHA0509044	0.0	0.0
PHA0510003A	0.0	1.0
PHA0510004B	0.0	0.0
PHA0510029	0.0	0.0
PHA0510050	0.0	0.0
PHA0610007A	0.0	0.0
PHA0610015	0.0	0.0
PHA0610018	0.0	0.0
PHA0610025	0.0	0.0
PHA0610026	0.0	0.0
PHA0710013	0.0	0.0
PHA0710016	0.0	0.0
PHA0710019	0.0	0.0
PHA0809010	0.0	0.0
PHA0810002	0.0	0.0
PHA0811017	0.0	0.0
PHA1109023	0.0	0.0
PHA1109025	0.0	0.0
PHA1109026	0.0	0.0
PHA1109027	0.0	0.0
PHA1110003A	1.0	1.0
PHA1110013	0.0	0.0
PHA1110014	0.0	0.0
PHA1110015	0.0	0.0
PHA1110021	0.0	0.0
PHA1111006B	0.0	0.0
VAR0909004	0.0	0.0
VAR0909007	0.0	0.0
VAR0910010	0.0	0.0
1325_1001015	2.0	2.0
1325_1001016	1.0	2.0
1325_1001018	2.0	2.0
1325_1001023	2.0	2.0
1325_1001029	2.0	2.0
1325_1001032	2.0	2.0
1325_1001035	3.0	3.0
1325_1001037	2.0	1.0
1325_1001040	2.0	2.0
1325_1001041	3.0	2.0
1325_1001043	2.0	2.0
1325_1001048	2.0	2.0
1325_1001051	1.0	2.0
1325_1001053	2.0	1.0
1325_1001054	3.0	2.0
1325_1001055	2.0	2.0
1325_1001057	2.0	2.0
1325_1001058	2.0	2.0
1325_1001080	2.0	2.0
1325_1001081	2.0	2.0
1325_1001090	2.0	2.0
1325_1001092	2.0	2.0
1325_1001094	2.0	2.0
1325_1001109	2.0	2.0
1325_1001119	2.0	2.0
1325_1001130	2.0	2.0
1325_1001132	2.0	2.0
1325_1001138	2.0	2.0
1325_1001139	2.0	2.0
1325_1001142	1.0	2.0
1325_1001143	2.0	3.0
1325_1001155	2.0	2.0
1325_1001156	2.0	2.0
1325_1001158	2.0	2.0
1325_1001165	1.0	2.0
1325_1001167	2.0	2.0
1325_1001170	2.0	2.0
1325_9000090	2.0	2.0
1325_9000095	2.0	2.0
1325_9000102	2.0	2.0
1325_9000186	3.0	3.0
1325_9000188	2.0	2.0
1325_9000213	3.0	2.0
1325_9000237	2.0	3.0
1325_9000314	2.0	2.0
1325_9000317	2.0	3.0
1325_9000684	2.0	3.0
1365_0100007	1.0	1.0
1365_0100019	1.0	2.0
1365_0100030	1.0	2.0
1365_0100056	1.0	2.0
1365_0100074	1.0	2.0
1365_0100099	2.0	2.0
1365_0100100	2.0	2.0
1365_0100105	3.0	2.0
1365_0100106	1.0	2.0
1365_0100120	2.0	2.0
1365_0100123	2.0	2.0
1365_0100136	2.0	2.0
1365_0100138	2.0	2.0
1365_0100139	2.0	2.0
1365_0100146	2.0	2.0
1365_0100148	1.0	2.0
1365_0100151	2.0	2.0
1365_0100165	2.0	2.0
1365_0100172	2.0	2.0
1365_0100179	2.0	2.0
1365_0100183	2.0	2.0
1365_0100186	2.0	2.0
1365_0100204	1.0	2.0
1365_0100217	2.0	2.0
1365_0100220	2.0	2.0
1365_0100221	2.0	2.0
1365_0100223	2.0	2.0
1365_0100224	2.0	2.0
1365_0100226	2.0	2.0
1365_0100251	2.0	2.0
1365_0100259	2.0	2.0
1365_0100268	2.0	2.0
1365_0100275	2.0	2.0
1365_0100276	2.0	2.0
1365_0100287	2.0	2.0
1365_0100288	2.0	2.0
1365_0100290	2.0	2.0
1365_0100447	2.0	2.0
1365_0100471	2.0	2.0
1365_0100476	2.0	2.0
1365_0100477	2.0	2.0
1365_0100478	2.0	2.0
1385_0000013	0.0	1.0
1385_0000017	1.0	1.0
1385_0000021	2.0	1.0
1385_0000043	2.0	2.0
1385_0000050	2.0	2.0
1385_0000103	1.0	1.0
1385_0000104	1.0	1.0
1385_0000126	1.0	1.0
1385_0001103	1.0	1.0
1385_0001104	1.0	0.0
1385_0001120	1.0	1.0
1385_0001122	1.0	1.0
1385_0001128	1.0	1.0
1385_0001147	1.0	1.0
1385_0001150	2.0	2.0
1385_0001151	2.0	1.0
1385_0001160	1.0	2.0
1385_0001167	1.0	1.0
1385_0001172	1.0	1.0
1385_0001175	1.0	1.0
1385_0001194	1.0	1.0
1385_0001197	1.0	1.0
1385_0001525	1.0	1.0
1385_0001526	0.0	1.0
1385_0001723	0.0	1.0
1385_0001729	1.0	1.0
1385_0001733	1.0	2.0
1385_0001734	1.0	2.0
1385_0001736	1.0	2.0
1385_0001739	0.0	1.0
1385_0001742	0.0	1.0
1385_0001756	1.0	1.0
1385_0001768	1.0	1.0
1385_0001771	1.0	1.0
1385_0001786	1.0	2.0
1385_0001789	1.0	2.0
1385_0001796	0.0	2.0
1385_0001799	1.0	2.0
1395_0000338	1.0	1.0
1395_0000359	1.0	1.0
1395_0000379	1.0	1.0
1395_0000409	2.0	1.0
1395_0000432	1.0	1.0
1395_0000447	1.0	1.0
1395_0000451	2.0	1.0
1395_0000458	1.0	1.0
1395_0000465	1.0	1.0
1395_0000516	1.0	1.0
1395_0000547	1.0	1.0
1395_0000554	2.0	1.0
1395_0000560	1.0	1.0
1395_0000572	1.0	1.0
1395_0000583	1.0	1.0
1395_0000587	0.0	1.0
1395_0000593	1.0	1.0
1395_0000597	1.0	1.0
1395_0000635	1.0	1.0
1395_0000636	1.0	1.0
1395_0001020	1.0	1.0
1395_0001034	1.0	1.0
1395_0001075	1.0	1.0
1395_0001080	2.0	1.0
1395_0001084	1.0	1.0
1395_0001093	1.0	1.0
1395_0001121	0.0	1.0
1395_0001122	1.0	1.0
1395_0001123	0.0	1.0
1395_0001161	1.0	1.0
1395_0001169	1.0	1.0
1395_0001171	1.0	1.0
Averaged weighted F1-scores 0.689409306507491
