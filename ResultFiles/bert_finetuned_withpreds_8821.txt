There are 1 GPU(s) available.
We will use the GPU: Tesla V100-SXM2-16GB
CROSSLINGUAL EXPERIMENTS
Train on DE, test on  CZ
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
144.90552995391704 65.35717405024758
LANGUAGE: IT
nan nan
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']


======== Epoch 1 / 4 ========
Training...
Elapsed time 11
Elapsed time 23
Elapsed time 34

  Average training loss: 1.07
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.31
              precision    recall  f1-score   support

         1.0       0.97      0.17      0.29       188
         2.0       0.32      0.40      0.35       165
         3.0       0.42      0.99      0.59        81

    accuracy                           0.41       434
   macro avg       0.57      0.52      0.41       434
weighted avg       0.62      0.41      0.37       434

[[ 32 142  14]
 [  1  66  98]
 [  0   1  80]]
0.3690121209774232
434 434 434



======== Epoch 2 / 4 ========
Training...
Elapsed time 11
Elapsed time 23
Elapsed time 35

  Average training loss: 0.73
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.88
              precision    recall  f1-score   support

         1.0       0.96      0.14      0.24       188
         2.0       0.22      0.18      0.20       165
         3.0       0.30      1.00      0.46        81

    accuracy                           0.32       434
   macro avg       0.49      0.44      0.30       434
weighted avg       0.56      0.32      0.27       434

[[ 26 104  58]
 [  1  30 134]
 [  0   0  81]]
0.2664698892742086
434 434 434



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.56
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.91
              precision    recall  f1-score   support

         1.0       1.00      0.14      0.25       188
         2.0       0.29      0.35      0.32       165
         3.0       0.38      1.00      0.55        81

    accuracy                           0.38       434
   macro avg       0.56      0.50      0.37       434
weighted avg       0.62      0.38      0.33       434

[[ 27 137  24]
 [  0  57 108]
 [  0   0  81]]
0.33236590066330585
434 434 434



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.42
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.74
              precision    recall  f1-score   support

         1.0       1.00      0.16      0.28       188
         2.0       0.31      0.40      0.35       165
         3.0       0.42      0.99      0.59        81

    accuracy                           0.41       434
   macro avg       0.58      0.52      0.40       434
weighted avg       0.63      0.41      0.36       434

[[ 30 145  13]
 [  0  66  99]
 [  0   1  80]]
0.36172241941868016
434 434 434
Filename	True Label	Prediction
0601	1.0	2.0
0602	1.0	3.0
0603	2.0	2.0
0604	2.0	2.0
0605	2.0	2.0
0606	2.0	2.0
0607	2.0	2.0
0608	1.0	2.0
0609	1.0	2.0
0610	1.0	3.0
0611	2.0	2.0
0612	1.0	2.0
0613	1.0	2.0
0614	2.0	3.0
0615	1.0	2.0
0616	1.0	2.0
0617	1.0	2.0
0618	1.0	2.0
0619	2.0	2.0
0620	1.0	2.0
0621	2.0	2.0
0622	1.0	2.0
0623	1.0	2.0
0624	2.0	2.0
0625	1.0	2.0
0626	2.0	2.0
0627	2.0	2.0
0628	2.0	2.0
0629	2.0	2.0
0630	1.0	1.0
0631	2.0	2.0
0632	1.0	2.0
0633	2.0	2.0
0634	2.0	2.0
0635	1.0	1.0
0636	2.0	2.0
0637	2.0	2.0
0638	1.0	2.0
0639	1.0	2.0
0640	2.0	2.0
0641	1.0	2.0
0642	1.0	2.0
0643	2.0	2.0
0644	1.0	2.0
0645	2.0	2.0
0714	2.0	2.0
0715	2.0	2.0
0716	2.0	2.0
0717	1.0	2.0
0718	1.0	2.0
0719	2.0	2.0
0720	1.0	2.0
0721	2.0	2.0
0722	2.0	2.0
0723	1.0	2.0
0724	2.0	3.0
0725	1.0	2.0
0801	1.0	2.0
0802	1.0	2.0
0803	1.0	2.0
0804	1.0	2.0
0805	2.0	2.0
0806	1.0	2.0
0807	2.0	2.0
0808	1.0	2.0
0809	2.0	2.0
0810	1.0	2.0
0811	2.0	2.0
0812	1.0	2.0
0813	1.0	2.0
0814	1.0	2.0
0815	2.0	2.0
0816	2.0	2.0
0817	1.0	2.0
0818	1.0	2.0
0819	3.0	2.0
0820	1.0	1.0
0821	2.0	2.0
0822	1.0	2.0
0823	2.0	2.0
0824	1.0	2.0
0825	1.0	2.0
0826	1.0	2.0
0827	1.0	2.0
0828	2.0	2.0
0829	1.0	2.0
0901	2.0	2.0
0902	2.0	2.0
0903	1.0	2.0
0904	1.0	2.0
0905	2.0	2.0
0906	2.0	2.0
0907	2.0	2.0
0910	1.0	2.0
0911	1.0	2.0
0912	2.0	2.0
0913	2.0	2.0
0914	1.0	2.0
0915	2.0	2.0
0916	1.0	2.0
0917	1.0	2.0
0918	1.0	2.0
0919	1.0	2.0
0920	2.0	2.0
0921	1.0	2.0
0922	1.0	2.0
0923	2.0	2.0
0924	1.0	2.0
0925	2.0	2.0
0926	2.0	2.0
0927	2.0	2.0
0928	2.0	2.0
0929	1.0	2.0
0930	1.0	2.0
1001	1.0	2.0
1002	2.0	2.0
1003	1.0	2.0
1004	1.0	2.0
1005	1.0	2.0
1006	2.0	2.0
1007	2.0	2.0
1008	1.0	2.0
1009	2.0	2.0
1010	1.0	2.0
1014	2.0	2.0
1015	1.0	2.0
1016	1.0	2.0
1017	1.0	2.0
1018	1.0	2.0
1019	1.0	2.0
1020	2.0	2.0
1021	1.0	2.0
1022	2.0	2.0
1023	1.0	2.0
1111	1.0	2.0
1112	1.0	3.0
1113	1.0	3.0
1114	2.0	2.0
1115	1.0	3.0
1116	1.0	2.0
1117	1.0	2.0
9999	1.0	2.0
BER0609003	2.0	3.0
BER0611003	2.0	3.0
BER0611005	2.0	3.0
BER0611006	2.0	3.0
BER0611007	2.0	3.0
KYJ0611003A	1.0	2.0
KYJ0611004A	1.0	2.0
KYJ0611005A	1.0	1.0
KYJ0611005B	1.0	2.0
KYJ0611006A	1.0	2.0
KYJ0611006B	1.0	2.0
KYJ0611009A	1.0	2.0
KYJ0611009B	1.0	2.0
LIB0611001A	1.0	2.0
LIB0611001B	1.0	2.0
LIB0611002A	1.0	2.0
LIB0611002B	1.0	2.0
LIB0611003A	1.0	2.0
LIB0611004A	1.0	1.0
LIB0611004B	1.0	2.0
LIB0611011	1.0	3.0
LON0610002A	1.0	1.0
LON0610002B	1.0	2.0
LON0611002A	1.0	1.0
LON0611002B	1.0	2.0
LON0611003	3.0	3.0
LON0611004A	1.0	1.0
LON0611004B	1.0	2.0
MOS0509001	2.0	3.0
MOS0509004	2.0	2.0
MOS0611012	2.0	3.0
MOS0611013	2.0	3.0
MOS0611014	1.0	3.0
MOS0611015	2.0	3.0
PAR1011008A	1.0	2.0
PAR1011009A	1.0	2.0
PAR1011009B	1.0	2.0
PAR1011013	2.0	3.0
PAR1011014	2.0	3.0
PAR1011015	2.0	3.0
PAR1011016	3.0	3.0
PAR1011017	3.0	3.0
PAR1011018	3.0	3.0
PHA0111001A	1.0	1.0
PHA0111001B	1.0	2.0
PHA0111002A	2.0	2.0
PHA0111002B	2.0	2.0
PHA0111003A	1.0	1.0
PHA0111003B	1.0	2.0
PHA0111004A	1.0	1.0
PHA0111004B	1.0	2.0
PHA0111005A	1.0	2.0
PHA0111005B	1.0	2.0
PHA0111010	3.0	3.0
PHA0111011	2.0	3.0
PHA0111012	2.0	3.0
PHA0111014	1.0	3.0
PHA0111015	3.0	3.0
PHA0111016	3.0	3.0
PHA0111018	2.0	3.0
PHA0112002A	1.0	1.0
PHA0112002B	1.0	3.0
PHA0112003A	1.0	1.0
PHA0112003B	1.0	2.0
PHA0112006A	2.0	2.0
PHA0112006B	2.0	3.0
PHA0112007A	1.0	1.0
PHA0112007B	1.0	2.0
PHA0112009A	2.0	2.0
PHA0112009B	2.0	2.0
PHA0112012A	1.0	2.0
PHA0112012B	1.0	2.0
PHA0209001	1.0	3.0
PHA0209008	1.0	2.0
PHA0209013	1.0	2.0
PHA0209024	2.0	3.0
PHA0209026	3.0	3.0
PHA0209028	2.0	3.0
PHA0209031	3.0	3.0
PHA0209034	2.0	3.0
PHA0209038	3.0	3.0
PHA0209039	2.0	3.0
PHA0210001	1.0	2.0
PHA0210004	1.0	2.0
PHA0210007	1.0	2.0
PHA0210008	1.0	2.0
PHA0411008A	1.0	1.0
PHA0411008B	1.0	2.0
PHA0411009A	1.0	2.0
PHA0411009B	1.0	2.0
PHA0411010A	1.0	1.0
PHA0411010B	1.0	2.0
PHA0411011A	1.0	1.0
PHA0411011B	1.0	2.0
PHA0411012A	1.0	1.0
PHA0411012B	1.0	2.0
PHA0411027	2.0	3.0
PHA0411028	2.0	3.0
PHA0411029	2.0	3.0
PHA0411030	3.0	3.0
PHA0411031	3.0	3.0
PHA0411032	2.0	3.0
PHA0411033	2.0	3.0
PHA0411034	1.0	2.0
PHA0411035	3.0	3.0
PHA0411036	3.0	3.0
PHA0411037	2.0	3.0
PHA0411038	3.0	3.0
PHA0411039	2.0	3.0
PHA0411041	3.0	3.0
PHA0411042	2.0	3.0
PHA0411043	2.0	3.0
PHA0411044	3.0	3.0
PHA0411045	2.0	3.0
PHA0411047	2.0	3.0
PHA0411051	3.0	3.0
PHA0411053	3.0	3.0
PHA0411054	3.0	3.0
PHA0411055	3.0	3.0
PHA0411056	3.0	3.0
PHA0411058	3.0	3.0
PHA0411059	3.0	3.0
PHA0411060	2.0	3.0
PHA0411061	3.0	3.0
PHA0411062	2.0	3.0
PHA0509002	1.0	1.0
PHA0509007	1.0	2.0
PHA0509013	1.0	2.0
PHA0509015	3.0	3.0
PHA0509017	2.0	3.0
PHA0509018	3.0	3.0
PHA0509019	2.0	3.0
PHA0509020	3.0	3.0
PHA0509021	2.0	3.0
PHA0509022	3.0	3.0
PHA0509024	2.0	3.0
PHA0509025	3.0	3.0
PHA0509026	3.0	3.0
PHA0509027	2.0	3.0
PHA0509028	3.0	3.0
PHA0509030	3.0	3.0
PHA0509031	2.0	3.0
PHA0509032	2.0	3.0
PHA0509033	1.0	3.0
PHA0509034	2.0	3.0
PHA0509035	2.0	3.0
PHA0509036	3.0	3.0
PHA0509037	3.0	3.0
PHA0509038	2.0	3.0
PHA0509039	3.0	3.0
PHA0509040	2.0	3.0
PHA0509041	2.0	3.0
PHA0509042	3.0	3.0
PHA0509043	3.0	3.0
PHA0509044	2.0	3.0
PHA0509045	2.0	3.0
PHA0510002A	1.0	2.0
PHA0510002B	1.0	2.0
PHA0510003A	1.0	2.0
PHA0510003B	1.0	2.0
PHA0510004A	1.0	2.0
PHA0510004B	1.0	2.0
PHA0510010A	1.0	1.0
PHA0510010B	1.0	1.0
PHA0510013A	1.0	2.0
PHA0510013B	1.0	2.0
PHA0510023	3.0	3.0
PHA0510027	2.0	3.0
PHA0510029	3.0	3.0
PHA0510030	2.0	3.0
PHA0510031	2.0	3.0
PHA0510032	3.0	3.0
PHA0510034	3.0	3.0
PHA0510035	3.0	3.0
PHA0510036	3.0	3.0
PHA0510037	2.0	3.0
PHA0510038	3.0	3.0
PHA0510039	2.0	3.0
PHA0510040	2.0	3.0
PHA0510046	2.0	3.0
PHA0510047	2.0	3.0
PHA0510048	2.0	3.0
PHA0510049	2.0	3.0
PHA0510050	2.0	3.0
PHA0610005A	1.0	1.0
PHA0610005B	1.0	1.0
PHA0610006A	1.0	2.0
PHA0610006B	1.0	2.0
PHA0610007A	1.0	1.0
PHA0610007B	1.0	3.0
PHA0610015	2.0	3.0
PHA0610016	3.0	3.0
PHA0610017	3.0	3.0
PHA0610018	3.0	3.0
PHA0610019A	1.0	2.0
PHA0610019B	1.0	3.0
PHA0610025	3.0	3.0
PHA0610026	3.0	3.0
PHA0709008	3.0	3.0
PHA0710009	2.0	3.0
PHA0710010	2.0	3.0
PHA0710011	3.0	3.0
PHA0710012	3.0	3.0
PHA0710013	3.0	3.0
PHA0710014	3.0	3.0
PHA0710015	3.0	3.0
PHA0710016	3.0	3.0
PHA0710017	3.0	3.0
PHA0710018	3.0	3.0
PHA0710019	3.0	3.0
PHA0710021	3.0	3.0
PHA0809009	2.0	3.0
PHA0809010	2.0	3.0
PHA0810001	3.0	3.0
PHA0810002	2.0	3.0
PHA0810003	2.0	3.0
PHA0810004	2.0	3.0
PHA0810006	2.0	3.0
PHA0810008	2.0	3.0
PHA0810009	3.0	3.0
PHA0810010	2.0	3.0
PHA0810011	2.0	3.0
PHA0810012	2.0	3.0
PHA0810015	3.0	3.0
PHA0811010	2.0	3.0
PHA0811012	3.0	3.0
PHA0811013	3.0	3.0
PHA0811014	2.0	3.0
PHA0811016	2.0	3.0
PHA0811017	3.0	3.0
PHA0811019	3.0	3.0
PHA0811020	2.0	3.0
PHA1109001	1.0	2.0
PHA1109002	3.0	3.0
PHA1109003	2.0	2.0
PHA1109004	3.0	3.0
PHA1109005	2.0	3.0
PHA1109006	2.0	3.0
PHA1109007	2.0	3.0
PHA1109008	1.0	2.0
PHA1109023	1.0	2.0
PHA1109024	3.0	3.0
PHA1109025	1.0	2.0
PHA1109026	3.0	3.0
PHA1109027	3.0	3.0
PHA1109028	2.0	3.0
PHA1110001A	1.0	2.0
PHA1110001B	1.0	2.0
PHA1110002A	2.0	2.0
PHA1110002B	2.0	3.0
PHA1110003A	1.0	2.0
PHA1110003B	1.0	2.0
PHA1110004A	1.0	1.0
PHA1110013	2.0	3.0
PHA1110014	2.0	3.0
PHA1110015	3.0	3.0
PHA1110016	2.0	3.0
PHA1110017	2.0	3.0
PHA1110019	2.0	3.0
PHA1110021	2.0	3.0
PHA1110022	3.0	3.0
PHA1111001A	1.0	2.0
PHA1111001B	1.0	2.0
PHA1111002A	1.0	1.0
PHA1111002B	1.0	1.0
PHA1111003A	1.0	1.0
PHA1111003B	1.0	2.0
PHA1111004A	1.0	1.0
PHA1111004B	1.0	2.0
PHA1111006A	1.0	1.0
PHA1111006B	1.0	2.0
PHA1111008A	1.0	2.0
PHA1111008B	1.0	2.0
PHA1111009A	1.0	2.0
ST071122B	1.0	2.0
TI071122B	1.0	2.0
VAR0209036	2.0	3.0
VAR0909003	2.0	3.0
VAR0909004	2.0	3.0
VAR0909005	2.0	3.0
VAR0909006	3.0	3.0
VAR0909007	2.0	3.0
VAR0909008	2.0	3.0
VAR0909009	3.0	3.0
VAR0909010	2.0	3.0
VAR0910004	3.0	3.0
VAR0910005	2.0	3.0
VAR0910006	2.0	3.0
VAR0910007	2.0	3.0
VAR0910009	3.0	3.0
VAR0910010	3.0	3.0
VAR0910011	2.0	3.0
Language = CZ, Weighted F1-score = 0.36172241941868016, Dimension = OverallCEFRrating

Train on DE, test on  IT
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
nan nan
LANGUAGE: IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']


======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 1.01
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 0.66
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        28
         1.0       0.71      0.95      0.81       380
         2.0       0.94      0.69      0.79       392
         3.0       0.00      0.00      0.00         0

    accuracy                           0.79       800
   macro avg       0.41      0.41      0.40       800
weighted avg       0.80      0.79      0.77       800

[[  0  28   0   0]
 [  0 362  18   0]
 [  0 120 269   3]
 [  0   0   0   0]]
0.7746519170624347
800 800 800



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.72
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 0.72
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        28
         1.0       0.81      0.78      0.80       380
         2.0       0.76      0.63      0.69       392
         3.0       0.00      0.00      0.00         0

    accuracy                           0.68       800
   macro avg       0.39      0.35      0.37       800
weighted avg       0.76      0.68      0.72       800

[[  0  28   0   0]
 [  0 297  79   4]
 [  0  40 247 105]
 [  0   0   0   0]]
0.7158557514348207
800 800 800



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.57
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 0.83
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        28
         1.0       0.80      0.52      0.63       380
         2.0       0.60      0.71      0.65       392
         3.0       0.00      0.00      0.00         0

    accuracy                           0.59       800
   macro avg       0.35      0.31      0.32       800
weighted avg       0.68      0.59      0.62       800

[[  0  26   2   0]
 [  0 198 179   3]
 [  0  23 277  92]
 [  0   0   0   0]]
0.6193647058823529
800 800 800



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.43
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 0.89
              precision    recall  f1-score   support

         0.0       0.30      0.11      0.16        28
         1.0       0.81      0.49      0.61       380
         2.0       0.59      0.67      0.63       392
         3.0       0.00      0.00      0.00         0

    accuracy                           0.56       800
   macro avg       0.42      0.32      0.35       800
weighted avg       0.68      0.56      0.60       800

[[  3  23   2   0]
 [  7 188 180   5]
 [  0  21 261 110]
 [  0   0   0   0]]
0.6036797344423641
800 800 800
Filename	True Label	Prediction
1325_1001008	2.0	2.0
1325_1001009	2.0	2.0
1325_1001010	2.0	3.0
1325_1001011	2.0	3.0
1325_1001012	2.0	3.0
1325_1001013	2.0	2.0
1325_1001014	2.0	2.0
1325_1001015	2.0	2.0
1325_1001016	2.0	2.0
1325_1001017	2.0	2.0
1325_1001018	2.0	3.0
1325_1001019	2.0	3.0
1325_1001020	2.0	2.0
1325_1001021	2.0	3.0
1325_1001022	2.0	2.0
1325_1001023	2.0	2.0
1325_1001024	2.0	2.0
1325_1001025	2.0	2.0
1325_1001027	2.0	2.0
1325_1001028	2.0	2.0
1325_1001029	2.0	3.0
1325_1001032	2.0	2.0
1325_1001033	2.0	3.0
1325_1001035	2.0	3.0
1325_1001036	2.0	3.0
1325_1001037	2.0	2.0
1325_1001039	2.0	3.0
1325_1001040	2.0	2.0
1325_1001041	2.0	2.0
1325_1001042	2.0	3.0
1325_1001043	2.0	2.0
1325_1001044	2.0	2.0
1325_1001045	2.0	2.0
1325_1001046	2.0	3.0
1325_1001047	2.0	2.0
1325_1001048	2.0	3.0
1325_1001050	2.0	3.0
1325_1001051	2.0	2.0
1325_1001052	2.0	2.0
1325_1001053	1.0	2.0
1325_1001054	2.0	2.0
1325_1001055	2.0	3.0
1325_1001056	2.0	3.0
1325_1001057	2.0	2.0
1325_1001058	2.0	2.0
1325_1001059	2.0	2.0
1325_1001062	2.0	3.0
1325_1001063	2.0	2.0
1325_1001075	1.0	2.0
1325_1001076	2.0	3.0
1325_1001077	2.0	2.0
1325_1001078	2.0	3.0
1325_1001079	2.0	3.0
1325_1001080	2.0	3.0
1325_1001081	2.0	3.0
1325_1001082	2.0	2.0
1325_1001083	2.0	2.0
1325_1001084	2.0	2.0
1325_1001085	2.0	2.0
1325_1001086	2.0	2.0
1325_1001087	2.0	2.0
1325_1001088	2.0	2.0
1325_1001089	2.0	2.0
1325_1001090	2.0	2.0
1325_1001091	2.0	3.0
1325_1001092	2.0	2.0
1325_1001093	2.0	3.0
1325_1001094	2.0	2.0
1325_1001095	2.0	2.0
1325_1001096	2.0	2.0
1325_1001097	1.0	2.0
1325_1001098	2.0	3.0
1325_1001099	2.0	3.0
1325_1001100	2.0	3.0
1325_1001101	2.0	3.0
1325_1001107	2.0	3.0
1325_1001108	2.0	3.0
1325_1001109	2.0	2.0
1325_1001110	2.0	3.0
1325_1001111	2.0	3.0
1325_1001113	2.0	3.0
1325_1001119	2.0	3.0
1325_1001120	2.0	3.0
1325_1001121	2.0	3.0
1325_1001122	2.0	2.0
1325_1001123	2.0	3.0
1325_1001124	2.0	2.0
1325_1001125	2.0	2.0
1325_1001126	2.0	2.0
1325_1001127	2.0	2.0
1325_1001128	2.0	2.0
1325_1001129	1.0	2.0
1325_1001130	2.0	2.0
1325_1001131	2.0	2.0
1325_1001132	2.0	3.0
1325_1001133	2.0	3.0
1325_1001134	2.0	3.0
1325_1001135	2.0	3.0
1325_1001136	2.0	2.0
1325_1001138	2.0	3.0
1325_1001139	2.0	3.0
1325_1001141	1.0	3.0
1325_1001142	2.0	3.0
1325_1001143	2.0	3.0
1325_1001144	2.0	2.0
1325_1001152	2.0	2.0
1325_1001153	2.0	2.0
1325_1001154	2.0	3.0
1325_1001155	2.0	3.0
1325_1001156	2.0	2.0
1325_1001157	2.0	3.0
1325_1001158	2.0	3.0
1325_1001159	2.0	3.0
1325_1001160	2.0	3.0
1325_1001161	2.0	3.0
1325_1001162	2.0	2.0
1325_1001163	2.0	3.0
1325_1001164	2.0	2.0
1325_1001165	2.0	2.0
1325_1001166	2.0	3.0
1325_1001167	2.0	3.0
1325_1001168	2.0	3.0
1325_1001169	2.0	3.0
1325_1001170	2.0	2.0
1325_9000059	2.0	2.0
1325_9000087	2.0	2.0
1325_9000088	2.0	2.0
1325_9000089	2.0	3.0
1325_9000090	2.0	3.0
1325_9000095	2.0	3.0
1325_9000099	2.0	2.0
1325_9000102	2.0	2.0
1325_9000104	2.0	2.0
1325_9000105	2.0	2.0
1325_9000106	2.0	2.0
1325_9000107	2.0	3.0
1325_9000136	2.0	3.0
1325_9000137	2.0	3.0
1325_9000138	2.0	3.0
1325_9000139	2.0	2.0
1325_9000140	2.0	3.0
1325_9000143	2.0	3.0
1325_9000144	2.0	3.0
1325_9000152	2.0	3.0
1325_9000185	2.0	2.0
1325_9000186	2.0	3.0
1325_9000187	2.0	3.0
1325_9000188	2.0	3.0
1325_9000209	2.0	2.0
1325_9000210	1.0	3.0
1325_9000211	2.0	3.0
1325_9000213	2.0	2.0
1325_9000214	2.0	2.0
1325_9000215	2.0	3.0
1325_9000237	2.0	3.0
1325_9000239	2.0	3.0
1325_9000240	2.0	2.0
1325_9000241	2.0	3.0
1325_9000278	2.0	3.0
1325_9000279	2.0	3.0
1325_9000296	2.0	3.0
1325_9000302	2.0	2.0
1325_9000303	2.0	2.0
1325_9000304	2.0	2.0
1325_9000314	2.0	2.0
1325_9000315	1.0	2.0
1325_9000316	2.0	2.0
1325_9000317	2.0	3.0
1325_9000318	2.0	2.0
1325_9000319	2.0	2.0
1325_9000320	2.0	2.0
1325_9000321	2.0	3.0
1325_9000322	2.0	3.0
1325_9000323	2.0	2.0
1325_9000503	2.0	3.0
1325_9000504	2.0	3.0
1325_9000505	2.0	3.0
1325_9000533	2.0	2.0
1325_9000534	2.0	3.0
1325_9000536	2.0	3.0
1325_9000554	2.0	2.0
1325_9000601	2.0	3.0
1325_9000602	2.0	3.0
1325_9000611	2.0	3.0
1325_9000612	1.0	2.0
1325_9000674	2.0	3.0
1325_9000675	2.0	2.0
1325_9000676	2.0	2.0
1325_9000677	2.0	3.0
1325_9000678	2.0	3.0
1325_9000684	2.0	3.0
1325_9000685	2.0	3.0
1325_9000686	2.0	3.0
1325_9000700	2.0	3.0
1325_9000750	2.0	1.0
1365_0100002	2.0	2.0
1365_0100003	1.0	1.0
1365_0100004	1.0	2.0
1365_0100005	1.0	1.0
1365_0100006	2.0	2.0
1365_0100007	1.0	1.0
1365_0100008	1.0	2.0
1365_0100009	1.0	1.0
1365_0100010	1.0	1.0
1365_0100011	2.0	2.0
1365_0100012	2.0	1.0
1365_0100013	2.0	2.0
1365_0100014	2.0	1.0
1365_0100015	1.0	1.0
1365_0100016	2.0	2.0
1365_0100017	2.0	2.0
1365_0100018	1.0	2.0
1365_0100019	1.0	1.0
1365_0100020	2.0	2.0
1365_0100021	2.0	1.0
1365_0100022	2.0	2.0
1365_0100023	1.0	2.0
1365_0100024	1.0	2.0
1365_0100026	1.0	1.0
1365_0100027	2.0	2.0
1365_0100028	2.0	2.0
1365_0100029	1.0	1.0
1365_0100030	1.0	2.0
1365_0100031	2.0	1.0
1365_0100051	1.0	2.0
1365_0100056	2.0	2.0
1365_0100057	2.0	3.0
1365_0100058	2.0	2.0
1365_0100061	2.0	2.0
1365_0100063	2.0	2.0
1365_0100064	2.0	2.0
1365_0100065	1.0	1.0
1365_0100066	1.0	2.0
1365_0100067	1.0	2.0
1365_0100069	2.0	2.0
1365_0100070	2.0	2.0
1365_0100071	2.0	2.0
1365_0100072	2.0	2.0
1365_0100073	2.0	2.0
1365_0100074	1.0	2.0
1365_0100079	2.0	2.0
1365_0100080	2.0	2.0
1365_0100092	2.0	2.0
1365_0100093	2.0	2.0
1365_0100094	2.0	2.0
1365_0100095	2.0	1.0
1365_0100096	2.0	2.0
1365_0100097	2.0	2.0
1365_0100098	1.0	2.0
1365_0100099	1.0	2.0
1365_0100100	2.0	3.0
1365_0100101	2.0	2.0
1365_0100102	2.0	2.0
1365_0100103	2.0	2.0
1365_0100104	1.0	3.0
1365_0100105	2.0	2.0
1365_0100106	1.0	2.0
1365_0100107	2.0	3.0
1365_0100116	2.0	2.0
1365_0100117	2.0	2.0
1365_0100118	2.0	2.0
1365_0100119	2.0	3.0
1365_0100120	2.0	3.0
1365_0100121	2.0	2.0
1365_0100123	2.0	2.0
1365_0100125	2.0	2.0
1365_0100133	2.0	2.0
1365_0100134	2.0	2.0
1365_0100135	2.0	2.0
1365_0100136	2.0	2.0
1365_0100137	2.0	2.0
1365_0100138	2.0	1.0
1365_0100139	2.0	1.0
1365_0100145	2.0	2.0
1365_0100146	2.0	2.0
1365_0100147	2.0	2.0
1365_0100148	2.0	2.0
1365_0100151	1.0	1.0
1365_0100162	2.0	2.0
1365_0100163	2.0	3.0
1365_0100164	2.0	3.0
1365_0100165	2.0	2.0
1365_0100166	1.0	2.0
1365_0100167	1.0	2.0
1365_0100168	2.0	2.0
1365_0100169	2.0	2.0
1365_0100170	1.0	2.0
1365_0100171	1.0	2.0
1365_0100172	2.0	2.0
1365_0100173	2.0	2.0
1365_0100174	1.0	2.0
1365_0100175	2.0	2.0
1365_0100176	2.0	2.0
1365_0100177	2.0	2.0
1365_0100178	2.0	2.0
1365_0100179	2.0	2.0
1365_0100180	1.0	2.0
1365_0100181	1.0	2.0
1365_0100182	2.0	2.0
1365_0100183	2.0	2.0
1365_0100184	2.0	2.0
1365_0100185	1.0	2.0
1365_0100186	2.0	2.0
1365_0100187	2.0	2.0
1365_0100188	2.0	2.0
1365_0100190	2.0	2.0
1365_0100191	1.0	2.0
1365_0100192	2.0	3.0
1365_0100194	2.0	2.0
1365_0100195	1.0	2.0
1365_0100196	1.0	2.0
1365_0100198	1.0	1.0
1365_0100199	2.0	2.0
1365_0100200	2.0	2.0
1365_0100201	2.0	2.0
1365_0100202	1.0	2.0
1365_0100203	2.0	2.0
1365_0100204	2.0	2.0
1365_0100205	2.0	1.0
1365_0100211	2.0	2.0
1365_0100212	2.0	3.0
1365_0100213	2.0	1.0
1365_0100215	2.0	2.0
1365_0100217	2.0	2.0
1365_0100218	2.0	2.0
1365_0100219	2.0	2.0
1365_0100220	2.0	2.0
1365_0100221	2.0	2.0
1365_0100222	2.0	2.0
1365_0100223	2.0	2.0
1365_0100224	2.0	3.0
1365_0100225	2.0	2.0
1365_0100226	2.0	2.0
1365_0100227	2.0	2.0
1365_0100228	1.0	2.0
1365_0100229	2.0	2.0
1365_0100230	2.0	2.0
1365_0100231	2.0	2.0
1365_0100232	2.0	2.0
1365_0100233	2.0	2.0
1365_0100251	2.0	3.0
1365_0100252	2.0	2.0
1365_0100253	1.0	2.0
1365_0100255	1.0	2.0
1365_0100256	2.0	1.0
1365_0100257	2.0	2.0
1365_0100258	2.0	2.0
1365_0100259	2.0	2.0
1365_0100260	2.0	2.0
1365_0100261	2.0	2.0
1365_0100262	2.0	2.0
1365_0100263	2.0	2.0
1365_0100265	2.0	2.0
1365_0100266	2.0	2.0
1365_0100267	2.0	2.0
1365_0100268	1.0	2.0
1365_0100269	2.0	2.0
1365_0100270	2.0	2.0
1365_0100274	2.0	3.0
1365_0100275	2.0	2.0
1365_0100276	2.0	2.0
1365_0100277	2.0	2.0
1365_0100278	2.0	2.0
1365_0100279	2.0	2.0
1365_0100280	1.0	2.0
1365_0100281	2.0	2.0
1365_0100282	2.0	2.0
1365_0100285	2.0	2.0
1365_0100286	1.0	2.0
1365_0100287	2.0	2.0
1365_0100288	2.0	1.0
1365_0100289	2.0	1.0
1365_0100290	2.0	2.0
1365_0100299	2.0	2.0
1365_0100447	2.0	2.0
1365_0100448	1.0	2.0
1365_0100451	2.0	2.0
1365_0100455	2.0	2.0
1365_0100456	2.0	2.0
1365_0100457	2.0	2.0
1365_0100458	2.0	2.0
1365_0100459	2.0	2.0
1365_0100461	2.0	2.0
1365_0100469	2.0	2.0
1365_0100470	2.0	2.0
1365_0100471	1.0	3.0
1365_0100472	2.0	2.0
1365_0100473	2.0	2.0
1365_0100474	2.0	2.0
1365_0100475	2.0	2.0
1365_0100476	2.0	2.0
1365_0100477	1.0	3.0
1365_0100478	2.0	2.0
1365_0100479	2.0	2.0
1365_0100480	2.0	2.0
1365_0100481	2.0	2.0
1365_0100482	2.0	2.0
1385_0000011	0.0	1.0
1385_0000012	1.0	1.0
1385_0000013	0.0	1.0
1385_0000016	1.0	1.0
1385_0000017	1.0	1.0
1385_0000020	1.0	1.0
1385_0000021	1.0	1.0
1385_0000022	1.0	1.0
1385_0000023	1.0	1.0
1385_0000033	1.0	2.0
1385_0000034	1.0	2.0
1385_0000035	1.0	1.0
1385_0000036	1.0	2.0
1385_0000037	1.0	1.0
1385_0000038	1.0	1.0
1385_0000039	1.0	1.0
1385_0000040	1.0	1.0
1385_0000041	1.0	2.0
1385_0000042	1.0	1.0
1385_0000043	1.0	1.0
1385_0000044	1.0	2.0
1385_0000045	1.0	2.0
1385_0000047	1.0	2.0
1385_0000048	1.0	1.0
1385_0000049	1.0	1.0
1385_0000050	1.0	1.0
1385_0000051	1.0	2.0
1385_0000052	1.0	0.0
1385_0000053	1.0	1.0
1385_0000054	1.0	2.0
1385_0000057	1.0	1.0
1385_0000058	1.0	1.0
1385_0000059	1.0	1.0
1385_0000095	1.0	1.0
1385_0000097	1.0	1.0
1385_0000098	1.0	1.0
1385_0000099	1.0	1.0
1385_0000100	1.0	1.0
1385_0000101	1.0	0.0
1385_0000102	1.0	2.0
1385_0000103	1.0	0.0
1385_0000104	1.0	1.0
1385_0000114	1.0	2.0
1385_0000119	1.0	1.0
1385_0000120	0.0	1.0
1385_0000122	1.0	2.0
1385_0000123	1.0	2.0
1385_0000124	1.0	2.0
1385_0000125	1.0	1.0
1385_0000126	1.0	1.0
1385_0000127	1.0	2.0
1385_0000128	1.0	2.0
1385_0000129	1.0	2.0
1385_0000130	1.0	1.0
1385_0001103	1.0	1.0
1385_0001104	1.0	1.0
1385_0001105	1.0	1.0
1385_0001107	1.0	1.0
1385_0001108	1.0	1.0
1385_0001109	1.0	1.0
1385_0001110	1.0	2.0
1385_0001111	1.0	1.0
1385_0001112	1.0	2.0
1385_0001113	1.0	1.0
1385_0001118	1.0	1.0
1385_0001119	1.0	1.0
1385_0001120	1.0	0.0
1385_0001121	1.0	2.0
1385_0001122	1.0	0.0
1385_0001123	1.0	0.0
1385_0001124	1.0	1.0
1385_0001125	1.0	1.0
1385_0001126	0.0	1.0
1385_0001127	1.0	2.0
1385_0001128	0.0	2.0
1385_0001129	1.0	1.0
1385_0001130	1.0	1.0
1385_0001131	1.0	1.0
1385_0001132	1.0	1.0
1385_0001133	1.0	2.0
1385_0001134	1.0	1.0
1385_0001135	1.0	2.0
1385_0001136	1.0	1.0
1385_0001137	1.0	1.0
1385_0001138	1.0	1.0
1385_0001147	1.0	1.0
1385_0001148	1.0	2.0
1385_0001149	1.0	2.0
1385_0001150	1.0	1.0
1385_0001151	1.0	2.0
1385_0001152	1.0	2.0
1385_0001153	2.0	1.0
1385_0001154	1.0	2.0
1385_0001155	1.0	1.0
1385_0001156	1.0	1.0
1385_0001157	1.0	0.0
1385_0001158	1.0	2.0
1385_0001159	1.0	1.0
1385_0001160	1.0	2.0
1385_0001161	1.0	2.0
1385_0001162	1.0	1.0
1385_0001163	1.0	1.0
1385_0001164	1.0	2.0
1385_0001165	1.0	2.0
1385_0001166	1.0	2.0
1385_0001167	1.0	2.0
1385_0001169	1.0	1.0
1385_0001170	1.0	1.0
1385_0001171	1.0	1.0
1385_0001172	1.0	1.0
1385_0001173	0.0	1.0
1385_0001174	1.0	1.0
1385_0001175	1.0	1.0
1385_0001178	0.0	1.0
1385_0001188	1.0	1.0
1385_0001189	1.0	1.0
1385_0001190	1.0	1.0
1385_0001191	1.0	1.0
1385_0001192	1.0	1.0
1385_0001193	2.0	2.0
1385_0001194	1.0	1.0
1385_0001195	1.0	2.0
1385_0001196	1.0	2.0
1385_0001197	1.0	1.0
1385_0001198	2.0	2.0
1385_0001199	1.0	1.0
1385_0001501	1.0	1.0
1385_0001503	1.0	2.0
1385_0001522	1.0	1.0
1385_0001523	1.0	2.0
1385_0001524	1.0	1.0
1385_0001525	1.0	2.0
1385_0001526	0.0	1.0
1385_0001527	1.0	1.0
1385_0001528	1.0	2.0
1385_0001712	1.0	2.0
1385_0001714	1.0	1.0
1385_0001715	1.0	2.0
1385_0001716	1.0	1.0
1385_0001717	2.0	2.0
1385_0001718	1.0	1.0
1385_0001719	1.0	1.0
1385_0001720	0.0	1.0
1385_0001723	0.0	1.0
1385_0001724	2.0	2.0
1385_0001725	1.0	1.0
1385_0001726	1.0	1.0
1385_0001727	0.0	1.0
1385_0001728	1.0	1.0
1385_0001729	1.0	2.0
1385_0001730	2.0	1.0
1385_0001732	1.0	2.0
1385_0001733	1.0	2.0
1385_0001734	1.0	1.0
1385_0001736	2.0	1.0
1385_0001737	1.0	2.0
1385_0001738	0.0	0.0
1385_0001739	1.0	2.0
1385_0001740	1.0	2.0
1385_0001741	0.0	1.0
1385_0001742	0.0	0.0
1385_0001744	0.0	2.0
1385_0001746	1.0	2.0
1385_0001747	1.0	1.0
1385_0001748	1.0	1.0
1385_0001749	1.0	2.0
1385_0001750	0.0	0.0
1385_0001751	1.0	2.0
1385_0001752	1.0	1.0
1385_0001753	1.0	2.0
1385_0001754	1.0	2.0
1385_0001756	1.0	1.0
1385_0001757	2.0	1.0
1385_0001758	1.0	1.0
1385_0001759	1.0	1.0
1385_0001760	1.0	1.0
1385_0001761	1.0	1.0
1385_0001762	1.0	1.0
1385_0001764	1.0	1.0
1385_0001765	0.0	1.0
1385_0001766	1.0	2.0
1385_0001767	1.0	2.0
1385_0001768	1.0	1.0
1385_0001771	1.0	1.0
1385_0001772	1.0	1.0
1385_0001773	1.0	1.0
1385_0001774	0.0	1.0
1385_0001775	1.0	2.0
1385_0001785	1.0	1.0
1385_0001786	1.0	1.0
1385_0001787	1.0	1.0
1385_0001788	1.0	1.0
1385_0001789	1.0	1.0
1385_0001790	1.0	2.0
1385_0001791	1.0	1.0
1385_0001792	1.0	2.0
1385_0001793	1.0	2.0
1385_0001794	1.0	2.0
1385_0001795	1.0	1.0
1385_0001796	1.0	2.0
1385_0001798	1.0	2.0
1385_0001799	1.0	2.0
1385_0001800	1.0	1.0
1395_0000333	1.0	2.0
1395_0000337	0.0	1.0
1395_0000338	1.0	1.0
1395_0000340	1.0	2.0
1395_0000341	1.0	1.0
1395_0000353	1.0	1.0
1395_0000354	1.0	1.0
1395_0000355	1.0	1.0
1395_0000356	1.0	1.0
1395_0000357	2.0	2.0
1395_0000359	1.0	2.0
1395_0000360	2.0	2.0
1395_0000361	1.0	1.0
1395_0000364	1.0	2.0
1395_0000365	2.0	2.0
1395_0000366	2.0	2.0
1395_0000368	0.0	1.0
1395_0000369	2.0	2.0
1395_0000376	2.0	2.0
1395_0000378	1.0	1.0
1395_0000379	1.0	1.0
1395_0000380	1.0	2.0
1395_0000383	1.0	1.0
1395_0000387	2.0	2.0
1395_0000388	2.0	2.0
1395_0000389	0.0	1.0
1395_0000390	1.0	1.0
1395_0000391	2.0	2.0
1395_0000392	1.0	2.0
1395_0000396	1.0	1.0
1395_0000398	2.0	2.0
1395_0000399	1.0	1.0
1395_0000402	1.0	1.0
1395_0000403	1.0	2.0
1395_0000404	1.0	2.0
1395_0000409	2.0	2.0
1395_0000413	1.0	2.0
1395_0000414	1.0	1.0
1395_0000415	1.0	1.0
1395_0000432	1.0	2.0
1395_0000438	2.0	2.0
1395_0000443	2.0	2.0
1395_0000446	2.0	2.0
1395_0000447	1.0	2.0
1395_0000448	1.0	1.0
1395_0000449	2.0	2.0
1395_0000450	1.0	2.0
1395_0000451	1.0	1.0
1395_0000452	1.0	1.0
1395_0000454	1.0	2.0
1395_0000455	1.0	2.0
1395_0000458	1.0	1.0
1395_0000460	1.0	1.0
1395_0000462	2.0	2.0
1395_0000465	1.0	1.0
1395_0000469	1.0	1.0
1395_0000470	1.0	1.0
1395_0000471	1.0	2.0
1395_0000499	1.0	1.0
1395_0000500	1.0	1.0
1395_0000504	1.0	1.0
1395_0000512	1.0	2.0
1395_0000513	2.0	2.0
1395_0000514	2.0	2.0
1395_0000515	2.0	2.0
1395_0000516	1.0	1.0
1395_0000518	2.0	2.0
1395_0000525	2.0	1.0
1395_0000526	1.0	1.0
1395_0000527	1.0	1.0
1395_0000528	2.0	1.0
1395_0000529	1.0	1.0
1395_0000531	1.0	1.0
1395_0000533	2.0	2.0
1395_0000534	1.0	2.0
1395_0000535	1.0	1.0
1395_0000537	1.0	2.0
1395_0000547	1.0	2.0
1395_0000548	1.0	2.0
1395_0000549	1.0	2.0
1395_0000550	1.0	2.0
1395_0000551	2.0	2.0
1395_0000552	1.0	2.0
1395_0000553	1.0	1.0
1395_0000554	1.0	2.0
1395_0000555	1.0	2.0
1395_0000556	1.0	2.0
1395_0000557	2.0	2.0
1395_0000559	1.0	2.0
1395_0000560	1.0	2.0
1395_0000563	1.0	2.0
1395_0000564	1.0	1.0
1395_0000565	1.0	2.0
1395_0000572	1.0	1.0
1395_0000575	1.0	2.0
1395_0000579	1.0	1.0
1395_0000581	1.0	2.0
1395_0000582	0.0	1.0
1395_0000583	1.0	2.0
1395_0000584	0.0	1.0
1395_0000585	1.0	2.0
1395_0000587	0.0	1.0
1395_0000591	0.0	1.0
1395_0000593	1.0	1.0
1395_0000595	0.0	1.0
1395_0000596	2.0	1.0
1395_0000597	1.0	2.0
1395_0000598	1.0	2.0
1395_0000599	1.0	1.0
1395_0000602	1.0	1.0
1395_0000604	1.0	1.0
1395_0000606	1.0	1.0
1395_0000607	1.0	1.0
1395_0000608	1.0	2.0
1395_0000609	1.0	1.0
1395_0000610	1.0	2.0
1395_0000611	1.0	1.0
1395_0000612	1.0	2.0
1395_0000626	2.0	2.0
1395_0000627	1.0	1.0
1395_0000628	1.0	2.0
1395_0000630	1.0	2.0
1395_0000631	1.0	2.0
1395_0000635	1.0	1.0
1395_0000636	1.0	1.0
1395_0000639	1.0	2.0
1395_0000642	1.0	2.0
1395_0000644	1.0	2.0
1395_0000646	1.0	1.0
1395_0000649	2.0	2.0
1395_0001010	2.0	2.0
1395_0001013	1.0	2.0
1395_0001015	1.0	2.0
1395_0001016	2.0	1.0
1395_0001017	1.0	1.0
1395_0001019	1.0	1.0
1395_0001020	1.0	2.0
1395_0001021	1.0	1.0
1395_0001022	1.0	2.0
1395_0001023	1.0	2.0
1395_0001024	1.0	1.0
1395_0001028	1.0	2.0
1395_0001033	1.0	2.0
1395_0001034	1.0	2.0
1395_0001040	1.0	1.0
1395_0001045	1.0	2.0
1395_0001058	1.0	1.0
1395_0001060	2.0	2.0
1395_0001061	2.0	2.0
1395_0001064	1.0	1.0
1395_0001065	1.0	2.0
1395_0001066	1.0	2.0
1395_0001067	1.0	2.0
1395_0001068	1.0	2.0
1395_0001069	1.0	2.0
1395_0001070	2.0	2.0
1395_0001071	1.0	1.0
1395_0001073	2.0	2.0
1395_0001074	1.0	1.0
1395_0001075	1.0	2.0
1395_0001076	1.0	2.0
1395_0001078	1.0	2.0
1395_0001080	1.0	2.0
1395_0001084	1.0	2.0
1395_0001090	2.0	2.0
1395_0001093	1.0	2.0
1395_0001101	1.0	2.0
1395_0001103	1.0	2.0
1395_0001104	1.0	1.0
1395_0001108	1.0	1.0
1395_0001109	1.0	2.0
1395_0001114	1.0	2.0
1395_0001115	2.0	2.0
1395_0001116	1.0	2.0
1395_0001117	1.0	1.0
1395_0001118	1.0	1.0
1395_0001119	2.0	2.0
1395_0001120	1.0	1.0
1395_0001121	0.0	1.0
1395_0001122	1.0	1.0
1395_0001123	1.0	2.0
1395_0001124	1.0	1.0
1395_0001126	1.0	1.0
1395_0001131	1.0	1.0
1395_0001132	2.0	2.0
1395_0001133	1.0	2.0
1395_0001141	1.0	1.0
1395_0001145	2.0	2.0
1395_0001146	0.0	1.0
1395_0001147	1.0	2.0
1395_0001149	1.0	2.0
1395_0001150	1.0	1.0
1395_0001158	2.0	2.0
1395_0001160	2.0	2.0
1395_0001161	1.0	2.0
1395_0001164	2.0	2.0
1395_0001167	1.0	2.0
1395_0001169	1.0	2.0
1395_0001170	1.0	2.0
1395_0001171	1.0	1.0
Language = IT, Weighted F1-score = 0.6036797344423641, Dimension = OverallCEFRrating

Train on DE, test on  CZ
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
144.90552995391704 65.35717405024758
LANGUAGE: IT
nan nan
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']


======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 1.12
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.32
              precision    recall  f1-score   support

         0.0       0.50      0.17      0.25         6
         1.0       0.68      0.15      0.24       185
         2.0       0.38      0.54      0.44       156
         3.0       0.47      0.96      0.63        82
         4.0       0.00      0.00      0.00         5

    accuracy                           0.44       434
   macro avg       0.40      0.36      0.31       434
weighted avg       0.52      0.44      0.38       434

[[  1   1   4   0   0]
 [  1  27 133  24   0]
 [  0  12  84  60   0]
 [  0   0   3  79   0]
 [  0   0   0   5   0]]
0.3840839194761096
434 434 434



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.92
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.75
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         6
         1.0       0.58      0.08      0.14       185
         2.0       0.37      0.49      0.42       156
         3.0       0.39      0.98      0.56        82
         4.0       0.00      0.00      0.00         5

    accuracy                           0.39       434
   macro avg       0.27      0.31      0.22       434
weighted avg       0.45      0.39      0.32       434

[[  0   2   3   1   0]
 [  0  15 123  47   0]
 [  0   9  76  71   0]
 [  0   0   2  80   0]
 [  0   0   0   5   0]]
0.31807398635100825
434 434 434



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.79
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.55
              precision    recall  f1-score   support

         0.0       1.00      0.17      0.29         6
         1.0       0.72      0.19      0.31       185
         2.0       0.40      0.51      0.45       156
         3.0       0.42      0.95      0.59        82
         4.0       0.00      0.00      0.00         5

    accuracy                           0.45       434
   macro avg       0.51      0.36      0.32       434
weighted avg       0.54      0.45      0.41       434

[[  1   2   3   0   0]
 [  0  36 113  36   0]
 [  0  12  79  65   0]
 [  0   0   4  78   0]
 [  0   0   0   5   0]]
0.40533720796663186
434 434 434



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.68
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.70
              precision    recall  f1-score   support

         0.0       1.00      0.17      0.29         6
         1.0       0.68      0.14      0.23       185
         2.0       0.38      0.50      0.43       156
         3.0       0.40      0.95      0.57        82
         4.0       0.00      0.00      0.00         5

    accuracy                           0.42       434
   macro avg       0.49      0.35      0.30       434
weighted avg       0.52      0.42      0.36       434

[[  1   2   3   0   0]
 [  0  25 118  42   0]
 [  0  10  78  68   0]
 [  0   0   4  78   0]
 [  0   0   0   5   0]]
0.3633309144077664
434 434 434
Filename	True Label	Prediction
0601	1.0	2.0
0602	2.0	2.0
0603	2.0	3.0
0604	2.0	2.0
0605	2.0	3.0
0606	1.0	2.0
0607	2.0	2.0
0608	1.0	2.0
0609	1.0	2.0
0610	2.0	2.0
0611	2.0	2.0
0612	1.0	2.0
0613	1.0	2.0
0614	1.0	2.0
0615	1.0	2.0
0616	1.0	2.0
0617	1.0	2.0
0618	1.0	2.0
0619	2.0	2.0
0620	1.0	2.0
0621	2.0	2.0
0622	1.0	2.0
0623	2.0	2.0
0624	2.0	2.0
0625	2.0	2.0
0626	2.0	2.0
0627	2.0	2.0
0628	2.0	2.0
0629	2.0	2.0
0630	1.0	1.0
0631	2.0	2.0
0632	1.0	2.0
0633	2.0	2.0
0634	2.0	2.0
0635	1.0	2.0
0636	2.0	2.0
0637	2.0	2.0
0638	2.0	2.0
0639	1.0	2.0
0640	2.0	2.0
0641	1.0	1.0
0642	1.0	2.0
0643	2.0	2.0
0644	1.0	2.0
0645	2.0	2.0
0714	2.0	2.0
0715	2.0	2.0
0716	2.0	2.0
0717	1.0	2.0
0718	1.0	2.0
0719	2.0	2.0
0720	1.0	2.0
0721	2.0	2.0
0722	2.0	2.0
0723	2.0	2.0
0724	2.0	2.0
0725	1.0	2.0
0801	1.0	2.0
0802	2.0	2.0
0803	2.0	2.0
0804	1.0	2.0
0805	2.0	2.0
0806	1.0	2.0
0807	2.0	2.0
0808	1.0	2.0
0809	2.0	2.0
0810	2.0	2.0
0811	2.0	2.0
0812	2.0	2.0
0813	1.0	2.0
0814	1.0	2.0
0815	2.0	2.0
0816	2.0	2.0
0817	1.0	2.0
0818	1.0	2.0
0819	3.0	2.0
0820	1.0	1.0
0821	2.0	2.0
0822	1.0	2.0
0823	2.0	2.0
0824	2.0	2.0
0825	1.0	2.0
0826	1.0	2.0
0827	1.0	2.0
0828	2.0	2.0
0829	1.0	3.0
0901	2.0	2.0
0902	2.0	2.0
0903	1.0	2.0
0904	1.0	2.0
0905	2.0	2.0
0906	2.0	2.0
0907	2.0	2.0
0910	1.0	2.0
0911	1.0	2.0
0912	2.0	2.0
0913	2.0	2.0
0914	1.0	2.0
0915	2.0	2.0
0916	1.0	2.0
0917	1.0	2.0
0918	1.0	2.0
0919	1.0	2.0
0920	2.0	3.0
0921	1.0	2.0
0922	1.0	2.0
0923	2.0	2.0
0924	1.0	2.0
0925	2.0	2.0
0926	2.0	2.0
0927	1.0	2.0
0928	1.0	2.0
0929	0.0	2.0
0930	2.0	2.0
1001	1.0	2.0
1002	2.0	2.0
1003	1.0	2.0
1004	1.0	2.0
1005	1.0	2.0
1006	2.0	2.0
1007	2.0	2.0
1008	2.0	2.0
1009	2.0	2.0
1010	1.0	2.0
1014	2.0	2.0
1015	1.0	3.0
1016	1.0	2.0
1017	1.0	2.0
1018	1.0	2.0
1019	1.0	2.0
1020	2.0	2.0
1021	1.0	2.0
1022	2.0	2.0
1023	1.0	3.0
1111	1.0	2.0
1112	1.0	2.0
1113	1.0	3.0
1114	2.0	3.0
1115	1.0	3.0
1116	1.0	2.0
1117	2.0	2.0
9999	1.0	2.0
BER0609003	2.0	2.0
BER0611003	2.0	3.0
BER0611005	2.0	3.0
BER0611006	2.0	3.0
BER0611007	2.0	3.0
KYJ0611003A	1.0	2.0
KYJ0611004A	1.0	1.0
KYJ0611005A	1.0	1.0
KYJ0611005B	1.0	3.0
KYJ0611006A	1.0	2.0
KYJ0611006B	1.0	2.0
KYJ0611009A	1.0	2.0
KYJ0611009B	1.0	2.0
LIB0611001A	1.0	1.0
LIB0611001B	1.0	2.0
LIB0611002A	1.0	1.0
LIB0611002B	2.0	2.0
LIB0611003A	1.0	2.0
LIB0611004A	1.0	1.0
LIB0611004B	1.0	3.0
LIB0611011	1.0	3.0
LON0610002A	2.0	1.0
LON0610002B	1.0	3.0
LON0611002A	1.0	1.0
LON0611002B	1.0	2.0
LON0611003	2.0	3.0
LON0611004A	1.0	1.0
LON0611004B	1.0	2.0
MOS0509001	1.0	3.0
MOS0509004	1.0	2.0
MOS0611012	2.0	3.0
MOS0611013	2.0	3.0
MOS0611014	1.0	3.0
MOS0611015	2.0	3.0
PAR1011008A	1.0	2.0
PAR1011009A	2.0	1.0
PAR1011009B	1.0	2.0
PAR1011013	2.0	3.0
PAR1011014	2.0	3.0
PAR1011015	2.0	3.0
PAR1011016	3.0	3.0
PAR1011017	3.0	3.0
PAR1011018	3.0	3.0
PHA0111001A	1.0	1.0
PHA0111001B	1.0	2.0
PHA0111002A	2.0	1.0
PHA0111002B	3.0	2.0
PHA0111003A	1.0	1.0
PHA0111003B	2.0	2.0
PHA0111004A	1.0	1.0
PHA0111004B	1.0	2.0
PHA0111005A	1.0	1.0
PHA0111005B	2.0	3.0
PHA0111010	3.0	3.0
PHA0111011	2.0	3.0
PHA0111012	1.0	3.0
PHA0111014	1.0	2.0
PHA0111015	3.0	3.0
PHA0111016	3.0	3.0
PHA0111018	1.0	3.0
PHA0112002A	2.0	1.0
PHA0112002B	1.0	3.0
PHA0112003A	1.0	1.0
PHA0112003B	1.0	2.0
PHA0112006A	3.0	2.0
PHA0112006B	2.0	3.0
PHA0112007A	1.0	1.0
PHA0112007B	1.0	2.0
PHA0112009A	2.0	2.0
PHA0112009B	2.0	3.0
PHA0112012A	1.0	2.0
PHA0112012B	1.0	3.0
PHA0209001	1.0	3.0
PHA0209008	1.0	2.0
PHA0209013	1.0	2.0
PHA0209024	1.0	3.0
PHA0209026	2.0	3.0
PHA0209028	2.0	3.0
PHA0209031	4.0	3.0
PHA0209034	2.0	3.0
PHA0209038	4.0	3.0
PHA0209039	2.0	3.0
PHA0210001	1.0	3.0
PHA0210004	1.0	2.0
PHA0210007	1.0	3.0
PHA0210008	1.0	2.0
PHA0411008A	1.0	1.0
PHA0411008B	1.0	2.0
PHA0411009A	2.0	2.0
PHA0411009B	1.0	3.0
PHA0411010A	0.0	1.0
PHA0411010B	0.0	2.0
PHA0411011A	1.0	1.0
PHA0411011B	1.0	2.0
PHA0411012A	1.0	1.0
PHA0411012B	1.0	2.0
PHA0411027	2.0	3.0
PHA0411028	2.0	2.0
PHA0411029	2.0	3.0
PHA0411030	3.0	3.0
PHA0411031	3.0	3.0
PHA0411032	1.0	3.0
PHA0411033	2.0	3.0
PHA0411034	1.0	2.0
PHA0411035	3.0	3.0
PHA0411036	3.0	3.0
PHA0411037	2.0	3.0
PHA0411038	3.0	3.0
PHA0411039	2.0	3.0
PHA0411041	3.0	3.0
PHA0411042	2.0	3.0
PHA0411043	1.0	3.0
PHA0411044	3.0	3.0
PHA0411045	3.0	3.0
PHA0411047	2.0	3.0
PHA0411051	3.0	3.0
PHA0411053	3.0	3.0
PHA0411054	3.0	3.0
PHA0411055	3.0	3.0
PHA0411056	3.0	3.0
PHA0411058	3.0	3.0
PHA0411059	3.0	3.0
PHA0411060	2.0	3.0
PHA0411061	3.0	3.0
PHA0411062	3.0	3.0
PHA0509002	1.0	2.0
PHA0509007	1.0	2.0
PHA0509013	1.0	2.0
PHA0509015	3.0	3.0
PHA0509017	2.0	3.0
PHA0509018	3.0	3.0
PHA0509019	3.0	3.0
PHA0509020	3.0	3.0
PHA0509021	1.0	3.0
PHA0509022	4.0	3.0
PHA0509024	2.0	3.0
PHA0509025	3.0	3.0
PHA0509026	3.0	3.0
PHA0509027	1.0	3.0
PHA0509028	2.0	3.0
PHA0509030	2.0	3.0
PHA0509031	1.0	3.0
PHA0509032	2.0	3.0
PHA0509033	1.0	2.0
PHA0509034	1.0	3.0
PHA0509035	2.0	3.0
PHA0509036	3.0	3.0
PHA0509037	3.0	3.0
PHA0509038	1.0	2.0
PHA0509039	3.0	3.0
PHA0509040	2.0	3.0
PHA0509041	2.0	3.0
PHA0509042	3.0	3.0
PHA0509043	2.0	3.0
PHA0509044	2.0	3.0
PHA0509045	1.0	2.0
PHA0510002A	2.0	1.0
PHA0510002B	2.0	2.0
PHA0510003A	1.0	1.0
PHA0510003B	1.0	2.0
PHA0510004A	1.0	2.0
PHA0510004B	0.0	2.0
PHA0510010A	1.0	1.0
PHA0510010B	0.0	0.0
PHA0510013A	2.0	1.0
PHA0510013B	1.0	3.0
PHA0510023	3.0	3.0
PHA0510027	1.0	2.0
PHA0510029	2.0	3.0
PHA0510030	2.0	3.0
PHA0510031	2.0	3.0
PHA0510032	3.0	3.0
PHA0510034	3.0	3.0
PHA0510035	3.0	3.0
PHA0510036	3.0	3.0
PHA0510037	1.0	3.0
PHA0510038	3.0	3.0
PHA0510039	3.0	3.0
PHA0510040	2.0	3.0
PHA0510046	2.0	3.0
PHA0510047	1.0	2.0
PHA0510048	1.0	3.0
PHA0510049	3.0	3.0
PHA0510050	2.0	3.0
PHA0610005A	1.0	1.0
PHA0610005B	0.0	1.0
PHA0610006A	1.0	2.0
PHA0610006B	1.0	3.0
PHA0610007A	1.0	1.0
PHA0610007B	1.0	2.0
PHA0610015	2.0	3.0
PHA0610016	2.0	3.0
PHA0610017	3.0	3.0
PHA0610018	2.0	3.0
PHA0610019A	2.0	1.0
PHA0610019B	2.0	3.0
PHA0610025	2.0	3.0
PHA0610026	3.0	3.0
PHA0709008	3.0	3.0
PHA0710009	3.0	3.0
PHA0710010	2.0	3.0
PHA0710011	3.0	3.0
PHA0710012	3.0	3.0
PHA0710013	4.0	3.0
PHA0710014	3.0	3.0
PHA0710015	2.0	3.0
PHA0710016	3.0	3.0
PHA0710017	3.0	3.0
PHA0710018	3.0	3.0
PHA0710019	3.0	3.0
PHA0710021	3.0	3.0
PHA0809009	2.0	3.0
PHA0809010	2.0	3.0
PHA0810001	3.0	3.0
PHA0810002	1.0	3.0
PHA0810003	3.0	3.0
PHA0810004	1.0	3.0
PHA0810006	2.0	3.0
PHA0810008	3.0	3.0
PHA0810009	3.0	3.0
PHA0810010	2.0	3.0
PHA0810011	2.0	3.0
PHA0810012	3.0	3.0
PHA0810015	3.0	3.0
PHA0811010	2.0	3.0
PHA0811012	3.0	3.0
PHA0811013	3.0	3.0
PHA0811014	1.0	3.0
PHA0811016	1.0	2.0
PHA0811017	3.0	3.0
PHA0811019	3.0	3.0
PHA0811020	1.0	3.0
PHA1109001	1.0	2.0
PHA1109002	3.0	3.0
PHA1109003	1.0	2.0
PHA1109004	3.0	3.0
PHA1109005	1.0	2.0
PHA1109006	2.0	2.0
PHA1109007	1.0	3.0
PHA1109008	1.0	2.0
PHA1109023	1.0	2.0
PHA1109024	4.0	3.0
PHA1109025	1.0	2.0
PHA1109026	3.0	3.0
PHA1109027	3.0	3.0
PHA1109028	3.0	3.0
PHA1110001A	2.0	2.0
PHA1110001B	1.0	2.0
PHA1110002A	2.0	2.0
PHA1110002B	2.0	3.0
PHA1110003A	1.0	2.0
PHA1110003B	1.0	3.0
PHA1110004A	1.0	2.0
PHA1110013	2.0	3.0
PHA1110014	2.0	3.0
PHA1110015	3.0	3.0
PHA1110016	1.0	3.0
PHA1110017	1.0	3.0
PHA1110019	2.0	3.0
PHA1110021	2.0	3.0
PHA1110022	3.0	3.0
PHA1111001A	1.0	2.0
PHA1111001B	1.0	2.0
PHA1111002A	2.0	1.0
PHA1111002B	1.0	1.0
PHA1111003A	2.0	1.0
PHA1111003B	1.0	3.0
PHA1111004A	1.0	2.0
PHA1111004B	1.0	2.0
PHA1111006A	1.0	1.0
PHA1111006B	1.0	2.0
PHA1111008A	2.0	1.0
PHA1111008B	1.0	3.0
PHA1111009A	1.0	2.0
ST071122B	1.0	2.0
TI071122B	1.0	2.0
VAR0209036	3.0	3.0
VAR0909003	2.0	3.0
VAR0909004	2.0	2.0
VAR0909005	1.0	3.0
VAR0909006	3.0	3.0
VAR0909007	2.0	3.0
VAR0909008	2.0	2.0
VAR0909009	3.0	3.0
VAR0909010	1.0	3.0
VAR0910004	3.0	3.0
VAR0910005	3.0	2.0
VAR0910006	3.0	3.0
VAR0910007	2.0	3.0
VAR0910009	3.0	3.0
VAR0910010	3.0	3.0
VAR0910011	3.0	3.0
Language = CZ, Weighted F1-score = 0.3633309144077664, Dimension = Grammaticalaccuracy

Train on DE, test on  IT
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
nan nan
LANGUAGE: IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']


======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36

  Average training loss: 1.09
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.02
              precision    recall  f1-score   support

         0.0       0.50      0.04      0.08        71
         1.0       0.47      0.75      0.58       242
         2.0       0.59      0.41      0.48       376
         3.0       0.38      0.51      0.44       111

    accuracy                           0.49       800
   macro avg       0.49      0.43      0.39       800
weighted avg       0.52      0.49      0.47       800

[[  3  63   5   0]
 [  2 181  54   5]
 [  1 133 154  88]
 [  0   6  48  57]]
0.4699797799441937
800 800 800



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.92
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.03
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        71
         1.0       0.47      0.68      0.56       242
         2.0       0.59      0.64      0.62       376
         3.0       0.52      0.22      0.31       111

    accuracy                           0.54       800
   macro avg       0.40      0.38      0.37       800
weighted avg       0.49      0.54      0.50       800

[[  0  66   5   0]
 [  0 165  77   0]
 [  0 114 240  22]
 [  0   5  82  24]]
0.5002744622071852
800 800 800



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.79
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.05
              precision    recall  f1-score   support

         0.0       0.33      0.11      0.17        71
         1.0       0.46      0.52      0.49       242
         2.0       0.56      0.49      0.52       376
         3.0       0.40      0.61      0.49       111

    accuracy                           0.48       800
   macro avg       0.44      0.44      0.42       800
weighted avg       0.49      0.48      0.48       800

[[  8  53  10   0]
 [ 14 127  97   4]
 [  2  92 185  97]
 [  0   2  41  68]]
0.47652029958453057
800 800 800



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.64
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.07
              precision    recall  f1-score   support

         0.0       0.42      0.07      0.12        71
         1.0       0.45      0.46      0.45       242
         2.0       0.54      0.59      0.56       376
         3.0       0.43      0.50      0.46       111

    accuracy                           0.49       800
   macro avg       0.46      0.40      0.40       800
weighted avg       0.48      0.49      0.47       800

[[  5  54  12   0]
 [  6 111 124   1]
 [  1  82 220  73]
 [  0   2  54  55]]
0.4741627408777692
800 800 800
Filename	True Label	Prediction
1325_1001008	2.0	2.0
1325_1001009	2.0	2.0
1325_1001010	2.0	3.0
1325_1001011	2.0	3.0
1325_1001012	2.0	3.0
1325_1001013	2.0	2.0
1325_1001014	3.0	3.0
1325_1001015	2.0	2.0
1325_1001016	2.0	2.0
1325_1001017	2.0	2.0
1325_1001018	2.0	3.0
1325_1001019	2.0	2.0
1325_1001020	2.0	2.0
1325_1001021	2.0	3.0
1325_1001022	2.0	3.0
1325_1001023	2.0	2.0
1325_1001024	2.0	2.0
1325_1001025	2.0	2.0
1325_1001027	3.0	2.0
1325_1001028	2.0	3.0
1325_1001029	2.0	2.0
1325_1001032	2.0	3.0
1325_1001033	3.0	3.0
1325_1001035	3.0	3.0
1325_1001036	2.0	3.0
1325_1001037	2.0	2.0
1325_1001039	3.0	3.0
1325_1001040	3.0	3.0
1325_1001041	3.0	2.0
1325_1001042	2.0	3.0
1325_1001043	2.0	3.0
1325_1001044	3.0	2.0
1325_1001045	2.0	2.0
1325_1001046	2.0	2.0
1325_1001047	1.0	2.0
1325_1001048	1.0	2.0
1325_1001050	2.0	3.0
1325_1001051	2.0	3.0
1325_1001052	2.0	2.0
1325_1001053	1.0	2.0
1325_1001054	2.0	2.0
1325_1001055	2.0	3.0
1325_1001056	2.0	3.0
1325_1001057	2.0	2.0
1325_1001058	2.0	2.0
1325_1001059	2.0	2.0
1325_1001062	2.0	3.0
1325_1001063	2.0	3.0
1325_1001075	1.0	2.0
1325_1001076	3.0	3.0
1325_1001077	2.0	2.0
1325_1001078	2.0	3.0
1325_1001079	3.0	3.0
1325_1001080	2.0	2.0
1325_1001081	3.0	2.0
1325_1001082	2.0	2.0
1325_1001083	2.0	2.0
1325_1001084	2.0	2.0
1325_1001085	2.0	2.0
1325_1001086	2.0	2.0
1325_1001087	2.0	3.0
1325_1001088	2.0	2.0
1325_1001089	2.0	2.0
1325_1001090	2.0	2.0
1325_1001091	3.0	3.0
1325_1001092	2.0	2.0
1325_1001093	2.0	3.0
1325_1001094	2.0	3.0
1325_1001095	2.0	2.0
1325_1001096	2.0	2.0
1325_1001097	1.0	2.0
1325_1001098	2.0	3.0
1325_1001099	3.0	3.0
1325_1001100	2.0	3.0
1325_1001101	3.0	3.0
1325_1001107	3.0	3.0
1325_1001108	3.0	3.0
1325_1001109	2.0	2.0
1325_1001110	3.0	3.0
1325_1001111	3.0	3.0
1325_1001113	3.0	3.0
1325_1001119	3.0	3.0
1325_1001120	3.0	3.0
1325_1001121	2.0	3.0
1325_1001122	2.0	2.0
1325_1001123	3.0	3.0
1325_1001124	2.0	2.0
1325_1001125	2.0	3.0
1325_1001126	2.0	2.0
1325_1001127	3.0	2.0
1325_1001128	2.0	2.0
1325_1001129	1.0	2.0
1325_1001130	2.0	2.0
1325_1001131	3.0	2.0
1325_1001132	2.0	3.0
1325_1001133	2.0	3.0
1325_1001134	2.0	3.0
1325_1001135	2.0	3.0
1325_1001136	2.0	2.0
1325_1001138	2.0	3.0
1325_1001139	2.0	3.0
1325_1001141	2.0	3.0
1325_1001142	2.0	3.0
1325_1001143	2.0	3.0
1325_1001144	3.0	2.0
1325_1001152	2.0	3.0
1325_1001153	2.0	2.0
1325_1001154	3.0	3.0
1325_1001155	2.0	3.0
1325_1001156	2.0	3.0
1325_1001157	2.0	3.0
1325_1001158	2.0	3.0
1325_1001159	3.0	3.0
1325_1001160	2.0	3.0
1325_1001161	2.0	3.0
1325_1001162	2.0	3.0
1325_1001163	2.0	3.0
1325_1001164	2.0	2.0
1325_1001165	2.0	2.0
1325_1001166	2.0	3.0
1325_1001167	3.0	3.0
1325_1001168	2.0	3.0
1325_1001169	2.0	3.0
1325_1001170	3.0	3.0
1325_9000059	2.0	3.0
1325_9000087	2.0	2.0
1325_9000088	3.0	2.0
1325_9000089	2.0	2.0
1325_9000090	2.0	3.0
1325_9000095	2.0	2.0
1325_9000099	2.0	3.0
1325_9000102	2.0	2.0
1325_9000104	2.0	2.0
1325_9000105	1.0	2.0
1325_9000106	2.0	2.0
1325_9000107	2.0	3.0
1325_9000136	2.0	3.0
1325_9000137	3.0	3.0
1325_9000138	3.0	3.0
1325_9000139	2.0	2.0
1325_9000140	3.0	3.0
1325_9000143	3.0	3.0
1325_9000144	3.0	3.0
1325_9000152	3.0	3.0
1325_9000185	3.0	2.0
1325_9000186	3.0	3.0
1325_9000187	3.0	3.0
1325_9000188	2.0	3.0
1325_9000209	2.0	3.0
1325_9000210	2.0	2.0
1325_9000211	2.0	3.0
1325_9000213	3.0	2.0
1325_9000214	2.0	2.0
1325_9000215	3.0	3.0
1325_9000237	3.0	3.0
1325_9000239	3.0	2.0
1325_9000240	2.0	3.0
1325_9000241	3.0	3.0
1325_9000278	3.0	3.0
1325_9000279	3.0	3.0
1325_9000296	2.0	2.0
1325_9000302	2.0	2.0
1325_9000303	3.0	2.0
1325_9000304	2.0	2.0
1325_9000314	2.0	2.0
1325_9000315	2.0	2.0
1325_9000316	1.0	2.0
1325_9000317	3.0	3.0
1325_9000318	3.0	2.0
1325_9000319	2.0	2.0
1325_9000320	3.0	2.0
1325_9000321	3.0	3.0
1325_9000322	3.0	3.0
1325_9000323	2.0	2.0
1325_9000503	3.0	3.0
1325_9000504	3.0	3.0
1325_9000505	2.0	3.0
1325_9000533	3.0	2.0
1325_9000534	2.0	3.0
1325_9000536	2.0	2.0
1325_9000554	2.0	2.0
1325_9000601	3.0	3.0
1325_9000602	3.0	3.0
1325_9000611	2.0	3.0
1325_9000612	2.0	2.0
1325_9000674	2.0	3.0
1325_9000675	3.0	3.0
1325_9000676	3.0	3.0
1325_9000677	3.0	2.0
1325_9000678	3.0	3.0
1325_9000684	3.0	3.0
1325_9000685	3.0	3.0
1325_9000686	3.0	3.0
1325_9000700	2.0	3.0
1325_9000750	3.0	2.0
1365_0100002	2.0	2.0
1365_0100003	1.0	1.0
1365_0100004	2.0	2.0
1365_0100005	2.0	1.0
1365_0100006	2.0	2.0
1365_0100007	1.0	1.0
1365_0100008	1.0	2.0
1365_0100009	2.0	1.0
1365_0100010	1.0	1.0
1365_0100011	1.0	2.0
1365_0100012	2.0	1.0
1365_0100013	3.0	2.0
1365_0100014	2.0	1.0
1365_0100015	1.0	1.0
1365_0100016	2.0	2.0
1365_0100017	2.0	2.0
1365_0100018	1.0	1.0
1365_0100019	1.0	2.0
1365_0100020	2.0	2.0
1365_0100021	2.0	2.0
1365_0100022	2.0	2.0
1365_0100023	2.0	2.0
1365_0100024	1.0	2.0
1365_0100026	1.0	1.0
1365_0100027	2.0	2.0
1365_0100028	2.0	1.0
1365_0100029	1.0	1.0
1365_0100030	1.0	2.0
1365_0100031	2.0	2.0
1365_0100051	1.0	2.0
1365_0100056	2.0	2.0
1365_0100057	2.0	3.0
1365_0100058	2.0	3.0
1365_0100061	3.0	2.0
1365_0100063	3.0	2.0
1365_0100064	2.0	2.0
1365_0100065	1.0	1.0
1365_0100066	1.0	2.0
1365_0100067	2.0	2.0
1365_0100069	1.0	2.0
1365_0100070	2.0	2.0
1365_0100071	2.0	2.0
1365_0100072	2.0	2.0
1365_0100073	2.0	2.0
1365_0100074	2.0	2.0
1365_0100079	2.0	2.0
1365_0100080	2.0	2.0
1365_0100092	2.0	2.0
1365_0100093	2.0	2.0
1365_0100094	2.0	2.0
1365_0100095	2.0	1.0
1365_0100096	2.0	2.0
1365_0100097	2.0	2.0
1365_0100098	1.0	2.0
1365_0100099	2.0	2.0
1365_0100100	2.0	3.0
1365_0100101	3.0	3.0
1365_0100102	3.0	2.0
1365_0100103	3.0	2.0
1365_0100104	2.0	3.0
1365_0100105	3.0	2.0
1365_0100106	2.0	2.0
1365_0100107	3.0	3.0
1365_0100116	2.0	2.0
1365_0100117	3.0	2.0
1365_0100118	3.0	2.0
1365_0100119	3.0	2.0
1365_0100120	3.0	3.0
1365_0100121	3.0	2.0
1365_0100123	2.0	2.0
1365_0100125	3.0	2.0
1365_0100133	2.0	2.0
1365_0100134	2.0	1.0
1365_0100135	2.0	2.0
1365_0100136	2.0	2.0
1365_0100137	2.0	1.0
1365_0100138	2.0	1.0
1365_0100139	2.0	1.0
1365_0100145	2.0	3.0
1365_0100146	2.0	2.0
1365_0100147	2.0	2.0
1365_0100148	2.0	2.0
1365_0100151	2.0	1.0
1365_0100162	2.0	3.0
1365_0100163	3.0	3.0
1365_0100164	2.0	3.0
1365_0100165	3.0	2.0
1365_0100166	2.0	2.0
1365_0100167	1.0	2.0
1365_0100168	3.0	2.0
1365_0100169	2.0	2.0
1365_0100170	1.0	2.0
1365_0100171	2.0	2.0
1365_0100172	2.0	2.0
1365_0100173	2.0	2.0
1365_0100174	2.0	2.0
1365_0100175	2.0	1.0
1365_0100176	2.0	2.0
1365_0100177	2.0	2.0
1365_0100178	2.0	2.0
1365_0100179	2.0	2.0
1365_0100180	1.0	2.0
1365_0100181	1.0	2.0
1365_0100182	2.0	2.0
1365_0100183	1.0	2.0
1365_0100184	2.0	2.0
1365_0100185	1.0	2.0
1365_0100186	2.0	2.0
1365_0100187	2.0	2.0
1365_0100188	2.0	2.0
1365_0100190	2.0	2.0
1365_0100191	2.0	2.0
1365_0100192	3.0	3.0
1365_0100194	3.0	2.0
1365_0100195	1.0	2.0
1365_0100196	1.0	2.0
1365_0100198	2.0	1.0
1365_0100199	2.0	2.0
1365_0100200	3.0	2.0
1365_0100201	2.0	2.0
1365_0100202	1.0	2.0
1365_0100203	2.0	2.0
1365_0100204	2.0	1.0
1365_0100205	3.0	1.0
1365_0100211	2.0	2.0
1365_0100212	3.0	2.0
1365_0100213	2.0	1.0
1365_0100215	2.0	2.0
1365_0100217	3.0	3.0
1365_0100218	3.0	2.0
1365_0100219	2.0	2.0
1365_0100220	2.0	2.0
1365_0100221	2.0	2.0
1365_0100222	1.0	2.0
1365_0100223	2.0	2.0
1365_0100224	2.0	3.0
1365_0100225	2.0	2.0
1365_0100226	2.0	2.0
1365_0100227	3.0	2.0
1365_0100228	1.0	2.0
1365_0100229	2.0	2.0
1365_0100230	2.0	2.0
1365_0100231	2.0	2.0
1365_0100232	3.0	2.0
1365_0100233	3.0	3.0
1365_0100251	2.0	3.0
1365_0100252	2.0	2.0
1365_0100253	2.0	2.0
1365_0100255	2.0	2.0
1365_0100256	2.0	2.0
1365_0100257	2.0	2.0
1365_0100258	2.0	2.0
1365_0100259	2.0	2.0
1365_0100260	2.0	2.0
1365_0100261	2.0	1.0
1365_0100262	2.0	2.0
1365_0100263	3.0	2.0
1365_0100265	3.0	2.0
1365_0100266	3.0	2.0
1365_0100267	2.0	2.0
1365_0100268	2.0	2.0
1365_0100269	2.0	2.0
1365_0100270	2.0	2.0
1365_0100274	2.0	3.0
1365_0100275	2.0	2.0
1365_0100276	3.0	2.0
1365_0100277	3.0	2.0
1365_0100278	2.0	2.0
1365_0100279	1.0	2.0
1365_0100280	1.0	2.0
1365_0100281	1.0	2.0
1365_0100282	2.0	2.0
1365_0100285	2.0	2.0
1365_0100286	1.0	2.0
1365_0100287	2.0	2.0
1365_0100288	2.0	2.0
1365_0100289	2.0	1.0
1365_0100290	2.0	2.0
1365_0100299	3.0	2.0
1365_0100447	3.0	2.0
1365_0100448	1.0	2.0
1365_0100451	2.0	2.0
1365_0100455	3.0	2.0
1365_0100456	2.0	2.0
1365_0100457	2.0	2.0
1365_0100458	2.0	2.0
1365_0100459	2.0	2.0
1365_0100461	2.0	3.0
1365_0100469	2.0	2.0
1365_0100470	2.0	2.0
1365_0100471	2.0	3.0
1365_0100472	2.0	2.0
1365_0100473	2.0	2.0
1365_0100474	2.0	2.0
1365_0100475	2.0	2.0
1365_0100476	2.0	3.0
1365_0100477	1.0	3.0
1365_0100478	2.0	2.0
1365_0100479	3.0	2.0
1365_0100480	2.0	2.0
1365_0100481	2.0	2.0
1365_0100482	2.0	2.0
1385_0000011	0.0	1.0
1385_0000012	2.0	1.0
1385_0000013	1.0	0.0
1385_0000016	2.0	1.0
1385_0000017	1.0	1.0
1385_0000020	2.0	1.0
1385_0000021	2.0	1.0
1385_0000022	1.0	1.0
1385_0000023	2.0	2.0
1385_0000033	1.0	1.0
1385_0000034	2.0	1.0
1385_0000035	2.0	1.0
1385_0000036	1.0	2.0
1385_0000037	1.0	2.0
1385_0000038	1.0	1.0
1385_0000039	1.0	1.0
1385_0000040	1.0	1.0
1385_0000041	2.0	1.0
1385_0000042	2.0	1.0
1385_0000043	2.0	1.0
1385_0000044	2.0	1.0
1385_0000045	2.0	1.0
1385_0000047	1.0	2.0
1385_0000048	2.0	1.0
1385_0000049	2.0	1.0
1385_0000050	2.0	1.0
1385_0000051	2.0	2.0
1385_0000052	1.0	1.0
1385_0000053	2.0	1.0
1385_0000054	2.0	1.0
1385_0000057	1.0	1.0
1385_0000058	2.0	1.0
1385_0000059	2.0	1.0
1385_0000095	1.0	1.0
1385_0000097	2.0	1.0
1385_0000098	1.0	1.0
1385_0000099	0.0	1.0
1385_0000100	1.0	1.0
1385_0000101	1.0	1.0
1385_0000102	1.0	2.0
1385_0000103	1.0	0.0
1385_0000104	2.0	1.0
1385_0000114	2.0	2.0
1385_0000119	2.0	1.0
1385_0000120	0.0	1.0
1385_0000122	2.0	1.0
1385_0000123	1.0	1.0
1385_0000124	2.0	2.0
1385_0000125	1.0	1.0
1385_0000126	2.0	1.0
1385_0000127	2.0	2.0
1385_0000128	1.0	2.0
1385_0000129	2.0	2.0
1385_0000130	2.0	1.0
1385_0001103	2.0	1.0
1385_0001104	1.0	1.0
1385_0001105	2.0	1.0
1385_0001107	2.0	1.0
1385_0001108	2.0	1.0
1385_0001109	2.0	1.0
1385_0001110	2.0	1.0
1385_0001111	2.0	1.0
1385_0001112	2.0	2.0
1385_0001113	1.0	1.0
1385_0001118	1.0	1.0
1385_0001119	1.0	2.0
1385_0001120	1.0	1.0
1385_0001121	1.0	2.0
1385_0001122	1.0	1.0
1385_0001123	2.0	0.0
1385_0001124	1.0	1.0
1385_0001125	1.0	1.0
1385_0001126	0.0	1.0
1385_0001127	2.0	2.0
1385_0001128	0.0	2.0
1385_0001129	1.0	1.0
1385_0001130	1.0	0.0
1385_0001131	1.0	1.0
1385_0001132	1.0	1.0
1385_0001133	2.0	2.0
1385_0001134	1.0	1.0
1385_0001135	1.0	2.0
1385_0001136	1.0	1.0
1385_0001137	1.0	1.0
1385_0001138	1.0	1.0
1385_0001147	1.0	1.0
1385_0001148	2.0	2.0
1385_0001149	2.0	2.0
1385_0001150	2.0	1.0
1385_0001151	2.0	2.0
1385_0001152	2.0	2.0
1385_0001153	3.0	1.0
1385_0001154	2.0	2.0
1385_0001155	2.0	1.0
1385_0001156	2.0	1.0
1385_0001157	1.0	1.0
1385_0001158	1.0	1.0
1385_0001159	1.0	1.0
1385_0001160	3.0	2.0
1385_0001161	2.0	2.0
1385_0001162	1.0	1.0
1385_0001163	2.0	2.0
1385_0001164	1.0	2.0
1385_0001165	1.0	1.0
1385_0001166	0.0	2.0
1385_0001167	1.0	2.0
1385_0001169	1.0	1.0
1385_0001170	1.0	0.0
1385_0001171	1.0	1.0
1385_0001172	0.0	1.0
1385_0001173	0.0	0.0
1385_0001174	0.0	1.0
1385_0001175	0.0	1.0
1385_0001178	0.0	1.0
1385_0001188	1.0	2.0
1385_0001189	0.0	1.0
1385_0001190	0.0	1.0
1385_0001191	1.0	2.0
1385_0001192	1.0	2.0
1385_0001193	2.0	2.0
1385_0001194	1.0	1.0
1385_0001195	1.0	2.0
1385_0001196	1.0	2.0
1385_0001197	1.0	2.0
1385_0001198	2.0	2.0
1385_0001199	1.0	2.0
1385_0001501	1.0	0.0
1385_0001503	1.0	2.0
1385_0001522	0.0	1.0
1385_0001523	1.0	2.0
1385_0001524	0.0	2.0
1385_0001525	1.0	2.0
1385_0001526	0.0	1.0
1385_0001527	1.0	1.0
1385_0001528	1.0	2.0
1385_0001712	1.0	2.0
1385_0001714	1.0	1.0
1385_0001715	1.0	2.0
1385_0001716	1.0	2.0
1385_0001717	2.0	2.0
1385_0001718	0.0	1.0
1385_0001719	0.0	1.0
1385_0001720	0.0	1.0
1385_0001723	0.0	1.0
1385_0001724	1.0	2.0
1385_0001725	1.0	1.0
1385_0001726	1.0	2.0
1385_0001727	0.0	1.0
1385_0001728	1.0	2.0
1385_0001729	1.0	1.0
1385_0001730	1.0	1.0
1385_0001732	1.0	2.0
1385_0001733	2.0	2.0
1385_0001734	1.0	1.0
1385_0001736	2.0	1.0
1385_0001737	2.0	1.0
1385_0001738	0.0	0.0
1385_0001739	0.0	2.0
1385_0001740	1.0	2.0
1385_0001741	0.0	1.0
1385_0001742	0.0	0.0
1385_0001744	0.0	1.0
1385_0001746	0.0	2.0
1385_0001747	1.0	1.0
1385_0001748	1.0	1.0
1385_0001749	1.0	1.0
1385_0001750	0.0	0.0
1385_0001751	1.0	2.0
1385_0001752	0.0	1.0
1385_0001753	1.0	2.0
1385_0001754	1.0	2.0
1385_0001756	1.0	1.0
1385_0001757	2.0	1.0
1385_0001758	1.0	1.0
1385_0001759	0.0	1.0
1385_0001760	1.0	2.0
1385_0001761	0.0	1.0
1385_0001762	1.0	1.0
1385_0001764	0.0	1.0
1385_0001765	0.0	0.0
1385_0001766	2.0	1.0
1385_0001767	0.0	2.0
1385_0001768	1.0	2.0
1385_0001771	0.0	1.0
1385_0001772	0.0	1.0
1385_0001773	0.0	1.0
1385_0001774	0.0	1.0
1385_0001775	0.0	1.0
1385_0001785	0.0	1.0
1385_0001786	1.0	2.0
1385_0001787	0.0	1.0
1385_0001788	1.0	1.0
1385_0001789	1.0	1.0
1385_0001790	2.0	1.0
1385_0001791	0.0	1.0
1385_0001792	0.0	2.0
1385_0001793	1.0	2.0
1385_0001794	0.0	1.0
1385_0001795	1.0	0.0
1385_0001796	2.0	1.0
1385_0001798	1.0	2.0
1385_0001799	1.0	2.0
1385_0001800	1.0	1.0
1395_0000333	2.0	2.0
1395_0000337	0.0	1.0
1395_0000338	1.0	1.0
1395_0000340	2.0	2.0
1395_0000341	1.0	1.0
1395_0000353	1.0	1.0
1395_0000354	1.0	1.0
1395_0000355	2.0	1.0
1395_0000356	1.0	1.0
1395_0000357	3.0	2.0
1395_0000359	2.0	2.0
1395_0000360	3.0	2.0
1395_0000361	2.0	1.0
1395_0000364	1.0	1.0
1395_0000365	2.0	2.0
1395_0000366	2.0	1.0
1395_0000368	0.0	1.0
1395_0000369	2.0	2.0
1395_0000376	2.0	2.0
1395_0000378	1.0	1.0
1395_0000379	1.0	1.0
1395_0000380	2.0	2.0
1395_0000383	2.0	1.0
1395_0000387	3.0	2.0
1395_0000388	2.0	1.0
1395_0000389	0.0	1.0
1395_0000390	1.0	1.0
1395_0000391	3.0	2.0
1395_0000392	2.0	1.0
1395_0000396	2.0	1.0
1395_0000398	2.0	2.0
1395_0000399	1.0	1.0
1395_0000402	1.0	1.0
1395_0000403	2.0	2.0
1395_0000404	2.0	1.0
1395_0000409	2.0	2.0
1395_0000413	2.0	1.0
1395_0000414	2.0	1.0
1395_0000415	1.0	1.0
1395_0000432	2.0	2.0
1395_0000438	3.0	2.0
1395_0000443	2.0	2.0
1395_0000446	2.0	2.0
1395_0000447	1.0	2.0
1395_0000448	1.0	2.0
1395_0000449	2.0	2.0
1395_0000450	1.0	2.0
1395_0000451	2.0	2.0
1395_0000452	1.0	1.0
1395_0000454	2.0	2.0
1395_0000455	1.0	1.0
1395_0000458	2.0	1.0
1395_0000460	1.0	1.0
1395_0000462	2.0	2.0
1395_0000465	1.0	1.0
1395_0000469	1.0	1.0
1395_0000470	1.0	1.0
1395_0000471	1.0	2.0
1395_0000499	1.0	1.0
1395_0000500	1.0	1.0
1395_0000504	1.0	2.0
1395_0000512	2.0	1.0
1395_0000513	2.0	2.0
1395_0000514	3.0	2.0
1395_0000515	2.0	1.0
1395_0000516	1.0	1.0
1395_0000518	2.0	2.0
1395_0000525	2.0	1.0
1395_0000526	1.0	1.0
1395_0000527	1.0	1.0
1395_0000528	2.0	1.0
1395_0000529	2.0	1.0
1395_0000531	2.0	1.0
1395_0000533	3.0	2.0
1395_0000534	2.0	2.0
1395_0000535	1.0	2.0
1395_0000537	1.0	2.0
1395_0000547	2.0	2.0
1395_0000548	2.0	2.0
1395_0000549	2.0	2.0
1395_0000550	1.0	2.0
1395_0000551	2.0	2.0
1395_0000552	2.0	2.0
1395_0000553	2.0	1.0
1395_0000554	2.0	2.0
1395_0000555	1.0	1.0
1395_0000556	1.0	2.0
1395_0000557	3.0	2.0
1395_0000559	2.0	1.0
1395_0000560	2.0	2.0
1395_0000563	1.0	2.0
1395_0000564	1.0	1.0
1395_0000565	1.0	2.0
1395_0000572	1.0	2.0
1395_0000575	1.0	2.0
1395_0000579	1.0	1.0
1395_0000581	1.0	2.0
1395_0000582	0.0	1.0
1395_0000583	1.0	2.0
1395_0000584	0.0	1.0
1395_0000585	1.0	2.0
1395_0000587	0.0	1.0
1395_0000591	0.0	1.0
1395_0000593	1.0	2.0
1395_0000595	0.0	1.0
1395_0000596	2.0	2.0
1395_0000597	1.0	2.0
1395_0000598	1.0	2.0
1395_0000599	1.0	1.0
1395_0000602	1.0	1.0
1395_0000604	1.0	1.0
1395_0000606	1.0	1.0
1395_0000607	1.0	1.0
1395_0000608	1.0	2.0
1395_0000609	0.0	1.0
1395_0000610	2.0	1.0
1395_0000611	1.0	1.0
1395_0000612	1.0	2.0
1395_0000626	1.0	2.0
1395_0000627	1.0	1.0
1395_0000628	0.0	1.0
1395_0000630	1.0	2.0
1395_0000631	1.0	1.0
1395_0000635	0.0	1.0
1395_0000636	0.0	1.0
1395_0000639	1.0	2.0
1395_0000642	0.0	1.0
1395_0000644	1.0	2.0
1395_0000646	1.0	1.0
1395_0000649	2.0	2.0
1395_0001010	1.0	2.0
1395_0001013	1.0	2.0
1395_0001015	1.0	2.0
1395_0001016	2.0	1.0
1395_0001017	1.0	1.0
1395_0001019	0.0	1.0
1395_0001020	1.0	2.0
1395_0001021	1.0	1.0
1395_0001022	1.0	2.0
1395_0001023	1.0	2.0
1395_0001024	1.0	1.0
1395_0001028	1.0	2.0
1395_0001033	1.0	2.0
1395_0001034	1.0	2.0
1395_0001040	0.0	1.0
1395_0001045	2.0	1.0
1395_0001058	1.0	1.0
1395_0001060	1.0	2.0
1395_0001061	1.0	2.0
1395_0001064	2.0	1.0
1395_0001065	1.0	2.0
1395_0001066	1.0	2.0
1395_0001067	1.0	1.0
1395_0001068	1.0	2.0
1395_0001069	2.0	2.0
1395_0001070	1.0	2.0
1395_0001071	1.0	1.0
1395_0001073	2.0	1.0
1395_0001074	1.0	1.0
1395_0001075	1.0	2.0
1395_0001076	0.0	2.0
1395_0001078	0.0	2.0
1395_0001080	1.0	2.0
1395_0001084	1.0	2.0
1395_0001090	2.0	2.0
1395_0001093	1.0	2.0
1395_0001101	1.0	2.0
1395_0001103	1.0	2.0
1395_0001104	0.0	2.0
1395_0001108	0.0	1.0
1395_0001109	0.0	2.0
1395_0001114	1.0	2.0
1395_0001115	1.0	1.0
1395_0001116	2.0	2.0
1395_0001117	1.0	1.0
1395_0001118	0.0	1.0
1395_0001119	1.0	2.0
1395_0001120	1.0	1.0
1395_0001121	0.0	1.0
1395_0001122	1.0	1.0
1395_0001123	1.0	2.0
1395_0001124	0.0	1.0
1395_0001126	1.0	2.0
1395_0001131	0.0	1.0
1395_0001132	1.0	2.0
1395_0001133	0.0	2.0
1395_0001141	1.0	1.0
1395_0001145	2.0	2.0
1395_0001146	0.0	1.0
1395_0001147	1.0	2.0
1395_0001149	0.0	1.0
1395_0001150	1.0	1.0
1395_0001158	2.0	1.0
1395_0001160	2.0	2.0
1395_0001161	1.0	1.0
1395_0001164	2.0	2.0
1395_0001167	1.0	2.0
1395_0001169	2.0	2.0
1395_0001170	1.0	1.0
1395_0001171	1.0	1.0
Language = IT, Weighted F1-score = 0.4741627408777692, Dimension = Grammaticalaccuracy

Train on DE, test on  CZ
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
144.90552995391704 65.35717405024758
LANGUAGE: IT
nan nan
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']


======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 1.26
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.43
              precision    recall  f1-score   support

         1.0       0.38      0.07      0.11        46
         2.0       0.73      0.29      0.41       269
         3.0       0.32      0.89      0.47       114
         4.0       0.00      0.00      0.00         5

    accuracy                           0.42       434
   macro avg       0.35      0.31      0.25       434
weighted avg       0.57      0.42      0.39       434

[[  3  19  24   0]
 [  2  77 190   0]
 [  3  10 101   0]
 [  0   0   5   0]]
0.3885722402731471
434 434 434



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 1.00
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.94
              precision    recall  f1-score   support

         1.0       0.67      0.04      0.08        46
         2.0       0.69      0.20      0.31       269
         3.0       0.15      0.31      0.20       114
         4.0       0.04      1.00      0.08         5

    accuracy                           0.22       434
   macro avg       0.39      0.39      0.17       434
weighted avg       0.54      0.22      0.25       434

[[  2  16  28   0]
 [  0  53 177  39]
 [  1   8  35  70]
 [  0   0   0   5]]
0.2514471143172272
434 434 434



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.83
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 2.05
              precision    recall  f1-score   support

         1.0       0.50      0.04      0.08        46
         2.0       0.74      0.36      0.49       269
         3.0       0.14      0.19      0.16       114
         4.0       0.04      1.00      0.07         5

    accuracy                           0.29       434
   macro avg       0.35      0.40      0.20       434
weighted avg       0.55      0.29      0.35       434

[[  2  23  21   0]
 [  0  98 116  55]
 [  2  11  22  79]
 [  0   0   0   5]]
0.3545671551934409
434 434 434



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.67
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 2.28
              precision    recall  f1-score   support

         1.0       1.00      0.04      0.08        46
         2.0       0.68      0.19      0.30       269
         3.0       0.16      0.34      0.22       114
         4.0       0.04      1.00      0.08         5

    accuracy                           0.23       434
   macro avg       0.47      0.39      0.17       434
weighted avg       0.57      0.23      0.25       434

[[  2  18  26   0]
 [  0  52 178  39]
 [  0   6  39  69]
 [  0   0   0   5]]
0.25404258120867856
434 434 434
Filename	True Label	Prediction
0601	2.0	3.0
0602	2.0	3.0
0603	2.0	3.0
0604	2.0	3.0
0605	2.0	3.0
0606	2.0	3.0
0607	3.0	3.0
0608	1.0	3.0
0609	2.0	3.0
0610	2.0	3.0
0611	2.0	3.0
0612	2.0	2.0
0613	2.0	2.0
0614	2.0	3.0
0615	2.0	3.0
0616	2.0	3.0
0617	2.0	3.0
0618	2.0	3.0
0619	2.0	3.0
0620	2.0	3.0
0621	2.0	3.0
0622	2.0	2.0
0623	2.0	3.0
0624	3.0	2.0
0625	2.0	2.0
0626	3.0	3.0
0627	2.0	3.0
0628	2.0	3.0
0629	2.0	3.0
0630	2.0	2.0
0631	2.0	3.0
0632	2.0	2.0
0633	3.0	3.0
0634	3.0	3.0
0635	2.0	3.0
0636	3.0	3.0
0637	2.0	3.0
0638	2.0	3.0
0639	2.0	3.0
0640	2.0	3.0
0641	1.0	2.0
0642	2.0	3.0
0643	2.0	3.0
0644	2.0	2.0
0645	3.0	3.0
0714	2.0	3.0
0715	2.0	3.0
0716	2.0	3.0
0717	2.0	2.0
0718	2.0	3.0
0719	2.0	3.0
0720	2.0	2.0
0721	3.0	3.0
0722	2.0	3.0
0723	2.0	3.0
0724	3.0	3.0
0725	2.0	3.0
0801	2.0	3.0
0802	2.0	3.0
0803	2.0	3.0
0804	2.0	2.0
0805	2.0	3.0
0806	2.0	3.0
0807	3.0	3.0
0808	2.0	2.0
0809	2.0	2.0
0810	2.0	2.0
0811	3.0	3.0
0812	2.0	2.0
0813	2.0	3.0
0814	2.0	2.0
0815	3.0	3.0
0816	3.0	3.0
0817	2.0	2.0
0818	2.0	3.0
0819	3.0	3.0
0820	1.0	2.0
0821	2.0	3.0
0822	2.0	3.0
0823	3.0	3.0
0824	2.0	3.0
0825	2.0	3.0
0826	2.0	2.0
0827	2.0	2.0
0828	2.0	3.0
0829	2.0	3.0
0901	3.0	3.0
0902	3.0	3.0
0903	2.0	3.0
0904	2.0	2.0
0905	2.0	3.0
0906	3.0	3.0
0907	2.0	3.0
0910	2.0	3.0
0911	2.0	3.0
0912	3.0	3.0
0913	2.0	3.0
0914	2.0	3.0
0915	2.0	3.0
0916	2.0	2.0
0917	2.0	3.0
0918	2.0	3.0
0919	2.0	3.0
0920	3.0	3.0
0921	2.0	3.0
0922	2.0	3.0
0923	2.0	3.0
0924	2.0	3.0
0925	2.0	3.0
0926	3.0	3.0
0927	3.0	3.0
0928	3.0	3.0
0929	1.0	2.0
0930	2.0	3.0
1001	2.0	3.0
1002	2.0	3.0
1003	1.0	3.0
1004	2.0	2.0
1005	2.0	3.0
1006	2.0	3.0
1007	2.0	3.0
1008	1.0	3.0
1009	2.0	3.0
1010	2.0	3.0
1014	3.0	3.0
1015	2.0	3.0
1016	2.0	3.0
1017	2.0	3.0
1018	2.0	3.0
1019	2.0	3.0
1020	2.0	3.0
1021	2.0	3.0
1022	2.0	3.0
1023	2.0	3.0
1111	2.0	3.0
1112	2.0	3.0
1113	2.0	3.0
1114	2.0	3.0
1115	2.0	3.0
1116	2.0	3.0
1117	2.0	3.0
9999	1.0	2.0
BER0609003	2.0	4.0
BER0611003	3.0	4.0
BER0611005	3.0	4.0
BER0611006	3.0	4.0
BER0611007	1.0	3.0
KYJ0611003A	2.0	2.0
KYJ0611004A	2.0	2.0
KYJ0611005A	1.0	2.0
KYJ0611005B	1.0	3.0
KYJ0611006A	1.0	2.0
KYJ0611006B	1.0	3.0
KYJ0611009A	2.0	2.0
KYJ0611009B	1.0	3.0
LIB0611001A	2.0	2.0
LIB0611001B	2.0	3.0
LIB0611002A	2.0	2.0
LIB0611002B	2.0	3.0
LIB0611003A	1.0	2.0
LIB0611004A	3.0	2.0
LIB0611004B	2.0	4.0
LIB0611011	1.0	3.0
LON0610002A	2.0	3.0
LON0610002B	2.0	3.0
LON0611002A	2.0	2.0
LON0611002B	1.0	3.0
LON0611003	3.0	4.0
LON0611004A	2.0	2.0
LON0611004B	1.0	2.0
MOS0509001	2.0	3.0
MOS0509004	2.0	3.0
MOS0611012	3.0	4.0
MOS0611013	3.0	3.0
MOS0611014	1.0	3.0
MOS0611015	3.0	4.0
PAR1011008A	2.0	2.0
PAR1011009A	2.0	2.0
PAR1011009B	2.0	3.0
PAR1011013	3.0	4.0
PAR1011014	2.0	4.0
PAR1011015	3.0	3.0
PAR1011016	3.0	4.0
PAR1011017	3.0	3.0
PAR1011018	2.0	3.0
PHA0111001A	2.0	2.0
PHA0111001B	2.0	4.0
PHA0111002A	3.0	2.0
PHA0111002B	2.0	3.0
PHA0111003A	1.0	2.0
PHA0111003B	2.0	3.0
PHA0111004A	3.0	2.0
PHA0111004B	2.0	3.0
PHA0111005A	3.0	3.0
PHA0111005B	2.0	3.0
PHA0111010	3.0	4.0
PHA0111011	2.0	3.0
PHA0111012	2.0	3.0
PHA0111014	1.0	3.0
PHA0111015	3.0	4.0
PHA0111016	3.0	4.0
PHA0111018	2.0	3.0
PHA0112002A	2.0	2.0
PHA0112002B	2.0	4.0
PHA0112003A	2.0	2.0
PHA0112003B	1.0	2.0
PHA0112006A	3.0	3.0
PHA0112006B	3.0	4.0
PHA0112007A	2.0	2.0
PHA0112007B	1.0	3.0
PHA0112009A	2.0	2.0
PHA0112009B	2.0	3.0
PHA0112012A	2.0	3.0
PHA0112012B	2.0	3.0
PHA0209001	2.0	3.0
PHA0209008	2.0	3.0
PHA0209013	2.0	2.0
PHA0209024	1.0	3.0
PHA0209026	3.0	4.0
PHA0209028	3.0	3.0
PHA0209031	3.0	4.0
PHA0209034	2.0	3.0
PHA0209038	3.0	4.0
PHA0209039	2.0	4.0
PHA0210001	2.0	3.0
PHA0210004	2.0	4.0
PHA0210007	2.0	3.0
PHA0210008	1.0	3.0
PHA0411008A	2.0	2.0
PHA0411008B	2.0	2.0
PHA0411009A	2.0	2.0
PHA0411009B	1.0	3.0
PHA0411010A	2.0	2.0
PHA0411010B	2.0	3.0
PHA0411011A	3.0	2.0
PHA0411011B	2.0	3.0
PHA0411012A	2.0	2.0
PHA0411012B	2.0	3.0
PHA0411027	2.0	4.0
PHA0411028	2.0	3.0
PHA0411029	2.0	2.0
PHA0411030	4.0	4.0
PHA0411031	3.0	4.0
PHA0411032	2.0	4.0
PHA0411033	2.0	3.0
PHA0411034	1.0	2.0
PHA0411035	3.0	3.0
PHA0411036	2.0	3.0
PHA0411037	2.0	3.0
PHA0411038	3.0	4.0
PHA0411039	2.0	3.0
PHA0411041	2.0	4.0
PHA0411042	3.0	4.0
PHA0411043	2.0	3.0
PHA0411044	4.0	4.0
PHA0411045	2.0	3.0
PHA0411047	2.0	4.0
PHA0411051	3.0	4.0
PHA0411053	3.0	4.0
PHA0411054	3.0	3.0
PHA0411055	3.0	4.0
PHA0411056	3.0	4.0
PHA0411058	3.0	4.0
PHA0411059	3.0	4.0
PHA0411060	3.0	4.0
PHA0411061	3.0	4.0
PHA0411062	2.0	4.0
PHA0509002	1.0	3.0
PHA0509007	2.0	3.0
PHA0509013	1.0	3.0
PHA0509015	2.0	3.0
PHA0509017	2.0	4.0
PHA0509018	3.0	4.0
PHA0509019	2.0	3.0
PHA0509020	3.0	4.0
PHA0509021	3.0	4.0
PHA0509022	3.0	4.0
PHA0509024	2.0	3.0
PHA0509025	4.0	4.0
PHA0509026	3.0	4.0
PHA0509027	2.0	3.0
PHA0509028	3.0	4.0
PHA0509030	3.0	4.0
PHA0509031	2.0	3.0
PHA0509032	3.0	4.0
PHA0509033	2.0	3.0
PHA0509034	2.0	4.0
PHA0509035	2.0	3.0
PHA0509036	3.0	4.0
PHA0509037	3.0	4.0
PHA0509038	2.0	3.0
PHA0509039	3.0	4.0
PHA0509040	2.0	3.0
PHA0509041	3.0	4.0
PHA0509042	3.0	4.0
PHA0509043	2.0	4.0
PHA0509044	2.0	4.0
PHA0509045	2.0	3.0
PHA0510002A	2.0	2.0
PHA0510002B	2.0	3.0
PHA0510003A	2.0	2.0
PHA0510003B	2.0	3.0
PHA0510004A	1.0	2.0
PHA0510004B	1.0	3.0
PHA0510010A	1.0	1.0
PHA0510010B	1.0	1.0
PHA0510013A	2.0	2.0
PHA0510013B	2.0	3.0
PHA0510023	3.0	4.0
PHA0510027	2.0	2.0
PHA0510029	3.0	4.0
PHA0510030	3.0	4.0
PHA0510031	2.0	2.0
PHA0510032	3.0	4.0
PHA0510034	3.0	3.0
PHA0510035	3.0	4.0
PHA0510036	3.0	4.0
PHA0510037	2.0	3.0
PHA0510038	3.0	4.0
PHA0510039	2.0	4.0
PHA0510040	3.0	4.0
PHA0510046	2.0	3.0
PHA0510047	2.0	3.0
PHA0510048	2.0	3.0
PHA0510049	3.0	3.0
PHA0510050	3.0	4.0
PHA0610005A	2.0	2.0
PHA0610005B	1.0	2.0
PHA0610006A	1.0	2.0
PHA0610006B	1.0	3.0
PHA0610007A	2.0	2.0
PHA0610007B	2.0	3.0
PHA0610015	3.0	3.0
PHA0610016	3.0	3.0
PHA0610017	3.0	4.0
PHA0610018	3.0	4.0
PHA0610019A	1.0	2.0
PHA0610019B	2.0	4.0
PHA0610025	3.0	4.0
PHA0610026	2.0	4.0
PHA0709008	2.0	3.0
PHA0710009	2.0	3.0
PHA0710010	3.0	4.0
PHA0710011	3.0	4.0
PHA0710012	3.0	4.0
PHA0710013	3.0	4.0
PHA0710014	3.0	4.0
PHA0710015	3.0	3.0
PHA0710016	3.0	3.0
PHA0710017	2.0	4.0
PHA0710018	3.0	4.0
PHA0710019	2.0	4.0
PHA0710021	3.0	4.0
PHA0809009	2.0	3.0
PHA0809010	2.0	3.0
PHA0810001	3.0	4.0
PHA0810002	2.0	3.0
PHA0810003	2.0	4.0
PHA0810004	2.0	3.0
PHA0810006	2.0	3.0
PHA0810008	2.0	4.0
PHA0810009	2.0	4.0
PHA0810010	2.0	3.0
PHA0810011	2.0	3.0
PHA0810012	2.0	4.0
PHA0810015	4.0	4.0
PHA0811010	2.0	3.0
PHA0811012	4.0	4.0
PHA0811013	3.0	4.0
PHA0811014	2.0	3.0
PHA0811016	2.0	3.0
PHA0811017	3.0	3.0
PHA0811019	3.0	4.0
PHA0811020	2.0	4.0
PHA1109001	2.0	3.0
PHA1109002	3.0	4.0
PHA1109003	2.0	3.0
PHA1109004	2.0	4.0
PHA1109005	2.0	3.0
PHA1109006	3.0	3.0
PHA1109007	2.0	3.0
PHA1109008	1.0	2.0
PHA1109023	2.0	3.0
PHA1109024	3.0	4.0
PHA1109025	1.0	3.0
PHA1109026	2.0	4.0
PHA1109027	2.0	4.0
PHA1109028	2.0	4.0
PHA1110001A	2.0	3.0
PHA1110001B	2.0	3.0
PHA1110002A	2.0	3.0
PHA1110002B	2.0	3.0
PHA1110003A	2.0	3.0
PHA1110003B	2.0	4.0
PHA1110004A	2.0	3.0
PHA1110013	3.0	4.0
PHA1110014	2.0	4.0
PHA1110015	3.0	4.0
PHA1110016	1.0	2.0
PHA1110017	2.0	3.0
PHA1110019	3.0	4.0
PHA1110021	2.0	3.0
PHA1110022	3.0	4.0
PHA1111001A	2.0	3.0
PHA1111001B	1.0	3.0
PHA1111002A	2.0	2.0
PHA1111002B	1.0	2.0
PHA1111003A	3.0	2.0
PHA1111003B	2.0	3.0
PHA1111004A	1.0	3.0
PHA1111004B	1.0	3.0
PHA1111006A	2.0	2.0
PHA1111006B	1.0	3.0
PHA1111008A	2.0	2.0
PHA1111008B	2.0	3.0
PHA1111009A	1.0	3.0
ST071122B	2.0	4.0
TI071122B	2.0	4.0
VAR0209036	3.0	4.0
VAR0909003	2.0	3.0
VAR0909004	2.0	3.0
VAR0909005	2.0	3.0
VAR0909006	3.0	4.0
VAR0909007	2.0	3.0
VAR0909008	2.0	3.0
VAR0909009	2.0	4.0
VAR0909010	2.0	3.0
VAR0910004	2.0	4.0
VAR0910005	1.0	3.0
VAR0910006	2.0	3.0
VAR0910007	2.0	4.0
VAR0910009	2.0	4.0
VAR0910010	2.0	4.0
VAR0910011	2.0	3.0
Language = CZ, Weighted F1-score = 0.25404258120867856, Dimension = Orthography

Train on DE, test on  IT
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
nan nan
LANGUAGE: IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']


======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 1.33
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.66
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.36      0.10      0.15       143
         2.0       0.42      0.89      0.57       220
         3.0       0.14      0.55      0.23        77
         4.0       0.00      0.00      0.00       262
         5.0       0.00      0.00      0.00        76

    accuracy                           0.31       800
   macro avg       0.15      0.25      0.16       800
weighted avg       0.19      0.31      0.21       800

[[  0   7  15   0   0   0]
 [  0  14 128   1   0   0]
 [  0  13 195  12   0   0]
 [  0   1  34  42   0   0]
 [  0   2  62 198   0   0]
 [  0   2  33  41   0   0]]
0.20540598994809262
800 800 800



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 1.04
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.72
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.48      0.19      0.27       143
         2.0       0.44      0.81      0.57       220
         3.0       0.14      0.61      0.23        77
         4.0       1.00      0.00      0.01       262
         5.0       0.00      0.00      0.00        76

    accuracy                           0.32       800
   macro avg       0.34      0.27      0.18       800
weighted avg       0.55      0.32      0.23       800

[[  0   9  13   0   0   0]
 [  0  27 106  10   0   0]
 [  0  15 179  26   0   0]
 [  0   1  29  47   0   0]
 [  0   2  52 207   1   0]
 [  0   2  26  48   0   0]]
0.2303167242414832
800 800 800



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.89
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.80
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.53      0.13      0.21       143
         2.0       0.43      0.88      0.58       220
         3.0       0.13      0.52      0.21        77
         4.0       0.58      0.03      0.05       262
         5.0       0.00      0.00      0.00        76

    accuracy                           0.33       800
   macro avg       0.28      0.26      0.18       800
weighted avg       0.42      0.33      0.23       800

[[  0   8  14   0   0   0]
 [  0  19 119   5   0   0]
 [  0   8 194  18   0   0]
 [  0   0  34  40   3   0]
 [  0   0  62 193   7   0]
 [  0   1  27  46   2   0]]
0.2342508580512139
800 800 800



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.73
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.81
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.44      0.08      0.13       143
         2.0       0.42      0.53      0.47       220
         3.0       0.14      0.74      0.24        77
         4.0       0.69      0.27      0.39       262
         5.0       0.00      0.00      0.00        76

    accuracy                           0.32       800
   macro avg       0.28      0.27      0.20       800
weighted avg       0.44      0.32      0.30       800

[[  0   6  15   1   0   0]
 [  0  11  99  32   1   0]
 [  0   7 117  94   2   0]
 [  0   0  12  57   8   0]
 [  0   0  24 168  70   0]
 [  0   1  11  44  20   0]]
0.3021308199286259
800 800 800
Filename	True Label	Prediction
1325_1001008	5.0	3.0
1325_1001009	5.0	3.0
1325_1001010	5.0	3.0
1325_1001011	4.0	3.0
1325_1001012	4.0	3.0
1325_1001013	4.0	3.0
1325_1001014	4.0	3.0
1325_1001015	4.0	3.0
1325_1001016	5.0	3.0
1325_1001017	4.0	3.0
1325_1001018	4.0	3.0
1325_1001019	4.0	3.0
1325_1001020	4.0	3.0
1325_1001021	5.0	4.0
1325_1001022	4.0	3.0
1325_1001023	4.0	3.0
1325_1001024	4.0	3.0
1325_1001025	4.0	3.0
1325_1001027	4.0	3.0
1325_1001028	4.0	4.0
1325_1001029	4.0	4.0
1325_1001032	4.0	3.0
1325_1001033	5.0	4.0
1325_1001035	5.0	4.0
1325_1001036	5.0	4.0
1325_1001037	5.0	4.0
1325_1001039	5.0	4.0
1325_1001040	4.0	3.0
1325_1001041	4.0	4.0
1325_1001042	5.0	4.0
1325_1001043	4.0	3.0
1325_1001044	3.0	3.0
1325_1001045	5.0	4.0
1325_1001046	4.0	4.0
1325_1001047	4.0	3.0
1325_1001048	4.0	3.0
1325_1001050	4.0	4.0
1325_1001051	4.0	4.0
1325_1001052	5.0	3.0
1325_1001053	5.0	3.0
1325_1001054	4.0	3.0
1325_1001055	5.0	3.0
1325_1001056	4.0	3.0
1325_1001057	4.0	3.0
1325_1001058	4.0	3.0
1325_1001059	4.0	3.0
1325_1001062	4.0	4.0
1325_1001063	4.0	4.0
1325_1001075	3.0	3.0
1325_1001076	5.0	3.0
1325_1001077	4.0	3.0
1325_1001078	4.0	3.0
1325_1001079	4.0	4.0
1325_1001080	4.0	4.0
1325_1001081	4.0	3.0
1325_1001082	5.0	3.0
1325_1001083	5.0	3.0
1325_1001084	4.0	4.0
1325_1001085	4.0	3.0
1325_1001086	4.0	4.0
1325_1001087	4.0	3.0
1325_1001088	4.0	3.0
1325_1001089	3.0	3.0
1325_1001090	4.0	3.0
1325_1001091	4.0	4.0
1325_1001092	4.0	3.0
1325_1001093	4.0	4.0
1325_1001094	4.0	3.0
1325_1001095	5.0	3.0
1325_1001096	4.0	3.0
1325_1001097	1.0	3.0
1325_1001098	4.0	4.0
1325_1001099	4.0	3.0
1325_1001100	4.0	3.0
1325_1001101	4.0	3.0
1325_1001107	4.0	4.0
1325_1001108	4.0	4.0
1325_1001109	4.0	3.0
1325_1001110	4.0	4.0
1325_1001111	4.0	4.0
1325_1001113	4.0	3.0
1325_1001119	4.0	4.0
1325_1001120	5.0	4.0
1325_1001121	4.0	4.0
1325_1001122	4.0	3.0
1325_1001123	4.0	3.0
1325_1001124	3.0	3.0
1325_1001125	4.0	3.0
1325_1001126	3.0	3.0
1325_1001127	4.0	3.0
1325_1001128	4.0	4.0
1325_1001129	3.0	4.0
1325_1001130	4.0	3.0
1325_1001131	3.0	4.0
1325_1001132	4.0	4.0
1325_1001133	5.0	3.0
1325_1001134	4.0	4.0
1325_1001135	3.0	4.0
1325_1001136	3.0	4.0
1325_1001138	4.0	4.0
1325_1001139	4.0	3.0
1325_1001141	2.0	3.0
1325_1001142	4.0	4.0
1325_1001143	4.0	3.0
1325_1001144	4.0	3.0
1325_1001152	4.0	3.0
1325_1001153	4.0	3.0
1325_1001154	4.0	4.0
1325_1001155	4.0	4.0
1325_1001156	4.0	4.0
1325_1001157	4.0	4.0
1325_1001158	4.0	3.0
1325_1001159	5.0	4.0
1325_1001160	4.0	4.0
1325_1001161	4.0	3.0
1325_1001162	4.0	3.0
1325_1001163	4.0	4.0
1325_1001164	3.0	3.0
1325_1001165	4.0	3.0
1325_1001166	4.0	3.0
1325_1001167	4.0	3.0
1325_1001168	4.0	4.0
1325_1001169	4.0	3.0
1325_1001170	4.0	4.0
1325_9000059	5.0	3.0
1325_9000087	5.0	3.0
1325_9000088	4.0	4.0
1325_9000089	4.0	3.0
1325_9000090	4.0	4.0
1325_9000095	4.0	4.0
1325_9000099	5.0	3.0
1325_9000102	3.0	2.0
1325_9000104	3.0	3.0
1325_9000105	4.0	3.0
1325_9000106	4.0	3.0
1325_9000107	4.0	4.0
1325_9000136	4.0	4.0
1325_9000137	4.0	4.0
1325_9000138	4.0	4.0
1325_9000139	4.0	3.0
1325_9000140	4.0	4.0
1325_9000143	4.0	3.0
1325_9000144	4.0	3.0
1325_9000152	4.0	4.0
1325_9000185	4.0	3.0
1325_9000186	4.0	4.0
1325_9000187	4.0	4.0
1325_9000188	4.0	4.0
1325_9000209	4.0	3.0
1325_9000210	3.0	3.0
1325_9000211	4.0	3.0
1325_9000213	4.0	3.0
1325_9000214	4.0	3.0
1325_9000215	4.0	3.0
1325_9000237	3.0	3.0
1325_9000239	3.0	3.0
1325_9000240	2.0	3.0
1325_9000241	4.0	4.0
1325_9000278	3.0	3.0
1325_9000279	3.0	3.0
1325_9000296	3.0	3.0
1325_9000302	3.0	3.0
1325_9000303	4.0	3.0
1325_9000304	3.0	3.0
1325_9000314	4.0	3.0
1325_9000315	3.0	3.0
1325_9000316	3.0	3.0
1325_9000317	4.0	4.0
1325_9000318	4.0	3.0
1325_9000319	4.0	3.0
1325_9000320	3.0	3.0
1325_9000321	4.0	4.0
1325_9000322	4.0	3.0
1325_9000323	4.0	3.0
1325_9000503	4.0	4.0
1325_9000504	4.0	4.0
1325_9000505	4.0	3.0
1325_9000533	4.0	4.0
1325_9000534	4.0	4.0
1325_9000536	4.0	3.0
1325_9000554	3.0	3.0
1325_9000601	4.0	4.0
1325_9000602	4.0	3.0
1325_9000611	3.0	3.0
1325_9000612	2.0	3.0
1325_9000674	4.0	4.0
1325_9000675	4.0	3.0
1325_9000676	3.0	3.0
1325_9000677	4.0	4.0
1325_9000678	4.0	4.0
1325_9000684	4.0	3.0
1325_9000685	4.0	4.0
1325_9000686	3.0	4.0
1325_9000700	4.0	3.0
1325_9000750	4.0	2.0
1365_0100002	4.0	3.0
1365_0100003	2.0	2.0
1365_0100004	3.0	3.0
1365_0100005	3.0	2.0
1365_0100006	4.0	3.0
1365_0100007	2.0	2.0
1365_0100008	4.0	2.0
1365_0100009	3.0	2.0
1365_0100010	1.0	2.0
1365_0100011	4.0	3.0
1365_0100012	4.0	2.0
1365_0100013	4.0	3.0
1365_0100014	4.0	2.0
1365_0100015	4.0	2.0
1365_0100016	4.0	3.0
1365_0100017	4.0	3.0
1365_0100018	3.0	2.0
1365_0100019	4.0	2.0
1365_0100020	3.0	3.0
1365_0100021	3.0	2.0
1365_0100022	4.0	3.0
1365_0100023	2.0	3.0
1365_0100024	4.0	3.0
1365_0100026	2.0	1.0
1365_0100027	4.0	3.0
1365_0100028	4.0	2.0
1365_0100029	1.0	2.0
1365_0100030	4.0	2.0
1365_0100031	5.0	2.0
1365_0100051	2.0	3.0
1365_0100056	4.0	3.0
1365_0100057	4.0	3.0
1365_0100058	4.0	4.0
1365_0100061	5.0	3.0
1365_0100063	4.0	3.0
1365_0100064	4.0	3.0
1365_0100065	2.0	2.0
1365_0100066	3.0	3.0
1365_0100067	3.0	3.0
1365_0100069	4.0	3.0
1365_0100070	4.0	3.0
1365_0100071	4.0	3.0
1365_0100072	4.0	3.0
1365_0100073	3.0	3.0
1365_0100074	3.0	3.0
1365_0100079	5.0	3.0
1365_0100080	4.0	3.0
1365_0100092	4.0	3.0
1365_0100093	4.0	3.0
1365_0100094	5.0	2.0
1365_0100095	4.0	2.0
1365_0100096	4.0	3.0
1365_0100097	4.0	3.0
1365_0100098	2.0	3.0
1365_0100099	3.0	3.0
1365_0100100	4.0	4.0
1365_0100101	4.0	3.0
1365_0100102	4.0	3.0
1365_0100103	5.0	3.0
1365_0100104	3.0	4.0
1365_0100105	5.0	4.0
1365_0100106	3.0	3.0
1365_0100107	4.0	4.0
1365_0100116	4.0	3.0
1365_0100117	5.0	3.0
1365_0100118	4.0	3.0
1365_0100119	5.0	4.0
1365_0100120	5.0	4.0
1365_0100121	4.0	3.0
1365_0100123	4.0	3.0
1365_0100125	4.0	3.0
1365_0100133	5.0	3.0
1365_0100134	5.0	3.0
1365_0100135	4.0	3.0
1365_0100136	4.0	3.0
1365_0100137	5.0	2.0
1365_0100138	5.0	2.0
1365_0100139	4.0	2.0
1365_0100145	5.0	4.0
1365_0100146	4.0	3.0
1365_0100147	4.0	3.0
1365_0100148	4.0	3.0
1365_0100151	4.0	2.0
1365_0100162	4.0	4.0
1365_0100163	5.0	4.0
1365_0100164	3.0	3.0
1365_0100165	4.0	3.0
1365_0100166	3.0	3.0
1365_0100167	2.0	3.0
1365_0100168	4.0	3.0
1365_0100169	5.0	3.0
1365_0100170	4.0	3.0
1365_0100171	4.0	3.0
1365_0100172	5.0	3.0
1365_0100173	5.0	3.0
1365_0100174	3.0	2.0
1365_0100175	4.0	3.0
1365_0100176	4.0	3.0
1365_0100177	5.0	3.0
1365_0100178	4.0	3.0
1365_0100179	4.0	3.0
1365_0100180	5.0	2.0
1365_0100181	3.0	3.0
1365_0100182	4.0	3.0
1365_0100183	4.0	3.0
1365_0100184	4.0	3.0
1365_0100185	3.0	3.0
1365_0100186	5.0	3.0
1365_0100187	5.0	2.0
1365_0100188	5.0	3.0
1365_0100190	4.0	2.0
1365_0100191	3.0	3.0
1365_0100192	4.0	3.0
1365_0100194	4.0	3.0
1365_0100195	4.0	3.0
1365_0100196	4.0	3.0
1365_0100198	4.0	2.0
1365_0100199	4.0	3.0
1365_0100200	5.0	3.0
1365_0100201	5.0	2.0
1365_0100202	3.0	2.0
1365_0100203	4.0	3.0
1365_0100204	5.0	2.0
1365_0100205	5.0	2.0
1365_0100211	4.0	3.0
1365_0100212	5.0	4.0
1365_0100213	4.0	2.0
1365_0100215	4.0	3.0
1365_0100217	5.0	4.0
1365_0100218	4.0	3.0
1365_0100219	4.0	3.0
1365_0100220	4.0	3.0
1365_0100221	3.0	3.0
1365_0100222	3.0	3.0
1365_0100223	4.0	4.0
1365_0100224	3.0	4.0
1365_0100225	4.0	3.0
1365_0100226	5.0	3.0
1365_0100227	4.0	3.0
1365_0100228	3.0	3.0
1365_0100229	5.0	4.0
1365_0100230	4.0	4.0
1365_0100231	4.0	3.0
1365_0100232	4.0	3.0
1365_0100233	4.0	3.0
1365_0100251	3.0	3.0
1365_0100252	4.0	3.0
1365_0100253	3.0	3.0
1365_0100255	3.0	3.0
1365_0100256	4.0	2.0
1365_0100257	3.0	3.0
1365_0100258	5.0	3.0
1365_0100259	4.0	3.0
1365_0100260	4.0	2.0
1365_0100261	5.0	3.0
1365_0100262	4.0	3.0
1365_0100263	4.0	3.0
1365_0100265	4.0	3.0
1365_0100266	3.0	3.0
1365_0100267	4.0	3.0
1365_0100268	3.0	3.0
1365_0100269	4.0	3.0
1365_0100270	3.0	3.0
1365_0100274	4.0	4.0
1365_0100275	4.0	3.0
1365_0100276	4.0	3.0
1365_0100277	4.0	4.0
1365_0100278	4.0	3.0
1365_0100279	4.0	3.0
1365_0100280	2.0	3.0
1365_0100281	5.0	3.0
1365_0100282	5.0	4.0
1365_0100285	3.0	3.0
1365_0100286	3.0	2.0
1365_0100287	4.0	2.0
1365_0100288	4.0	2.0
1365_0100289	5.0	2.0
1365_0100290	4.0	3.0
1365_0100299	5.0	3.0
1365_0100447	5.0	3.0
1365_0100448	2.0	3.0
1365_0100451	2.0	3.0
1365_0100455	2.0	4.0
1365_0100456	2.0	4.0
1365_0100457	2.0	3.0
1365_0100458	4.0	4.0
1365_0100459	4.0	3.0
1365_0100461	2.0	3.0
1365_0100469	5.0	3.0
1365_0100470	4.0	4.0
1365_0100471	3.0	4.0
1365_0100472	3.0	3.0
1365_0100473	4.0	3.0
1365_0100474	4.0	3.0
1365_0100475	4.0	4.0
1365_0100476	4.0	3.0
1365_0100477	4.0	3.0
1365_0100478	4.0	4.0
1365_0100479	4.0	3.0
1365_0100480	5.0	3.0
1365_0100481	4.0	3.0
1365_0100482	4.0	3.0
1385_0000011	1.0	2.0
1385_0000012	2.0	2.0
1385_0000013	1.0	2.0
1385_0000016	1.0	2.0
1385_0000017	1.0	2.0
1385_0000020	2.0	2.0
1385_0000021	1.0	2.0
1385_0000022	0.0	2.0
1385_0000023	2.0	3.0
1385_0000033	1.0	3.0
1385_0000034	2.0	3.0
1385_0000035	2.0	2.0
1385_0000036	2.0	3.0
1385_0000037	2.0	3.0
1385_0000038	2.0	2.0
1385_0000039	2.0	2.0
1385_0000040	1.0	2.0
1385_0000041	2.0	3.0
1385_0000042	2.0	2.0
1385_0000043	2.0	2.0
1385_0000044	2.0	3.0
1385_0000045	2.0	3.0
1385_0000047	2.0	3.0
1385_0000048	2.0	3.0
1385_0000049	2.0	2.0
1385_0000050	2.0	2.0
1385_0000051	2.0	3.0
1385_0000052	1.0	1.0
1385_0000053	1.0	2.0
1385_0000054	2.0	3.0
1385_0000057	1.0	2.0
1385_0000058	2.0	3.0
1385_0000059	2.0	2.0
1385_0000095	1.0	2.0
1385_0000097	2.0	2.0
1385_0000098	2.0	2.0
1385_0000099	1.0	2.0
1385_0000100	2.0	2.0
1385_0000101	1.0	1.0
1385_0000102	2.0	3.0
1385_0000103	2.0	1.0
1385_0000104	2.0	2.0
1385_0000114	2.0	3.0
1385_0000119	2.0	3.0
1385_0000120	1.0	2.0
1385_0000122	2.0	2.0
1385_0000123	2.0	3.0
1385_0000124	2.0	3.0
1385_0000125	2.0	2.0
1385_0000126	2.0	2.0
1385_0000127	2.0	2.0
1385_0000128	1.0	2.0
1385_0000129	2.0	2.0
1385_0000130	2.0	2.0
1385_0001103	2.0	2.0
1385_0001104	1.0	2.0
1385_0001105	2.0	2.0
1385_0001107	2.0	2.0
1385_0001108	2.0	3.0
1385_0001109	2.0	2.0
1385_0001110	2.0	2.0
1385_0001111	2.0	2.0
1385_0001112	2.0	2.0
1385_0001113	1.0	2.0
1385_0001118	2.0	2.0
1385_0001119	2.0	2.0
1385_0001120	2.0	1.0
1385_0001121	2.0	2.0
1385_0001122	2.0	1.0
1385_0001123	2.0	1.0
1385_0001124	1.0	2.0
1385_0001125	2.0	2.0
1385_0001126	1.0	1.0
1385_0001127	2.0	3.0
1385_0001128	1.0	2.0
1385_0001129	1.0	2.0
1385_0001130	1.0	1.0
1385_0001131	2.0	2.0
1385_0001132	2.0	2.0
1385_0001133	2.0	2.0
1385_0001134	2.0	2.0
1385_0001135	2.0	2.0
1385_0001136	2.0	2.0
1385_0001137	2.0	2.0
1385_0001138	2.0	2.0
1385_0001147	2.0	2.0
1385_0001148	2.0	3.0
1385_0001149	2.0	3.0
1385_0001150	2.0	2.0
1385_0001151	2.0	3.0
1385_0001152	2.0	3.0
1385_0001153	2.0	2.0
1385_0001154	2.0	3.0
1385_0001155	2.0	2.0
1385_0001156	2.0	2.0
1385_0001157	2.0	1.0
1385_0001158	2.0	3.0
1385_0001159	1.0	2.0
1385_0001160	1.0	4.0
1385_0001161	2.0	3.0
1385_0001162	1.0	2.0
1385_0001163	1.0	2.0
1385_0001164	1.0	3.0
1385_0001165	2.0	2.0
1385_0001166	2.0	2.0
1385_0001167	2.0	3.0
1385_0001169	2.0	2.0
1385_0001170	1.0	1.0
1385_0001171	1.0	2.0
1385_0001172	1.0	2.0
1385_0001173	0.0	2.0
1385_0001174	1.0	2.0
1385_0001175	1.0	2.0
1385_0001178	1.0	2.0
1385_0001188	2.0	2.0
1385_0001189	1.0	2.0
1385_0001190	1.0	2.0
1385_0001191	2.0	2.0
1385_0001192	1.0	2.0
1385_0001193	2.0	2.0
1385_0001194	2.0	2.0
1385_0001195	2.0	3.0
1385_0001196	2.0	2.0
1385_0001197	2.0	2.0
1385_0001198	2.0	3.0
1385_0001199	2.0	2.0
1385_0001501	1.0	2.0
1385_0001503	2.0	3.0
1385_0001522	1.0	2.0
1385_0001523	2.0	2.0
1385_0001524	1.0	2.0
1385_0001525	2.0	3.0
1385_0001526	0.0	2.0
1385_0001527	2.0	2.0
1385_0001528	2.0	3.0
1385_0001712	1.0	3.0
1385_0001714	1.0	2.0
1385_0001715	1.0	2.0
1385_0001716	2.0	2.0
1385_0001717	2.0	2.0
1385_0001718	1.0	2.0
1385_0001719	1.0	2.0
1385_0001720	1.0	2.0
1385_0001723	0.0	2.0
1385_0001724	2.0	3.0
1385_0001725	1.0	2.0
1385_0001726	1.0	3.0
1385_0001727	1.0	2.0
1385_0001728	2.0	2.0
1385_0001729	2.0	3.0
1385_0001730	2.0	3.0
1385_0001732	1.0	3.0
1385_0001733	1.0	2.0
1385_0001734	2.0	3.0
1385_0001736	2.0	2.0
1385_0001737	2.0	3.0
1385_0001738	0.0	1.0
1385_0001739	1.0	3.0
1385_0001740	2.0	2.0
1385_0001741	1.0	2.0
1385_0001742	0.0	1.0
1385_0001744	0.0	2.0
1385_0001746	1.0	2.0
1385_0001747	0.0	2.0
1385_0001748	1.0	2.0
1385_0001749	1.0	2.0
1385_0001750	0.0	1.0
1385_0001751	1.0	3.0
1385_0001752	1.0	2.0
1385_0001753	1.0	2.0
1385_0001754	1.0	2.0
1385_0001756	2.0	2.0
1385_0001757	2.0	2.0
1385_0001758	1.0	2.0
1385_0001759	1.0	2.0
1385_0001760	0.0	2.0
1385_0001761	1.0	2.0
1385_0001762	2.0	2.0
1385_0001764	1.0	2.0
1385_0001765	0.0	1.0
1385_0001766	2.0	3.0
1385_0001767	1.0	2.0
1385_0001768	2.0	2.0
1385_0001771	1.0	2.0
1385_0001772	1.0	2.0
1385_0001773	1.0	2.0
1385_0001774	1.0	1.0
1385_0001775	2.0	2.0
1385_0001785	1.0	2.0
1385_0001786	2.0	3.0
1385_0001787	1.0	2.0
1385_0001788	1.0	2.0
1385_0001789	2.0	3.0
1385_0001790	2.0	3.0
1385_0001791	1.0	2.0
1385_0001792	1.0	3.0
1385_0001793	1.0	3.0
1385_0001794	1.0	2.0
1385_0001795	1.0	1.0
1385_0001796	1.0	2.0
1385_0001798	1.0	3.0
1385_0001799	2.0	3.0
1385_0001800	1.0	2.0
1395_0000333	2.0	3.0
1395_0000337	1.0	2.0
1395_0000338	2.0	2.0
1395_0000340	2.0	3.0
1395_0000341	2.0	2.0
1395_0000353	2.0	2.0
1395_0000354	1.0	2.0
1395_0000355	2.0	2.0
1395_0000356	2.0	2.0
1395_0000357	3.0	3.0
1395_0000359	3.0	3.0
1395_0000360	3.0	3.0
1395_0000361	4.0	2.0
1395_0000364	2.0	3.0
1395_0000365	3.0	3.0
1395_0000366	3.0	3.0
1395_0000368	1.0	1.0
1395_0000369	4.0	3.0
1395_0000376	5.0	3.0
1395_0000378	2.0	2.0
1395_0000379	1.0	2.0
1395_0000380	5.0	3.0
1395_0000383	5.0	3.0
1395_0000387	5.0	3.0
1395_0000388	4.0	2.0
1395_0000389	1.0	1.0
1395_0000390	1.0	1.0
1395_0000391	3.0	3.0
1395_0000392	5.0	3.0
1395_0000396	2.0	2.0
1395_0000398	5.0	2.0
1395_0000399	2.0	2.0
1395_0000402	2.0	2.0
1395_0000403	5.0	3.0
1395_0000404	4.0	3.0
1395_0000409	3.0	2.0
1395_0000413	2.0	3.0
1395_0000414	2.0	2.0
1395_0000415	2.0	2.0
1395_0000432	5.0	3.0
1395_0000438	5.0	4.0
1395_0000443	5.0	3.0
1395_0000446	3.0	3.0
1395_0000447	2.0	3.0
1395_0000448	2.0	3.0
1395_0000449	4.0	3.0
1395_0000450	2.0	2.0
1395_0000451	2.0	2.0
1395_0000452	2.0	1.0
1395_0000454	2.0	3.0
1395_0000455	2.0	3.0
1395_0000458	2.0	2.0
1395_0000460	2.0	2.0
1395_0000462	4.0	2.0
1395_0000465	2.0	2.0
1395_0000469	2.0	2.0
1395_0000470	2.0	2.0
1395_0000471	2.0	2.0
1395_0000499	2.0	3.0
1395_0000500	2.0	2.0
1395_0000504	2.0	3.0
1395_0000512	3.0	3.0
1395_0000513	4.0	3.0
1395_0000514	4.0	3.0
1395_0000515	4.0	2.0
1395_0000516	1.0	2.0
1395_0000518	4.0	3.0
1395_0000525	5.0	1.0
1395_0000526	2.0	2.0
1395_0000527	1.0	2.0
1395_0000528	4.0	2.0
1395_0000529	2.0	2.0
1395_0000531	2.0	2.0
1395_0000533	4.0	2.0
1395_0000534	2.0	2.0
1395_0000535	2.0	2.0
1395_0000537	2.0	2.0
1395_0000547	3.0	2.0
1395_0000548	2.0	3.0
1395_0000549	2.0	3.0
1395_0000550	2.0	3.0
1395_0000551	3.0	2.0
1395_0000552	3.0	3.0
1395_0000553	2.0	2.0
1395_0000554	2.0	2.0
1395_0000555	2.0	2.0
1395_0000556	2.0	3.0
1395_0000557	4.0	3.0
1395_0000559	2.0	3.0
1395_0000560	3.0	3.0
1395_0000563	2.0	2.0
1395_0000564	2.0	2.0
1395_0000565	2.0	2.0
1395_0000572	2.0	3.0
1395_0000575	2.0	3.0
1395_0000579	1.0	2.0
1395_0000581	1.0	3.0
1395_0000582	0.0	2.0
1395_0000583	1.0	3.0
1395_0000584	0.0	1.0
1395_0000585	1.0	3.0
1395_0000587	0.0	2.0
1395_0000591	0.0	1.0
1395_0000593	0.0	2.0
1395_0000595	0.0	2.0
1395_0000596	3.0	2.0
1395_0000597	1.0	3.0
1395_0000598	1.0	2.0
1395_0000599	1.0	2.0
1395_0000602	1.0	2.0
1395_0000604	1.0	2.0
1395_0000606	0.0	2.0
1395_0000607	1.0	2.0
1395_0000608	1.0	3.0
1395_0000609	1.0	2.0
1395_0000610	2.0	3.0
1395_0000611	1.0	2.0
1395_0000612	0.0	3.0
1395_0000626	2.0	3.0
1395_0000627	1.0	2.0
1395_0000628	1.0	2.0
1395_0000630	1.0	3.0
1395_0000631	1.0	2.0
1395_0000635	1.0	2.0
1395_0000636	1.0	2.0
1395_0000639	1.0	3.0
1395_0000642	1.0	2.0
1395_0000644	2.0	3.0
1395_0000646	2.0	2.0
1395_0000649	2.0	2.0
1395_0001010	2.0	3.0
1395_0001013	2.0	3.0
1395_0001015	2.0	2.0
1395_0001016	2.0	3.0
1395_0001017	2.0	3.0
1395_0001019	2.0	2.0
1395_0001020	1.0	3.0
1395_0001021	2.0	2.0
1395_0001022	2.0	3.0
1395_0001023	2.0	2.0
1395_0001024	1.0	2.0
1395_0001028	2.0	3.0
1395_0001033	2.0	3.0
1395_0001034	2.0	3.0
1395_0001040	1.0	1.0
1395_0001045	2.0	2.0
1395_0001058	2.0	2.0
1395_0001060	2.0	2.0
1395_0001061	2.0	3.0
1395_0001064	2.0	2.0
1395_0001065	1.0	3.0
1395_0001066	1.0	2.0
1395_0001067	1.0	2.0
1395_0001068	1.0	3.0
1395_0001069	2.0	2.0
1395_0001070	2.0	3.0
1395_0001071	2.0	2.0
1395_0001073	1.0	2.0
1395_0001074	1.0	2.0
1395_0001075	1.0	2.0
1395_0001076	1.0	3.0
1395_0001078	2.0	3.0
1395_0001080	2.0	3.0
1395_0001084	2.0	3.0
1395_0001090	2.0	3.0
1395_0001093	2.0	2.0
1395_0001101	2.0	3.0
1395_0001103	1.0	3.0
1395_0001104	1.0	3.0
1395_0001108	1.0	2.0
1395_0001109	0.0	2.0
1395_0001114	1.0	3.0
1395_0001115	2.0	3.0
1395_0001116	2.0	3.0
1395_0001117	1.0	2.0
1395_0001118	1.0	2.0
1395_0001119	2.0	3.0
1395_0001120	1.0	2.0
1395_0001121	1.0	2.0
1395_0001122	1.0	2.0
1395_0001123	1.0	3.0
1395_0001124	0.0	2.0
1395_0001126	1.0	2.0
1395_0001131	1.0	2.0
1395_0001132	1.0	3.0
1395_0001133	1.0	3.0
1395_0001141	2.0	2.0
1395_0001145	3.0	3.0
1395_0001146	0.0	2.0
1395_0001147	1.0	3.0
1395_0001149	1.0	2.0
1395_0001150	1.0	3.0
1395_0001158	2.0	2.0
1395_0001160	2.0	2.0
1395_0001161	1.0	3.0
1395_0001164	2.0	3.0
1395_0001167	1.0	2.0
1395_0001169	2.0	3.0
1395_0001170	1.0	2.0
1395_0001171	1.0	3.0
Language = IT, Weighted F1-score = 0.3021308199286259, Dimension = Orthography

Train on DE, test on  CZ
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
144.90552995391704 65.35717405024758
LANGUAGE: IT
nan nan
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']


======== Epoch 1 / 4 ========
Training...
Elapsed time 11
Elapsed time 22
Elapsed time 34

  Average training loss: 1.05
  Training epoch took: 36
Running Validation...
  Average evaluation loss: 2.30
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.00      0.00      0.00       132
         2.0       0.18      0.09      0.12       141
         3.0       0.02      0.02      0.02       143
         4.0       0.06      1.00      0.11        10

    accuracy                           0.06       434
   macro avg       0.05      0.22      0.05       434
weighted avg       0.07      0.06      0.05       434

[[  0   0   5   2   1]
 [  0   0  53  77   2]
 [  0   0  13 101  27]
 [  0   0   0   3 140]
 [  0   0   0   0  10]]
0.04833406969960103
434 434 434



======== Epoch 2 / 4 ========
Training...
Elapsed time 11
Elapsed time 23
Elapsed time 34

  Average training loss: 0.79
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.91
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.00      0.00      0.00       132
         2.0       0.26      0.17      0.21       141
         3.0       0.32      0.62      0.42       143
         4.0       0.12      0.80      0.21        10

    accuracy                           0.28       434
   macro avg       0.14      0.32      0.17       434
weighted avg       0.19      0.28      0.21       434

[[  0   1   4   3   0]
 [  0   0  63  69   0]
 [  0   1  24 113   3]
 [  0   0   0  88  55]
 [  0   0   0   2   8]]
0.21080231167462596
434 434 434



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.68
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.83
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.71      0.04      0.07       132
         2.0       0.36      0.34      0.35       141
         3.0       0.18      0.20      0.19       143
         4.0       0.08      1.00      0.14        10

    accuracy                           0.21       434
   macro avg       0.27      0.32      0.15       434
weighted avg       0.40      0.21      0.20       434

[[  0   1   4   3   0]
 [  0   5  81  46   0]
 [  0   1  48  83   9]
 [  0   0   0  29 114]
 [  0   0   0   0  10]]
0.20179575775842382
434 434 434



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.57
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 2.19
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.33      0.01      0.01       132
         2.0       0.32      0.24      0.27       141
         3.0       0.21      0.31      0.25       143
         4.0       0.09      1.00      0.16        10

    accuracy                           0.21       434
   macro avg       0.19      0.31      0.14       434
weighted avg       0.28      0.21      0.18       434

[[ 0  1  4  3  0]
 [ 0  1 69 62  0]
 [ 0  1 34 99  7]
 [ 0  0  0 44 99]
 [ 0  0  0  0 10]]
0.17985253298914658
434 434 434
Filename	True Label	Prediction
0601	2.0	2.0
0602	2.0	3.0
0603	2.0	3.0
0604	2.0	3.0
0605	2.0	3.0
0606	2.0	3.0
0607	2.0	3.0
0608	1.0	3.0
0609	1.0	3.0
0610	2.0	3.0
0611	2.0	3.0
0612	1.0	3.0
0613	1.0	2.0
0614	2.0	3.0
0615	2.0	2.0
0616	1.0	3.0
0617	1.0	3.0
0618	2.0	3.0
0619	2.0	2.0
0620	2.0	3.0
0621	2.0	3.0
0622	1.0	2.0
0623	2.0	3.0
0624	2.0	3.0
0625	2.0	2.0
0626	2.0	3.0
0627	2.0	3.0
0628	1.0	3.0
0629	1.0	3.0
0630	1.0	2.0
0631	2.0	2.0
0632	1.0	2.0
0633	2.0	3.0
0634	2.0	3.0
0635	1.0	2.0
0636	2.0	3.0
0637	2.0	3.0
0638	1.0	2.0
0639	1.0	2.0
0640	2.0	3.0
0641	1.0	2.0
0642	2.0	3.0
0643	2.0	3.0
0644	2.0	2.0
0645	2.0	3.0
0714	2.0	3.0
0715	2.0	3.0
0716	2.0	3.0
0717	2.0	3.0
0718	1.0	3.0
0719	2.0	3.0
0720	2.0	3.0
0721	2.0	3.0
0722	2.0	2.0
0723	1.0	3.0
0724	2.0	3.0
0725	2.0	3.0
0801	2.0	2.0
0802	1.0	2.0
0803	1.0	3.0
0804	1.0	3.0
0805	1.0	3.0
0806	1.0	3.0
0807	2.0	2.0
0808	1.0	3.0
0809	2.0	2.0
0810	2.0	3.0
0811	2.0	3.0
0812	2.0	2.0
0813	2.0	2.0
0814	1.0	2.0
0815	2.0	3.0
0816	2.0	3.0
0817	2.0	2.0
0818	2.0	2.0
0819	2.0	3.0
0820	1.0	2.0
0821	2.0	3.0
0822	1.0	3.0
0823	2.0	3.0
0824	2.0	3.0
0825	1.0	3.0
0826	1.0	2.0
0827	1.0	2.0
0828	2.0	2.0
0829	2.0	3.0
0901	2.0	3.0
0902	1.0	2.0
0903	2.0	3.0
0904	2.0	2.0
0905	2.0	3.0
0906	2.0	3.0
0907	2.0	3.0
0910	1.0	2.0
0911	2.0	3.0
0912	2.0	3.0
0913	2.0	2.0
0914	2.0	3.0
0915	2.0	3.0
0916	1.0	3.0
0917	1.0	2.0
0918	2.0	3.0
0919	1.0	2.0
0920	2.0	3.0
0921	2.0	2.0
0922	2.0	2.0
0923	1.0	3.0
0924	2.0	2.0
0925	1.0	3.0
0926	2.0	3.0
0927	1.0	2.0
0928	2.0	3.0
0929	1.0	3.0
0930	1.0	3.0
1001	2.0	3.0
1002	2.0	3.0
1003	2.0	2.0
1004	2.0	2.0
1005	2.0	3.0
1006	2.0	2.0
1007	2.0	3.0
1008	2.0	3.0
1009	2.0	3.0
1010	2.0	3.0
1014	2.0	2.0
1015	2.0	3.0
1016	1.0	3.0
1017	1.0	3.0
1018	1.0	3.0
1019	2.0	3.0
1020	2.0	3.0
1021	2.0	3.0
1022	2.0	3.0
1023	2.0	3.0
1111	2.0	2.0
1112	2.0	3.0
1113	2.0	3.0
1114	2.0	3.0
1115	2.0	3.0
1116	2.0	3.0
1117	1.0	2.0
9999	0.0	2.0
BER0609003	3.0	3.0
BER0611003	2.0	4.0
BER0611005	2.0	3.0
BER0611006	2.0	4.0
BER0611007	3.0	4.0
KYJ0611003A	1.0	2.0
KYJ0611004A	1.0	2.0
KYJ0611005A	1.0	2.0
KYJ0611005B	1.0	3.0
KYJ0611006A	0.0	2.0
KYJ0611006B	0.0	3.0
KYJ0611009A	1.0	2.0
KYJ0611009B	1.0	3.0
LIB0611001A	1.0	2.0
LIB0611001B	1.0	3.0
LIB0611002A	1.0	2.0
LIB0611002B	1.0	3.0
LIB0611003A	1.0	2.0
LIB0611004A	1.0	2.0
LIB0611004B	1.0	3.0
LIB0611011	2.0	3.0
LON0610002A	1.0	2.0
LON0610002B	1.0	3.0
LON0611002A	1.0	2.0
LON0611002B	1.0	3.0
LON0611003	3.0	4.0
LON0611004A	1.0	2.0
LON0611004B	1.0	2.0
MOS0509001	2.0	3.0
MOS0509004	3.0	3.0
MOS0611012	3.0	3.0
MOS0611013	3.0	4.0
MOS0611014	2.0	3.0
MOS0611015	3.0	4.0
PAR1011008A	2.0	2.0
PAR1011009A	1.0	2.0
PAR1011009B	1.0	3.0
PAR1011013	3.0	4.0
PAR1011014	2.0	4.0
PAR1011015	3.0	3.0
PAR1011016	3.0	4.0
PAR1011017	3.0	4.0
PAR1011018	4.0	4.0
PHA0111001A	1.0	2.0
PHA0111001B	1.0	3.0
PHA0111002A	1.0	2.0
PHA0111002B	2.0	3.0
PHA0111003A	1.0	2.0
PHA0111003B	1.0	2.0
PHA0111004A	1.0	2.0
PHA0111004B	1.0	3.0
PHA0111005A	1.0	2.0
PHA0111005B	1.0	3.0
PHA0111010	2.0	4.0
PHA0111011	3.0	3.0
PHA0111012	2.0	4.0
PHA0111014	2.0	3.0
PHA0111015	3.0	4.0
PHA0111016	3.0	4.0
PHA0111018	3.0	4.0
PHA0112002A	1.0	2.0
PHA0112002B	1.0	3.0
PHA0112003A	1.0	2.0
PHA0112003B	1.0	2.0
PHA0112006A	2.0	2.0
PHA0112006B	2.0	3.0
PHA0112007A	1.0	2.0
PHA0112007B	1.0	2.0
PHA0112009A	2.0	2.0
PHA0112009B	1.0	3.0
PHA0112012A	2.0	3.0
PHA0112012B	1.0	3.0
PHA0209001	1.0	3.0
PHA0209008	1.0	3.0
PHA0209013	1.0	2.0
PHA0209024	3.0	3.0
PHA0209026	3.0	4.0
PHA0209028	3.0	4.0
PHA0209031	4.0	4.0
PHA0209034	3.0	4.0
PHA0209038	4.0	4.0
PHA0209039	3.0	4.0
PHA0210001	1.0	3.0
PHA0210004	1.0	3.0
PHA0210007	1.0	3.0
PHA0210008	1.0	2.0
PHA0411008A	1.0	2.0
PHA0411008B	2.0	2.0
PHA0411009A	1.0	2.0
PHA0411009B	1.0	3.0
PHA0411010A	0.0	2.0
PHA0411010B	0.0	3.0
PHA0411011A	1.0	2.0
PHA0411011B	1.0	2.0
PHA0411012A	1.0	2.0
PHA0411012B	1.0	3.0
PHA0411027	2.0	3.0
PHA0411028	2.0	3.0
PHA0411029	3.0	3.0
PHA0411030	3.0	4.0
PHA0411031	3.0	4.0
PHA0411032	3.0	4.0
PHA0411033	3.0	3.0
PHA0411034	2.0	3.0
PHA0411035	2.0	3.0
PHA0411036	3.0	4.0
PHA0411037	3.0	4.0
PHA0411038	3.0	4.0
PHA0411039	3.0	3.0
PHA0411041	3.0	4.0
PHA0411042	3.0	4.0
PHA0411043	3.0	3.0
PHA0411044	4.0	4.0
PHA0411045	3.0	3.0
PHA0411047	3.0	4.0
PHA0411051	4.0	4.0
PHA0411053	4.0	4.0
PHA0411054	3.0	3.0
PHA0411055	3.0	4.0
PHA0411056	3.0	4.0
PHA0411058	3.0	4.0
PHA0411059	3.0	4.0
PHA0411060	2.0	4.0
PHA0411061	3.0	4.0
PHA0411062	3.0	4.0
PHA0509002	1.0	2.0
PHA0509007	1.0	3.0
PHA0509013	1.0	2.0
PHA0509015	3.0	3.0
PHA0509017	3.0	4.0
PHA0509018	3.0	4.0
PHA0509019	3.0	3.0
PHA0509020	3.0	4.0
PHA0509021	2.0	3.0
PHA0509022	4.0	4.0
PHA0509024	3.0	4.0
PHA0509025	3.0	4.0
PHA0509026	3.0	4.0
PHA0509027	3.0	4.0
PHA0509028	3.0	3.0
PHA0509030	3.0	4.0
PHA0509031	2.0	3.0
PHA0509032	3.0	4.0
PHA0509033	2.0	3.0
PHA0509034	3.0	3.0
PHA0509035	3.0	4.0
PHA0509036	3.0	4.0
PHA0509037	2.0	3.0
PHA0509038	2.0	3.0
PHA0509039	3.0	4.0
PHA0509040	3.0	4.0
PHA0509041	3.0	4.0
PHA0509042	3.0	4.0
PHA0509043	3.0	3.0
PHA0509044	3.0	4.0
PHA0509045	3.0	3.0
PHA0510002A	1.0	2.0
PHA0510002B	1.0	3.0
PHA0510003A	1.0	2.0
PHA0510003B	1.0	3.0
PHA0510004A	1.0	2.0
PHA0510004B	1.0	3.0
PHA0510010A	2.0	1.0
PHA0510010B	0.0	1.0
PHA0510013A	1.0	2.0
PHA0510013B	1.0	3.0
PHA0510023	3.0	4.0
PHA0510027	3.0	3.0
PHA0510029	3.0	4.0
PHA0510030	3.0	3.0
PHA0510031	3.0	3.0
PHA0510032	3.0	4.0
PHA0510034	3.0	4.0
PHA0510035	3.0	4.0
PHA0510036	3.0	4.0
PHA0510037	2.0	3.0
PHA0510038	3.0	4.0
PHA0510039	3.0	4.0
PHA0510040	3.0	4.0
PHA0510046	3.0	3.0
PHA0510047	2.0	3.0
PHA0510048	3.0	3.0
PHA0510049	2.0	4.0
PHA0510050	3.0	4.0
PHA0610005A	1.0	2.0
PHA0610005B	0.0	2.0
PHA0610006A	2.0	2.0
PHA0610006B	1.0	3.0
PHA0610007A	1.0	1.0
PHA0610007B	1.0	3.0
PHA0610015	3.0	4.0
PHA0610016	3.0	4.0
PHA0610017	3.0	4.0
PHA0610018	3.0	4.0
PHA0610019A	2.0	2.0
PHA0610019B	1.0	3.0
PHA0610025	3.0	4.0
PHA0610026	3.0	4.0
PHA0709008	3.0	4.0
PHA0710009	3.0	4.0
PHA0710010	3.0	4.0
PHA0710011	3.0	4.0
PHA0710012	3.0	3.0
PHA0710013	4.0	4.0
PHA0710014	3.0	4.0
PHA0710015	3.0	4.0
PHA0710016	3.0	4.0
PHA0710017	3.0	4.0
PHA0710018	3.0	4.0
PHA0710019	3.0	4.0
PHA0710021	3.0	4.0
PHA0809009	3.0	3.0
PHA0809010	2.0	3.0
PHA0810001	3.0	4.0
PHA0810002	3.0	3.0
PHA0810003	3.0	4.0
PHA0810004	3.0	3.0
PHA0810006	3.0	4.0
PHA0810008	3.0	4.0
PHA0810009	3.0	4.0
PHA0810010	3.0	4.0
PHA0810011	3.0	4.0
PHA0810012	3.0	4.0
PHA0810015	3.0	4.0
PHA0811010	3.0	3.0
PHA0811012	3.0	4.0
PHA0811013	4.0	4.0
PHA0811014	3.0	3.0
PHA0811016	3.0	3.0
PHA0811017	3.0	4.0
PHA0811019	4.0	4.0
PHA0811020	3.0	3.0
PHA1109001	1.0	3.0
PHA1109002	3.0	3.0
PHA1109003	2.0	3.0
PHA1109004	3.0	4.0
PHA1109005	2.0	3.0
PHA1109006	2.0	3.0
PHA1109007	2.0	3.0
PHA1109008	1.0	2.0
PHA1109023	2.0	2.0
PHA1109024	3.0	4.0
PHA1109025	2.0	2.0
PHA1109026	3.0	4.0
PHA1109027	3.0	4.0
PHA1109028	3.0	4.0
PHA1110001A	1.0	2.0
PHA1110001B	1.0	3.0
PHA1110002A	2.0	3.0
PHA1110002B	1.0	3.0
PHA1110003A	1.0	2.0
PHA1110003B	0.0	3.0
PHA1110004A	1.0	2.0
PHA1110013	3.0	4.0
PHA1110014	3.0	4.0
PHA1110015	3.0	4.0
PHA1110016	3.0	3.0
PHA1110017	3.0	3.0
PHA1110019	3.0	4.0
PHA1110021	3.0	3.0
PHA1110022	3.0	4.0
PHA1111001A	2.0	2.0
PHA1111001B	1.0	3.0
PHA1111002A	1.0	2.0
PHA1111002B	1.0	2.0
PHA1111003A	1.0	2.0
PHA1111003B	1.0	3.0
PHA1111004A	1.0	2.0
PHA1111004B	1.0	2.0
PHA1111006A	1.0	2.0
PHA1111006B	1.0	3.0
PHA1111008A	1.0	2.0
PHA1111008B	1.0	3.0
PHA1111009A	1.0	2.0
ST071122B	1.0	3.0
TI071122B	1.0	3.0
VAR0209036	2.0	3.0
VAR0909003	3.0	3.0
VAR0909004	3.0	3.0
VAR0909005	3.0	3.0
VAR0909006	3.0	4.0
VAR0909007	3.0	4.0
VAR0909008	3.0	3.0
VAR0909009	3.0	3.0
VAR0909010	3.0	3.0
VAR0910004	3.0	4.0
VAR0910005	3.0	3.0
VAR0910006	3.0	4.0
VAR0910007	3.0	4.0
VAR0910009	3.0	4.0
VAR0910010	3.0	3.0
VAR0910011	3.0	3.0
Language = CZ, Weighted F1-score = 0.17985253298914658, Dimension = Vocabularyrange

Train on DE, test on  IT
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
nan nan
LANGUAGE: IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']


======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 1.15
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 0.96
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        39
         1.0       0.63      0.14      0.22       243
         2.0       0.50      0.88      0.64       328
         3.0       0.77      0.70      0.73       188
         4.0       0.00      0.00      0.00         2

    accuracy                           0.57       800
   macro avg       0.38      0.34      0.32       800
weighted avg       0.58      0.57      0.50       800

[[  0  18  21   0   0]
 [  0  33 210   0   0]
 [  0   1 289  38   0]
 [  0   0  56 132   0]
 [  0   0   0   2   0]]
0.5024369781510925
800 800 800



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.85
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.29
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        39
         1.0       0.58      0.21      0.31       243
         2.0       0.46      0.54      0.50       328
         3.0       0.55      0.94      0.70       188
         4.0       0.17      0.50      0.25         2

    accuracy                           0.51       800
   macro avg       0.35      0.44      0.35       800
weighted avg       0.50      0.51      0.46       800

[[  0  26  13   0   0]
 [  0  52 189   2   0]
 [  0  11 177 139   1]
 [  0   0   7 177   4]
 [  0   0   0   1   1]]
0.4631357535706947
800 800 800



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.73
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.13
              precision    recall  f1-score   support

         0.0       0.67      0.05      0.10        39
         1.0       0.53      0.16      0.25       243
         2.0       0.49      0.67      0.57       328
         3.0       0.64      0.91      0.75       188
         4.0       0.33      0.50      0.40         2

    accuracy                           0.54       800
   macro avg       0.53      0.46      0.41       800
weighted avg       0.54      0.54      0.49       800

[[  2  24  13   0   0]
 [  1  39 203   0   0]
 [  0  11 221  96   0]
 [  0   0  15 171   2]
 [  0   0   0   1   1]]
0.48896593811025985
800 800 800



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.62
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.26
              precision    recall  f1-score   support

         0.0       0.25      0.05      0.09        39
         1.0       0.52      0.13      0.21       243
         2.0       0.48      0.65      0.55       328
         3.0       0.62      0.94      0.75       188
         4.0       0.50      0.50      0.50         2

    accuracy                           0.53       800
   macro avg       0.47      0.45      0.42       800
weighted avg       0.51      0.53      0.47       800

[[  2  24  13   0   0]
 [  3  32 208   0   0]
 [  3   6 212 107   0]
 [  0   0  11 176   1]
 [  0   0   0   1   1]]
0.4695722255266402
800 800 800
Filename	True Label	Prediction
1325_1001008	3.0	3.0
1325_1001009	3.0	3.0
1325_1001010	3.0	3.0
1325_1001011	3.0	3.0
1325_1001012	3.0	3.0
1325_1001013	3.0	3.0
1325_1001014	3.0	3.0
1325_1001015	3.0	3.0
1325_1001016	2.0	3.0
1325_1001017	3.0	3.0
1325_1001018	3.0	3.0
1325_1001019	3.0	3.0
1325_1001020	3.0	3.0
1325_1001021	3.0	3.0
1325_1001022	3.0	3.0
1325_1001023	3.0	3.0
1325_1001024	3.0	3.0
1325_1001025	2.0	3.0
1325_1001027	3.0	3.0
1325_1001028	3.0	3.0
1325_1001029	3.0	3.0
1325_1001032	3.0	3.0
1325_1001033	3.0	3.0
1325_1001035	3.0	4.0
1325_1001036	3.0	3.0
1325_1001037	2.0	3.0
1325_1001039	3.0	3.0
1325_1001040	3.0	3.0
1325_1001041	3.0	3.0
1325_1001042	2.0	3.0
1325_1001043	3.0	3.0
1325_1001044	3.0	3.0
1325_1001045	3.0	3.0
1325_1001046	2.0	3.0
1325_1001047	3.0	2.0
1325_1001048	2.0	3.0
1325_1001050	3.0	3.0
1325_1001051	3.0	3.0
1325_1001052	2.0	3.0
1325_1001053	2.0	2.0
1325_1001054	3.0	3.0
1325_1001055	3.0	3.0
1325_1001056	3.0	3.0
1325_1001057	2.0	3.0
1325_1001058	2.0	3.0
1325_1001059	3.0	3.0
1325_1001062	3.0	3.0
1325_1001063	2.0	3.0
1325_1001075	2.0	3.0
1325_1001076	3.0	3.0
1325_1001077	3.0	3.0
1325_1001078	3.0	3.0
1325_1001079	3.0	3.0
1325_1001080	2.0	3.0
1325_1001081	3.0	3.0
1325_1001082	3.0	3.0
1325_1001083	3.0	3.0
1325_1001084	3.0	3.0
1325_1001085	3.0	3.0
1325_1001086	3.0	3.0
1325_1001087	2.0	3.0
1325_1001088	2.0	3.0
1325_1001089	3.0	3.0
1325_1001090	2.0	3.0
1325_1001091	3.0	3.0
1325_1001092	2.0	3.0
1325_1001093	2.0	3.0
1325_1001094	2.0	3.0
1325_1001095	2.0	3.0
1325_1001096	3.0	3.0
1325_1001097	1.0	2.0
1325_1001098	3.0	3.0
1325_1001099	3.0	3.0
1325_1001100	2.0	3.0
1325_1001101	3.0	3.0
1325_1001107	3.0	3.0
1325_1001108	3.0	3.0
1325_1001109	2.0	3.0
1325_1001110	3.0	3.0
1325_1001111	3.0	3.0
1325_1001113	3.0	3.0
1325_1001119	3.0	3.0
1325_1001120	3.0	3.0
1325_1001121	2.0	3.0
1325_1001122	3.0	3.0
1325_1001123	3.0	3.0
1325_1001124	3.0	3.0
1325_1001125	3.0	3.0
1325_1001126	2.0	3.0
1325_1001127	3.0	3.0
1325_1001128	3.0	3.0
1325_1001129	2.0	3.0
1325_1001130	3.0	3.0
1325_1001131	3.0	3.0
1325_1001132	3.0	3.0
1325_1001133	3.0	3.0
1325_1001134	2.0	3.0
1325_1001135	3.0	3.0
1325_1001136	3.0	3.0
1325_1001138	2.0	3.0
1325_1001139	3.0	3.0
1325_1001141	2.0	3.0
1325_1001142	3.0	3.0
1325_1001143	3.0	3.0
1325_1001144	3.0	3.0
1325_1001152	3.0	3.0
1325_1001153	2.0	3.0
1325_1001154	3.0	3.0
1325_1001155	3.0	3.0
1325_1001156	3.0	3.0
1325_1001157	3.0	3.0
1325_1001158	3.0	3.0
1325_1001159	3.0	3.0
1325_1001160	3.0	3.0
1325_1001161	3.0	3.0
1325_1001162	3.0	3.0
1325_1001163	2.0	3.0
1325_1001164	3.0	3.0
1325_1001165	2.0	3.0
1325_1001166	3.0	3.0
1325_1001167	3.0	3.0
1325_1001168	3.0	3.0
1325_1001169	3.0	3.0
1325_1001170	3.0	3.0
1325_9000059	3.0	3.0
1325_9000087	2.0	3.0
1325_9000088	2.0	3.0
1325_9000089	2.0	3.0
1325_9000090	2.0	3.0
1325_9000095	3.0	3.0
1325_9000099	2.0	3.0
1325_9000102	3.0	2.0
1325_9000104	3.0	3.0
1325_9000105	2.0	3.0
1325_9000106	3.0	3.0
1325_9000107	3.0	3.0
1325_9000136	3.0	3.0
1325_9000137	3.0	3.0
1325_9000138	4.0	4.0
1325_9000139	3.0	3.0
1325_9000140	3.0	3.0
1325_9000143	3.0	3.0
1325_9000144	3.0	3.0
1325_9000152	3.0	3.0
1325_9000185	3.0	3.0
1325_9000186	3.0	3.0
1325_9000187	3.0	3.0
1325_9000188	3.0	3.0
1325_9000209	3.0	3.0
1325_9000210	2.0	3.0
1325_9000211	3.0	3.0
1325_9000213	3.0	3.0
1325_9000214	3.0	3.0
1325_9000215	3.0	3.0
1325_9000237	3.0	3.0
1325_9000239	3.0	3.0
1325_9000240	3.0	3.0
1325_9000241	3.0	3.0
1325_9000278	3.0	3.0
1325_9000279	3.0	3.0
1325_9000296	3.0	3.0
1325_9000302	3.0	3.0
1325_9000303	3.0	3.0
1325_9000304	3.0	3.0
1325_9000314	3.0	3.0
1325_9000315	2.0	2.0
1325_9000316	3.0	3.0
1325_9000317	3.0	3.0
1325_9000318	3.0	3.0
1325_9000319	3.0	3.0
1325_9000320	3.0	2.0
1325_9000321	3.0	3.0
1325_9000322	3.0	3.0
1325_9000323	3.0	3.0
1325_9000503	3.0	3.0
1325_9000504	3.0	3.0
1325_9000505	3.0	3.0
1325_9000533	3.0	3.0
1325_9000534	3.0	3.0
1325_9000536	3.0	3.0
1325_9000554	3.0	3.0
1325_9000601	3.0	3.0
1325_9000602	4.0	3.0
1325_9000611	3.0	3.0
1325_9000612	2.0	3.0
1325_9000674	3.0	3.0
1325_9000675	3.0	3.0
1325_9000676	3.0	3.0
1325_9000677	3.0	3.0
1325_9000678	3.0	3.0
1325_9000684	3.0	3.0
1325_9000685	3.0	3.0
1325_9000686	3.0	3.0
1325_9000700	3.0	3.0
1325_9000750	3.0	2.0
1365_0100002	2.0	2.0
1365_0100003	2.0	2.0
1365_0100004	2.0	3.0
1365_0100005	2.0	2.0
1365_0100006	2.0	2.0
1365_0100007	1.0	2.0
1365_0100008	2.0	2.0
1365_0100009	2.0	2.0
1365_0100010	2.0	2.0
1365_0100011	2.0	2.0
1365_0100012	2.0	2.0
1365_0100013	3.0	3.0
1365_0100014	2.0	2.0
1365_0100015	2.0	2.0
1365_0100016	2.0	3.0
1365_0100017	2.0	3.0
1365_0100018	2.0	2.0
1365_0100019	2.0	2.0
1365_0100020	2.0	3.0
1365_0100021	2.0	2.0
1365_0100022	2.0	2.0
1365_0100023	2.0	2.0
1365_0100024	2.0	2.0
1365_0100026	2.0	1.0
1365_0100027	2.0	2.0
1365_0100028	2.0	2.0
1365_0100029	1.0	2.0
1365_0100030	2.0	2.0
1365_0100031	2.0	2.0
1365_0100051	2.0	2.0
1365_0100056	2.0	3.0
1365_0100057	2.0	3.0
1365_0100058	2.0	3.0
1365_0100061	3.0	3.0
1365_0100063	3.0	3.0
1365_0100064	2.0	2.0
1365_0100065	1.0	2.0
1365_0100066	2.0	2.0
1365_0100067	2.0	2.0
1365_0100069	2.0	2.0
1365_0100070	2.0	2.0
1365_0100071	3.0	3.0
1365_0100072	2.0	2.0
1365_0100073	2.0	2.0
1365_0100074	2.0	3.0
1365_0100079	2.0	2.0
1365_0100080	2.0	3.0
1365_0100092	2.0	2.0
1365_0100093	2.0	2.0
1365_0100094	2.0	2.0
1365_0100095	2.0	2.0
1365_0100096	2.0	3.0
1365_0100097	2.0	2.0
1365_0100098	2.0	3.0
1365_0100099	2.0	2.0
1365_0100100	2.0	3.0
1365_0100101	3.0	3.0
1365_0100102	3.0	3.0
1365_0100103	2.0	3.0
1365_0100104	2.0	3.0
1365_0100105	3.0	3.0
1365_0100106	2.0	3.0
1365_0100107	2.0	3.0
1365_0100116	3.0	3.0
1365_0100117	2.0	2.0
1365_0100118	2.0	3.0
1365_0100119	3.0	3.0
1365_0100120	3.0	3.0
1365_0100121	2.0	3.0
1365_0100123	2.0	3.0
1365_0100125	3.0	3.0
1365_0100133	2.0	2.0
1365_0100134	2.0	2.0
1365_0100135	2.0	2.0
1365_0100136	2.0	2.0
1365_0100137	2.0	2.0
1365_0100138	2.0	2.0
1365_0100139	2.0	2.0
1365_0100145	2.0	3.0
1365_0100146	2.0	2.0
1365_0100147	2.0	3.0
1365_0100148	2.0	3.0
1365_0100151	2.0	2.0
1365_0100162	2.0	3.0
1365_0100163	3.0	3.0
1365_0100164	2.0	3.0
1365_0100165	3.0	3.0
1365_0100166	2.0	2.0
1365_0100167	2.0	2.0
1365_0100168	2.0	2.0
1365_0100169	2.0	2.0
1365_0100170	2.0	3.0
1365_0100171	2.0	2.0
1365_0100172	2.0	3.0
1365_0100173	2.0	2.0
1365_0100174	2.0	2.0
1365_0100175	2.0	2.0
1365_0100176	2.0	2.0
1365_0100177	2.0	2.0
1365_0100178	2.0	2.0
1365_0100179	2.0	2.0
1365_0100180	2.0	2.0
1365_0100181	2.0	2.0
1365_0100182	2.0	3.0
1365_0100183	2.0	3.0
1365_0100184	2.0	2.0
1365_0100185	2.0	2.0
1365_0100186	2.0	2.0
1365_0100187	2.0	2.0
1365_0100188	2.0	2.0
1365_0100190	2.0	2.0
1365_0100191	2.0	3.0
1365_0100192	3.0	3.0
1365_0100194	2.0	3.0
1365_0100195	2.0	2.0
1365_0100196	2.0	3.0
1365_0100198	2.0	2.0
1365_0100199	2.0	3.0
1365_0100200	3.0	3.0
1365_0100201	2.0	2.0
1365_0100202	2.0	2.0
1365_0100203	2.0	2.0
1365_0100204	2.0	2.0
1365_0100205	2.0	2.0
1365_0100211	3.0	3.0
1365_0100212	3.0	3.0
1365_0100213	2.0	2.0
1365_0100215	2.0	2.0
1365_0100217	3.0	3.0
1365_0100218	2.0	2.0
1365_0100219	2.0	3.0
1365_0100220	3.0	3.0
1365_0100221	2.0	2.0
1365_0100222	3.0	3.0
1365_0100223	2.0	3.0
1365_0100224	3.0	3.0
1365_0100225	2.0	2.0
1365_0100226	3.0	2.0
1365_0100227	3.0	3.0
1365_0100228	2.0	3.0
1365_0100229	2.0	3.0
1365_0100230	2.0	3.0
1365_0100231	2.0	2.0
1365_0100232	2.0	3.0
1365_0100233	2.0	3.0
1365_0100251	2.0	3.0
1365_0100252	2.0	3.0
1365_0100253	2.0	2.0
1365_0100255	2.0	2.0
1365_0100256	2.0	2.0
1365_0100257	2.0	2.0
1365_0100258	2.0	2.0
1365_0100259	2.0	2.0
1365_0100260	2.0	2.0
1365_0100261	2.0	2.0
1365_0100262	3.0	3.0
1365_0100263	3.0	3.0
1365_0100265	2.0	3.0
1365_0100266	2.0	3.0
1365_0100267	2.0	3.0
1365_0100268	2.0	2.0
1365_0100269	2.0	2.0
1365_0100270	2.0	3.0
1365_0100274	2.0	3.0
1365_0100275	3.0	3.0
1365_0100276	3.0	3.0
1365_0100277	3.0	3.0
1365_0100278	3.0	2.0
1365_0100279	2.0	3.0
1365_0100280	1.0	2.0
1365_0100281	2.0	2.0
1365_0100282	2.0	3.0
1365_0100285	2.0	2.0
1365_0100286	2.0	2.0
1365_0100287	2.0	2.0
1365_0100288	2.0	2.0
1365_0100289	2.0	2.0
1365_0100290	2.0	2.0
1365_0100299	2.0	2.0
1365_0100447	2.0	2.0
1365_0100448	2.0	3.0
1365_0100451	2.0	3.0
1365_0100455	2.0	3.0
1365_0100456	2.0	3.0
1365_0100457	2.0	3.0
1365_0100458	2.0	2.0
1365_0100459	3.0	3.0
1365_0100461	2.0	3.0
1365_0100469	2.0	2.0
1365_0100470	2.0	3.0
1365_0100471	2.0	3.0
1365_0100472	2.0	2.0
1365_0100473	2.0	3.0
1365_0100474	2.0	3.0
1365_0100475	2.0	3.0
1365_0100476	2.0	3.0
1365_0100477	2.0	3.0
1365_0100478	2.0	3.0
1365_0100479	2.0	3.0
1365_0100480	2.0	3.0
1365_0100481	2.0	3.0
1365_0100482	2.0	3.0
1385_0000011	0.0	1.0
1385_0000012	1.0	2.0
1385_0000013	1.0	1.0
1385_0000016	1.0	2.0
1385_0000017	1.0	1.0
1385_0000020	1.0	2.0
1385_0000021	1.0	2.0
1385_0000022	1.0	2.0
1385_0000023	1.0	2.0
1385_0000033	1.0	2.0
1385_0000034	1.0	2.0
1385_0000035	1.0	1.0
1385_0000036	1.0	2.0
1385_0000037	1.0	2.0
1385_0000038	1.0	2.0
1385_0000039	1.0	2.0
1385_0000040	1.0	1.0
1385_0000041	1.0	2.0
1385_0000042	1.0	2.0
1385_0000043	1.0	1.0
1385_0000044	2.0	2.0
1385_0000045	2.0	2.0
1385_0000047	1.0	2.0
1385_0000048	1.0	2.0
1385_0000049	1.0	2.0
1385_0000050	1.0	2.0
1385_0000051	2.0	2.0
1385_0000052	1.0	1.0
1385_0000053	1.0	2.0
1385_0000054	2.0	2.0
1385_0000057	1.0	2.0
1385_0000058	1.0	2.0
1385_0000059	1.0	2.0
1385_0000095	1.0	1.0
1385_0000097	2.0	2.0
1385_0000098	2.0	1.0
1385_0000099	1.0	2.0
1385_0000100	1.0	1.0
1385_0000101	1.0	0.0
1385_0000102	2.0	2.0
1385_0000103	1.0	0.0
1385_0000104	2.0	2.0
1385_0000114	2.0	2.0
1385_0000119	1.0	2.0
1385_0000120	0.0	2.0
1385_0000122	1.0	2.0
1385_0000123	1.0	2.0
1385_0000124	2.0	2.0
1385_0000125	2.0	2.0
1385_0000126	1.0	2.0
1385_0000127	2.0	2.0
1385_0000128	1.0	2.0
1385_0000129	1.0	2.0
1385_0000130	1.0	2.0
1385_0001103	1.0	1.0
1385_0001104	1.0	1.0
1385_0001105	1.0	2.0
1385_0001107	1.0	2.0
1385_0001108	1.0	2.0
1385_0001109	1.0	2.0
1385_0001110	2.0	2.0
1385_0001111	2.0	2.0
1385_0001112	2.0	2.0
1385_0001113	1.0	2.0
1385_0001118	2.0	2.0
1385_0001119	2.0	2.0
1385_0001120	2.0	0.0
1385_0001121	2.0	2.0
1385_0001122	2.0	0.0
1385_0001123	2.0	0.0
1385_0001124	1.0	1.0
1385_0001125	1.0	2.0
1385_0001126	0.0	1.0
1385_0001127	2.0	2.0
1385_0001128	1.0	2.0
1385_0001129	1.0	2.0
1385_0001130	1.0	1.0
1385_0001131	1.0	1.0
1385_0001132	1.0	1.0
1385_0001133	2.0	2.0
1385_0001134	1.0	1.0
1385_0001135	1.0	2.0
1385_0001136	1.0	2.0
1385_0001137	1.0	2.0
1385_0001138	1.0	2.0
1385_0001147	1.0	2.0
1385_0001148	2.0	2.0
1385_0001149	2.0	2.0
1385_0001150	1.0	1.0
1385_0001151	2.0	2.0
1385_0001152	2.0	2.0
1385_0001153	2.0	1.0
1385_0001154	1.0	2.0
1385_0001155	1.0	2.0
1385_0001156	1.0	2.0
1385_0001157	1.0	0.0
1385_0001158	1.0	2.0
1385_0001159	1.0	2.0
1385_0001160	1.0	2.0
1385_0001161	1.0	2.0
1385_0001162	1.0	2.0
1385_0001163	1.0	2.0
1385_0001164	1.0	2.0
1385_0001165	1.0	2.0
1385_0001166	1.0	2.0
1385_0001167	1.0	2.0
1385_0001169	1.0	1.0
1385_0001170	1.0	1.0
1385_0001171	0.0	1.0
1385_0001172	1.0	2.0
1385_0001173	0.0	1.0
1385_0001174	1.0	1.0
1385_0001175	0.0	1.0
1385_0001178	1.0	1.0
1385_0001188	1.0	2.0
1385_0001189	1.0	2.0
1385_0001190	1.0	2.0
1385_0001191	1.0	2.0
1385_0001192	1.0	2.0
1385_0001193	1.0	2.0
1385_0001194	1.0	2.0
1385_0001195	2.0	2.0
1385_0001196	1.0	2.0
1385_0001197	1.0	2.0
1385_0001198	1.0	2.0
1385_0001199	1.0	2.0
1385_0001501	1.0	1.0
1385_0001503	1.0	2.0
1385_0001522	0.0	1.0
1385_0001523	1.0	2.0
1385_0001524	1.0	2.0
1385_0001525	1.0	2.0
1385_0001526	0.0	1.0
1385_0001527	2.0	2.0
1385_0001528	1.0	2.0
1385_0001712	1.0	2.0
1385_0001714	0.0	1.0
1385_0001715	1.0	2.0
1385_0001716	1.0	2.0
1385_0001717	1.0	2.0
1385_0001718	0.0	1.0
1385_0001719	1.0	2.0
1385_0001720	0.0	1.0
1385_0001723	0.0	1.0
1385_0001724	1.0	2.0
1385_0001725	1.0	1.0
1385_0001726	1.0	2.0
1385_0001727	0.0	2.0
1385_0001728	1.0	2.0
1385_0001729	1.0	2.0
1385_0001730	1.0	2.0
1385_0001732	1.0	2.0
1385_0001733	1.0	2.0
1385_0001734	1.0	2.0
1385_0001736	1.0	2.0
1385_0001737	1.0	2.0
1385_0001738	0.0	0.0
1385_0001739	1.0	2.0
1385_0001740	1.0	2.0
1385_0001741	0.0	1.0
1385_0001742	0.0	0.0
1385_0001744	0.0	2.0
1385_0001746	1.0	2.0
1385_0001747	1.0	2.0
1385_0001748	1.0	1.0
1385_0001749	1.0	2.0
1385_0001750	0.0	1.0
1385_0001751	1.0	2.0
1385_0001752	1.0	2.0
1385_0001753	1.0	2.0
1385_0001754	1.0	2.0
1385_0001756	1.0	2.0
1385_0001757	1.0	1.0
1385_0001758	1.0	1.0
1385_0001759	1.0	2.0
1385_0001760	1.0	2.0
1385_0001761	1.0	2.0
1385_0001762	1.0	2.0
1385_0001764	1.0	2.0
1385_0001765	0.0	1.0
1385_0001766	2.0	2.0
1385_0001767	1.0	2.0
1385_0001768	2.0	2.0
1385_0001771	1.0	2.0
1385_0001772	1.0	1.0
1385_0001773	0.0	1.0
1385_0001774	0.0	1.0
1385_0001775	1.0	2.0
1385_0001785	1.0	2.0
1385_0001786	1.0	2.0
1385_0001787	0.0	1.0
1385_0001788	1.0	2.0
1385_0001789	1.0	2.0
1385_0001790	1.0	2.0
1385_0001791	1.0	2.0
1385_0001792	1.0	2.0
1385_0001793	1.0	2.0
1385_0001794	1.0	2.0
1385_0001795	0.0	1.0
1385_0001796	1.0	2.0
1385_0001798	1.0	2.0
1385_0001799	2.0	2.0
1385_0001800	1.0	2.0
1395_0000333	1.0	2.0
1395_0000337	1.0	1.0
1395_0000338	2.0	2.0
1395_0000340	2.0	2.0
1395_0000341	2.0	2.0
1395_0000353	1.0	2.0
1395_0000354	1.0	2.0
1395_0000355	2.0	1.0
1395_0000356	1.0	2.0
1395_0000357	3.0	2.0
1395_0000359	2.0	2.0
1395_0000360	3.0	2.0
1395_0000361	2.0	2.0
1395_0000364	2.0	2.0
1395_0000365	2.0	2.0
1395_0000366	2.0	2.0
1395_0000368	0.0	1.0
1395_0000369	2.0	2.0
1395_0000376	2.0	2.0
1395_0000378	2.0	2.0
1395_0000379	2.0	2.0
1395_0000380	2.0	2.0
1395_0000383	2.0	2.0
1395_0000387	3.0	2.0
1395_0000388	2.0	2.0
1395_0000389	1.0	1.0
1395_0000390	1.0	1.0
1395_0000391	3.0	2.0
1395_0000392	2.0	2.0
1395_0000396	2.0	2.0
1395_0000398	2.0	2.0
1395_0000399	2.0	2.0
1395_0000402	2.0	2.0
1395_0000403	2.0	2.0
1395_0000404	2.0	2.0
1395_0000409	2.0	2.0
1395_0000413	2.0	2.0
1395_0000414	2.0	2.0
1395_0000415	2.0	2.0
1395_0000432	2.0	2.0
1395_0000438	3.0	3.0
1395_0000443	2.0	2.0
1395_0000446	2.0	3.0
1395_0000447	2.0	2.0
1395_0000448	2.0	2.0
1395_0000449	2.0	2.0
1395_0000450	1.0	2.0
1395_0000451	2.0	2.0
1395_0000452	1.0	2.0
1395_0000454	2.0	2.0
1395_0000455	2.0	2.0
1395_0000458	1.0	2.0
1395_0000460	1.0	2.0
1395_0000462	2.0	2.0
1395_0000465	1.0	2.0
1395_0000469	2.0	2.0
1395_0000470	2.0	2.0
1395_0000471	2.0	2.0
1395_0000499	2.0	2.0
1395_0000500	1.0	2.0
1395_0000504	2.0	2.0
1395_0000512	2.0	2.0
1395_0000513	2.0	2.0
1395_0000514	3.0	2.0
1395_0000515	2.0	2.0
1395_0000516	1.0	1.0
1395_0000518	2.0	2.0
1395_0000525	2.0	1.0
1395_0000526	1.0	1.0
1395_0000527	1.0	1.0
1395_0000528	2.0	2.0
1395_0000529	2.0	2.0
1395_0000531	2.0	2.0
1395_0000533	2.0	2.0
1395_0000534	2.0	2.0
1395_0000535	1.0	2.0
1395_0000537	2.0	2.0
1395_0000547	2.0	2.0
1395_0000548	2.0	2.0
1395_0000549	2.0	2.0
1395_0000550	2.0	2.0
1395_0000551	2.0	2.0
1395_0000552	2.0	2.0
1395_0000553	2.0	2.0
1395_0000554	2.0	2.0
1395_0000555	1.0	2.0
1395_0000556	1.0	2.0
1395_0000557	2.0	2.0
1395_0000559	2.0	2.0
1395_0000560	2.0	2.0
1395_0000563	2.0	2.0
1395_0000564	2.0	2.0
1395_0000565	1.0	2.0
1395_0000572	1.0	2.0
1395_0000575	1.0	2.0
1395_0000579	1.0	2.0
1395_0000581	1.0	2.0
1395_0000582	0.0	2.0
1395_0000583	1.0	2.0
1395_0000584	1.0	2.0
1395_0000585	1.0	2.0
1395_0000587	0.0	2.0
1395_0000591	0.0	1.0
1395_0000593	1.0	2.0
1395_0000595	1.0	2.0
1395_0000596	2.0	2.0
1395_0000597	1.0	2.0
1395_0000598	1.0	2.0
1395_0000599	1.0	2.0
1395_0000602	1.0	2.0
1395_0000604	0.0	2.0
1395_0000606	0.0	2.0
1395_0000607	0.0	2.0
1395_0000608	1.0	2.0
1395_0000609	1.0	2.0
1395_0000610	2.0	2.0
1395_0000611	1.0	2.0
1395_0000612	0.0	2.0
1395_0000626	2.0	2.0
1395_0000627	1.0	2.0
1395_0000628	1.0	2.0
1395_0000630	1.0	2.0
1395_0000631	1.0	2.0
1395_0000635	1.0	2.0
1395_0000636	1.0	2.0
1395_0000639	1.0	2.0
1395_0000642	1.0	2.0
1395_0000644	1.0	2.0
1395_0000646	1.0	2.0
1395_0000649	1.0	2.0
1395_0001010	1.0	2.0
1395_0001013	1.0	2.0
1395_0001015	1.0	2.0
1395_0001016	1.0	2.0
1395_0001017	1.0	2.0
1395_0001019	1.0	2.0
1395_0001020	1.0	2.0
1395_0001021	1.0	2.0
1395_0001022	1.0	2.0
1395_0001023	1.0	2.0
1395_0001024	2.0	2.0
1395_0001028	1.0	2.0
1395_0001033	1.0	2.0
1395_0001034	1.0	2.0
1395_0001040	0.0	1.0
1395_0001045	2.0	2.0
1395_0001058	1.0	2.0
1395_0001060	1.0	2.0
1395_0001061	2.0	3.0
1395_0001064	2.0	2.0
1395_0001065	1.0	2.0
1395_0001066	1.0	2.0
1395_0001067	1.0	2.0
1395_0001068	1.0	2.0
1395_0001069	2.0	2.0
1395_0001070	2.0	2.0
1395_0001071	2.0	1.0
1395_0001073	1.0	2.0
1395_0001074	1.0	2.0
1395_0001075	1.0	2.0
1395_0001076	1.0	2.0
1395_0001078	0.0	2.0
1395_0001080	2.0	2.0
1395_0001084	1.0	2.0
1395_0001090	1.0	2.0
1395_0001093	1.0	2.0
1395_0001101	2.0	2.0
1395_0001103	1.0	2.0
1395_0001104	1.0	2.0
1395_0001108	1.0	2.0
1395_0001109	1.0	2.0
1395_0001114	0.0	2.0
1395_0001115	2.0	2.0
1395_0001116	2.0	2.0
1395_0001117	1.0	2.0
1395_0001118	1.0	2.0
1395_0001119	2.0	2.0
1395_0001120	1.0	2.0
1395_0001121	0.0	1.0
1395_0001122	0.0	2.0
1395_0001123	1.0	2.0
1395_0001124	1.0	2.0
1395_0001126	1.0	2.0
1395_0001131	0.0	1.0
1395_0001132	2.0	2.0
1395_0001133	1.0	2.0
1395_0001141	2.0	2.0
1395_0001145	2.0	3.0
1395_0001146	0.0	1.0
1395_0001147	2.0	2.0
1395_0001149	1.0	2.0
1395_0001150	0.0	2.0
1395_0001158	1.0	2.0
1395_0001160	1.0	2.0
1395_0001161	1.0	2.0
1395_0001164	2.0	2.0
1395_0001167	1.0	2.0
1395_0001169	2.0	2.0
1395_0001170	1.0	2.0
1395_0001171	1.0	2.0
Language = IT, Weighted F1-score = 0.4695722255266402, Dimension = Vocabularyrange

Train on DE, test on  CZ
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
144.90552995391704 65.35717405024758
LANGUAGE: IT
nan nan
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']


======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 1.21
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.71
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.00      0.00      0.00       131
         2.0       0.41      0.40      0.40       182
         3.0       0.37      0.95      0.53       100
         4.0       0.00      0.00      0.00        16

    accuracy                           0.38       434
   macro avg       0.16      0.27      0.19       434
weighted avg       0.26      0.38      0.29       434

[[  0   0   4   1   0]
 [  0   0  95  36   0]
 [  0   0  72 110   0]
 [  0   0   5  95   0]
 [  0   0   0  16   0]]
0.29096619725561873
434 434 434



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 1.02
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.27
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.63      0.13      0.22       131
         2.0       0.44      0.54      0.49       182
         3.0       0.48      0.89      0.63       100
         4.0       0.00      0.00      0.00        16

    accuracy                           0.47       434
   macro avg       0.31      0.31      0.27       434
weighted avg       0.49      0.47      0.41       434

[[  0   2   3   0   0]
 [  0  17 111   3   0]
 [  0   7  99  76   0]
 [  0   1  10  89   0]
 [  0   0   0  16   0]]
0.4143864228883121
434 434 434



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.88
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.39
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.58      0.19      0.29       131
         2.0       0.43      0.44      0.43       182
         3.0       0.44      0.90      0.59       100
         4.0       0.00      0.00      0.00        16

    accuracy                           0.45       434
   macro avg       0.29      0.31      0.26       434
weighted avg       0.46      0.45      0.41       434

[[ 0  2  3  0  0]
 [ 0 25 94 12  0]
 [ 0 15 80 87  0]
 [ 0  1  9 90  0]
 [ 0  0  0 16  0]]
0.40504724973767553
434 434 434



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.77
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.74
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.67      0.08      0.14       131
         2.0       0.45      0.41      0.43       182
         3.0       0.37      0.94      0.53       100
         4.0       0.00      0.00      0.00        16

    accuracy                           0.41       434
   macro avg       0.30      0.29      0.22       434
weighted avg       0.48      0.41      0.34       434

[[  0   2   2   1   0]
 [  0  10  84  37   0]
 [  0   2  75 105   0]
 [  0   1   5  94   0]
 [  0   0   0  16   0]]
0.3448185984096612
434 434 434
Filename	True Label	Prediction
0601	1.0	2.0
0602	1.0	3.0
0603	2.0	3.0
0604	2.0	3.0
0605	2.0	3.0
0606	2.0	2.0
0607	3.0	2.0
0608	1.0	2.0
0609	2.0	2.0
0610	1.0	3.0
0611	2.0	2.0
0612	1.0	2.0
0613	1.0	2.0
0614	2.0	3.0
0615	1.0	2.0
0616	1.0	2.0
0617	1.0	2.0
0618	1.0	3.0
0619	1.0	2.0
0620	1.0	2.0
0621	1.0	2.0
0622	1.0	2.0
0623	1.0	2.0
0624	2.0	2.0
0625	1.0	2.0
0626	2.0	2.0
0627	2.0	3.0
0628	2.0	2.0
0629	2.0	2.0
0630	1.0	2.0
0631	2.0	2.0
0632	1.0	2.0
0633	2.0	2.0
0634	3.0	3.0
0635	2.0	2.0
0636	2.0	2.0
0637	2.0	2.0
0638	2.0	2.0
0639	1.0	2.0
0640	2.0	2.0
0641	1.0	2.0
0642	1.0	2.0
0643	2.0	2.0
0644	1.0	3.0
0645	2.0	2.0
0714	2.0	3.0
0715	2.0	2.0
0716	2.0	2.0
0717	1.0	2.0
0718	2.0	2.0
0719	2.0	2.0
0720	1.0	2.0
0721	2.0	2.0
0722	2.0	2.0
0723	2.0	2.0
0724	3.0	3.0
0725	1.0	3.0
0801	2.0	2.0
0802	2.0	2.0
0803	2.0	2.0
0804	1.0	2.0
0805	2.0	2.0
0806	2.0	3.0
0807	2.0	2.0
0808	2.0	2.0
0809	2.0	2.0
0810	1.0	2.0
0811	2.0	2.0
0812	1.0	2.0
0813	1.0	2.0
0814	2.0	2.0
0815	2.0	3.0
0816	3.0	2.0
0817	1.0	2.0
0818	1.0	2.0
0819	3.0	2.0
0820	2.0	2.0
0821	1.0	3.0
0822	1.0	2.0
0823	1.0	2.0
0824	1.0	2.0
0825	2.0	2.0
0826	2.0	2.0
0827	2.0	2.0
0828	2.0	2.0
0829	1.0	3.0
0901	2.0	3.0
0902	2.0	2.0
0903	1.0	2.0
0904	1.0	2.0
0905	2.0	2.0
0906	2.0	3.0
0907	2.0	2.0
0910	1.0	2.0
0911	1.0	2.0
0912	1.0	2.0
0913	2.0	2.0
0914	1.0	2.0
0915	3.0	2.0
0916	2.0	2.0
0917	1.0	2.0
0918	1.0	2.0
0919	2.0	2.0
0920	2.0	3.0
0921	1.0	2.0
0922	1.0	2.0
0923	2.0	2.0
0924	1.0	2.0
0925	2.0	2.0
0926	2.0	3.0
0927	2.0	2.0
0928	1.0	3.0
0929	1.0	3.0
0930	2.0	2.0
1001	1.0	2.0
1002	2.0	3.0
1003	1.0	2.0
1004	1.0	2.0
1005	1.0	2.0
1006	2.0	2.0
1007	2.0	2.0
1008	1.0	3.0
1009	2.0	3.0
1010	2.0	2.0
1014	2.0	2.0
1015	1.0	3.0
1016	2.0	2.0
1017	2.0	2.0
1018	2.0	3.0
1019	2.0	3.0
1020	2.0	2.0
1021	2.0	3.0
1022	2.0	2.0
1023	2.0	3.0
1111	1.0	2.0
1112	1.0	3.0
1113	2.0	3.0
1114	3.0	3.0
1115	2.0	3.0
1116	2.0	2.0
1117	1.0	2.0
9999	1.0	2.0
BER0609003	3.0	3.0
BER0611003	2.0	3.0
BER0611005	3.0	3.0
BER0611006	3.0	3.0
BER0611007	3.0	3.0
KYJ0611003A	1.0	2.0
KYJ0611004A	1.0	2.0
KYJ0611005A	0.0	1.0
KYJ0611005B	1.0	3.0
KYJ0611006A	1.0	2.0
KYJ0611006B	0.0	2.0
KYJ0611009A	1.0	2.0
KYJ0611009B	0.0	3.0
LIB0611001A	1.0	2.0
LIB0611001B	1.0	3.0
LIB0611002A	1.0	2.0
LIB0611002B	1.0	3.0
LIB0611003A	1.0	2.0
LIB0611004A	1.0	1.0
LIB0611004B	1.0	2.0
LIB0611011	2.0	3.0
LON0610002A	1.0	2.0
LON0610002B	2.0	3.0
LON0611002A	1.0	2.0
LON0611002B	0.0	2.0
LON0611003	3.0	3.0
LON0611004A	0.0	1.0
LON0611004B	1.0	2.0
MOS0509001	2.0	3.0
MOS0509004	2.0	3.0
MOS0611012	2.0	3.0
MOS0611013	2.0	3.0
MOS0611014	2.0	3.0
MOS0611015	2.0	3.0
PAR1011008A	1.0	2.0
PAR1011009A	2.0	2.0
PAR1011009B	1.0	3.0
PAR1011013	2.0	3.0
PAR1011014	3.0	3.0
PAR1011015	2.0	3.0
PAR1011016	2.0	3.0
PAR1011017	3.0	3.0
PAR1011018	3.0	3.0
PHA0111001A	1.0	1.0
PHA0111001B	1.0	3.0
PHA0111002A	2.0	2.0
PHA0111002B	3.0	3.0
PHA0111003A	1.0	1.0
PHA0111003B	1.0	3.0
PHA0111004A	1.0	1.0
PHA0111004B	1.0	3.0
PHA0111005A	1.0	2.0
PHA0111005B	1.0	3.0
PHA0111010	4.0	3.0
PHA0111011	3.0	3.0
PHA0111012	2.0	3.0
PHA0111014	1.0	3.0
PHA0111015	4.0	3.0
PHA0111016	4.0	3.0
PHA0111018	2.0	3.0
PHA0112002A	2.0	2.0
PHA0112002B	1.0	3.0
PHA0112003A	2.0	1.0
PHA0112003B	1.0	2.0
PHA0112006A	3.0	2.0
PHA0112006B	3.0	3.0
PHA0112007A	2.0	2.0
PHA0112007B	1.0	2.0
PHA0112009A	2.0	2.0
PHA0112009B	1.0	2.0
PHA0112012A	2.0	3.0
PHA0112012B	1.0	3.0
PHA0209001	2.0	3.0
PHA0209008	2.0	2.0
PHA0209013	1.0	2.0
PHA0209024	3.0	3.0
PHA0209026	3.0	3.0
PHA0209028	2.0	3.0
PHA0209031	4.0	3.0
PHA0209034	2.0	3.0
PHA0209038	4.0	3.0
PHA0209039	3.0	3.0
PHA0210001	1.0	3.0
PHA0210004	2.0	3.0
PHA0210007	2.0	3.0
PHA0210008	2.0	2.0
PHA0411008A	2.0	2.0
PHA0411008B	1.0	3.0
PHA0411009A	1.0	2.0
PHA0411009B	1.0	2.0
PHA0411010A	1.0	1.0
PHA0411010B	1.0	2.0
PHA0411011A	1.0	1.0
PHA0411011B	1.0	2.0
PHA0411012A	1.0	1.0
PHA0411012B	1.0	3.0
PHA0411027	2.0	3.0
PHA0411028	2.0	3.0
PHA0411029	3.0	3.0
PHA0411030	3.0	3.0
PHA0411031	3.0	3.0
PHA0411032	2.0	3.0
PHA0411033	2.0	3.0
PHA0411034	2.0	3.0
PHA0411035	3.0	3.0
PHA0411036	3.0	3.0
PHA0411037	2.0	3.0
PHA0411038	3.0	3.0
PHA0411039	3.0	3.0
PHA0411041	3.0	3.0
PHA0411042	2.0	3.0
PHA0411043	3.0	3.0
PHA0411044	3.0	3.0
PHA0411045	2.0	3.0
PHA0411047	3.0	3.0
PHA0411051	3.0	3.0
PHA0411053	3.0	3.0
PHA0411054	3.0	3.0
PHA0411055	3.0	3.0
PHA0411056	4.0	3.0
PHA0411058	3.0	3.0
PHA0411059	2.0	3.0
PHA0411060	3.0	3.0
PHA0411061	2.0	3.0
PHA0411062	2.0	3.0
PHA0509002	1.0	2.0
PHA0509007	1.0	2.0
PHA0509013	1.0	2.0
PHA0509015	3.0	3.0
PHA0509017	3.0	3.0
PHA0509018	3.0	3.0
PHA0509019	2.0	3.0
PHA0509020	3.0	3.0
PHA0509021	3.0	3.0
PHA0509022	4.0	3.0
PHA0509024	3.0	3.0
PHA0509025	4.0	3.0
PHA0509026	4.0	3.0
PHA0509027	2.0	3.0
PHA0509028	3.0	3.0
PHA0509030	3.0	3.0
PHA0509031	2.0	3.0
PHA0509032	2.0	3.0
PHA0509033	2.0	3.0
PHA0509034	2.0	3.0
PHA0509035	3.0	3.0
PHA0509036	2.0	3.0
PHA0509037	3.0	3.0
PHA0509038	2.0	3.0
PHA0509039	2.0	3.0
PHA0509040	2.0	3.0
PHA0509041	3.0	3.0
PHA0509042	3.0	3.0
PHA0509043	3.0	3.0
PHA0509044	2.0	3.0
PHA0509045	2.0	3.0
PHA0510002A	2.0	2.0
PHA0510002B	1.0	3.0
PHA0510003A	2.0	2.0
PHA0510003B	1.0	3.0
PHA0510004A	1.0	2.0
PHA0510004B	1.0	3.0
PHA0510010A	2.0	1.0
PHA0510010B	1.0	1.0
PHA0510013A	1.0	2.0
PHA0510013B	1.0	3.0
PHA0510023	4.0	3.0
PHA0510027	2.0	3.0
PHA0510029	3.0	3.0
PHA0510030	2.0	3.0
PHA0510031	3.0	3.0
PHA0510032	2.0	3.0
PHA0510034	3.0	3.0
PHA0510035	2.0	3.0
PHA0510036	3.0	3.0
PHA0510037	2.0	3.0
PHA0510038	3.0	3.0
PHA0510039	2.0	3.0
PHA0510040	2.0	3.0
PHA0510046	2.0	3.0
PHA0510047	2.0	3.0
PHA0510048	3.0	3.0
PHA0510049	2.0	3.0
PHA0510050	2.0	3.0
PHA0610005A	1.0	1.0
PHA0610005B	1.0	2.0
PHA0610006A	2.0	2.0
PHA0610006B	1.0	3.0
PHA0610007A	3.0	1.0
PHA0610007B	1.0	3.0
PHA0610015	2.0	3.0
PHA0610016	3.0	3.0
PHA0610017	3.0	3.0
PHA0610018	3.0	3.0
PHA0610019A	2.0	2.0
PHA0610019B	2.0	3.0
PHA0610025	3.0	3.0
PHA0610026	3.0	3.0
PHA0709008	3.0	3.0
PHA0710009	2.0	3.0
PHA0710010	2.0	3.0
PHA0710011	3.0	3.0
PHA0710012	3.0	3.0
PHA0710013	4.0	3.0
PHA0710014	3.0	3.0
PHA0710015	3.0	3.0
PHA0710016	3.0	3.0
PHA0710017	3.0	3.0
PHA0710018	3.0	3.0
PHA0710019	4.0	3.0
PHA0710021	4.0	3.0
PHA0809009	2.0	3.0
PHA0809010	3.0	3.0
PHA0810001	2.0	3.0
PHA0810002	2.0	3.0
PHA0810003	2.0	3.0
PHA0810004	3.0	3.0
PHA0810006	2.0	3.0
PHA0810008	2.0	3.0
PHA0810009	3.0	3.0
PHA0810010	3.0	3.0
PHA0810011	2.0	3.0
PHA0810012	2.0	3.0
PHA0810015	2.0	3.0
PHA0811010	2.0	3.0
PHA0811012	4.0	3.0
PHA0811013	3.0	3.0
PHA0811014	2.0	3.0
PHA0811016	3.0	3.0
PHA0811017	3.0	3.0
PHA0811019	3.0	3.0
PHA0811020	2.0	3.0
PHA1109001	2.0	3.0
PHA1109002	3.0	3.0
PHA1109003	2.0	3.0
PHA1109004	3.0	3.0
PHA1109005	3.0	3.0
PHA1109006	2.0	3.0
PHA1109007	3.0	3.0
PHA1109008	1.0	2.0
PHA1109023	2.0	2.0
PHA1109024	4.0	3.0
PHA1109025	1.0	2.0
PHA1109026	3.0	3.0
PHA1109027	4.0	3.0
PHA1109028	2.0	3.0
PHA1110001A	2.0	2.0
PHA1110001B	1.0	2.0
PHA1110002A	2.0	2.0
PHA1110002B	2.0	3.0
PHA1110003A	1.0	2.0
PHA1110003B	1.0	3.0
PHA1110004A	2.0	2.0
PHA1110013	2.0	3.0
PHA1110014	3.0	3.0
PHA1110015	3.0	3.0
PHA1110016	2.0	3.0
PHA1110017	2.0	3.0
PHA1110019	2.0	3.0
PHA1110021	3.0	3.0
PHA1110022	3.0	3.0
PHA1111001A	2.0	2.0
PHA1111001B	1.0	3.0
PHA1111002A	1.0	1.0
PHA1111002B	1.0	2.0
PHA1111003A	1.0	2.0
PHA1111003B	1.0	3.0
PHA1111004A	2.0	2.0
PHA1111004B	1.0	2.0
PHA1111006A	1.0	2.0
PHA1111006B	1.0	3.0
PHA1111008A	2.0	2.0
PHA1111008B	1.0	3.0
PHA1111009A	1.0	2.0
ST071122B	1.0	2.0
TI071122B	2.0	3.0
VAR0209036	3.0	3.0
VAR0909003	3.0	3.0
VAR0909004	3.0	3.0
VAR0909005	2.0	3.0
VAR0909006	2.0	3.0
VAR0909007	3.0	3.0
VAR0909008	3.0	3.0
VAR0909009	3.0	3.0
VAR0909010	3.0	3.0
VAR0910004	3.0	3.0
VAR0910005	3.0	3.0
VAR0910006	2.0	3.0
VAR0910007	2.0	3.0
VAR0910009	3.0	3.0
VAR0910010	3.0	3.0
VAR0910011	2.0	3.0
Language = CZ, Weighted F1-score = 0.3448185984096612, Dimension = Vocabularycontrol

Train on DE, test on  IT
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
nan nan
LANGUAGE: IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']


======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 1.20
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.04
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        70
         1.0       0.49      0.81      0.61       204
         2.0       0.57      0.40      0.47       327
         3.0       0.62      0.73      0.67       196
         4.0       0.00      0.00      0.00         3

    accuracy                           0.55       800
   macro avg       0.34      0.39      0.35       800
weighted avg       0.51      0.55      0.51       800

[[  0  60  10   0   0]
 [  0 165  39   0   0]
 [  0 112 132  83   0]
 [  0   3  50 143   0]
 [  0   0   0   3   0]]
0.5129451850094877
800 800 800



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.99
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 1.05
              precision    recall  f1-score   support

         0.0       0.67      0.03      0.05        70
         1.0       0.47      0.88      0.61       204
         2.0       0.56      0.52      0.54       327
         3.0       0.76      0.43      0.55       196
         4.0       0.00      0.00      0.00         3

    accuracy                           0.54       800
   macro avg       0.49      0.37      0.35       800
weighted avg       0.59      0.54      0.51       800

[[  2  62   6   0   0]
 [  0 180  24   0   0]
 [  1 132 169  25   0]
 [  0   8 104  84   0]
 [  0   0   1   2   0]]
0.514471551127422
800 800 800



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.88
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.53      0.14      0.22        70
         1.0       0.51      0.79      0.62       204
         2.0       0.58      0.58      0.58       327
         3.0       0.75      0.53      0.62       196
         4.0       0.00      0.00      0.00         3

    accuracy                           0.58       800
   macro avg       0.47      0.41      0.41       800
weighted avg       0.60      0.58      0.57       800

[[ 10  50  10   0   0]
 [  5 162  37   0   0]
 [  4 101 190  32   0]
 [  0   4  88 104   0]
 [  0   0   0   3   0]]
0.5685905064513078
800 800 800



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.77
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.03
              precision    recall  f1-score   support

         0.0       0.47      0.13      0.20        70
         1.0       0.50      0.81      0.62       204
         2.0       0.58      0.44      0.50       327
         3.0       0.64      0.67      0.66       196
         4.0       0.00      0.00      0.00         3

    accuracy                           0.56       800
   macro avg       0.44      0.41      0.39       800
weighted avg       0.56      0.56      0.54       800

[[  9  52   9   0   0]
 [  6 165  33   0   0]
 [  4 110 143  70   0]
 [  0   4  61 131   0]
 [  0   0   0   3   0]]
0.5394796734469813
800 800 800
Filename	True Label	Prediction
1325_1001008	3.0	3.0
1325_1001009	3.0	2.0
1325_1001010	3.0	3.0
1325_1001011	3.0	3.0
1325_1001012	3.0	3.0
1325_1001013	3.0	3.0
1325_1001014	3.0	3.0
1325_1001015	2.0	3.0
1325_1001016	2.0	3.0
1325_1001017	3.0	3.0
1325_1001018	2.0	3.0
1325_1001019	3.0	3.0
1325_1001020	2.0	2.0
1325_1001021	3.0	3.0
1325_1001022	3.0	3.0
1325_1001023	3.0	2.0
1325_1001024	2.0	3.0
1325_1001025	2.0	3.0
1325_1001027	3.0	3.0
1325_1001028	3.0	3.0
1325_1001029	3.0	3.0
1325_1001032	2.0	3.0
1325_1001033	3.0	3.0
1325_1001035	3.0	3.0
1325_1001036	3.0	3.0
1325_1001037	3.0	2.0
1325_1001039	3.0	3.0
1325_1001040	3.0	3.0
1325_1001041	3.0	3.0
1325_1001042	3.0	3.0
1325_1001043	3.0	3.0
1325_1001044	3.0	3.0
1325_1001045	3.0	3.0
1325_1001046	2.0	3.0
1325_1001047	2.0	2.0
1325_1001048	2.0	3.0
1325_1001050	3.0	3.0
1325_1001051	3.0	3.0
1325_1001052	2.0	3.0
1325_1001053	2.0	2.0
1325_1001054	3.0	3.0
1325_1001055	3.0	3.0
1325_1001056	2.0	3.0
1325_1001057	2.0	2.0
1325_1001058	3.0	3.0
1325_1001059	2.0	3.0
1325_1001062	3.0	3.0
1325_1001063	2.0	3.0
1325_1001075	2.0	3.0
1325_1001076	2.0	3.0
1325_1001077	3.0	2.0
1325_1001078	3.0	3.0
1325_1001079	3.0	3.0
1325_1001080	3.0	3.0
1325_1001081	2.0	3.0
1325_1001082	2.0	3.0
1325_1001083	2.0	3.0
1325_1001084	3.0	2.0
1325_1001085	3.0	2.0
1325_1001086	3.0	3.0
1325_1001087	3.0	3.0
1325_1001088	2.0	2.0
1325_1001089	2.0	2.0
1325_1001090	3.0	2.0
1325_1001091	2.0	3.0
1325_1001092	2.0	2.0
1325_1001093	2.0	3.0
1325_1001094	2.0	3.0
1325_1001095	2.0	3.0
1325_1001096	2.0	3.0
1325_1001097	1.0	2.0
1325_1001098	2.0	3.0
1325_1001099	3.0	3.0
1325_1001100	3.0	3.0
1325_1001101	3.0	3.0
1325_1001107	3.0	3.0
1325_1001108	3.0	3.0
1325_1001109	2.0	2.0
1325_1001110	3.0	3.0
1325_1001111	3.0	3.0
1325_1001113	3.0	3.0
1325_1001119	3.0	3.0
1325_1001120	3.0	3.0
1325_1001121	3.0	3.0
1325_1001122	2.0	2.0
1325_1001123	3.0	3.0
1325_1001124	3.0	2.0
1325_1001125	3.0	3.0
1325_1001126	2.0	2.0
1325_1001127	3.0	3.0
1325_1001128	3.0	3.0
1325_1001129	2.0	2.0
1325_1001130	3.0	2.0
1325_1001131	3.0	3.0
1325_1001132	3.0	3.0
1325_1001133	3.0	3.0
1325_1001134	3.0	3.0
1325_1001135	3.0	3.0
1325_1001136	2.0	3.0
1325_1001138	2.0	3.0
1325_1001139	3.0	3.0
1325_1001141	2.0	3.0
1325_1001142	2.0	3.0
1325_1001143	3.0	3.0
1325_1001144	3.0	2.0
1325_1001152	3.0	3.0
1325_1001153	3.0	3.0
1325_1001154	3.0	3.0
1325_1001155	3.0	3.0
1325_1001156	2.0	3.0
1325_1001157	2.0	3.0
1325_1001158	3.0	3.0
1325_1001159	3.0	3.0
1325_1001160	3.0	3.0
1325_1001161	2.0	3.0
1325_1001162	2.0	3.0
1325_1001163	2.0	3.0
1325_1001164	3.0	3.0
1325_1001165	2.0	3.0
1325_1001166	3.0	3.0
1325_1001167	3.0	3.0
1325_1001168	2.0	3.0
1325_1001169	3.0	3.0
1325_1001170	3.0	3.0
1325_9000059	2.0	3.0
1325_9000087	2.0	3.0
1325_9000088	2.0	3.0
1325_9000089	3.0	3.0
1325_9000090	3.0	3.0
1325_9000095	2.0	3.0
1325_9000099	3.0	3.0
1325_9000102	3.0	2.0
1325_9000104	3.0	3.0
1325_9000105	2.0	2.0
1325_9000106	3.0	3.0
1325_9000107	3.0	3.0
1325_9000136	3.0	3.0
1325_9000137	3.0	3.0
1325_9000138	4.0	3.0
1325_9000139	3.0	3.0
1325_9000140	3.0	3.0
1325_9000143	3.0	3.0
1325_9000144	3.0	3.0
1325_9000152	3.0	3.0
1325_9000185	3.0	3.0
1325_9000186	3.0	3.0
1325_9000187	3.0	3.0
1325_9000188	3.0	3.0
1325_9000209	3.0	3.0
1325_9000210	2.0	3.0
1325_9000211	2.0	3.0
1325_9000213	3.0	2.0
1325_9000214	3.0	2.0
1325_9000215	3.0	3.0
1325_9000237	3.0	3.0
1325_9000239	3.0	3.0
1325_9000240	2.0	3.0
1325_9000241	3.0	3.0
1325_9000278	3.0	3.0
1325_9000279	3.0	3.0
1325_9000296	2.0	3.0
1325_9000302	2.0	3.0
1325_9000303	2.0	3.0
1325_9000304	3.0	2.0
1325_9000314	2.0	2.0
1325_9000315	2.0	2.0
1325_9000316	3.0	3.0
1325_9000317	3.0	3.0
1325_9000318	3.0	3.0
1325_9000319	2.0	3.0
1325_9000320	3.0	2.0
1325_9000321	3.0	3.0
1325_9000322	3.0	3.0
1325_9000323	2.0	3.0
1325_9000503	3.0	3.0
1325_9000504	3.0	3.0
1325_9000505	3.0	3.0
1325_9000533	3.0	3.0
1325_9000534	3.0	3.0
1325_9000536	3.0	3.0
1325_9000554	2.0	3.0
1325_9000601	3.0	3.0
1325_9000602	4.0	3.0
1325_9000611	2.0	3.0
1325_9000612	2.0	3.0
1325_9000674	3.0	3.0
1325_9000675	3.0	3.0
1325_9000676	3.0	3.0
1325_9000677	3.0	3.0
1325_9000678	3.0	3.0
1325_9000684	3.0	3.0
1325_9000685	4.0	3.0
1325_9000686	2.0	3.0
1325_9000700	3.0	3.0
1325_9000750	3.0	2.0
1365_0100002	3.0	2.0
1365_0100003	2.0	1.0
1365_0100004	2.0	2.0
1365_0100005	1.0	1.0
1365_0100006	2.0	2.0
1365_0100007	2.0	1.0
1365_0100008	2.0	2.0
1365_0100009	2.0	1.0
1365_0100010	2.0	1.0
1365_0100011	2.0	2.0
1365_0100012	2.0	1.0
1365_0100013	2.0	2.0
1365_0100014	2.0	1.0
1365_0100015	1.0	1.0
1365_0100016	2.0	2.0
1365_0100017	2.0	2.0
1365_0100018	2.0	2.0
1365_0100019	2.0	1.0
1365_0100020	2.0	2.0
1365_0100021	2.0	2.0
1365_0100022	2.0	2.0
1365_0100023	2.0	2.0
1365_0100024	2.0	1.0
1365_0100026	2.0	1.0
1365_0100027	3.0	2.0
1365_0100028	2.0	2.0
1365_0100029	1.0	1.0
1365_0100030	2.0	1.0
1365_0100031	2.0	1.0
1365_0100051	2.0	2.0
1365_0100056	2.0	2.0
1365_0100057	2.0	3.0
1365_0100058	3.0	2.0
1365_0100061	3.0	2.0
1365_0100063	3.0	3.0
1365_0100064	3.0	2.0
1365_0100065	1.0	1.0
1365_0100066	2.0	2.0
1365_0100067	2.0	2.0
1365_0100069	3.0	2.0
1365_0100070	2.0	2.0
1365_0100071	3.0	2.0
1365_0100072	2.0	2.0
1365_0100073	3.0	2.0
1365_0100074	2.0	3.0
1365_0100079	2.0	1.0
1365_0100080	2.0	2.0
1365_0100092	2.0	2.0
1365_0100093	2.0	1.0
1365_0100094	2.0	1.0
1365_0100095	2.0	1.0
1365_0100096	2.0	2.0
1365_0100097	2.0	1.0
1365_0100098	2.0	2.0
1365_0100099	2.0	2.0
1365_0100100	3.0	3.0
1365_0100101	2.0	2.0
1365_0100102	2.0	2.0
1365_0100103	3.0	2.0
1365_0100104	2.0	3.0
1365_0100105	3.0	2.0
1365_0100106	2.0	2.0
1365_0100107	3.0	3.0
1365_0100116	3.0	2.0
1365_0100117	3.0	2.0
1365_0100118	2.0	2.0
1365_0100119	3.0	3.0
1365_0100120	3.0	3.0
1365_0100121	3.0	2.0
1365_0100123	2.0	2.0
1365_0100125	3.0	2.0
1365_0100133	2.0	2.0
1365_0100134	2.0	1.0
1365_0100135	2.0	1.0
1365_0100136	2.0	2.0
1365_0100137	2.0	1.0
1365_0100138	2.0	1.0
1365_0100139	2.0	2.0
1365_0100145	3.0	2.0
1365_0100146	3.0	1.0
1365_0100147	3.0	1.0
1365_0100148	3.0	2.0
1365_0100151	2.0	1.0
1365_0100162	3.0	3.0
1365_0100163	3.0	3.0
1365_0100164	3.0	3.0
1365_0100165	3.0	2.0
1365_0100166	2.0	2.0
1365_0100167	2.0	2.0
1365_0100168	3.0	1.0
1365_0100169	2.0	2.0
1365_0100170	2.0	3.0
1365_0100171	2.0	2.0
1365_0100172	2.0	1.0
1365_0100173	2.0	1.0
1365_0100174	2.0	2.0
1365_0100175	2.0	1.0
1365_0100176	2.0	2.0
1365_0100177	3.0	2.0
1365_0100178	2.0	2.0
1365_0100179	2.0	2.0
1365_0100180	2.0	1.0
1365_0100181	2.0	2.0
1365_0100182	2.0	2.0
1365_0100183	2.0	2.0
1365_0100184	2.0	2.0
1365_0100185	2.0	1.0
1365_0100186	2.0	2.0
1365_0100187	3.0	2.0
1365_0100188	2.0	2.0
1365_0100190	3.0	2.0
1365_0100191	2.0	2.0
1365_0100192	3.0	3.0
1365_0100194	3.0	2.0
1365_0100195	2.0	1.0
1365_0100196	2.0	2.0
1365_0100198	2.0	2.0
1365_0100199	2.0	2.0
1365_0100200	3.0	2.0
1365_0100201	2.0	2.0
1365_0100202	2.0	1.0
1365_0100203	2.0	2.0
1365_0100204	2.0	1.0
1365_0100205	2.0	1.0
1365_0100211	3.0	3.0
1365_0100212	3.0	3.0
1365_0100213	2.0	1.0
1365_0100215	2.0	2.0
1365_0100217	3.0	2.0
1365_0100218	3.0	2.0
1365_0100219	2.0	3.0
1365_0100220	3.0	2.0
1365_0100221	2.0	2.0
1365_0100222	2.0	3.0
1365_0100223	2.0	3.0
1365_0100224	3.0	3.0
1365_0100225	3.0	2.0
1365_0100226	3.0	2.0
1365_0100227	3.0	2.0
1365_0100228	2.0	3.0
1365_0100229	2.0	3.0
1365_0100230	2.0	3.0
1365_0100231	2.0	2.0
1365_0100232	2.0	2.0
1365_0100233	3.0	3.0
1365_0100251	3.0	3.0
1365_0100252	3.0	2.0
1365_0100253	2.0	2.0
1365_0100255	2.0	2.0
1365_0100256	2.0	2.0
1365_0100257	2.0	2.0
1365_0100258	2.0	2.0
1365_0100259	2.0	2.0
1365_0100260	2.0	2.0
1365_0100261	2.0	2.0
1365_0100262	3.0	2.0
1365_0100263	3.0	2.0
1365_0100265	3.0	2.0
1365_0100266	2.0	3.0
1365_0100267	2.0	3.0
1365_0100268	2.0	2.0
1365_0100269	2.0	2.0
1365_0100270	2.0	2.0
1365_0100274	3.0	3.0
1365_0100275	3.0	2.0
1365_0100276	3.0	3.0
1365_0100277	3.0	2.0
1365_0100278	3.0	2.0
1365_0100279	2.0	2.0
1365_0100280	1.0	2.0
1365_0100281	2.0	2.0
1365_0100282	2.0	2.0
1365_0100285	2.0	2.0
1365_0100286	2.0	1.0
1365_0100287	2.0	2.0
1365_0100288	2.0	1.0
1365_0100289	2.0	1.0
1365_0100290	2.0	2.0
1365_0100299	3.0	2.0
1365_0100447	3.0	2.0
1365_0100448	2.0	2.0
1365_0100451	3.0	2.0
1365_0100455	3.0	3.0
1365_0100456	3.0	3.0
1365_0100457	3.0	2.0
1365_0100458	2.0	2.0
1365_0100459	3.0	3.0
1365_0100461	3.0	3.0
1365_0100469	2.0	2.0
1365_0100470	2.0	3.0
1365_0100471	2.0	3.0
1365_0100472	2.0	2.0
1365_0100473	2.0	2.0
1365_0100474	2.0	3.0
1365_0100475	2.0	3.0
1365_0100476	2.0	3.0
1365_0100477	2.0	3.0
1365_0100478	2.0	3.0
1365_0100479	2.0	2.0
1365_0100480	2.0	3.0
1365_0100481	2.0	2.0
1365_0100482	2.0	2.0
1385_0000011	1.0	1.0
1385_0000012	1.0	1.0
1385_0000013	1.0	0.0
1385_0000016	1.0	1.0
1385_0000017	1.0	0.0
1385_0000020	1.0	1.0
1385_0000021	1.0	1.0
1385_0000022	1.0	1.0
1385_0000023	1.0	1.0
1385_0000033	2.0	1.0
1385_0000034	1.0	1.0
1385_0000035	1.0	1.0
1385_0000036	1.0	1.0
1385_0000037	1.0	1.0
1385_0000038	1.0	1.0
1385_0000039	1.0	1.0
1385_0000040	1.0	1.0
1385_0000041	1.0	1.0
1385_0000042	1.0	1.0
1385_0000043	1.0	1.0
1385_0000044	2.0	1.0
1385_0000045	2.0	1.0
1385_0000047	1.0	1.0
1385_0000048	1.0	1.0
1385_0000049	1.0	1.0
1385_0000050	1.0	1.0
1385_0000051	2.0	1.0
1385_0000052	1.0	0.0
1385_0000053	2.0	1.0
1385_0000054	2.0	1.0
1385_0000057	1.0	1.0
1385_0000058	1.0	1.0
1385_0000059	2.0	1.0
1385_0000095	1.0	1.0
1385_0000097	2.0	1.0
1385_0000098	2.0	1.0
1385_0000099	1.0	1.0
1385_0000100	2.0	1.0
1385_0000101	1.0	0.0
1385_0000102	2.0	2.0
1385_0000103	2.0	0.0
1385_0000104	2.0	1.0
1385_0000114	2.0	1.0
1385_0000119	1.0	1.0
1385_0000120	0.0	1.0
1385_0000122	1.0	1.0
1385_0000123	1.0	1.0
1385_0000124	1.0	1.0
1385_0000125	2.0	1.0
1385_0000126	1.0	1.0
1385_0000127	2.0	1.0
1385_0000128	1.0	2.0
1385_0000129	2.0	1.0
1385_0000130	2.0	1.0
1385_0001103	2.0	1.0
1385_0001104	1.0	1.0
1385_0001105	1.0	1.0
1385_0001107	2.0	1.0
1385_0001108	2.0	1.0
1385_0001109	2.0	1.0
1385_0001110	2.0	1.0
1385_0001111	2.0	1.0
1385_0001112	2.0	1.0
1385_0001113	1.0	1.0
1385_0001118	2.0	1.0
1385_0001119	2.0	1.0
1385_0001120	2.0	0.0
1385_0001121	2.0	1.0
1385_0001122	2.0	0.0
1385_0001123	2.0	0.0
1385_0001124	2.0	1.0
1385_0001125	2.0	1.0
1385_0001126	0.0	1.0
1385_0001127	2.0	1.0
1385_0001128	1.0	2.0
1385_0001129	1.0	1.0
1385_0001130	1.0	0.0
1385_0001131	1.0	1.0
1385_0001132	1.0	1.0
1385_0001133	2.0	1.0
1385_0001134	1.0	1.0
1385_0001135	1.0	1.0
1385_0001136	1.0	1.0
1385_0001137	2.0	1.0
1385_0001138	1.0	1.0
1385_0001147	1.0	1.0
1385_0001148	2.0	1.0
1385_0001149	2.0	1.0
1385_0001150	1.0	1.0
1385_0001151	2.0	1.0
1385_0001152	2.0	2.0
1385_0001153	2.0	1.0
1385_0001154	1.0	1.0
1385_0001155	1.0	1.0
1385_0001156	1.0	1.0
1385_0001157	1.0	0.0
1385_0001158	1.0	1.0
1385_0001159	1.0	1.0
1385_0001160	1.0	2.0
1385_0001161	1.0	1.0
1385_0001162	1.0	1.0
1385_0001163	1.0	1.0
1385_0001164	1.0	1.0
1385_0001165	1.0	1.0
1385_0001166	1.0	1.0
1385_0001167	1.0	1.0
1385_0001169	1.0	1.0
1385_0001170	0.0	0.0
1385_0001171	0.0	0.0
1385_0001172	0.0	1.0
1385_0001173	0.0	0.0
1385_0001174	0.0	1.0
1385_0001175	0.0	1.0
1385_0001178	0.0	1.0
1385_0001188	0.0	1.0
1385_0001189	0.0	1.0
1385_0001190	0.0	1.0
1385_0001191	1.0	1.0
1385_0001192	1.0	1.0
1385_0001193	1.0	1.0
1385_0001194	1.0	1.0
1385_0001195	2.0	2.0
1385_0001196	1.0	1.0
1385_0001197	1.0	1.0
1385_0001198	1.0	1.0
1385_0001199	1.0	1.0
1385_0001501	1.0	1.0
1385_0001503	1.0	1.0
1385_0001522	1.0	1.0
1385_0001523	1.0	1.0
1385_0001524	1.0	1.0
1385_0001525	1.0	1.0
1385_0001526	0.0	1.0
1385_0001527	2.0	1.0
1385_0001528	1.0	1.0
1385_0001712	1.0	1.0
1385_0001714	1.0	1.0
1385_0001715	0.0	1.0
1385_0001716	1.0	1.0
1385_0001717	1.0	1.0
1385_0001718	0.0	1.0
1385_0001719	1.0	1.0
1385_0001720	0.0	1.0
1385_0001723	0.0	1.0
1385_0001724	1.0	1.0
1385_0001725	1.0	1.0
1385_0001726	1.0	2.0
1385_0001727	0.0	1.0
1385_0001728	1.0	1.0
1385_0001729	1.0	1.0
1385_0001730	2.0	1.0
1385_0001732	0.0	1.0
1385_0001733	1.0	1.0
1385_0001734	1.0	1.0
1385_0001736	1.0	1.0
1385_0001737	2.0	1.0
1385_0001738	0.0	0.0
1385_0001739	1.0	2.0
1385_0001740	1.0	1.0
1385_0001741	0.0	1.0
1385_0001742	0.0	0.0
1385_0001744	0.0	1.0
1385_0001746	1.0	1.0
1385_0001747	1.0	1.0
1385_0001748	1.0	1.0
1385_0001749	1.0	1.0
1385_0001750	0.0	0.0
1385_0001751	0.0	2.0
1385_0001752	1.0	1.0
1385_0001753	1.0	1.0
1385_0001754	1.0	1.0
1385_0001756	2.0	1.0
1385_0001757	1.0	1.0
1385_0001758	0.0	1.0
1385_0001759	0.0	1.0
1385_0001760	1.0	1.0
1385_0001761	0.0	1.0
1385_0001762	2.0	1.0
1385_0001764	1.0	1.0
1385_0001765	0.0	0.0
1385_0001766	2.0	1.0
1385_0001767	0.0	1.0
1385_0001768	2.0	1.0
1385_0001771	1.0	1.0
1385_0001772	1.0	1.0
1385_0001773	0.0	1.0
1385_0001774	0.0	0.0
1385_0001775	0.0	1.0
1385_0001785	1.0	1.0
1385_0001786	1.0	1.0
1385_0001787	1.0	1.0
1385_0001788	1.0	1.0
1385_0001789	1.0	1.0
1385_0001790	1.0	1.0
1385_0001791	0.0	1.0
1385_0001792	1.0	2.0
1385_0001793	0.0	1.0
1385_0001794	1.0	1.0
1385_0001795	0.0	0.0
1385_0001796	1.0	1.0
1385_0001798	1.0	1.0
1385_0001799	1.0	2.0
1385_0001800	1.0	1.0
1395_0000333	1.0	1.0
1395_0000337	1.0	1.0
1395_0000338	2.0	1.0
1395_0000340	2.0	1.0
1395_0000341	2.0	1.0
1395_0000353	1.0	1.0
1395_0000354	1.0	1.0
1395_0000355	2.0	1.0
1395_0000356	1.0	1.0
1395_0000357	3.0	2.0
1395_0000359	2.0	2.0
1395_0000360	3.0	1.0
1395_0000361	2.0	1.0
1395_0000364	2.0	1.0
1395_0000365	3.0	2.0
1395_0000366	2.0	2.0
1395_0000368	1.0	1.0
1395_0000369	2.0	2.0
1395_0000376	2.0	2.0
1395_0000378	2.0	1.0
1395_0000379	2.0	1.0
1395_0000380	2.0	2.0
1395_0000383	2.0	2.0
1395_0000387	3.0	2.0
1395_0000388	2.0	2.0
1395_0000389	1.0	1.0
1395_0000390	1.0	1.0
1395_0000391	3.0	2.0
1395_0000392	2.0	1.0
1395_0000396	1.0	1.0
1395_0000398	2.0	2.0
1395_0000399	2.0	1.0
1395_0000402	1.0	1.0
1395_0000403	2.0	2.0
1395_0000404	2.0	2.0
1395_0000409	2.0	2.0
1395_0000413	2.0	1.0
1395_0000414	2.0	2.0
1395_0000415	1.0	1.0
1395_0000432	2.0	2.0
1395_0000438	2.0	2.0
1395_0000443	2.0	2.0
1395_0000446	2.0	2.0
1395_0000447	2.0	2.0
1395_0000448	2.0	1.0
1395_0000449	2.0	2.0
1395_0000450	1.0	1.0
1395_0000451	1.0	2.0
1395_0000452	1.0	1.0
1395_0000454	2.0	1.0
1395_0000455	2.0	1.0
1395_0000458	2.0	1.0
1395_0000460	1.0	1.0
1395_0000462	2.0	1.0
1395_0000465	1.0	1.0
1395_0000469	1.0	1.0
1395_0000470	2.0	1.0
1395_0000471	2.0	2.0
1395_0000499	1.0	1.0
1395_0000500	2.0	1.0
1395_0000504	2.0	2.0
1395_0000512	2.0	2.0
1395_0000513	2.0	2.0
1395_0000514	3.0	2.0
1395_0000515	2.0	1.0
1395_0000516	1.0	1.0
1395_0000518	2.0	2.0
1395_0000525	2.0	1.0
1395_0000526	1.0	1.0
1395_0000527	1.0	1.0
1395_0000528	2.0	1.0
1395_0000529	2.0	1.0
1395_0000531	2.0	2.0
1395_0000533	3.0	2.0
1395_0000534	2.0	2.0
1395_0000535	1.0	1.0
1395_0000537	2.0	1.0
1395_0000547	2.0	1.0
1395_0000548	2.0	2.0
1395_0000549	2.0	2.0
1395_0000550	2.0	2.0
1395_0000551	2.0	1.0
1395_0000552	2.0	2.0
1395_0000553	2.0	1.0
1395_0000554	2.0	2.0
1395_0000555	1.0	1.0
1395_0000556	1.0	2.0
1395_0000557	2.0	2.0
1395_0000559	2.0	1.0
1395_0000560	2.0	2.0
1395_0000563	2.0	2.0
1395_0000564	1.0	1.0
1395_0000565	2.0	1.0
1395_0000572	1.0	1.0
1395_0000575	1.0	2.0
1395_0000579	1.0	1.0
1395_0000581	2.0	2.0
1395_0000582	0.0	1.0
1395_0000583	1.0	2.0
1395_0000584	0.0	1.0
1395_0000585	0.0	2.0
1395_0000587	0.0	1.0
1395_0000591	0.0	1.0
1395_0000593	0.0	1.0
1395_0000595	0.0	1.0
1395_0000596	2.0	1.0
1395_0000597	1.0	2.0
1395_0000598	0.0	2.0
1395_0000599	1.0	1.0
1395_0000602	1.0	1.0
1395_0000604	0.0	1.0
1395_0000606	0.0	1.0
1395_0000607	0.0	1.0
1395_0000608	0.0	2.0
1395_0000609	1.0	1.0
1395_0000610	2.0	2.0
1395_0000611	0.0	1.0
1395_0000612	0.0	2.0
1395_0000626	1.0	2.0
1395_0000627	1.0	1.0
1395_0000628	0.0	1.0
1395_0000630	0.0	1.0
1395_0000631	0.0	1.0
1395_0000635	0.0	1.0
1395_0000636	0.0	1.0
1395_0000639	0.0	2.0
1395_0000642	1.0	1.0
1395_0000644	1.0	2.0
1395_0000646	1.0	1.0
1395_0000649	1.0	2.0
1395_0001010	1.0	2.0
1395_0001013	1.0	2.0
1395_0001015	1.0	2.0
1395_0001016	1.0	1.0
1395_0001017	1.0	1.0
1395_0001019	1.0	1.0
1395_0001020	1.0	1.0
1395_0001021	1.0	1.0
1395_0001022	1.0	1.0
1395_0001023	1.0	1.0
1395_0001024	1.0	1.0
1395_0001028	1.0	2.0
1395_0001033	1.0	1.0
1395_0001034	1.0	1.0
1395_0001040	0.0	1.0
1395_0001045	2.0	1.0
1395_0001058	1.0	1.0
1395_0001060	2.0	2.0
1395_0001061	2.0	2.0
1395_0001064	2.0	1.0
1395_0001065	1.0	2.0
1395_0001066	1.0	1.0
1395_0001067	0.0	1.0
1395_0001068	0.0	1.0
1395_0001069	2.0	1.0
1395_0001070	2.0	2.0
1395_0001071	2.0	1.0
1395_0001073	2.0	2.0
1395_0001074	1.0	1.0
1395_0001075	0.0	2.0
1395_0001076	0.0	2.0
1395_0001078	1.0	2.0
1395_0001080	2.0	2.0
1395_0001084	1.0	2.0
1395_0001090	1.0	1.0
1395_0001093	1.0	2.0
1395_0001101	1.0	1.0
1395_0001103	1.0	2.0
1395_0001104	0.0	2.0
1395_0001108	0.0	1.0
1395_0001109	0.0	1.0
1395_0001114	1.0	2.0
1395_0001115	1.0	2.0
1395_0001116	2.0	2.0
1395_0001117	1.0	1.0
1395_0001118	0.0	1.0
1395_0001119	2.0	2.0
1395_0001120	1.0	1.0
1395_0001121	0.0	1.0
1395_0001122	0.0	1.0
1395_0001123	1.0	2.0
1395_0001124	0.0	1.0
1395_0001126	1.0	1.0
1395_0001131	0.0	1.0
1395_0001132	2.0	2.0
1395_0001133	1.0	1.0
1395_0001141	2.0	1.0
1395_0001145	1.0	2.0
1395_0001146	0.0	1.0
1395_0001147	1.0	2.0
1395_0001149	1.0	1.0
1395_0001150	1.0	1.0
1395_0001158	1.0	2.0
1395_0001160	1.0	1.0
1395_0001161	1.0	1.0
1395_0001164	2.0	2.0
1395_0001167	1.0	2.0
1395_0001169	2.0	2.0
1395_0001170	1.0	1.0
1395_0001171	1.0	1.0
Language = IT, Weighted F1-score = 0.5394796734469813, Dimension = Vocabularycontrol

Train on DE, test on  CZ
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
144.90552995391704 65.35717405024758
LANGUAGE: IT
nan nan
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']


======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36

  Average training loss: 1.18
  Training epoch took: 38
Running Validation...
  Average evaluation loss: 0.84
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.67      0.38      0.48       101
         2.0       0.55      0.86      0.67       171
         3.0       0.91      0.63      0.74       156
         4.0       0.00      0.00      0.00         5

    accuracy                           0.65       434
   macro avg       0.42      0.37      0.38       434
weighted avg       0.70      0.65      0.64       434

[[  0   1   0   0   0]
 [  0  38  62   1   0]
 [  0  18 147   6   0]
 [  0   0  58  98   0]
 [  0   0   2   3   0]]
0.6420726986366021
434 434 434



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.95
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.38
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.81      0.13      0.22       101
         2.0       0.42      0.19      0.27       171
         3.0       0.46      0.99      0.62       156
         4.0       0.00      0.00      0.00         5

    accuracy                           0.46       434
   macro avg       0.34      0.26      0.22       434
weighted avg       0.52      0.46      0.38       434

[[  0   1   0   0   0]
 [  0  13  44  44   0]
 [  0   2  33 136   0]
 [  0   0   1 155   0]
 [  0   0   0   5   0]]
0.38080586555129886
434 434 434



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.82
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.12
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.75      0.12      0.21       101
         2.0       0.57      0.64      0.60       171
         3.0       0.66      0.83      0.73       156
         4.0       0.08      0.40      0.13         5

    accuracy                           0.59       434
   macro avg       0.41      0.40      0.33       434
weighted avg       0.63      0.59      0.55       434

[[  0   1   0   0   0]
 [  0  12  82   7   0]
 [  0   3 110  58   0]
 [  0   0   2 130  24]
 [  0   0   0   3   2]]
0.5507093878926196
434 434 434



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.72
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.12
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.83      0.15      0.25       101
         2.0       0.58      0.68      0.63       171
         3.0       0.65      0.69      0.67       156
         4.0       0.06      0.60      0.11         5

    accuracy                           0.56       434
   macro avg       0.42      0.42      0.33       434
weighted avg       0.66      0.56      0.55       434

[[  0   1   0   0   0]
 [  0  15  82   4   0]
 [  0   2 116  52   1]
 [  0   0   2 108  46]
 [  0   0   0   2   3]]
0.5474336790497222
434 434 434
Filename	True Label	Prediction
0601	2.0	2.0
0602	2.0	3.0
0603	2.0	3.0
0604	2.0	2.0
0605	2.0	3.0
0606	2.0	2.0
0607	2.0	2.0
0608	1.0	2.0
0609	2.0	2.0
0610	2.0	3.0
0611	2.0	2.0
0612	2.0	2.0
0613	2.0	2.0
0614	2.0	3.0
0615	2.0	2.0
0616	2.0	2.0
0617	2.0	2.0
0618	2.0	2.0
0619	2.0	2.0
0620	2.0	2.0
0621	2.0	2.0
0622	2.0	2.0
0623	2.0	2.0
0624	2.0	2.0
0625	1.0	2.0
0626	2.0	2.0
0627	2.0	2.0
0628	2.0	2.0
0629	2.0	2.0
0630	1.0	1.0
0631	2.0	2.0
0632	1.0	2.0
0633	2.0	2.0
0634	2.0	3.0
0635	2.0	2.0
0636	2.0	2.0
0637	2.0	2.0
0638	2.0	2.0
0639	2.0	2.0
0640	2.0	2.0
0641	2.0	2.0
0642	2.0	2.0
0643	2.0	2.0
0644	2.0	2.0
0645	2.0	3.0
0714	2.0	2.0
0715	2.0	3.0
0716	2.0	2.0
0717	2.0	2.0
0718	2.0	2.0
0719	2.0	2.0
0720	2.0	2.0
0721	2.0	2.0
0722	2.0	2.0
0723	1.0	2.0
0724	3.0	2.0
0725	2.0	3.0
0801	1.0	2.0
0802	1.0	2.0
0803	1.0	2.0
0804	2.0	2.0
0805	2.0	2.0
0806	2.0	2.0
0807	2.0	2.0
0808	2.0	2.0
0809	2.0	2.0
0810	2.0	2.0
0811	2.0	2.0
0812	1.0	2.0
0813	2.0	2.0
0814	1.0	2.0
0815	2.0	2.0
0816	2.0	2.0
0817	2.0	2.0
0818	1.0	2.0
0819	3.0	3.0
0820	1.0	2.0
0821	2.0	3.0
0822	2.0	2.0
0823	2.0	2.0
0824	2.0	2.0
0825	2.0	2.0
0826	2.0	2.0
0827	2.0	2.0
0828	2.0	2.0
0829	2.0	3.0
0901	3.0	3.0
0902	2.0	2.0
0903	2.0	3.0
0904	1.0	2.0
0905	2.0	2.0
0906	2.0	3.0
0907	2.0	3.0
0910	1.0	2.0
0911	2.0	2.0
0912	2.0	3.0
0913	2.0	2.0
0914	2.0	2.0
0915	2.0	2.0
0916	1.0	2.0
0917	2.0	2.0
0918	2.0	2.0
0919	2.0	2.0
0920	2.0	2.0
0921	2.0	2.0
0922	1.0	2.0
0923	2.0	2.0
0924	1.0	2.0
0925	2.0	2.0
0926	2.0	2.0
0927	2.0	2.0
0928	2.0	3.0
0929	1.0	2.0
0930	2.0	2.0
1001	2.0	3.0
1002	2.0	2.0
1003	2.0	2.0
1004	2.0	2.0
1005	2.0	2.0
1006	2.0	2.0
1007	2.0	2.0
1008	2.0	2.0
1009	2.0	2.0
1010	1.0	2.0
1014	2.0	2.0
1015	2.0	3.0
1016	2.0	2.0
1017	2.0	2.0
1018	2.0	3.0
1019	2.0	2.0
1020	2.0	2.0
1021	2.0	2.0
1022	2.0	2.0
1023	2.0	3.0
1111	2.0	2.0
1112	2.0	3.0
1113	2.0	3.0
1114	2.0	3.0
1115	2.0	3.0
1116	2.0	3.0
1117	2.0	2.0
9999	1.0	2.0
BER0609003	3.0	3.0
BER0611003	3.0	3.0
BER0611005	3.0	3.0
BER0611006	3.0	3.0
BER0611007	3.0	3.0
KYJ0611003A	1.0	2.0
KYJ0611004A	1.0	2.0
KYJ0611005A	1.0	1.0
KYJ0611005B	1.0	2.0
KYJ0611006A	1.0	1.0
KYJ0611006B	1.0	2.0
KYJ0611009A	2.0	2.0
KYJ0611009B	1.0	2.0
LIB0611001A	1.0	2.0
LIB0611001B	1.0	2.0
LIB0611002A	1.0	2.0
LIB0611002B	1.0	2.0
LIB0611003A	2.0	2.0
LIB0611004A	1.0	1.0
LIB0611004B	2.0	2.0
LIB0611011	2.0	3.0
LON0610002A	2.0	2.0
LON0610002B	1.0	2.0
LON0611002A	1.0	1.0
LON0611002B	1.0	2.0
LON0611003	3.0	4.0
LON0611004A	1.0	2.0
LON0611004B	1.0	2.0
MOS0509001	2.0	3.0
MOS0509004	2.0	3.0
MOS0611012	3.0	3.0
MOS0611013	3.0	3.0
MOS0611014	1.0	3.0
MOS0611015	3.0	3.0
PAR1011008A	2.0	2.0
PAR1011009A	2.0	2.0
PAR1011009B	1.0	2.0
PAR1011013	3.0	3.0
PAR1011014	3.0	3.0
PAR1011015	2.0	3.0
PAR1011016	3.0	4.0
PAR1011017	3.0	3.0
PAR1011018	3.0	3.0
PHA0111001A	1.0	1.0
PHA0111001B	1.0	2.0
PHA0111002A	1.0	1.0
PHA0111002B	2.0	3.0
PHA0111003A	1.0	1.0
PHA0111003B	1.0	2.0
PHA0111004A	1.0	1.0
PHA0111004B	1.0	3.0
PHA0111005A	2.0	2.0
PHA0111005B	2.0	2.0
PHA0111010	3.0	3.0
PHA0111011	3.0	3.0
PHA0111012	3.0	3.0
PHA0111014	2.0	3.0
PHA0111015	4.0	3.0
PHA0111016	3.0	4.0
PHA0111018	2.0	4.0
PHA0112002A	1.0	2.0
PHA0112002B	2.0	3.0
PHA0112003A	1.0	2.0
PHA0112003B	1.0	2.0
PHA0112006A	3.0	2.0
PHA0112006B	2.0	2.0
PHA0112007A	1.0	2.0
PHA0112007B	1.0	2.0
PHA0112009A	2.0	2.0
PHA0112009B	2.0	2.0
PHA0112012A	1.0	3.0
PHA0112012B	1.0	2.0
PHA0209001	2.0	3.0
PHA0209008	1.0	2.0
PHA0209013	1.0	2.0
PHA0209024	3.0	3.0
PHA0209026	3.0	3.0
PHA0209028	3.0	4.0
PHA0209031	3.0	4.0
PHA0209034	3.0	3.0
PHA0209038	3.0	4.0
PHA0209039	3.0	4.0
PHA0210001	1.0	2.0
PHA0210004	1.0	2.0
PHA0210007	1.0	3.0
PHA0210008	1.0	2.0
PHA0411008A	2.0	1.0
PHA0411008B	1.0	2.0
PHA0411009A	1.0	2.0
PHA0411009B	2.0	2.0
PHA0411010A	0.0	1.0
PHA0411010B	1.0	2.0
PHA0411011A	1.0	1.0
PHA0411011B	1.0	2.0
PHA0411012A	2.0	1.0
PHA0411012B	1.0	2.0
PHA0411027	3.0	3.0
PHA0411028	2.0	3.0
PHA0411029	3.0	3.0
PHA0411030	3.0	4.0
PHA0411031	3.0	4.0
PHA0411032	3.0	3.0
PHA0411033	3.0	3.0
PHA0411034	3.0	3.0
PHA0411035	3.0	3.0
PHA0411036	3.0	3.0
PHA0411037	2.0	3.0
PHA0411038	3.0	4.0
PHA0411039	3.0	3.0
PHA0411041	3.0	3.0
PHA0411042	3.0	4.0
PHA0411043	3.0	3.0
PHA0411044	4.0	4.0
PHA0411045	3.0	3.0
PHA0411047	3.0	3.0
PHA0411051	3.0	4.0
PHA0411053	3.0	4.0
PHA0411054	3.0	3.0
PHA0411055	3.0	4.0
PHA0411056	3.0	3.0
PHA0411058	3.0	4.0
PHA0411059	3.0	4.0
PHA0411060	3.0	3.0
PHA0411061	3.0	4.0
PHA0411062	3.0	3.0
PHA0509002	1.0	2.0
PHA0509007	1.0	2.0
PHA0509013	1.0	2.0
PHA0509015	3.0	3.0
PHA0509017	3.0	4.0
PHA0509018	3.0	4.0
PHA0509019	3.0	3.0
PHA0509020	3.0	4.0
PHA0509021	2.0	3.0
PHA0509022	3.0	4.0
PHA0509024	3.0	3.0
PHA0509025	3.0	4.0
PHA0509026	3.0	4.0
PHA0509027	2.0	3.0
PHA0509028	3.0	3.0
PHA0509030	3.0	3.0
PHA0509031	2.0	3.0
PHA0509032	3.0	4.0
PHA0509033	2.0	3.0
PHA0509034	2.0	3.0
PHA0509035	3.0	3.0
PHA0509036	3.0	3.0
PHA0509037	3.0	3.0
PHA0509038	2.0	3.0
PHA0509039	3.0	4.0
PHA0509040	3.0	3.0
PHA0509041	3.0	3.0
PHA0509042	3.0	4.0
PHA0509043	3.0	3.0
PHA0509044	3.0	4.0
PHA0509045	3.0	3.0
PHA0510002A	1.0	2.0
PHA0510002B	2.0	2.0
PHA0510003A	1.0	2.0
PHA0510003B	1.0	2.0
PHA0510004A	1.0	2.0
PHA0510004B	1.0	2.0
PHA0510010A	1.0	2.0
PHA0510010B	1.0	1.0
PHA0510013A	2.0	2.0
PHA0510013B	1.0	2.0
PHA0510023	3.0	3.0
PHA0510027	3.0	3.0
PHA0510029	3.0	3.0
PHA0510030	3.0	3.0
PHA0510031	3.0	3.0
PHA0510032	3.0	3.0
PHA0510034	3.0	3.0
PHA0510035	3.0	3.0
PHA0510036	3.0	4.0
PHA0510037	2.0	3.0
PHA0510038	3.0	4.0
PHA0510039	3.0	3.0
PHA0510040	3.0	3.0
PHA0510046	2.0	3.0
PHA0510047	2.0	3.0
PHA0510048	3.0	3.0
PHA0510049	3.0	3.0
PHA0510050	3.0	3.0
PHA0610005A	1.0	1.0
PHA0610005B	1.0	2.0
PHA0610006A	1.0	2.0
PHA0610006B	1.0	2.0
PHA0610007A	1.0	1.0
PHA0610007B	1.0	2.0
PHA0610015	3.0	4.0
PHA0610016	3.0	3.0
PHA0610017	3.0	4.0
PHA0610018	3.0	4.0
PHA0610019A	1.0	2.0
PHA0610019B	2.0	3.0
PHA0610025	3.0	4.0
PHA0610026	3.0	4.0
PHA0709008	3.0	3.0
PHA0710009	3.0	3.0
PHA0710010	3.0	3.0
PHA0710011	3.0	3.0
PHA0710012	3.0	3.0
PHA0710013	3.0	4.0
PHA0710014	3.0	3.0
PHA0710015	3.0	3.0
PHA0710016	3.0	3.0
PHA0710017	3.0	3.0
PHA0710018	3.0	4.0
PHA0710019	3.0	3.0
PHA0710021	4.0	4.0
PHA0809009	3.0	3.0
PHA0809010	2.0	3.0
PHA0810001	3.0	4.0
PHA0810002	3.0	3.0
PHA0810003	3.0	3.0
PHA0810004	3.0	3.0
PHA0810006	3.0	3.0
PHA0810008	3.0	3.0
PHA0810009	3.0	4.0
PHA0810010	3.0	3.0
PHA0810011	3.0	3.0
PHA0810012	3.0	3.0
PHA0810015	3.0	4.0
PHA0811010	3.0	3.0
PHA0811012	3.0	4.0
PHA0811013	3.0	4.0
PHA0811014	3.0	3.0
PHA0811016	3.0	3.0
PHA0811017	4.0	3.0
PHA0811019	3.0	3.0
PHA0811020	2.0	3.0
PHA1109001	2.0	2.0
PHA1109002	3.0	3.0
PHA1109003	2.0	3.0
PHA1109004	3.0	3.0
PHA1109005	3.0	3.0
PHA1109006	2.0	3.0
PHA1109007	2.0	3.0
PHA1109008	1.0	2.0
PHA1109023	1.0	2.0
PHA1109024	3.0	4.0
PHA1109025	1.0	2.0
PHA1109026	3.0	4.0
PHA1109027	3.0	4.0
PHA1109028	3.0	3.0
PHA1110001A	2.0	2.0
PHA1110001B	2.0	2.0
PHA1110002A	2.0	2.0
PHA1110002B	2.0	2.0
PHA1110003A	1.0	2.0
PHA1110003B	1.0	2.0
PHA1110004A	1.0	2.0
PHA1110013	3.0	3.0
PHA1110014	3.0	3.0
PHA1110015	3.0	3.0
PHA1110016	2.0	3.0
PHA1110017	3.0	3.0
PHA1110019	3.0	3.0
PHA1110021	3.0	3.0
PHA1110022	4.0	4.0
PHA1111001A	2.0	2.0
PHA1111001B	1.0	2.0
PHA1111002A	1.0	1.0
PHA1111002B	1.0	2.0
PHA1111003A	1.0	2.0
PHA1111003B	1.0	2.0
PHA1111004A	1.0	2.0
PHA1111004B	1.0	2.0
PHA1111006A	1.0	1.0
PHA1111006B	2.0	2.0
PHA1111008A	2.0	2.0
PHA1111008B	1.0	2.0
PHA1111009A	1.0	2.0
ST071122B	1.0	2.0
TI071122B	1.0	2.0
VAR0209036	2.0	3.0
VAR0909003	3.0	3.0
VAR0909004	3.0	3.0
VAR0909005	3.0	3.0
VAR0909006	3.0	4.0
VAR0909007	3.0	3.0
VAR0909008	3.0	3.0
VAR0909009	3.0	3.0
VAR0909010	3.0	3.0
VAR0910004	3.0	3.0
VAR0910005	3.0	3.0
VAR0910006	3.0	3.0
VAR0910007	3.0	3.0
VAR0910009	3.0	4.0
VAR0910010	3.0	3.0
VAR0910011	3.0	3.0
Language = CZ, Weighted F1-score = 0.5474336790497222, Dimension = CoherenceCohesion

Train on DE, test on  IT
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
nan nan
LANGUAGE: IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']


======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 1.23
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.16
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00       109
         1.0       0.64      0.36      0.46       334
         2.0       0.40      0.59      0.48       300
         3.0       0.23      0.68      0.34        57

    accuracy                           0.42       800
   macro avg       0.32      0.41      0.32       800
weighted avg       0.43      0.42      0.39       800

[[  0  67  41   1]
 [  0 120 206   8]
 [  0   1 176 123]
 [  0   0  18  39]]
0.3944666748103681
800 800 800



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.91
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.04
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00       109
         1.0       0.66      0.42      0.51       334
         2.0       0.50      0.93      0.65       300
         3.0       0.34      0.19      0.25        57

    accuracy                           0.54       800
   macro avg       0.38      0.39      0.35       800
weighted avg       0.49      0.54      0.48       800

[[  0  73  36   0]
 [  0 141 192   1]
 [  0   1 279  20]
 [  0   0  46  11]]
0.47737657969290737
800 800 800



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.82
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.32
              precision    recall  f1-score   support

         0.0       0.75      0.03      0.05       109
         1.0       0.56      0.17      0.26       334
         2.0       0.39      0.77      0.52       300
         3.0       0.30      0.56      0.39        57

    accuracy                           0.40       800
   macro avg       0.50      0.38      0.31       800
weighted avg       0.50      0.40      0.34       800

[[  3  43  63   0]
 [  1  56 270   7]
 [  0   1 231  68]
 [  0   0  25  32]]
0.33766321657076726
800 800 800



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.71
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.07
              precision    recall  f1-score   support

         0.0       0.79      0.10      0.18       109
         1.0       0.70      0.54      0.61       334
         2.0       0.54      0.77      0.63       300
         3.0       0.31      0.54      0.40        57

    accuracy                           0.57       800
   macro avg       0.58      0.49      0.45       800
weighted avg       0.62      0.57      0.54       800

[[ 11  70  28   0]
 [  3 181 145   5]
 [  0   7 230  63]
 [  0   0  26  31]]
0.544608348902895
800 800 800
Filename	True Label	Prediction
1325_1001008	2.0	2.0
1325_1001009	3.0	2.0
1325_1001010	2.0	3.0
1325_1001011	2.0	2.0
1325_1001012	2.0	3.0
1325_1001013	2.0	2.0
1325_1001014	3.0	2.0
1325_1001015	2.0	2.0
1325_1001016	2.0	2.0
1325_1001017	2.0	2.0
1325_1001018	2.0	3.0
1325_1001019	2.0	2.0
1325_1001020	2.0	2.0
1325_1001021	2.0	3.0
1325_1001022	2.0	2.0
1325_1001023	2.0	2.0
1325_1001024	2.0	2.0
1325_1001025	2.0	2.0
1325_1001027	3.0	2.0
1325_1001028	2.0	3.0
1325_1001029	2.0	2.0
1325_1001032	2.0	2.0
1325_1001033	2.0	2.0
1325_1001035	3.0	3.0
1325_1001036	2.0	3.0
1325_1001037	2.0	2.0
1325_1001039	3.0	3.0
1325_1001040	2.0	3.0
1325_1001041	3.0	3.0
1325_1001042	2.0	3.0
1325_1001043	2.0	2.0
1325_1001044	2.0	2.0
1325_1001045	2.0	2.0
1325_1001046	2.0	3.0
1325_1001047	1.0	2.0
1325_1001048	2.0	3.0
1325_1001050	2.0	2.0
1325_1001051	2.0	2.0
1325_1001052	2.0	2.0
1325_1001053	1.0	2.0
1325_1001054	2.0	2.0
1325_1001055	2.0	3.0
1325_1001056	2.0	3.0
1325_1001057	1.0	2.0
1325_1001058	2.0	2.0
1325_1001059	2.0	2.0
1325_1001062	2.0	3.0
1325_1001063	2.0	2.0
1325_1001075	1.0	2.0
1325_1001076	2.0	3.0
1325_1001077	2.0	2.0
1325_1001078	2.0	3.0
1325_1001079	2.0	3.0
1325_1001080	2.0	2.0
1325_1001081	2.0	2.0
1325_1001082	2.0	3.0
1325_1001083	2.0	2.0
1325_1001084	2.0	2.0
1325_1001085	2.0	2.0
1325_1001086	2.0	2.0
1325_1001087	2.0	2.0
1325_1001088	2.0	2.0
1325_1001089	2.0	2.0
1325_1001090	2.0	2.0
1325_1001091	2.0	2.0
1325_1001092	2.0	2.0
1325_1001093	2.0	2.0
1325_1001094	2.0	2.0
1325_1001095	2.0	2.0
1325_1001096	2.0	2.0
1325_1001097	0.0	2.0
1325_1001098	2.0	2.0
1325_1001099	3.0	2.0
1325_1001100	2.0	2.0
1325_1001101	3.0	3.0
1325_1001107	2.0	3.0
1325_1001108	3.0	3.0
1325_1001109	2.0	2.0
1325_1001110	2.0	3.0
1325_1001111	3.0	3.0
1325_1001113	3.0	3.0
1325_1001119	2.0	2.0
1325_1001120	2.0	3.0
1325_1001121	2.0	3.0
1325_1001122	2.0	2.0
1325_1001123	2.0	3.0
1325_1001124	2.0	2.0
1325_1001125	3.0	3.0
1325_1001126	2.0	2.0
1325_1001127	3.0	2.0
1325_1001128	2.0	2.0
1325_1001129	1.0	2.0
1325_1001130	2.0	2.0
1325_1001131	2.0	2.0
1325_1001132	2.0	3.0
1325_1001133	2.0	2.0
1325_1001134	2.0	3.0
1325_1001135	2.0	3.0
1325_1001136	2.0	2.0
1325_1001138	2.0	3.0
1325_1001139	2.0	3.0
1325_1001141	2.0	3.0
1325_1001142	2.0	3.0
1325_1001143	2.0	3.0
1325_1001144	2.0	2.0
1325_1001152	2.0	3.0
1325_1001153	2.0	2.0
1325_1001154	3.0	3.0
1325_1001155	2.0	3.0
1325_1001156	2.0	2.0
1325_1001157	2.0	2.0
1325_1001158	2.0	3.0
1325_1001159	2.0	3.0
1325_1001160	2.0	3.0
1325_1001161	2.0	3.0
1325_1001162	2.0	2.0
1325_1001163	2.0	2.0
1325_1001164	2.0	2.0
1325_1001165	2.0	2.0
1325_1001166	2.0	2.0
1325_1001167	2.0	3.0
1325_1001168	2.0	3.0
1325_1001169	2.0	2.0
1325_1001170	2.0	2.0
1325_9000059	2.0	2.0
1325_9000087	2.0	2.0
1325_9000088	2.0	2.0
1325_9000089	2.0	3.0
1325_9000090	2.0	2.0
1325_9000095	2.0	3.0
1325_9000099	3.0	2.0
1325_9000102	2.0	3.0
1325_9000104	2.0	2.0
1325_9000105	2.0	2.0
1325_9000106	2.0	2.0
1325_9000107	2.0	2.0
1325_9000136	2.0	3.0
1325_9000137	2.0	3.0
1325_9000138	3.0	3.0
1325_9000139	2.0	2.0
1325_9000140	3.0	3.0
1325_9000143	3.0	3.0
1325_9000144	3.0	3.0
1325_9000152	2.0	3.0
1325_9000185	3.0	3.0
1325_9000186	3.0	3.0
1325_9000187	2.0	3.0
1325_9000188	2.0	3.0
1325_9000209	2.0	3.0
1325_9000210	1.0	2.0
1325_9000211	2.0	2.0
1325_9000213	3.0	2.0
1325_9000214	3.0	2.0
1325_9000215	3.0	3.0
1325_9000237	2.0	3.0
1325_9000239	2.0	2.0
1325_9000240	3.0	2.0
1325_9000241	3.0	3.0
1325_9000278	3.0	2.0
1325_9000279	3.0	3.0
1325_9000296	1.0	3.0
1325_9000302	2.0	2.0
1325_9000303	2.0	2.0
1325_9000304	2.0	2.0
1325_9000314	2.0	2.0
1325_9000315	2.0	2.0
1325_9000316	2.0	3.0
1325_9000317	3.0	3.0
1325_9000318	3.0	2.0
1325_9000319	2.0	2.0
1325_9000320	3.0	2.0
1325_9000321	3.0	3.0
1325_9000322	3.0	3.0
1325_9000323	2.0	3.0
1325_9000503	3.0	3.0
1325_9000504	2.0	2.0
1325_9000505	3.0	2.0
1325_9000533	2.0	2.0
1325_9000534	2.0	3.0
1325_9000536	3.0	2.0
1325_9000554	2.0	2.0
1325_9000601	2.0	3.0
1325_9000602	3.0	3.0
1325_9000611	2.0	3.0
1325_9000612	2.0	2.0
1325_9000674	3.0	3.0
1325_9000675	2.0	2.0
1325_9000676	3.0	2.0
1325_9000677	2.0	2.0
1325_9000678	3.0	3.0
1325_9000684	3.0	3.0
1325_9000685	3.0	3.0
1325_9000686	2.0	3.0
1325_9000700	2.0	3.0
1325_9000750	3.0	3.0
1365_0100002	2.0	2.0
1365_0100003	1.0	2.0
1365_0100004	2.0	2.0
1365_0100005	1.0	2.0
1365_0100006	2.0	2.0
1365_0100007	1.0	1.0
1365_0100008	2.0	2.0
1365_0100009	1.0	1.0
1365_0100010	1.0	1.0
1365_0100011	2.0	2.0
1365_0100012	1.0	2.0
1365_0100013	2.0	2.0
1365_0100014	2.0	2.0
1365_0100015	2.0	2.0
1365_0100016	2.0	2.0
1365_0100017	2.0	2.0
1365_0100018	2.0	2.0
1365_0100019	1.0	2.0
1365_0100020	2.0	2.0
1365_0100021	2.0	2.0
1365_0100022	2.0	2.0
1365_0100023	1.0	2.0
1365_0100024	1.0	2.0
1365_0100026	1.0	1.0
1365_0100027	2.0	2.0
1365_0100028	1.0	2.0
1365_0100029	1.0	1.0
1365_0100030	1.0	2.0
1365_0100031	2.0	2.0
1365_0100051	0.0	2.0
1365_0100056	2.0	2.0
1365_0100057	2.0	3.0
1365_0100058	2.0	2.0
1365_0100061	3.0	2.0
1365_0100063	2.0	2.0
1365_0100064	2.0	2.0
1365_0100065	1.0	1.0
1365_0100066	1.0	2.0
1365_0100067	1.0	2.0
1365_0100069	1.0	2.0
1365_0100070	2.0	2.0
1365_0100071	2.0	2.0
1365_0100072	2.0	2.0
1365_0100073	2.0	2.0
1365_0100074	2.0	2.0
1365_0100079	2.0	2.0
1365_0100080	2.0	2.0
1365_0100092	2.0	2.0
1365_0100093	1.0	2.0
1365_0100094	2.0	2.0
1365_0100095	2.0	2.0
1365_0100096	2.0	2.0
1365_0100097	2.0	2.0
1365_0100098	1.0	2.0
1365_0100099	1.0	2.0
1365_0100100	2.0	3.0
1365_0100101	2.0	2.0
1365_0100102	2.0	2.0
1365_0100103	2.0	2.0
1365_0100104	2.0	3.0
1365_0100105	3.0	2.0
1365_0100106	2.0	2.0
1365_0100107	2.0	3.0
1365_0100116	2.0	2.0
1365_0100117	2.0	2.0
1365_0100118	2.0	2.0
1365_0100119	3.0	2.0
1365_0100120	3.0	3.0
1365_0100121	2.0	2.0
1365_0100123	2.0	2.0
1365_0100125	2.0	2.0
1365_0100133	2.0	2.0
1365_0100134	2.0	2.0
1365_0100135	2.0	2.0
1365_0100136	1.0	2.0
1365_0100137	1.0	2.0
1365_0100138	2.0	1.0
1365_0100139	1.0	2.0
1365_0100145	2.0	2.0
1365_0100146	2.0	2.0
1365_0100147	2.0	2.0
1365_0100148	2.0	2.0
1365_0100151	1.0	1.0
1365_0100162	2.0	2.0
1365_0100163	2.0	2.0
1365_0100164	2.0	2.0
1365_0100165	2.0	2.0
1365_0100166	1.0	2.0
1365_0100167	1.0	2.0
1365_0100168	2.0	2.0
1365_0100169	1.0	2.0
1365_0100170	2.0	2.0
1365_0100171	1.0	2.0
1365_0100172	1.0	2.0
1365_0100173	1.0	2.0
1365_0100174	1.0	2.0
1365_0100175	1.0	1.0
1365_0100176	2.0	2.0
1365_0100177	2.0	2.0
1365_0100178	1.0	2.0
1365_0100179	1.0	2.0
1365_0100180	1.0	2.0
1365_0100181	1.0	2.0
1365_0100182	2.0	2.0
1365_0100183	2.0	2.0
1365_0100184	1.0	2.0
1365_0100185	1.0	2.0
1365_0100186	1.0	2.0
1365_0100187	2.0	2.0
1365_0100188	2.0	2.0
1365_0100190	2.0	3.0
1365_0100191	2.0	2.0
1365_0100192	3.0	3.0
1365_0100194	2.0	2.0
1365_0100195	1.0	2.0
1365_0100196	1.0	2.0
1365_0100198	1.0	2.0
1365_0100199	2.0	2.0
1365_0100200	2.0	2.0
1365_0100201	1.0	2.0
1365_0100202	1.0	2.0
1365_0100203	2.0	2.0
1365_0100204	1.0	1.0
1365_0100205	2.0	1.0
1365_0100211	3.0	2.0
1365_0100212	3.0	3.0
1365_0100213	1.0	1.0
1365_0100215	2.0	2.0
1365_0100217	3.0	2.0
1365_0100218	2.0	2.0
1365_0100219	2.0	2.0
1365_0100220	2.0	2.0
1365_0100221	3.0	2.0
1365_0100222	2.0	3.0
1365_0100223	2.0	2.0
1365_0100224	2.0	3.0
1365_0100225	2.0	2.0
1365_0100226	2.0	2.0
1365_0100227	2.0	2.0
1365_0100228	1.0	2.0
1365_0100229	2.0	2.0
1365_0100230	2.0	2.0
1365_0100231	2.0	2.0
1365_0100232	1.0	2.0
1365_0100233	2.0	2.0
1365_0100251	2.0	2.0
1365_0100252	2.0	2.0
1365_0100253	1.0	2.0
1365_0100255	1.0	2.0
1365_0100256	2.0	2.0
1365_0100257	2.0	2.0
1365_0100258	2.0	2.0
1365_0100259	2.0	2.0
1365_0100260	2.0	2.0
1365_0100261	2.0	2.0
1365_0100262	2.0	2.0
1365_0100263	3.0	2.0
1365_0100265	2.0	2.0
1365_0100266	2.0	2.0
1365_0100267	2.0	2.0
1365_0100268	1.0	2.0
1365_0100269	2.0	2.0
1365_0100270	2.0	2.0
1365_0100274	2.0	3.0
1365_0100275	2.0	2.0
1365_0100276	3.0	2.0
1365_0100277	3.0	2.0
1365_0100278	2.0	2.0
1365_0100279	2.0	2.0
1365_0100280	1.0	2.0
1365_0100281	2.0	2.0
1365_0100282	2.0	2.0
1365_0100285	2.0	2.0
1365_0100286	1.0	2.0
1365_0100287	1.0	2.0
1365_0100288	1.0	2.0
1365_0100289	2.0	2.0
1365_0100290	1.0	2.0
1365_0100299	2.0	2.0
1365_0100447	2.0	2.0
1365_0100448	1.0	2.0
1365_0100451	2.0	2.0
1365_0100455	2.0	2.0
1365_0100456	2.0	2.0
1365_0100457	2.0	2.0
1365_0100458	2.0	2.0
1365_0100459	2.0	2.0
1365_0100461	2.0	2.0
1365_0100469	2.0	2.0
1365_0100470	1.0	3.0
1365_0100471	1.0	3.0
1365_0100472	2.0	2.0
1365_0100473	2.0	2.0
1365_0100474	2.0	2.0
1365_0100475	2.0	2.0
1365_0100476	2.0	2.0
1365_0100477	1.0	3.0
1365_0100478	1.0	2.0
1365_0100479	2.0	2.0
1365_0100480	1.0	2.0
1365_0100481	1.0	2.0
1365_0100482	2.0	2.0
1385_0000011	0.0	1.0
1385_0000012	1.0	1.0
1385_0000013	1.0	1.0
1385_0000016	1.0	1.0
1385_0000017	1.0	0.0
1385_0000020	1.0	1.0
1385_0000021	1.0	1.0
1385_0000022	1.0	1.0
1385_0000023	1.0	1.0
1385_0000033	1.0	1.0
1385_0000034	1.0	1.0
1385_0000035	1.0	2.0
1385_0000036	1.0	1.0
1385_0000037	1.0	2.0
1385_0000038	1.0	1.0
1385_0000039	1.0	1.0
1385_0000040	1.0	0.0
1385_0000041	1.0	1.0
1385_0000042	1.0	1.0
1385_0000043	1.0	1.0
1385_0000044	1.0	1.0
1385_0000045	1.0	1.0
1385_0000047	1.0	1.0
1385_0000048	1.0	1.0
1385_0000049	1.0	1.0
1385_0000050	1.0	1.0
1385_0000051	1.0	1.0
1385_0000052	1.0	1.0
1385_0000053	1.0	1.0
1385_0000054	1.0	1.0
1385_0000057	1.0	1.0
1385_0000058	1.0	1.0
1385_0000059	1.0	1.0
1385_0000095	1.0	1.0
1385_0000097	1.0	1.0
1385_0000098	1.0	1.0
1385_0000099	1.0	1.0
1385_0000100	1.0	1.0
1385_0000101	1.0	1.0
1385_0000102	1.0	2.0
1385_0000103	1.0	1.0
1385_0000104	1.0	1.0
1385_0000114	1.0	1.0
1385_0000119	1.0	1.0
1385_0000120	0.0	1.0
1385_0000122	1.0	1.0
1385_0000123	1.0	1.0
1385_0000124	1.0	2.0
1385_0000125	1.0	1.0
1385_0000126	1.0	1.0
1385_0000127	1.0	1.0
1385_0000128	1.0	1.0
1385_0000129	1.0	2.0
1385_0000130	1.0	1.0
1385_0001103	1.0	1.0
1385_0001104	0.0	1.0
1385_0001105	1.0	1.0
1385_0001107	1.0	1.0
1385_0001108	1.0	1.0
1385_0001109	1.0	1.0
1385_0001110	1.0	1.0
1385_0001111	1.0	1.0
1385_0001112	1.0	1.0
1385_0001113	1.0	1.0
1385_0001118	1.0	1.0
1385_0001119	1.0	1.0
1385_0001120	1.0	1.0
1385_0001121	1.0	1.0
1385_0001122	1.0	1.0
1385_0001123	1.0	1.0
1385_0001124	1.0	1.0
1385_0001125	1.0	1.0
1385_0001126	0.0	1.0
1385_0001127	1.0	2.0
1385_0001128	0.0	2.0
1385_0001129	0.0	1.0
1385_0001130	1.0	0.0
1385_0001131	1.0	1.0
1385_0001132	1.0	1.0
1385_0001133	1.0	1.0
1385_0001134	1.0	1.0
1385_0001135	1.0	1.0
1385_0001136	1.0	1.0
1385_0001137	1.0	1.0
1385_0001138	1.0	1.0
1385_0001147	1.0	1.0
1385_0001148	1.0	1.0
1385_0001149	1.0	1.0
1385_0001150	1.0	1.0
1385_0001151	1.0	2.0
1385_0001152	1.0	2.0
1385_0001153	2.0	1.0
1385_0001154	1.0	1.0
1385_0001155	1.0	1.0
1385_0001156	1.0	1.0
1385_0001157	1.0	1.0
1385_0001158	1.0	1.0
1385_0001159	1.0	1.0
1385_0001160	1.0	2.0
1385_0001161	1.0	1.0
1385_0001162	1.0	1.0
1385_0001163	1.0	1.0
1385_0001164	1.0	1.0
1385_0001165	1.0	1.0
1385_0001166	1.0	1.0
1385_0001167	0.0	1.0
1385_0001169	1.0	1.0
1385_0001170	0.0	0.0
1385_0001171	0.0	0.0
1385_0001172	0.0	1.0
1385_0001173	0.0	0.0
1385_0001174	0.0	1.0
1385_0001175	0.0	1.0
1385_0001178	0.0	0.0
1385_0001188	1.0	1.0
1385_0001189	0.0	1.0
1385_0001190	0.0	1.0
1385_0001191	1.0	1.0
1385_0001192	1.0	1.0
1385_0001193	1.0	1.0
1385_0001194	1.0	1.0
1385_0001195	1.0	2.0
1385_0001196	0.0	1.0
1385_0001197	0.0	1.0
1385_0001198	2.0	2.0
1385_0001199	1.0	1.0
1385_0001501	0.0	1.0
1385_0001503	1.0	2.0
1385_0001522	0.0	1.0
1385_0001523	1.0	2.0
1385_0001524	0.0	1.0
1385_0001525	1.0	2.0
1385_0001526	0.0	0.0
1385_0001527	1.0	1.0
1385_0001528	1.0	2.0
1385_0001712	1.0	2.0
1385_0001714	1.0	1.0
1385_0001715	1.0	1.0
1385_0001716	1.0	1.0
1385_0001717	1.0	2.0
1385_0001718	0.0	1.0
1385_0001719	1.0	1.0
1385_0001720	0.0	1.0
1385_0001723	0.0	1.0
1385_0001724	2.0	2.0
1385_0001725	0.0	1.0
1385_0001726	1.0	2.0
1385_0001727	0.0	1.0
1385_0001728	0.0	2.0
1385_0001729	1.0	2.0
1385_0001730	1.0	2.0
1385_0001732	0.0	1.0
1385_0001733	0.0	1.0
1385_0001734	0.0	1.0
1385_0001736	1.0	3.0
1385_0001737	1.0	1.0
1385_0001738	0.0	0.0
1385_0001739	1.0	2.0
1385_0001740	1.0	1.0
1385_0001741	0.0	1.0
1385_0001742	0.0	0.0
1385_0001744	0.0	1.0
1385_0001746	1.0	2.0
1385_0001747	0.0	1.0
1385_0001748	2.0	2.0
1385_0001749	0.0	1.0
1385_0001750	0.0	0.0
1385_0001751	0.0	2.0
1385_0001752	1.0	1.0
1385_0001753	1.0	1.0
1385_0001754	0.0	1.0
1385_0001756	1.0	1.0
1385_0001757	1.0	1.0
1385_0001758	0.0	1.0
1385_0001759	1.0	1.0
1385_0001760	1.0	2.0
1385_0001761	0.0	1.0
1385_0001762	1.0	1.0
1385_0001764	0.0	1.0
1385_0001765	0.0	1.0
1385_0001766	1.0	2.0
1385_0001767	0.0	2.0
1385_0001768	1.0	1.0
1385_0001771	0.0	1.0
1385_0001772	1.0	1.0
1385_0001773	0.0	1.0
1385_0001774	0.0	0.0
1385_0001775	1.0	1.0
1385_0001785	0.0	1.0
1385_0001786	1.0	1.0
1385_0001787	0.0	1.0
1385_0001788	0.0	2.0
1385_0001789	1.0	1.0
1385_0001790	0.0	1.0
1385_0001791	1.0	1.0
1385_0001792	1.0	2.0
1385_0001793	0.0	1.0
1385_0001794	1.0	1.0
1385_0001795	0.0	0.0
1385_0001796	1.0	2.0
1385_0001798	1.0	2.0
1385_0001799	1.0	1.0
1385_0001800	1.0	1.0
1395_0000333	1.0	2.0
1395_0000337	0.0	1.0
1395_0000338	1.0	1.0
1395_0000340	1.0	2.0
1395_0000341	1.0	1.0
1395_0000353	1.0	1.0
1395_0000354	1.0	1.0
1395_0000355	1.0	2.0
1395_0000356	1.0	1.0
1395_0000357	2.0	2.0
1395_0000359	1.0	2.0
1395_0000360	2.0	2.0
1395_0000361	1.0	1.0
1395_0000364	1.0	2.0
1395_0000365	2.0	2.0
1395_0000366	2.0	1.0
1395_0000368	0.0	1.0
1395_0000369	2.0	2.0
1395_0000376	2.0	2.0
1395_0000378	1.0	1.0
1395_0000379	1.0	1.0
1395_0000380	1.0	2.0
1395_0000383	1.0	1.0
1395_0000387	3.0	2.0
1395_0000388	2.0	1.0
1395_0000389	0.0	0.0
1395_0000390	1.0	1.0
1395_0000391	3.0	2.0
1395_0000392	1.0	2.0
1395_0000396	1.0	1.0
1395_0000398	2.0	2.0
1395_0000399	1.0	1.0
1395_0000402	1.0	1.0
1395_0000403	1.0	2.0
1395_0000404	1.0	2.0
1395_0000409	2.0	2.0
1395_0000413	1.0	2.0
1395_0000414	1.0	2.0
1395_0000415	1.0	1.0
1395_0000432	1.0	2.0
1395_0000438	2.0	2.0
1395_0000443	2.0	2.0
1395_0000446	2.0	2.0
1395_0000447	1.0	2.0
1395_0000448	1.0	1.0
1395_0000449	2.0	2.0
1395_0000450	1.0	2.0
1395_0000451	1.0	1.0
1395_0000452	1.0	1.0
1395_0000454	1.0	1.0
1395_0000455	1.0	1.0
1395_0000458	1.0	1.0
1395_0000460	1.0	1.0
1395_0000462	1.0	1.0
1395_0000465	1.0	1.0
1395_0000469	1.0	1.0
1395_0000470	1.0	1.0
1395_0000471	1.0	1.0
1395_0000499	1.0	2.0
1395_0000500	1.0	1.0
1395_0000504	1.0	1.0
1395_0000512	1.0	2.0
1395_0000513	2.0	1.0
1395_0000514	2.0	2.0
1395_0000515	2.0	2.0
1395_0000516	1.0	1.0
1395_0000518	1.0	2.0
1395_0000525	1.0	1.0
1395_0000526	1.0	1.0
1395_0000527	0.0	1.0
1395_0000528	1.0	1.0
1395_0000529	1.0	1.0
1395_0000531	1.0	2.0
1395_0000533	2.0	2.0
1395_0000534	1.0	2.0
1395_0000535	1.0	1.0
1395_0000537	1.0	2.0
1395_0000547	1.0	2.0
1395_0000548	1.0	2.0
1395_0000549	1.0	2.0
1395_0000550	1.0	2.0
1395_0000551	2.0	2.0
1395_0000552	1.0	2.0
1395_0000553	1.0	1.0
1395_0000554	1.0	2.0
1395_0000555	1.0	1.0
1395_0000556	1.0	2.0
1395_0000557	2.0	2.0
1395_0000559	1.0	2.0
1395_0000560	2.0	2.0
1395_0000563	1.0	2.0
1395_0000564	1.0	1.0
1395_0000565	1.0	2.0
1395_0000572	1.0	2.0
1395_0000575	1.0	2.0
1395_0000579	1.0	1.0
1395_0000581	1.0	2.0
1395_0000582	0.0	1.0
1395_0000583	1.0	2.0
1395_0000584	0.0	1.0
1395_0000585	0.0	2.0
1395_0000587	0.0	1.0
1395_0000591	0.0	1.0
1395_0000593	0.0	2.0
1395_0000595	0.0	1.0
1395_0000596	2.0	1.0
1395_0000597	1.0	2.0
1395_0000598	1.0	2.0
1395_0000599	1.0	1.0
1395_0000602	1.0	1.0
1395_0000604	0.0	1.0
1395_0000606	0.0	1.0
1395_0000607	0.0	1.0
1395_0000608	0.0	2.0
1395_0000609	1.0	1.0
1395_0000610	1.0	2.0
1395_0000611	0.0	1.0
1395_0000612	1.0	2.0
1395_0000626	1.0	2.0
1395_0000627	1.0	1.0
1395_0000628	0.0	2.0
1395_0000630	0.0	1.0
1395_0000631	0.0	2.0
1395_0000635	0.0	1.0
1395_0000636	0.0	1.0
1395_0000639	0.0	2.0
1395_0000642	0.0	1.0
1395_0000644	1.0	2.0
1395_0000646	0.0	1.0
1395_0000649	2.0	2.0
1395_0001010	1.0	2.0
1395_0001013	1.0	2.0
1395_0001015	0.0	2.0
1395_0001016	1.0	1.0
1395_0001017	0.0	2.0
1395_0001019	0.0	1.0
1395_0001020	1.0	1.0
1395_0001021	1.0	1.0
1395_0001022	1.0	2.0
1395_0001023	1.0	2.0
1395_0001024	1.0	1.0
1395_0001028	1.0	2.0
1395_0001033	1.0	2.0
1395_0001034	0.0	2.0
1395_0001040	0.0	1.0
1395_0001045	1.0	2.0
1395_0001058	1.0	1.0
1395_0001060	1.0	2.0
1395_0001061	1.0	2.0
1395_0001064	1.0	1.0
1395_0001065	0.0	2.0
1395_0001066	0.0	2.0
1395_0001067	0.0	1.0
1395_0001068	0.0	2.0
1395_0001069	2.0	2.0
1395_0001070	1.0	2.0
1395_0001071	1.0	1.0
1395_0001073	1.0	1.0
1395_0001074	1.0	1.0
1395_0001075	0.0	2.0
1395_0001076	1.0	2.0
1395_0001078	0.0	2.0
1395_0001080	1.0	2.0
1395_0001084	0.0	2.0
1395_0001090	1.0	2.0
1395_0001093	0.0	1.0
1395_0001101	1.0	2.0
1395_0001103	0.0	2.0
1395_0001104	0.0	1.0
1395_0001108	0.0	1.0
1395_0001109	0.0	2.0
1395_0001114	0.0	2.0
1395_0001115	1.0	2.0
1395_0001116	1.0	1.0
1395_0001117	0.0	1.0
1395_0001118	0.0	1.0
1395_0001119	1.0	2.0
1395_0001120	0.0	1.0
1395_0001121	0.0	1.0
1395_0001122	0.0	1.0
1395_0001123	0.0	2.0
1395_0001124	1.0	1.0
1395_0001126	1.0	1.0
1395_0001131	0.0	1.0
1395_0001132	2.0	2.0
1395_0001133	1.0	2.0
1395_0001141	1.0	1.0
1395_0001145	1.0	2.0
1395_0001146	0.0	1.0
1395_0001147	0.0	2.0
1395_0001149	0.0	2.0
1395_0001150	0.0	1.0
1395_0001158	2.0	2.0
1395_0001160	1.0	2.0
1395_0001161	1.0	2.0
1395_0001164	1.0	2.0
1395_0001167	2.0	2.0
1395_0001169	1.0	1.0
1395_0001170	1.0	2.0
1395_0001171	1.0	2.0
Language = IT, Weighted F1-score = 0.544608348902895, Dimension = CoherenceCohesion

Train on DE, test on  CZ
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
144.90552995391704 65.35717405024758
LANGUAGE: IT
nan nan
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']


======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 1.18
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 2.85
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00       261
         1.0       0.48      0.30      0.37       106
         2.0       0.33      0.22      0.26        63
         3.0       0.01      0.75      0.02         4

    accuracy                           0.11       434
   macro avg       0.20      0.32      0.16       434
weighted avg       0.16      0.11      0.13       434

[[  0  32  10 219]
 [  0  32  18  56]
 [  0   3  14  46]
 [  0   0   1   3]]
0.12886763363603573
434 434 434



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.95
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 3.29
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00       261
         1.0       0.57      0.23      0.32       106
         2.0       0.29      0.83      0.42        63
         3.0       0.00      0.25      0.01         4

    accuracy                           0.18       434
   macro avg       0.22      0.33      0.19       434
weighted avg       0.18      0.18      0.14       434

[[  0  17  59 185]
 [  0  24  68  14]
 [  0   1  52  10]
 [  0   0   3   1]]
0.14091847626452905
434 434 434



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.81
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 3.66
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00       261
         1.0       0.75      0.14      0.24       106
         2.0       0.24      0.52      0.33        63
         3.0       0.01      0.75      0.02         4

    accuracy                           0.12       434
   macro avg       0.25      0.35      0.15       434
weighted avg       0.22      0.12      0.11       434

[[  0   5  47 209]
 [  0  15  59  32]
 [  0   0  33  30]
 [  0   0   1   3]]
0.10554650874202005
434 434 434



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.70
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 3.77
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00       261
         1.0       0.71      0.11      0.20       106
         2.0       0.23      0.40      0.29        63
         3.0       0.01      0.75      0.02         4
         4.0       0.00      0.00      0.00         0

    accuracy                           0.09       434
   macro avg       0.19      0.25      0.10       434
weighted avg       0.21      0.09      0.09       434

[[  0   5  37 217   2]
 [  0  12  44  50   0]
 [  0   0  25  38   0]
 [  0   0   1   3   0]
 [  0   0   0   0   0]]
0.09052825269791996
434 434 434
Filename	True Label	Prediction
0601	1.0	2.0
0602	1.0	3.0
0603	2.0	3.0
0604	1.0	3.0
0605	2.0	3.0
0606	2.0	2.0
0607	2.0	2.0
0608	0.0	3.0
0609	1.0	3.0
0610	1.0	3.0
0611	1.0	3.0
0612	2.0	2.0
0613	0.0	2.0
0614	2.0	3.0
0615	1.0	3.0
0616	2.0	3.0
0617	0.0	2.0
0618	2.0	3.0
0619	2.0	2.0
0620	1.0	3.0
0621	2.0	3.0
0622	1.0	2.0
0623	0.0	3.0
0624	2.0	3.0
0625	1.0	2.0
0626	2.0	3.0
0627	2.0	3.0
0628	2.0	3.0
0629	2.0	2.0
0630	0.0	2.0
0631	2.0	2.0
0632	1.0	2.0
0633	2.0	3.0
0634	3.0	3.0
0635	2.0	2.0
0636	2.0	3.0
0637	2.0	3.0
0638	1.0	3.0
0639	1.0	3.0
0640	2.0	3.0
0641	1.0	2.0
0642	2.0	3.0
0643	1.0	3.0
0644	1.0	3.0
0645	2.0	3.0
0714	2.0	3.0
0715	2.0	3.0
0716	2.0	3.0
0717	1.0	2.0
0718	1.0	3.0
0719	1.0	3.0
0720	2.0	2.0
0721	2.0	3.0
0722	2.0	2.0
0723	2.0	3.0
0724	2.0	3.0
0725	1.0	3.0
0801	1.0	3.0
0802	1.0	2.0
0803	1.0	2.0
0804	2.0	3.0
0805	2.0	2.0
0806	2.0	3.0
0807	2.0	2.0
0808	1.0	2.0
0809	1.0	2.0
0810	1.0	2.0
0811	2.0	3.0
0812	1.0	2.0
0813	2.0	2.0
0814	1.0	2.0
0815	2.0	3.0
0816	3.0	3.0
0817	1.0	2.0
0818	1.0	2.0
0819	3.0	3.0
0820	1.0	2.0
0821	2.0	3.0
0822	1.0	3.0
0823	1.0	3.0
0824	1.0	3.0
0825	1.0	3.0
0826	2.0	2.0
0827	2.0	2.0
0828	1.0	2.0
0829	1.0	3.0
0901	2.0	3.0
0902	2.0	2.0
0903	1.0	2.0
0904	1.0	2.0
0905	1.0	3.0
0906	2.0	3.0
0907	2.0	3.0
0910	2.0	2.0
0911	2.0	2.0
0912	2.0	3.0
0913	2.0	2.0
0914	1.0	2.0
0915	2.0	3.0
0916	1.0	3.0
0917	2.0	2.0
0918	1.0	3.0
0919	2.0	2.0
0920	2.0	3.0
0921	2.0	2.0
0922	1.0	3.0
0923	2.0	3.0
0924	1.0	2.0
0925	2.0	3.0
0926	2.0	3.0
0927	2.0	2.0
0928	2.0	3.0
0929	0.0	3.0
0930	1.0	3.0
1001	1.0	3.0
1002	1.0	3.0
1003	1.0	3.0
1004	1.0	2.0
1005	1.0	3.0
1006	1.0	3.0
1007	1.0	2.0
1008	1.0	3.0
1009	2.0	3.0
1010	1.0	3.0
1014	1.0	3.0
1015	1.0	3.0
1016	1.0	3.0
1017	1.0	3.0
1018	1.0	3.0
1019	1.0	3.0
1020	1.0	3.0
1021	1.0	3.0
1022	1.0	3.0
1023	1.0	3.0
1111	1.0	2.0
1112	1.0	3.0
1113	1.0	3.0
1114	1.0	3.0
1115	1.0	3.0
1116	1.0	3.0
1117	1.0	3.0
9999	0.0	2.0
BER0609003	0.0	3.0
BER0611003	0.0	3.0
BER0611005	0.0	3.0
BER0611006	0.0	3.0
BER0611007	0.0	3.0
KYJ0611003A	1.0	2.0
KYJ0611004A	1.0	2.0
KYJ0611005A	1.0	2.0
KYJ0611005B	0.0	2.0
KYJ0611006A	1.0	1.0
KYJ0611006B	0.0	2.0
KYJ0611009A	1.0	2.0
KYJ0611009B	0.0	3.0
LIB0611001A	0.0	2.0
LIB0611001B	0.0	3.0
LIB0611002A	0.0	2.0
LIB0611002B	0.0	3.0
LIB0611003A	1.0	2.0
LIB0611004A	1.0	1.0
LIB0611004B	0.0	3.0
LIB0611011	0.0	3.0
LON0610002A	1.0	2.0
LON0610002B	0.0	2.0
LON0611002A	1.0	2.0
LON0611002B	0.0	3.0
LON0611003	0.0	4.0
LON0611004A	1.0	2.0
LON0611004B	0.0	2.0
MOS0509001	0.0	3.0
MOS0509004	0.0	3.0
MOS0611012	0.0	3.0
MOS0611013	0.0	3.0
MOS0611014	0.0	3.0
MOS0611015	0.0	3.0
PAR1011008A	2.0	2.0
PAR1011009A	2.0	2.0
PAR1011009B	0.0	3.0
PAR1011013	0.0	3.0
PAR1011014	0.0	3.0
PAR1011015	0.0	3.0
PAR1011016	0.0	3.0
PAR1011017	0.0	3.0
PAR1011018	0.0	3.0
PHA0111001A	1.0	1.0
PHA0111001B	0.0	3.0
PHA0111002A	0.0	2.0
PHA0111002B	0.0	3.0
PHA0111003A	1.0	1.0
PHA0111003B	0.0	2.0
PHA0111004A	1.0	1.0
PHA0111004B	0.0	3.0
PHA0111005A	1.0	2.0
PHA0111005B	0.0	3.0
PHA0111010	0.0	3.0
PHA0111011	0.0	3.0
PHA0111012	0.0	3.0
PHA0111014	0.0	3.0
PHA0111015	0.0	3.0
PHA0111016	0.0	3.0
PHA0111018	0.0	3.0
PHA0112002A	1.0	2.0
PHA0112002B	0.0	3.0
PHA0112003A	1.0	2.0
PHA0112003B	0.0	2.0
PHA0112006A	3.0	2.0
PHA0112006B	0.0	3.0
PHA0112007A	1.0	1.0
PHA0112007B	0.0	2.0
PHA0112009A	2.0	2.0
PHA0112009B	0.0	2.0
PHA0112012A	1.0	3.0
PHA0112012B	0.0	3.0
PHA0209001	0.0	3.0
PHA0209008	0.0	2.0
PHA0209013	0.0	2.0
PHA0209024	0.0	3.0
PHA0209026	0.0	3.0
PHA0209028	0.0	3.0
PHA0209031	0.0	3.0
PHA0209034	0.0	3.0
PHA0209038	0.0	3.0
PHA0209039	0.0	3.0
PHA0210001	0.0	3.0
PHA0210004	0.0	3.0
PHA0210007	0.0	3.0
PHA0210008	0.0	2.0
PHA0411008A	0.0	1.0
PHA0411008B	0.0	2.0
PHA0411009A	1.0	2.0
PHA0411009B	0.0	2.0
PHA0411010A	1.0	1.0
PHA0411010B	0.0	2.0
PHA0411011A	1.0	1.0
PHA0411011B	0.0	2.0
PHA0411012A	1.0	1.0
PHA0411012B	0.0	3.0
PHA0411027	0.0	3.0
PHA0411028	0.0	3.0
PHA0411029	0.0	3.0
PHA0411030	0.0	3.0
PHA0411031	0.0	3.0
PHA0411032	0.0	3.0
PHA0411033	0.0	3.0
PHA0411034	0.0	2.0
PHA0411035	0.0	3.0
PHA0411036	0.0	3.0
PHA0411037	0.0	3.0
PHA0411038	0.0	3.0
PHA0411039	0.0	3.0
PHA0411041	0.0	3.0
PHA0411042	0.0	3.0
PHA0411043	0.0	3.0
PHA0411044	0.0	3.0
PHA0411045	0.0	3.0
PHA0411047	0.0	3.0
PHA0411051	0.0	3.0
PHA0411053	0.0	3.0
PHA0411054	0.0	3.0
PHA0411055	0.0	3.0
PHA0411056	0.0	3.0
PHA0411058	0.0	3.0
PHA0411059	0.0	3.0
PHA0411060	0.0	3.0
PHA0411061	0.0	3.0
PHA0411062	0.0	3.0
PHA0509002	0.0	2.0
PHA0509007	0.0	3.0
PHA0509013	0.0	2.0
PHA0509015	0.0	3.0
PHA0509017	0.0	3.0
PHA0509018	0.0	3.0
PHA0509019	0.0	3.0
PHA0509020	0.0	3.0
PHA0509021	0.0	3.0
PHA0509022	0.0	3.0
PHA0509024	0.0	3.0
PHA0509025	0.0	3.0
PHA0509026	0.0	3.0
PHA0509027	0.0	3.0
PHA0509028	0.0	3.0
PHA0509030	0.0	3.0
PHA0509031	0.0	3.0
PHA0509032	0.0	3.0
PHA0509033	0.0	3.0
PHA0509034	0.0	3.0
PHA0509035	0.0	3.0
PHA0509036	0.0	3.0
PHA0509037	0.0	3.0
PHA0509038	0.0	3.0
PHA0509039	0.0	3.0
PHA0509040	0.0	3.0
PHA0509041	0.0	3.0
PHA0509042	0.0	3.0
PHA0509043	0.0	3.0
PHA0509044	0.0	3.0
PHA0509045	0.0	3.0
PHA0510002A	0.0	1.0
PHA0510002B	0.0	2.0
PHA0510003A	0.0	2.0
PHA0510003B	0.0	3.0
PHA0510004A	0.0	2.0
PHA0510004B	0.0	3.0
PHA0510010A	1.0	1.0
PHA0510010B	0.0	1.0
PHA0510013A	1.0	2.0
PHA0510013B	0.0	3.0
PHA0510023	0.0	3.0
PHA0510027	0.0	3.0
PHA0510029	0.0	3.0
PHA0510030	0.0	3.0
PHA0510031	0.0	3.0
PHA0510032	0.0	3.0
PHA0510034	0.0	3.0
PHA0510035	0.0	3.0
PHA0510036	0.0	3.0
PHA0510037	0.0	3.0
PHA0510038	0.0	3.0
PHA0510039	0.0	3.0
PHA0510040	0.0	3.0
PHA0510046	0.0	3.0
PHA0510047	0.0	3.0
PHA0510048	0.0	3.0
PHA0510049	0.0	3.0
PHA0510050	0.0	3.0
PHA0610005A	1.0	2.0
PHA0610005B	0.0	2.0
PHA0610006A	0.0	2.0
PHA0610006B	0.0	3.0
PHA0610007A	0.0	1.0
PHA0610007B	0.0	3.0
PHA0610015	0.0	3.0
PHA0610016	0.0	3.0
PHA0610017	0.0	3.0
PHA0610018	0.0	3.0
PHA0610019A	0.0	2.0
PHA0610019B	0.0	3.0
PHA0610025	0.0	3.0
PHA0610026	0.0	3.0
PHA0709008	0.0	3.0
PHA0710009	0.0	3.0
PHA0710010	0.0	3.0
PHA0710011	0.0	3.0
PHA0710012	0.0	3.0
PHA0710013	0.0	4.0
PHA0710014	0.0	3.0
PHA0710015	0.0	3.0
PHA0710016	0.0	3.0
PHA0710017	0.0	3.0
PHA0710018	0.0	3.0
PHA0710019	0.0	3.0
PHA0710021	0.0	3.0
PHA0809009	0.0	3.0
PHA0809010	0.0	3.0
PHA0810001	0.0	3.0
PHA0810002	0.0	3.0
PHA0810003	0.0	3.0
PHA0810004	0.0	3.0
PHA0810006	0.0	3.0
PHA0810008	0.0	3.0
PHA0810009	0.0	3.0
PHA0810010	0.0	3.0
PHA0810011	0.0	3.0
PHA0810012	0.0	3.0
PHA0810015	0.0	3.0
PHA0811010	0.0	3.0
PHA0811012	0.0	3.0
PHA0811013	0.0	3.0
PHA0811014	0.0	3.0
PHA0811016	0.0	3.0
PHA0811017	0.0	3.0
PHA0811019	0.0	3.0
PHA0811020	0.0	3.0
PHA1109001	0.0	3.0
PHA1109002	0.0	3.0
PHA1109003	0.0	3.0
PHA1109004	0.0	3.0
PHA1109005	0.0	3.0
PHA1109006	0.0	3.0
PHA1109007	0.0	3.0
PHA1109008	0.0	2.0
PHA1109023	0.0	3.0
PHA1109024	0.0	3.0
PHA1109025	0.0	2.0
PHA1109026	0.0	3.0
PHA1109027	0.0	3.0
PHA1109028	0.0	3.0
PHA1110001A	1.0	2.0
PHA1110001B	0.0	3.0
PHA1110002A	1.0	3.0
PHA1110002B	0.0	3.0
PHA1110003A	1.0	2.0
PHA1110003B	0.0	3.0
PHA1110004A	1.0	1.0
PHA1110013	0.0	3.0
PHA1110014	0.0	3.0
PHA1110015	0.0	3.0
PHA1110016	0.0	3.0
PHA1110017	0.0	3.0
PHA1110019	0.0	3.0
PHA1110021	0.0	3.0
PHA1110022	0.0	3.0
PHA1111001A	1.0	2.0
PHA1111001B	0.0	3.0
PHA1111002A	1.0	1.0
PHA1111002B	0.0	2.0
PHA1111003A	1.0	2.0
PHA1111003B	0.0	3.0
PHA1111004A	1.0	2.0
PHA1111004B	0.0	3.0
PHA1111006A	0.0	1.0
PHA1111006B	0.0	2.0
PHA1111008A	1.0	2.0
PHA1111008B	0.0	3.0
PHA1111009A	0.0	2.0
ST071122B	0.0	2.0
TI071122B	0.0	3.0
VAR0209036	0.0	3.0
VAR0909003	0.0	3.0
VAR0909004	0.0	3.0
VAR0909005	0.0	3.0
VAR0909006	0.0	3.0
VAR0909007	0.0	3.0
VAR0909008	0.0	3.0
VAR0909009	0.0	3.0
VAR0909010	0.0	3.0
VAR0910004	0.0	3.0
VAR0910005	0.0	3.0
VAR0910006	0.0	3.0
VAR0910007	0.0	3.0
VAR0910009	0.0	3.0
VAR0910010	0.0	3.0
VAR0910011	0.0	3.0
Language = CZ, Weighted F1-score = 0.09052825269791996, Dimension = Sociolinguisticappropriateness

Train on DE, test on  IT
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
nan nan
LANGUAGE: IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']


======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 1.18
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.32
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        56
         1.0       0.64      0.10      0.17       336
         2.0       0.41      0.62      0.49       372
         3.0       0.14      0.69      0.23        36

    accuracy                           0.36       800
   macro avg       0.30      0.35      0.22       800
weighted avg       0.47      0.36      0.31       800

[[  0  17  39   0]
 [  0  34 283  19]
 [  0   2 231 139]
 [  0   0  11  25]]
0.31321222650821723
800 800 800



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 1.00
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.18
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        56
         1.0       0.61      0.37      0.46       336
         2.0       0.46      0.49      0.47       372
         3.0       0.14      0.78      0.24        36

    accuracy                           0.42       800
   macro avg       0.30      0.41      0.29       800
weighted avg       0.47      0.42      0.42       800

[[  0  39  17   0]
 [  0 123 193  20]
 [  0  40 183 149]
 [  0   0   8  28]]
0.4230282362471086
800 800 800



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.90
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.24
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        56
         1.0       0.62      0.32      0.42       336
         2.0       0.41      0.43      0.42       372
         3.0       0.12      0.81      0.21        36

    accuracy                           0.37       800
   macro avg       0.29      0.39      0.26       800
weighted avg       0.45      0.37      0.38       800

[[  0  35  21   0]
 [  0 108 203  25]
 [  0  32 159 181]
 [  0   0   7  29]]
0.3812203609955418
800 800 800



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35

  Average training loss: 0.81
  Training epoch took: 37
Running Validation...
  Average evaluation loss: 1.49
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        56
         1.0       0.64      0.24      0.34       336
         2.0       0.34      0.36      0.35       372
         3.0       0.11      0.89      0.20        36

    accuracy                           0.31       800
   macro avg       0.27      0.37      0.22       800
weighted avg       0.43      0.31      0.32       800

[[  0  26  30   0]
 [  0  79 228  29]
 [  0  19 135 218]
 [  0   0   4  32]]
0.31666770590183263
800 800 800
Filename	True Label	Prediction
1325_1001008	2.0	3.0
1325_1001009	2.0	3.0
1325_1001010	2.0	3.0
1325_1001011	3.0	3.0
1325_1001012	2.0	3.0
1325_1001013	3.0	3.0
1325_1001014	2.0	3.0
1325_1001015	2.0	3.0
1325_1001016	1.0	3.0
1325_1001017	1.0	3.0
1325_1001018	2.0	3.0
1325_1001019	2.0	3.0
1325_1001020	2.0	3.0
1325_1001021	2.0	3.0
1325_1001022	2.0	3.0
1325_1001023	2.0	3.0
1325_1001024	2.0	3.0
1325_1001025	2.0	3.0
1325_1001027	2.0	3.0
1325_1001028	1.0	3.0
1325_1001029	2.0	3.0
1325_1001032	2.0	3.0
1325_1001033	2.0	3.0
1325_1001035	3.0	3.0
1325_1001036	2.0	3.0
1325_1001037	2.0	3.0
1325_1001039	2.0	3.0
1325_1001040	2.0	3.0
1325_1001041	3.0	3.0
1325_1001042	2.0	3.0
1325_1001043	2.0	3.0
1325_1001044	2.0	3.0
1325_1001045	2.0	3.0
1325_1001046	1.0	3.0
1325_1001047	2.0	2.0
1325_1001048	2.0	3.0
1325_1001050	2.0	3.0
1325_1001051	1.0	3.0
1325_1001052	2.0	3.0
1325_1001053	2.0	2.0
1325_1001054	3.0	3.0
1325_1001055	2.0	3.0
1325_1001056	2.0	3.0
1325_1001057	2.0	3.0
1325_1001058	2.0	3.0
1325_1001059	2.0	3.0
1325_1001062	2.0	3.0
1325_1001063	1.0	3.0
1325_1001075	2.0	3.0
1325_1001076	2.0	3.0
1325_1001077	2.0	3.0
1325_1001078	2.0	3.0
1325_1001079	2.0	3.0
1325_1001080	2.0	3.0
1325_1001081	2.0	3.0
1325_1001082	2.0	3.0
1325_1001083	2.0	3.0
1325_1001084	2.0	3.0
1325_1001085	2.0	3.0
1325_1001086	2.0	3.0
1325_1001087	2.0	3.0
1325_1001088	2.0	2.0
1325_1001089	2.0	3.0
1325_1001090	2.0	3.0
1325_1001091	2.0	3.0
1325_1001092	2.0	2.0
1325_1001093	2.0	3.0
1325_1001094	2.0	3.0
1325_1001095	2.0	3.0
1325_1001096	2.0	3.0
1325_1001097	1.0	2.0
1325_1001098	2.0	3.0
1325_1001099	3.0	3.0
1325_1001100	2.0	3.0
1325_1001101	3.0	3.0
1325_1001107	2.0	3.0
1325_1001108	2.0	3.0
1325_1001109	2.0	3.0
1325_1001110	2.0	3.0
1325_1001111	3.0	3.0
1325_1001113	3.0	3.0
1325_1001119	2.0	3.0
1325_1001120	2.0	3.0
1325_1001121	2.0	3.0
1325_1001122	2.0	3.0
1325_1001123	2.0	3.0
1325_1001124	1.0	3.0
1325_1001125	2.0	3.0
1325_1001126	2.0	3.0
1325_1001127	2.0	3.0
1325_1001128	2.0	3.0
1325_1001129	1.0	3.0
1325_1001130	2.0	3.0
1325_1001131	2.0	3.0
1325_1001132	2.0	3.0
1325_1001133	2.0	3.0
1325_1001134	2.0	3.0
1325_1001135	2.0	3.0
1325_1001136	2.0	3.0
1325_1001138	2.0	3.0
1325_1001139	2.0	3.0
1325_1001141	1.0	3.0
1325_1001142	1.0	3.0
1325_1001143	2.0	3.0
1325_1001144	2.0	3.0
1325_1001152	2.0	3.0
1325_1001153	2.0	3.0
1325_1001154	3.0	3.0
1325_1001155	2.0	3.0
1325_1001156	2.0	3.0
1325_1001157	2.0	3.0
1325_1001158	2.0	3.0
1325_1001159	2.0	3.0
1325_1001160	2.0	3.0
1325_1001161	2.0	3.0
1325_1001162	2.0	3.0
1325_1001163	1.0	3.0
1325_1001164	2.0	3.0
1325_1001165	1.0	2.0
1325_1001166	2.0	3.0
1325_1001167	2.0	3.0
1325_1001168	1.0	3.0
1325_1001169	2.0	3.0
1325_1001170	2.0	3.0
1325_9000059	3.0	3.0
1325_9000087	2.0	3.0
1325_9000088	2.0	3.0
1325_9000089	2.0	3.0
1325_9000090	2.0	3.0
1325_9000095	2.0	3.0
1325_9000099	2.0	3.0
1325_9000102	2.0	2.0
1325_9000104	2.0	3.0
1325_9000105	2.0	2.0
1325_9000106	2.0	3.0
1325_9000107	3.0	3.0
1325_9000136	2.0	3.0
1325_9000137	2.0	3.0
1325_9000138	2.0	3.0
1325_9000139	2.0	3.0
1325_9000140	3.0	3.0
1325_9000143	3.0	3.0
1325_9000144	3.0	3.0
1325_9000152	2.0	3.0
1325_9000185	3.0	3.0
1325_9000186	3.0	3.0
1325_9000187	2.0	3.0
1325_9000188	2.0	3.0
1325_9000209	2.0	3.0
1325_9000210	1.0	3.0
1325_9000211	2.0	3.0
1325_9000213	3.0	3.0
1325_9000214	2.0	3.0
1325_9000215	2.0	3.0
1325_9000237	2.0	3.0
1325_9000239	3.0	3.0
1325_9000240	2.0	3.0
1325_9000241	3.0	3.0
1325_9000278	3.0	3.0
1325_9000279	2.0	3.0
1325_9000296	2.0	3.0
1325_9000302	2.0	3.0
1325_9000303	2.0	3.0
1325_9000304	2.0	3.0
1325_9000314	2.0	3.0
1325_9000315	1.0	2.0
1325_9000316	2.0	3.0
1325_9000317	2.0	3.0
1325_9000318	2.0	3.0
1325_9000319	2.0	3.0
1325_9000320	3.0	2.0
1325_9000321	3.0	3.0
1325_9000322	2.0	3.0
1325_9000323	1.0	3.0
1325_9000503	3.0	3.0
1325_9000504	2.0	3.0
1325_9000505	2.0	3.0
1325_9000533	2.0	3.0
1325_9000534	1.0	3.0
1325_9000536	3.0	3.0
1325_9000554	2.0	3.0
1325_9000601	2.0	3.0
1325_9000602	2.0	3.0
1325_9000611	2.0	3.0
1325_9000612	1.0	3.0
1325_9000674	3.0	3.0
1325_9000675	2.0	3.0
1325_9000676	2.0	3.0
1325_9000677	2.0	3.0
1325_9000678	3.0	3.0
1325_9000684	2.0	3.0
1325_9000685	3.0	3.0
1325_9000686	2.0	3.0
1325_9000700	2.0	3.0
1325_9000750	3.0	2.0
1365_0100002	1.0	2.0
1365_0100003	2.0	2.0
1365_0100004	1.0	3.0
1365_0100005	2.0	2.0
1365_0100006	2.0	3.0
1365_0100007	1.0	2.0
1365_0100008	1.0	2.0
1365_0100009	2.0	1.0
1365_0100010	1.0	2.0
1365_0100011	2.0	2.0
1365_0100012	2.0	2.0
1365_0100013	2.0	3.0
1365_0100014	2.0	2.0
1365_0100015	2.0	2.0
1365_0100016	1.0	3.0
1365_0100017	2.0	3.0
1365_0100018	2.0	2.0
1365_0100019	1.0	2.0
1365_0100020	2.0	2.0
1365_0100021	2.0	2.0
1365_0100022	1.0	2.0
1365_0100023	1.0	2.0
1365_0100024	1.0	2.0
1365_0100026	2.0	1.0
1365_0100027	2.0	2.0
1365_0100028	2.0	2.0
1365_0100029	2.0	1.0
1365_0100030	1.0	2.0
1365_0100031	1.0	2.0
1365_0100051	1.0	2.0
1365_0100056	1.0	2.0
1365_0100057	1.0	3.0
1365_0100058	2.0	3.0
1365_0100061	3.0	2.0
1365_0100063	2.0	3.0
1365_0100064	2.0	2.0
1365_0100065	1.0	2.0
1365_0100066	1.0	2.0
1365_0100067	1.0	2.0
1365_0100069	2.0	2.0
1365_0100070	2.0	2.0
1365_0100071	2.0	3.0
1365_0100072	2.0	2.0
1365_0100073	2.0	2.0
1365_0100074	1.0	3.0
1365_0100079	1.0	2.0
1365_0100080	2.0	3.0
1365_0100092	2.0	2.0
1365_0100093	2.0	2.0
1365_0100094	2.0	2.0
1365_0100095	2.0	2.0
1365_0100096	2.0	3.0
1365_0100097	2.0	2.0
1365_0100098	2.0	2.0
1365_0100099	2.0	2.0
1365_0100100	2.0	3.0
1365_0100101	2.0	3.0
1365_0100102	2.0	3.0
1365_0100103	2.0	3.0
1365_0100104	1.0	3.0
1365_0100105	3.0	3.0
1365_0100106	1.0	3.0
1365_0100107	2.0	3.0
1365_0100116	2.0	2.0
1365_0100117	2.0	2.0
1365_0100118	2.0	3.0
1365_0100119	3.0	3.0
1365_0100120	2.0	3.0
1365_0100121	2.0	3.0
1365_0100123	2.0	3.0
1365_0100125	2.0	3.0
1365_0100133	2.0	2.0
1365_0100134	2.0	2.0
1365_0100135	1.0	2.0
1365_0100136	2.0	2.0
1365_0100137	2.0	2.0
1365_0100138	2.0	2.0
1365_0100139	2.0	2.0
1365_0100145	2.0	3.0
1365_0100146	2.0	2.0
1365_0100147	2.0	2.0
1365_0100148	1.0	3.0
1365_0100151	2.0	1.0
1365_0100162	2.0	3.0
1365_0100163	2.0	3.0
1365_0100164	1.0	3.0
1365_0100165	2.0	3.0
1365_0100166	2.0	2.0
1365_0100167	1.0	2.0
1365_0100168	2.0	2.0
1365_0100169	2.0	2.0
1365_0100170	1.0	3.0
1365_0100171	1.0	2.0
1365_0100172	2.0	2.0
1365_0100173	1.0	2.0
1365_0100174	1.0	2.0
1365_0100175	2.0	2.0
1365_0100176	2.0	2.0
1365_0100177	2.0	2.0
1365_0100178	2.0	2.0
1365_0100179	2.0	2.0
1365_0100180	1.0	2.0
1365_0100181	1.0	2.0
1365_0100182	2.0	3.0
1365_0100183	2.0	3.0
1365_0100184	2.0	2.0
1365_0100185	2.0	2.0
1365_0100186	2.0	2.0
1365_0100187	2.0	2.0
1365_0100188	1.0	2.0
1365_0100190	2.0	2.0
1365_0100191	1.0	3.0
1365_0100192	3.0	3.0
1365_0100194	2.0	2.0
1365_0100195	1.0	2.0
1365_0100196	1.0	3.0
1365_0100198	1.0	2.0
1365_0100199	2.0	3.0
1365_0100200	2.0	3.0
1365_0100201	2.0	2.0
1365_0100202	2.0	2.0
1365_0100203	2.0	2.0
1365_0100204	1.0	2.0
1365_0100205	2.0	1.0
1365_0100211	2.0	3.0
1365_0100212	3.0	3.0
1365_0100213	2.0	2.0
1365_0100215	2.0	2.0
1365_0100217	2.0	3.0
1365_0100218	2.0	2.0
1365_0100219	2.0	3.0
1365_0100220	2.0	3.0
1365_0100221	2.0	2.0
1365_0100222	2.0	3.0
1365_0100223	2.0	3.0
1365_0100224	2.0	3.0
1365_0100225	2.0	2.0
1365_0100226	2.0	2.0
1365_0100227	2.0	3.0
1365_0100228	2.0	3.0
1365_0100229	2.0	3.0
1365_0100230	2.0	3.0
1365_0100231	2.0	3.0
1365_0100232	2.0	3.0
1365_0100233	2.0	3.0
1365_0100251	2.0	3.0
1365_0100252	2.0	3.0
1365_0100253	1.0	2.0
1365_0100255	1.0	2.0
1365_0100256	2.0	2.0
1365_0100257	2.0	3.0
1365_0100258	2.0	2.0
1365_0100259	2.0	2.0
1365_0100260	1.0	2.0
1365_0100261	2.0	2.0
1365_0100262	2.0	3.0
1365_0100263	3.0	3.0
1365_0100265	2.0	3.0
1365_0100266	1.0	3.0
1365_0100267	2.0	3.0
1365_0100268	2.0	2.0
1365_0100269	2.0	2.0
1365_0100270	2.0	3.0
1365_0100274	2.0	3.0
1365_0100275	2.0	3.0
1365_0100276	2.0	3.0
1365_0100277	2.0	3.0
1365_0100278	2.0	2.0
1365_0100279	2.0	3.0
1365_0100280	1.0	2.0
1365_0100281	2.0	2.0
1365_0100282	2.0	3.0
1365_0100285	2.0	2.0
1365_0100286	1.0	2.0
1365_0100287	2.0	2.0
1365_0100288	2.0	2.0
1365_0100289	2.0	2.0
1365_0100290	2.0	2.0
1365_0100299	1.0	2.0
1365_0100447	2.0	2.0
1365_0100448	2.0	3.0
1365_0100451	2.0	3.0
1365_0100455	2.0	3.0
1365_0100456	2.0	3.0
1365_0100457	2.0	3.0
1365_0100458	2.0	3.0
1365_0100459	2.0	3.0
1365_0100461	2.0	3.0
1365_0100469	2.0	3.0
1365_0100470	2.0	3.0
1365_0100471	2.0	3.0
1365_0100472	2.0	3.0
1365_0100473	2.0	3.0
1365_0100474	2.0	3.0
1365_0100475	2.0	3.0
1365_0100476	2.0	3.0
1365_0100477	2.0	3.0
1365_0100478	2.0	3.0
1365_0100479	2.0	3.0
1365_0100480	2.0	3.0
1365_0100481	2.0	3.0
1365_0100482	2.0	3.0
1385_0000011	1.0	1.0
1385_0000012	1.0	1.0
1385_0000013	0.0	1.0
1385_0000016	1.0	1.0
1385_0000017	1.0	1.0
1385_0000020	2.0	1.0
1385_0000021	2.0	2.0
1385_0000022	0.0	2.0
1385_0000023	1.0	1.0
1385_0000033	1.0	2.0
1385_0000034	1.0	2.0
1385_0000035	1.0	1.0
1385_0000036	1.0	2.0
1385_0000037	0.0	2.0
1385_0000038	2.0	1.0
1385_0000039	1.0	1.0
1385_0000040	1.0	1.0
1385_0000041	2.0	2.0
1385_0000042	2.0	1.0
1385_0000043	2.0	1.0
1385_0000044	2.0	2.0
1385_0000045	2.0	2.0
1385_0000047	2.0	2.0
1385_0000048	2.0	2.0
1385_0000049	2.0	1.0
1385_0000050	2.0	2.0
1385_0000051	2.0	2.0
1385_0000052	2.0	1.0
1385_0000053	1.0	1.0
1385_0000054	2.0	2.0
1385_0000057	2.0	2.0
1385_0000058	2.0	2.0
1385_0000059	2.0	2.0
1385_0000095	1.0	1.0
1385_0000097	1.0	1.0
1385_0000098	1.0	1.0
1385_0000099	1.0	2.0
1385_0000100	1.0	1.0
1385_0000101	1.0	1.0
1385_0000102	1.0	2.0
1385_0000103	1.0	1.0
1385_0000104	1.0	1.0
1385_0000114	2.0	2.0
1385_0000119	2.0	2.0
1385_0000120	0.0	1.0
1385_0000122	1.0	2.0
1385_0000123	1.0	2.0
1385_0000124	2.0	2.0
1385_0000125	1.0	2.0
1385_0000126	1.0	2.0
1385_0000127	1.0	2.0
1385_0000128	1.0	2.0
1385_0000129	2.0	2.0
1385_0000130	2.0	1.0
1385_0001103	1.0	1.0
1385_0001104	1.0	1.0
1385_0001105	1.0	2.0
1385_0001107	2.0	2.0
1385_0001108	1.0	2.0
1385_0001109	1.0	2.0
1385_0001110	1.0	2.0
1385_0001111	2.0	2.0
1385_0001112	1.0	2.0
1385_0001113	1.0	1.0
1385_0001118	1.0	1.0
1385_0001119	1.0	2.0
1385_0001120	1.0	1.0
1385_0001121	1.0	2.0
1385_0001122	1.0	1.0
1385_0001123	1.0	1.0
1385_0001124	1.0	1.0
1385_0001125	1.0	1.0
1385_0001126	1.0	1.0
1385_0001127	1.0	2.0
1385_0001128	1.0	2.0
1385_0001129	1.0	1.0
1385_0001130	1.0	1.0
1385_0001131	1.0	1.0
1385_0001132	1.0	1.0
1385_0001133	1.0	2.0
1385_0001134	1.0	1.0
1385_0001135	1.0	2.0
1385_0001136	1.0	1.0
1385_0001137	1.0	1.0
1385_0001138	1.0	2.0
1385_0001147	1.0	1.0
1385_0001148	1.0	2.0
1385_0001149	2.0	2.0
1385_0001150	2.0	1.0
1385_0001151	2.0	2.0
1385_0001152	2.0	2.0
1385_0001153	2.0	1.0
1385_0001154	2.0	2.0
1385_0001155	2.0	1.0
1385_0001156	2.0	2.0
1385_0001157	2.0	1.0
1385_0001158	2.0	2.0
1385_0001159	2.0	2.0
1385_0001160	1.0	2.0
1385_0001161	2.0	2.0
1385_0001162	1.0	1.0
1385_0001163	1.0	2.0
1385_0001164	1.0	2.0
1385_0001165	1.0	2.0
1385_0001166	1.0	2.0
1385_0001167	1.0	2.0
1385_0001169	1.0	1.0
1385_0001170	0.0	1.0
1385_0001171	1.0	1.0
1385_0001172	1.0	1.0
1385_0001173	0.0	1.0
1385_0001174	1.0	1.0
1385_0001175	1.0	1.0
1385_0001178	0.0	1.0
1385_0001188	1.0	2.0
1385_0001189	0.0	2.0
1385_0001190	0.0	1.0
1385_0001191	1.0	2.0
1385_0001192	0.0	2.0
1385_0001193	1.0	2.0
1385_0001194	1.0	2.0
1385_0001195	2.0	2.0
1385_0001196	1.0	2.0
1385_0001197	1.0	2.0
1385_0001198	1.0	2.0
1385_0001199	0.0	2.0
1385_0001501	0.0	1.0
1385_0001503	1.0	2.0
1385_0001522	1.0	1.0
1385_0001523	1.0	2.0
1385_0001524	1.0	2.0
1385_0001525	1.0	2.0
1385_0001526	0.0	1.0
1385_0001527	1.0	2.0
1385_0001528	1.0	2.0
1385_0001712	1.0	2.0
1385_0001714	0.0	1.0
1385_0001715	0.0	2.0
1385_0001716	1.0	2.0
1385_0001717	1.0	2.0
1385_0001718	1.0	1.0
1385_0001719	1.0	1.0
1385_0001720	0.0	1.0
1385_0001723	0.0	1.0
1385_0001724	1.0	2.0
1385_0001725	1.0	1.0
1385_0001726	0.0	2.0
1385_0001727	0.0	1.0
1385_0001728	1.0	2.0
1385_0001729	1.0	2.0
1385_0001730	1.0	2.0
1385_0001732	1.0	2.0
1385_0001733	1.0	2.0
1385_0001734	1.0	2.0
1385_0001736	1.0	1.0
1385_0001737	2.0	2.0
1385_0001738	0.0	1.0
1385_0001739	0.0	2.0
1385_0001740	1.0	2.0
1385_0001741	0.0	1.0
1385_0001742	0.0	1.0
1385_0001744	0.0	2.0
1385_0001746	1.0	2.0
1385_0001747	0.0	2.0
1385_0001748	1.0	1.0
1385_0001749	0.0	2.0
1385_0001750	0.0	1.0
1385_0001751	1.0	2.0
1385_0001752	0.0	2.0
1385_0001753	1.0	2.0
1385_0001754	0.0	2.0
1385_0001756	1.0	1.0
1385_0001757	1.0	1.0
1385_0001758	1.0	1.0
1385_0001759	1.0	1.0
1385_0001760	0.0	2.0
1385_0001761	1.0	1.0
1385_0001762	1.0	1.0
1385_0001764	1.0	2.0
1385_0001765	0.0	1.0
1385_0001766	2.0	2.0
1385_0001767	1.0	2.0
1385_0001768	1.0	1.0
1385_0001771	1.0	2.0
1385_0001772	0.0	1.0
1385_0001773	1.0	1.0
1385_0001774	0.0	1.0
1385_0001775	1.0	2.0
1385_0001785	0.0	2.0
1385_0001786	1.0	2.0
1385_0001787	0.0	1.0
1385_0001788	0.0	2.0
1385_0001789	1.0	2.0
1385_0001790	1.0	2.0
1385_0001791	1.0	1.0
1385_0001792	1.0	2.0
1385_0001793	1.0	2.0
1385_0001794	1.0	2.0
1385_0001795	1.0	1.0
1385_0001796	0.0	2.0
1385_0001798	1.0	2.0
1385_0001799	1.0	2.0
1385_0001800	1.0	2.0
1395_0000333	1.0	2.0
1395_0000337	1.0	1.0
1395_0000338	1.0	2.0
1395_0000340	1.0	2.0
1395_0000341	1.0	2.0
1395_0000353	1.0	2.0
1395_0000354	0.0	1.0
1395_0000355	1.0	1.0
1395_0000356	1.0	1.0
1395_0000357	2.0	2.0
1395_0000359	1.0	2.0
1395_0000360	2.0	2.0
1395_0000361	1.0	2.0
1395_0000364	1.0	2.0
1395_0000365	2.0	2.0
1395_0000366	2.0	2.0
1395_0000368	0.0	1.0
1395_0000369	2.0	2.0
1395_0000376	2.0	2.0
1395_0000378	1.0	1.0
1395_0000379	1.0	2.0
1395_0000380	1.0	2.0
1395_0000383	1.0	2.0
1395_0000387	3.0	2.0
1395_0000388	1.0	2.0
1395_0000389	1.0	1.0
1395_0000390	1.0	1.0
1395_0000391	2.0	2.0
1395_0000392	1.0	2.0
1395_0000396	1.0	2.0
1395_0000398	1.0	2.0
1395_0000399	1.0	1.0
1395_0000402	1.0	2.0
1395_0000403	1.0	2.0
1395_0000404	1.0	2.0
1395_0000409	2.0	2.0
1395_0000413	1.0	2.0
1395_0000414	1.0	2.0
1395_0000415	1.0	1.0
1395_0000432	1.0	2.0
1395_0000438	2.0	3.0
1395_0000443	1.0	2.0
1395_0000446	2.0	3.0
1395_0000447	1.0	2.0
1395_0000448	1.0	2.0
1395_0000449	1.0	2.0
1395_0000450	1.0	2.0
1395_0000451	2.0	2.0
1395_0000452	1.0	2.0
1395_0000454	2.0	2.0
1395_0000455	1.0	2.0
1395_0000458	1.0	2.0
1395_0000460	1.0	2.0
1395_0000462	2.0	2.0
1395_0000465	1.0	1.0
1395_0000469	1.0	1.0
1395_0000470	1.0	2.0
1395_0000471	1.0	2.0
1395_0000499	1.0	2.0
1395_0000500	1.0	2.0
1395_0000504	1.0	2.0
1395_0000512	2.0	2.0
1395_0000513	1.0	2.0
1395_0000514	2.0	2.0
1395_0000515	1.0	2.0
1395_0000516	1.0	1.0
1395_0000518	2.0	2.0
1395_0000525	2.0	1.0
1395_0000526	1.0	1.0
1395_0000527	1.0	1.0
1395_0000528	2.0	1.0
1395_0000529	2.0	2.0
1395_0000531	2.0	2.0
1395_0000533	2.0	2.0
1395_0000534	1.0	2.0
1395_0000535	1.0	2.0
1395_0000537	1.0	2.0
1395_0000547	1.0	2.0
1395_0000548	1.0	2.0
1395_0000549	1.0	2.0
1395_0000550	1.0	2.0
1395_0000551	2.0	2.0
1395_0000552	2.0	2.0
1395_0000553	1.0	2.0
1395_0000554	2.0	2.0
1395_0000555	2.0	1.0
1395_0000556	1.0	2.0
1395_0000557	2.0	2.0
1395_0000559	1.0	2.0
1395_0000560	1.0	2.0
1395_0000563	2.0	2.0
1395_0000564	1.0	2.0
1395_0000565	1.0	2.0
1395_0000572	1.0	2.0
1395_0000575	1.0	2.0
1395_0000579	1.0	2.0
1395_0000581	1.0	2.0
1395_0000582	1.0	1.0
1395_0000583	1.0	2.0
1395_0000584	0.0	1.0
1395_0000585	1.0	2.0
1395_0000587	0.0	1.0
1395_0000591	0.0	1.0
1395_0000593	1.0	2.0
1395_0000595	0.0	2.0
1395_0000596	2.0	2.0
1395_0000597	1.0	2.0
1395_0000598	1.0	2.0
1395_0000599	1.0	2.0
1395_0000602	1.0	2.0
1395_0000604	0.0	2.0
1395_0000606	1.0	2.0
1395_0000607	0.0	2.0
1395_0000608	1.0	2.0
1395_0000609	0.0	2.0
1395_0000610	2.0	2.0
1395_0000611	1.0	1.0
1395_0000612	1.0	2.0
1395_0000626	0.0	2.0
1395_0000627	1.0	1.0
1395_0000628	1.0	2.0
1395_0000630	1.0	2.0
1395_0000631	1.0	2.0
1395_0000635	1.0	1.0
1395_0000636	1.0	1.0
1395_0000639	0.0	2.0
1395_0000642	1.0	2.0
1395_0000644	1.0	2.0
1395_0000646	1.0	2.0
1395_0000649	1.0	2.0
1395_0001010	2.0	2.0
1395_0001013	1.0	2.0
1395_0001015	1.0	2.0
1395_0001016	1.0	2.0
1395_0001017	1.0	2.0
1395_0001019	1.0	2.0
1395_0001020	1.0	2.0
1395_0001021	1.0	2.0
1395_0001022	1.0	2.0
1395_0001023	0.0	2.0
1395_0001024	0.0	2.0
1395_0001028	1.0	2.0
1395_0001033	1.0	2.0
1395_0001034	1.0	2.0
1395_0001040	1.0	1.0
1395_0001045	2.0	2.0
1395_0001058	1.0	2.0
1395_0001060	1.0	2.0
1395_0001061	1.0	3.0
1395_0001064	1.0	2.0
1395_0001065	1.0	2.0
1395_0001066	1.0	2.0
1395_0001067	1.0	2.0
1395_0001068	0.0	2.0
1395_0001069	1.0	2.0
1395_0001070	1.0	2.0
1395_0001071	1.0	1.0
1395_0001073	1.0	2.0
1395_0001074	1.0	2.0
1395_0001075	1.0	2.0
1395_0001076	1.0	2.0
1395_0001078	1.0	2.0
1395_0001080	2.0	2.0
1395_0001084	1.0	2.0
1395_0001090	1.0	2.0
1395_0001093	1.0	2.0
1395_0001101	1.0	2.0
1395_0001103	1.0	2.0
1395_0001104	1.0	2.0
1395_0001108	1.0	1.0
1395_0001109	1.0	2.0
1395_0001114	1.0	2.0
1395_0001115	1.0	2.0
1395_0001116	2.0	2.0
1395_0001117	1.0	2.0
1395_0001118	1.0	2.0
1395_0001119	1.0	2.0
1395_0001120	1.0	2.0
1395_0001121	0.0	1.0
1395_0001122	1.0	1.0
1395_0001123	0.0	2.0
1395_0001124	0.0	2.0
1395_0001126	1.0	2.0
1395_0001131	1.0	1.0
1395_0001132	1.0	2.0
1395_0001133	1.0	2.0
1395_0001141	1.0	2.0
1395_0001145	1.0	2.0
1395_0001146	0.0	2.0
1395_0001147	0.0	2.0
1395_0001149	1.0	2.0
1395_0001150	1.0	2.0
1395_0001158	1.0	2.0
1395_0001160	1.0	2.0
1395_0001161	1.0	2.0
1395_0001164	2.0	2.0
1395_0001167	1.0	2.0
1395_0001169	1.0	2.0
1395_0001170	1.0	2.0
1395_0001171	1.0	2.0
Language = IT, Weighted F1-score = 0.31666770590183263, Dimension = Sociolinguisticappropriateness

MONOLINGUAL EXPERIMENTS
MONOLINGUAL Experiments with:  DE
130.68810916179336 82.83744971317162
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']
LANGUAGE: DE, 0th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.11
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.84
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        12
         1.0       0.62      0.95      0.75        61
         2.0       0.83      0.38      0.52        66
         3.0       0.69      0.97      0.80        59
         4.0       0.00      0.00      0.00         8

    accuracy                           0.68       206
   macro avg       0.43      0.46      0.42       206
weighted avg       0.65      0.68      0.62       206

[[ 0 12  0  0  0]
 [ 0 58  3  0  0]
 [ 0 23 25 18  0]
 [ 0  0  2 57  0]
 [ 0  0  0  8  0]]
0.619850723937624
206 206 206



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.79
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.67
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        12
         1.0       0.70      0.74      0.72        61
         2.0       0.71      0.76      0.74        66
         3.0       0.76      0.93      0.84        59
         4.0       0.00      0.00      0.00         8

    accuracy                           0.73       206
   macro avg       0.44      0.49      0.46       206
weighted avg       0.66      0.73      0.69       206

[[ 0 12  0  0  0]
 [ 0 45 16  0  0]
 [ 0  7 50  9  0]
 [ 0  0  4 55  0]
 [ 0  0  0  8  0]]
0.689278623774419
206 206 206



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.66
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.64
              precision    recall  f1-score   support

         0.0       0.41      0.58      0.48        12
         1.0       0.73      0.67      0.70        61
         2.0       0.77      0.71      0.74        66
         3.0       0.76      0.93      0.84        59
         4.0       0.00      0.00      0.00         8

    accuracy                           0.73       206
   macro avg       0.54      0.58      0.55       206
weighted avg       0.71      0.73      0.71       206

[[ 7  5  0  0  0]
 [10 41 10  0  0]
 [ 0 10 47  9  0]
 [ 0  0  4 55  0]
 [ 0  0  0  8  0]]
0.7132894108447788
206 206 206



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.54
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.65
              precision    recall  f1-score   support

         0.0       0.50      0.08      0.14        12
         1.0       0.73      0.77      0.75        61
         2.0       0.75      0.74      0.75        66
         3.0       0.75      0.95      0.84        59
         4.0       0.00      0.00      0.00         8

    accuracy                           0.74       206
   macro avg       0.55      0.51      0.50       206
weighted avg       0.70      0.74      0.71       206

[[ 1 11  0  0  0]
 [ 1 47 13  0  0]
 [ 0  6 49 11  0]
 [ 0  0  3 56  0]
 [ 0  0  0  8  0]]
0.7100668172409385
206 206 206
Filename	True Label	Prediction
1023_0001416	3.0	3.0
1023_0101753	3.0	3.0
1023_0101893	3.0	3.0
1023_0101909	4.0	3.0
1023_0103829	2.0	3.0
1023_0103830	3.0	3.0
1023_0103833	4.0	3.0
1023_0103880	3.0	3.0
1023_0107042	3.0	3.0
1023_0107244	3.0	3.0
1023_0107672	2.0	3.0
1023_0107727	3.0	3.0
1023_0108306	3.0	3.0
1023_0108641	4.0	3.0
1023_0108649	3.0	3.0
1023_0108752	3.0	3.0
1023_0108814	3.0	3.0
1023_0108815	3.0	3.0
1023_0108888	3.0	3.0
1023_0108889	3.0	3.0
1023_0109026	2.0	3.0
1023_0109030	3.0	3.0
1023_0109151	3.0	3.0
1023_0109392	3.0	3.0
1023_0109396	2.0	3.0
1023_0109401	3.0	3.0
1023_0109422	3.0	3.0
1023_0109516	3.0	3.0
1023_0109519	2.0	2.0
1023_0109614	2.0	2.0
1023_0109651	3.0	3.0
1023_0109671	3.0	3.0
1023_0109721	2.0	3.0
1023_0109917	3.0	3.0
1023_0109954	3.0	3.0
1031_0002004	3.0	3.0
1031_0002005	3.0	3.0
1031_0002061	3.0	3.0
1031_0002089	3.0	3.0
1031_0002196	3.0	3.0
1031_0002199	3.0	3.0
1031_0003042	3.0	3.0
1031_0003053	3.0	3.0
1031_0003072	3.0	3.0
1031_0003074	3.0	3.0
1031_0003077	3.0	3.0
1031_0003092	2.0	3.0
1031_0003106	3.0	3.0
1031_0003130	4.0	3.0
1031_0003133	4.0	3.0
1031_0003141	3.0	3.0
1031_0003144	3.0	3.0
1031_0003156	3.0	3.0
1031_0003173	3.0	3.0
1031_0003181	4.0	3.0
1031_0003190	3.0	3.0
1031_0003207	4.0	3.0
1031_0003220	3.0	3.0
1031_0003230	3.0	3.0
1031_0003231	3.0	3.0
1031_0003233	3.0	3.0
1031_0003234	3.0	3.0
1031_0003244	4.0	3.0
1031_0003249	3.0	3.0
1031_0003261	3.0	3.0
1031_0003274	3.0	3.0
1031_0003310	3.0	3.0
1031_0003327	2.0	3.0
1031_0003354	3.0	3.0
1031_0003355	3.0	3.0
1031_0003356	3.0	3.0
1031_0003386	2.0	3.0
1031_0003393	3.0	3.0
1031_0003414	3.0	3.0
1061_0120271	2.0	2.0
1061_0120277	1.0	2.0
1061_0120285	1.0	2.0
1061_0120301	2.0	2.0
1061_0120312	1.0	1.0
1061_0120325	2.0	2.0
1061_0120326	2.0	2.0
1061_0120333	2.0	3.0
1061_0120335	2.0	3.0
1061_0120351	2.0	2.0
1061_0120353	1.0	1.0
1061_0120359	1.0	2.0
1061_0120367	3.0	2.0
1061_0120369	2.0	2.0
1061_0120372	2.0	2.0
1061_0120388	1.0	2.0
1061_0120423	2.0	3.0
1061_0120428	2.0	2.0
1061_0120431	2.0	2.0
1061_0120433	1.0	1.0
1061_0120440	1.0	1.0
1061_0120443	0.0	1.0
1061_0120455	2.0	2.0
1061_0120458	3.0	2.0
1061_0120459	2.0	2.0
1061_0120482	2.0	2.0
1061_0120484	2.0	2.0
1061_0120486	2.0	2.0
1061_0120493	2.0	2.0
1061_0120495	2.0	2.0
1061_0120498	2.0	2.0
1061_0120853	2.0	2.0
1061_0120883	2.0	2.0
1061_0120887	1.0	1.0
1061_1029113	1.0	2.0
1061_1029120	1.0	2.0
1061_1202910	2.0	2.0
1061_1202912	2.0	2.0
1061_1202913	2.0	1.0
1071_0020001	1.0	1.0
1071_0024680	2.0	2.0
1071_0024688	1.0	1.0
1071_0024701	2.0	2.0
1071_0024761	1.0	1.0
1071_0024765	0.0	1.0
1071_0024766	1.0	1.0
1071_0024769	0.0	1.0
1071_0024777	1.0	1.0
1071_0024778	0.0	1.0
1071_0024784	1.0	1.0
1071_0024802	1.0	2.0
1071_0024822	0.0	1.0
1071_0024826	1.0	1.0
1071_0024827	1.0	1.0
1071_0024833	1.0	1.0
1071_0024835	1.0	1.0
1071_0024837	0.0	0.0
1071_0024844	1.0	1.0
1071_0024848	1.0	1.0
1071_0024854	0.0	1.0
1071_0024857	1.0	1.0
1071_0024863	1.0	1.0
1071_0024866	2.0	2.0
1071_0024874	1.0	1.0
1071_0024878	2.0	2.0
1071_0024879	1.0	1.0
1071_0242022	0.0	1.0
1071_0242091	1.0	0.0
1071_0243593	1.0	1.0
1071_0248307	2.0	1.0
1071_0248308	1.0	1.0
1071_0248321	1.0	1.0
1071_0248329	1.0	1.0
1071_0248332	2.0	2.0
1071_0248340	0.0	1.0
1071_0248343	1.0	1.0
1071_0248344	1.0	1.0
1071_0248345	1.0	2.0
1071_0248347	1.0	1.0
1071_0248349	1.0	1.0
1091_0000001	1.0	1.0
1091_0000007	2.0	2.0
1091_0000009	0.0	1.0
1091_0000014	0.0	1.0
1091_0000030	1.0	1.0
1091_0000036	1.0	1.0
1091_0000037	1.0	1.0
1091_0000038	1.0	1.0
1091_0000039	1.0	1.0
1091_0000041	1.0	1.0
1091_0000047	1.0	1.0
1091_0000048	1.0	1.0
1091_0000052	0.0	1.0
1091_0000054	1.0	1.0
1091_0000062	1.0	1.0
1091_0000067	2.0	1.0
1091_0000071	2.0	2.0
1091_0000072	2.0	2.0
1091_0000075	2.0	2.0
1091_0000076	3.0	2.0
1091_0000079	2.0	2.0
1091_0000102	1.0	2.0
1091_0000114	2.0	2.0
1091_0000123	2.0	1.0
1091_0000145	1.0	1.0
1091_0000151	1.0	1.0
1091_0000153	2.0	2.0
1091_0000154	2.0	2.0
1091_0000161	2.0	2.0
1091_0000165	1.0	1.0
1091_0000166	2.0	2.0
1091_0000172	2.0	1.0
1091_0000194	2.0	2.0
1091_0000196	2.0	1.0
1091_0000219	2.0	2.0
1091_0000221	1.0	1.0
1091_0000224	1.0	1.0
1091_0000227	1.0	2.0
1091_0000231	2.0	2.0
1091_0000236	1.0	2.0
1091_0000237	2.0	2.0
1091_0000240	1.0	1.0
1091_0000242	2.0	2.0
1091_0000247	2.0	2.0
1091_0000251	2.0	2.0
1091_0000254	1.0	1.0
1091_0000260	2.0	2.0
1091_0000261	2.0	2.0
1091_0000264	1.0	2.0
1091_0000268	2.0	2.0
1091_0000273	2.0	2.0
1091_0000275	1.0	2.0
LANGUAGE: DE, 1th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.02
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.75
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.68      0.70      0.69        61
         2.0       0.64      0.68      0.66        65
         3.0       0.73      0.90      0.80        59
         4.0       0.00      0.00      0.00         9

    accuracy                           0.68       205
   macro avg       0.41      0.46      0.43       205
weighted avg       0.61      0.68      0.65       205

[[ 0 10  1  0  0]
 [ 0 43 18  0  0]
 [ 0 10 44 11  0]
 [ 0  0  6 53  0]
 [ 0  0  0  9  0]]
0.6457161300286351
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.73
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.76
              precision    recall  f1-score   support

         0.0       1.00      0.18      0.31        11
         1.0       0.75      0.66      0.70        61
         2.0       0.67      0.68      0.67        65
         3.0       0.70      1.00      0.83        59
         4.0       0.00      0.00      0.00         9

    accuracy                           0.71       205
   macro avg       0.62      0.50      0.50       205
weighted avg       0.69      0.71      0.68       205

[[ 2  8  1  0  0]
 [ 0 40 21  0  0]
 [ 0  5 44 16  0]
 [ 0  0  0 59  0]
 [ 0  0  0  9  0]]
0.6758100963472893
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.58
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.81
              precision    recall  f1-score   support

         0.0       1.00      0.27      0.43        11
         1.0       0.77      0.59      0.67        61
         2.0       0.64      0.72      0.68        65
         3.0       0.72      1.00      0.84        59
         4.0       0.00      0.00      0.00         9

    accuracy                           0.71       205
   macro avg       0.63      0.52      0.52       205
weighted avg       0.69      0.71      0.68       205

[[ 3  7  1  0  0]
 [ 0 36 25  0  0]
 [ 0  4 47 14  0]
 [ 0  0  0 59  0]
 [ 0  0  0  9  0]]
0.6782058596322714
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.46
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.71
              precision    recall  f1-score   support

         0.0       0.80      0.36      0.50        11
         1.0       0.78      0.69      0.73        61
         2.0       0.68      0.74      0.71        65
         3.0       0.73      0.93      0.82        59
         4.0       0.00      0.00      0.00         9

    accuracy                           0.73       205
   macro avg       0.60      0.54      0.55       205
weighted avg       0.70      0.73      0.70       205

[[ 4  6  1  0  0]
 [ 1 42 18  0  0]
 [ 0  6 48 11  0]
 [ 0  0  4 55  0]
 [ 0  0  0  9  0]]
0.7042522463473289
205 205 205
Filename	True Label	Prediction
1023_0001418	3.0	3.0
1023_0001422	2.0	3.0
1023_0101683	3.0	2.0
1023_0101684	2.0	2.0
1023_0101690	2.0	3.0
1023_0101691	3.0	3.0
1023_0101693	3.0	3.0
1023_0101695	2.0	2.0
1023_0101700	3.0	2.0
1023_0101843	3.0	3.0
1023_0101847	3.0	3.0
1023_0101851	3.0	3.0
1023_0101852	3.0	3.0
1023_0101853	2.0	3.0
1023_0101854	2.0	2.0
1023_0101897	2.0	3.0
1023_0101901	3.0	3.0
1023_0101907	3.0	3.0
1023_0103823	3.0	3.0
1023_0103831	3.0	3.0
1023_0103838	3.0	3.0
1023_0103883	3.0	3.0
1023_0104203	3.0	2.0
1023_0104206	3.0	2.0
1023_0107729	3.0	3.0
1023_0107740	3.0	3.0
1023_0107780	3.0	3.0
1023_0108305	3.0	3.0
1023_0108510	3.0	3.0
1023_0108520	3.0	3.0
1023_0108751	3.0	3.0
1023_0108753	2.0	2.0
1023_0108890	3.0	3.0
1023_0108935	2.0	2.0
1023_0108955	3.0	3.0
1023_0108992	3.0	3.0
1023_0109038	3.0	3.0
1023_0109039	3.0	3.0
1023_0109518	2.0	2.0
1023_0109606	3.0	3.0
1023_0109946	2.0	3.0
1023_0111896	2.0	3.0
1031_0002006	4.0	3.0
1031_0002079	4.0	3.0
1031_0002087	3.0	3.0
1031_0002187	3.0	3.0
1031_0002198	3.0	3.0
1031_0003023	3.0	3.0
1031_0003048	4.0	3.0
1031_0003063	4.0	3.0
1031_0003071	3.0	3.0
1031_0003078	3.0	3.0
1031_0003088	4.0	3.0
1031_0003091	2.0	3.0
1031_0003132	3.0	3.0
1031_0003140	3.0	3.0
1031_0003165	2.0	3.0
1031_0003179	3.0	3.0
1031_0003183	4.0	3.0
1031_0003186	3.0	3.0
1031_0003187	3.0	3.0
1031_0003191	3.0	3.0
1031_0003214	3.0	3.0
1031_0003219	3.0	3.0
1031_0003225	3.0	3.0
1031_0003226	3.0	3.0
1031_0003239	4.0	3.0
1031_0003260	3.0	3.0
1031_0003273	3.0	3.0
1031_0003330	3.0	3.0
1031_0003336	3.0	3.0
1031_0003339	3.0	3.0
1031_0003352	2.0	3.0
1031_0003353	2.0	3.0
1031_0003357	3.0	3.0
1031_0003365	3.0	3.0
1031_0003369	3.0	3.0
1031_0003387	3.0	3.0
1031_0003388	3.0	3.0
1031_0003389	3.0	3.0
1031_0003391	2.0	3.0
1031_0003407	3.0	3.0
1031_0003409	4.0	3.0
1031_0003415	4.0	3.0
1061_0120272	1.0	1.0
1061_0120273	1.0	2.0
1061_0120279	1.0	1.0
1061_0120283	1.0	1.0
1061_0120286	1.0	1.0
1061_0120290	1.0	2.0
1061_0120291	1.0	1.0
1061_0120295	0.0	2.0
1061_0120300	2.0	2.0
1061_0120304	2.0	2.0
1061_0120308	2.0	2.0
1061_0120313	2.0	1.0
1061_0120316	2.0	2.0
1061_0120327	2.0	2.0
1061_0120330	2.0	2.0
1061_0120346	2.0	2.0
1061_0120348	1.0	1.0
1061_0120354	1.0	1.0
1061_0120355	1.0	1.0
1061_0120357	3.0	3.0
1061_0120373	2.0	2.0
1061_0120375	2.0	1.0
1061_0120376	2.0	2.0
1061_0120382	1.0	2.0
1061_0120384	1.0	1.0
1061_0120386	1.0	2.0
1061_0120390	2.0	2.0
1061_0120404	1.0	1.0
1061_0120410	2.0	2.0
1061_0120430	2.0	2.0
1061_0120432	2.0	2.0
1061_0120449	2.0	2.0
1061_0120450	2.0	2.0
1061_0120457	2.0	2.0
1061_0120487	2.0	2.0
1061_0120488	2.0	2.0
1061_0120491	2.0	2.0
1061_0120496	2.0	2.0
1061_0120499	2.0	2.0
1061_0120500	2.0	2.0
1061_0120857	2.0	2.0
1061_0120876	2.0	2.0
1061_0120877	2.0	2.0
1061_0120888	1.0	2.0
1061_0120889	1.0	1.0
1061_1029118	1.0	2.0
1061_1202916	2.0	2.0
1061_1202918	1.0	2.0
1071_0024687	1.0	1.0
1071_0024690	1.0	2.0
1071_0024691	1.0	2.0
1071_0024699	1.0	1.0
1071_0024702	1.0	2.0
1071_0024704	1.0	1.0
1071_0024705	1.0	2.0
1071_0024714	2.0	2.0
1071_0024759	0.0	1.0
1071_0024768	1.0	1.0
1071_0024774	0.0	0.0
1071_0024797	0.0	1.0
1071_0024808	1.0	1.0
1071_0024814	1.0	1.0
1071_0024816	1.0	1.0
1071_0024818	2.0	1.0
1071_0024834	2.0	2.0
1071_0024845	0.0	1.0
1071_0024855	1.0	1.0
1071_0024859	1.0	2.0
1071_0024860	1.0	1.0
1071_0024861	0.0	1.0
1071_0024862	1.0	1.0
1071_0024865	2.0	2.0
1071_0024873	1.0	1.0
1071_0024875	1.0	1.0
1071_0241831	1.0	2.0
1071_0242013	1.0	1.0
1071_0242092	0.0	0.0
1071_0243501	1.0	1.0
1071_0248302	1.0	0.0
1071_0248311	1.0	1.0
1071_0248312	1.0	1.0
1071_0248313	1.0	1.0
1071_0248319	0.0	0.0
1071_0248320	0.0	0.0
1071_0248323	1.0	1.0
1071_0248327	0.0	1.0
1071_0248334	2.0	2.0
1091_0000002	2.0	2.0
1091_0000011	2.0	1.0
1091_0000015	1.0	2.0
1091_0000016	0.0	1.0
1091_0000023	1.0	1.0
1091_0000025	1.0	1.0
1091_0000029	1.0	1.0
1091_0000033	1.0	1.0
1091_0000034	1.0	1.0
1091_0000044	1.0	2.0
1091_0000045	2.0	2.0
1091_0000049	1.0	1.0
1091_0000086	1.0	1.0
1091_0000095	2.0	2.0
1091_0000125	2.0	2.0
1091_0000126	2.0	2.0
1091_0000144	1.0	1.0
1091_0000174	1.0	1.0
1091_0000197	2.0	2.0
1091_0000198	1.0	2.0
1091_0000204	1.0	1.0
1091_0000206	1.0	1.0
1091_0000207	2.0	2.0
1091_0000208	2.0	1.0
1091_0000212	2.0	2.0
1091_0000213	1.0	2.0
1091_0000215	2.0	2.0
1091_0000216	2.0	1.0
1091_0000218	1.0	2.0
1091_0000225	1.0	1.0
1091_0000244	2.0	2.0
1091_0000248	1.0	1.0
1091_0000257	2.0	2.0
1091_0000266	2.0	2.0
LANGUAGE: DE, 2th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.06
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.89
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.67      0.79      0.72        61
         2.0       0.62      0.55      0.59        65
         3.0       0.65      0.83      0.73        59
         4.0       0.00      0.00      0.00         9

    accuracy                           0.65       205
   macro avg       0.39      0.43      0.41       205
weighted avg       0.58      0.65      0.61       205

[[ 0 11  0  0  0]
 [ 0 48 13  0  0]
 [ 0 12 36 17  0]
 [ 0  1  9 49  0]
 [ 0  0  0  9  0]]
0.6108688263761859
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.81
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.93
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.59      1.00      0.74        61
         2.0       0.64      0.43      0.51        65
         3.0       0.71      0.69      0.70        59
         4.0       0.00      0.00      0.00         9

    accuracy                           0.63       205
   macro avg       0.39      0.43      0.39       205
weighted avg       0.58      0.63      0.59       205

[[ 0 11  0  0  0]
 [ 0 61  0  0  0]
 [ 0 29 28  8  0]
 [ 0  2 16 41  0]
 [ 0  0  0  9  0]]
0.5859657148476058
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.72
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.83
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.73      0.67      0.70        61
         2.0       0.61      0.68      0.64        65
         3.0       0.66      0.86      0.75        59
         4.0       0.00      0.00      0.00         9

    accuracy                           0.66       205
   macro avg       0.40      0.44      0.42       205
weighted avg       0.60      0.66      0.63       205

[[ 0 11  0  0  0]
 [ 0 41 20  0  0]
 [ 0  4 44 17  0]
 [ 0  0  8 51  0]
 [ 0  0  0  9  0]]
0.6280681052178292
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.61
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       1.00      0.09      0.17        11
         1.0       0.76      0.72      0.74        61
         2.0       0.63      0.65      0.64        65
         3.0       0.65      0.86      0.74        59
         4.0       0.00      0.00      0.00         9

    accuracy                           0.67       205
   macro avg       0.61      0.46      0.46       205
weighted avg       0.66      0.67      0.64       205

[[ 1 10  0  0  0]
 [ 0 44 17  0  0]
 [ 0  4 42 19  0]
 [ 0  0  8 51  0]
 [ 0  0  0  9  0]]
0.6434873612030371
205 205 205
Filename	True Label	Prediction
1023_0001419	3.0	3.0
1023_0001420	3.0	3.0
1023_0101688	3.0	3.0
1023_0101694	3.0	3.0
1023_0101752	3.0	3.0
1023_0101841	2.0	3.0
1023_0101848	2.0	2.0
1023_0101855	2.0	3.0
1023_0101894	2.0	3.0
1023_0101895	3.0	3.0
1023_0101899	2.0	3.0
1023_0101906	2.0	3.0
1023_0103821	3.0	3.0
1023_0103822	2.0	2.0
1023_0103824	3.0	3.0
1023_0103827	3.0	3.0
1023_0103832	2.0	2.0
1023_0103841	3.0	3.0
1023_0104207	2.0	3.0
1023_0107074	3.0	3.0
1023_0107682	2.0	2.0
1023_0107725	2.0	3.0
1023_0107781	3.0	3.0
1023_0107783	3.0	2.0
1023_0107784	2.0	2.0
1023_0108648	3.0	3.0
1023_0108811	3.0	3.0
1023_0108813	3.0	3.0
1023_0108887	2.0	3.0
1023_0108932	2.0	3.0
1023_0108933	3.0	3.0
1023_0108958	2.0	3.0
1023_0109096	3.0	3.0
1023_0109249	3.0	3.0
1023_0109250	2.0	3.0
1023_0109267	2.0	3.0
1023_0109399	2.0	3.0
1023_0109400	3.0	3.0
1023_0109402	2.0	3.0
1023_0109495	3.0	3.0
1023_0109500	2.0	3.0
1023_0109505	3.0	3.0
1023_0109522	3.0	3.0
1023_0109524	3.0	3.0
1023_0109590	2.0	3.0
1023_0109716	3.0	3.0
1023_0109717	3.0	3.0
1023_0109947	2.0	3.0
1031_0001703	4.0	3.0
1031_0001949	3.0	3.0
1031_0001997	3.0	3.0
1031_0002003	3.0	3.0
1031_0002043	4.0	3.0
1031_0002091	3.0	3.0
1031_0002131	3.0	3.0
1031_0002197	3.0	3.0
1031_0003076	4.0	3.0
1031_0003095	3.0	3.0
1031_0003097	3.0	3.0
1031_0003098	4.0	3.0
1031_0003127	4.0	3.0
1031_0003129	3.0	3.0
1031_0003131	3.0	3.0
1031_0003135	3.0	3.0
1031_0003136	4.0	3.0
1031_0003146	4.0	3.0
1031_0003149	3.0	3.0
1031_0003155	3.0	3.0
1031_0003157	4.0	3.0
1031_0003160	3.0	3.0
1031_0003166	2.0	3.0
1031_0003169	3.0	3.0
1031_0003180	4.0	3.0
1031_0003185	3.0	3.0
1031_0003189	3.0	3.0
1031_0003211	2.0	3.0
1031_0003218	3.0	3.0
1031_0003236	3.0	3.0
1031_0003237	3.0	3.0
1031_0003242	3.0	3.0
1031_0003246	3.0	3.0
1031_0003272	3.0	3.0
1031_0003315	3.0	3.0
1031_0003368	3.0	3.0
1061_0012029	3.0	2.0
1061_0120276	2.0	2.0
1061_0120278	2.0	2.0
1061_0120296	2.0	2.0
1061_0120303	1.0	2.0
1061_0120306	3.0	2.0
1061_0120319	2.0	2.0
1061_0120329	2.0	2.0
1061_0120332	1.0	2.0
1061_0120337	2.0	2.0
1061_0120347	1.0	2.0
1061_0120368	2.0	2.0
1061_0120387	1.0	2.0
1061_0120389	2.0	2.0
1061_0120403	3.0	2.0
1061_0120408	2.0	2.0
1061_0120411	3.0	2.0
1061_0120413	1.0	1.0
1061_0120425	2.0	2.0
1061_0120438	2.0	2.0
1061_0120439	1.0	1.0
1061_0120442	2.0	2.0
1061_0120448	3.0	2.0
1061_0120453	2.0	2.0
1061_0120478	2.0	2.0
1061_0120481	3.0	2.0
1061_0120485	2.0	2.0
1061_0120855	1.0	2.0
1061_0120858	2.0	2.0
1061_0120859	2.0	2.0
1061_0120882	3.0	2.0
1061_0120885	2.0	2.0
1061_0120890	1.0	2.0
1061_0120894	2.0	2.0
1061_1029112	3.0	3.0
1061_1029114	1.0	1.0
1061_1029119	1.0	2.0
1061_1202917	1.0	2.0
1071_0024678	1.0	1.0
1071_0024683	0.0	1.0
1071_0024685	2.0	2.0
1071_0024693	1.0	1.0
1071_0024694	1.0	2.0
1071_0024708	1.0	1.0
1071_0024715	1.0	2.0
1071_0024716	1.0	1.0
1071_0024756	1.0	1.0
1071_0024758	2.0	2.0
1071_0024770	1.0	1.0
1071_0024775	0.0	1.0
1071_0024779	1.0	1.0
1071_0024782	0.0	1.0
1071_0024799	2.0	2.0
1071_0024803	1.0	1.0
1071_0024809	1.0	1.0
1071_0024810	1.0	1.0
1071_0024813	0.0	1.0
1071_0024817	1.0	1.0
1071_0024824	1.0	1.0
1071_0024838	0.0	0.0
1071_0024840	1.0	1.0
1071_0024864	0.0	1.0
1071_0242011	1.0	1.0
1071_0242012	1.0	1.0
1071_0242021	1.0	1.0
1071_0242042	1.0	1.0
1071_0242093	0.0	1.0
1071_0243581	1.0	1.0
1071_0243582	1.0	1.0
1071_0243621	2.0	1.0
1071_0243623	1.0	1.0
1071_0248301	1.0	1.0
1071_0248309	2.0	1.0
1071_0248324	0.0	1.0
1071_0248326	1.0	1.0
1071_0248328	0.0	1.0
1071_0248330	2.0	1.0
1071_0248331	1.0	1.0
1071_0248338	1.0	1.0
1071_0248341	1.0	1.0
1071_0248350	1.0	1.0
1091_0000006	0.0	1.0
1091_0000018	2.0	2.0
1091_0000031	1.0	1.0
1091_0000050	1.0	1.0
1091_0000051	0.0	1.0
1091_0000053	1.0	1.0
1091_0000063	1.0	1.0
1091_0000069	1.0	1.0
1091_0000070	1.0	1.0
1091_0000078	1.0	1.0
1091_0000087	1.0	2.0
1091_0000092	2.0	2.0
1091_0000127	1.0	1.0
1091_0000155	2.0	2.0
1091_0000157	2.0	2.0
1091_0000159	2.0	2.0
1091_0000164	1.0	1.0
1091_0000168	2.0	2.0
1091_0000173	2.0	2.0
1091_0000185	1.0	1.0
1091_0000192	1.0	2.0
1091_0000193	1.0	1.0
1091_0000200	1.0	2.0
1091_0000201	1.0	2.0
1091_0000203	1.0	2.0
1091_0000211	2.0	2.0
1091_0000217	1.0	1.0
1091_0000220	2.0	2.0
1091_0000233	1.0	2.0
1091_0000239	2.0	2.0
1091_0000245	2.0	2.0
1091_0000246	2.0	2.0
1091_0000255	1.0	2.0
1091_0000256	2.0	1.0
1091_0000259	2.0	2.0
1091_0000262	1.0	1.0
1091_0000263	2.0	2.0
1091_0000269	2.0	2.0
1091_0000271	1.0	1.0
1091_0000272	1.0	1.0
LANGUAGE: DE, 3th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.09
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.91
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.57      0.98      0.72        62
         2.0       0.65      0.39      0.49        66
         3.0       0.74      0.74      0.74        58
         4.0       0.00      0.00      0.00         8

    accuracy                           0.63       205
   macro avg       0.39      0.42      0.39       205
weighted avg       0.59      0.63      0.59       205

[[ 0 11  0  0  0]
 [ 0 61  1  0  0]
 [ 0 33 26  7  0]
 [ 0  2 13 43  0]
 [ 0  0  0  8  0]]
0.5860231948305862
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.74
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.75
              precision    recall  f1-score   support

         0.0       0.50      0.27      0.35        11
         1.0       0.74      0.74      0.74        62
         2.0       0.66      0.77      0.71        66
         3.0       0.75      0.78      0.76        58
         4.0       0.00      0.00      0.00         8

    accuracy                           0.71       205
   macro avg       0.53      0.51      0.51       205
weighted avg       0.68      0.71      0.69       205

[[ 3  8  0  0  0]
 [ 3 46 13  0  0]
 [ 0  8 51  7  0]
 [ 0  0 13 45  0]
 [ 0  0  0  8  0]]
0.6887637275789892
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.59
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.81
              precision    recall  f1-score   support

         0.0       0.60      0.55      0.57        11
         1.0       0.81      0.48      0.61        62
         2.0       0.59      0.73      0.65        66
         3.0       0.69      0.91      0.79        58
         4.0       0.00      0.00      0.00         8

    accuracy                           0.67       205
   macro avg       0.54      0.53      0.52       205
weighted avg       0.66      0.67      0.65       205

[[ 6  5  0  0  0]
 [ 4 30 28  0  0]
 [ 0  2 48 16  0]
 [ 0  0  5 53  0]
 [ 0  0  0  8  0]]
0.6463622117977518
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.49
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.71
              precision    recall  f1-score   support

         0.0       0.80      0.36      0.50        11
         1.0       0.80      0.76      0.78        62
         2.0       0.71      0.74      0.73        66
         3.0       0.72      0.90      0.80        58
         4.0       0.00      0.00      0.00         8

    accuracy                           0.74       205
   macro avg       0.61      0.55      0.56       205
weighted avg       0.72      0.74      0.72       205

[[ 4  7  0  0  0]
 [ 1 47 14  0  0]
 [ 0  5 49 12  0]
 [ 0  0  6 52  0]
 [ 0  0  0  8  0]]
0.7218360993527291
205 205 205
Filename	True Label	Prediction
1023_0001575	3.0	3.0
1023_0101675	3.0	3.0
1023_0101845	3.0	3.0
1023_0101856	2.0	2.0
1023_0101896	2.0	2.0
1023_0101900	3.0	3.0
1023_0101904	2.0	2.0
1023_0102118	3.0	3.0
1023_0103825	3.0	3.0
1023_0103826	3.0	3.0
1023_0103828	1.0	2.0
1023_0103836	3.0	3.0
1023_0103839	3.0	3.0
1023_0103843	2.0	3.0
1023_0103844	4.0	3.0
1023_0104209	3.0	3.0
1023_0107075	2.0	3.0
1023_0107726	3.0	3.0
1023_0107773	2.0	3.0
1023_0107787	2.0	2.0
1023_0108304	3.0	3.0
1023_0108423	3.0	2.0
1023_0108518	3.0	3.0
1023_0108766	2.0	3.0
1023_0108810	3.0	3.0
1023_0108885	2.0	2.0
1023_0108908	3.0	3.0
1023_0108931	3.0	3.0
1023_0108934	3.0	3.0
1023_0109022	3.0	3.0
1023_0109029	1.0	2.0
1023_0109033	4.0	3.0
1023_0109192	3.0	3.0
1023_0109248	2.0	3.0
1023_0109391	2.0	3.0
1023_0109496	3.0	3.0
1023_0109515	3.0	3.0
1023_0109528	3.0	3.0
1023_0109588	3.0	3.0
1023_0109591	3.0	3.0
1023_0109878	2.0	3.0
1023_0109890	3.0	3.0
1023_0109914	2.0	3.0
1023_0109945	3.0	3.0
1031_0001950	3.0	3.0
1031_0001998	4.0	3.0
1031_0002002	2.0	3.0
1031_0002010	3.0	3.0
1031_0002011	4.0	3.0
1031_0002032	3.0	3.0
1031_0002042	3.0	3.0
1031_0002083	3.0	3.0
1031_0002084	3.0	3.0
1031_0002085	3.0	3.0
1031_0002086	3.0	3.0
1031_0002088	3.0	3.0
1031_0002092	4.0	3.0
1031_0003013	4.0	3.0
1031_0003043	4.0	3.0
1031_0003052	3.0	3.0
1031_0003054	3.0	3.0
1031_0003161	3.0	3.0
1031_0003162	3.0	3.0
1031_0003164	3.0	3.0
1031_0003167	3.0	3.0
1031_0003170	3.0	3.0
1031_0003172	3.0	3.0
1031_0003174	3.0	3.0
1031_0003182	4.0	3.0
1031_0003206	3.0	3.0
1031_0003212	2.0	3.0
1031_0003216	3.0	3.0
1031_0003240	2.0	3.0
1031_0003245	3.0	3.0
1031_0003262	3.0	3.0
1031_0003314	3.0	3.0
1031_0003338	3.0	3.0
1031_0003359	2.0	3.0
1031_0003383	3.0	3.0
1061_0120274	1.0	1.0
1061_0120275	2.0	2.0
1061_0120280	1.0	1.0
1061_0120281	1.0	1.0
1061_0120282	0.0	1.0
1061_0120284	0.0	1.0
1061_0120287	1.0	2.0
1061_0120288	2.0	2.0
1061_0120289	1.0	1.0
1061_0120297	2.0	2.0
1061_0120298	2.0	2.0
1061_0120309	1.0	1.0
1061_0120310	2.0	2.0
1061_0120311	3.0	2.0
1061_0120314	2.0	2.0
1061_0120315	2.0	1.0
1061_0120317	2.0	2.0
1061_0120318	2.0	2.0
1061_0120320	3.0	2.0
1061_0120323	1.0	1.0
1061_0120331	1.0	1.0
1061_0120341	1.0	1.0
1061_0120345	2.0	2.0
1061_0120350	2.0	2.0
1061_0120352	1.0	1.0
1061_0120356	2.0	2.0
1061_0120358	1.0	1.0
1061_0120361	2.0	2.0
1061_0120370	2.0	2.0
1061_0120391	1.0	1.0
1061_0120394	2.0	2.0
1061_0120405	2.0	2.0
1061_0120406	2.0	2.0
1061_0120414	2.0	2.0
1061_0120421	2.0	2.0
1061_0120424	2.0	2.0
1061_0120427	2.0	2.0
1061_0120456	2.0	2.0
1061_0120483	1.0	2.0
1061_0120489	2.0	2.0
1061_0120490	2.0	2.0
1061_0120494	2.0	2.0
1061_0120497	3.0	2.0
1061_0120856	1.0	2.0
1061_0120878	1.0	1.0
1061_0120880	3.0	3.0
1061_0120881	2.0	2.0
1061_0120886	2.0	2.0
1061_1029115	2.0	1.0
1061_1029116	1.0	2.0
1061_1202911	1.0	2.0
1061_1202914	1.0	2.0
1061_1202915	1.0	1.0
1061_1202919	2.0	2.0
1071_0024689	1.0	1.0
1071_0024712	1.0	1.0
1071_0024757	2.0	2.0
1071_0024762	1.0	1.0
1071_0024763	1.0	1.0
1071_0024773	1.0	1.0
1071_0024776	0.0	0.0
1071_0024783	0.0	0.0
1071_0024798	0.0	1.0
1071_0024804	1.0	1.0
1071_0024807	1.0	1.0
1071_0024812	1.0	1.0
1071_0024815	0.0	1.0
1071_0024831	0.0	1.0
1071_0024843	1.0	1.0
1071_0024846	1.0	1.0
1071_0024847	1.0	1.0
1071_0024851	1.0	1.0
1071_0024853	1.0	1.0
1071_0024867	2.0	1.0
1071_0024871	1.0	1.0
1071_0024876	1.0	1.0
1071_0024877	1.0	1.0
1071_0024881	2.0	1.0
1071_0241832	1.0	1.0
1071_0241833	1.0	1.0
1071_0242041	1.0	1.0
1071_0242072	0.0	0.0
1071_0243592	1.0	1.0
1071_0248303	1.0	1.0
1071_0248305	0.0	1.0
1071_0248310	1.0	1.0
1071_0248314	1.0	1.0
1071_0248315	0.0	0.0
1071_0248333	2.0	1.0
1071_0248346	1.0	1.0
1091_0000004	1.0	1.0
1091_0000005	2.0	2.0
1091_0000010	2.0	2.0
1091_0000021	2.0	2.0
1091_0000022	3.0	2.0
1091_0000061	1.0	0.0
1091_0000064	1.0	1.0
1091_0000068	2.0	2.0
1091_0000073	1.0	1.0
1091_0000074	2.0	2.0
1091_0000101	1.0	1.0
1091_0000116	2.0	2.0
1091_0000148	1.0	1.0
1091_0000156	2.0	2.0
1091_0000158	2.0	2.0
1091_0000163	1.0	2.0
1091_0000167	2.0	2.0
1091_0000169	1.0	2.0
1091_0000171	2.0	2.0
1091_0000190	1.0	2.0
1091_0000195	1.0	1.0
1091_0000202	2.0	2.0
1091_0000210	1.0	1.0
1091_0000222	1.0	1.0
1091_0000223	2.0	2.0
1091_0000229	2.0	2.0
1091_0000230	2.0	2.0
1091_0000235	1.0	1.0
1091_0000238	1.0	2.0
1091_0000249	2.0	2.0
1091_0000250	1.0	2.0
1091_0000252	2.0	2.0
1091_0000253	0.0	1.0
1091_0000267	1.0	2.0
1091_0000270	1.0	1.0
1091_0000276	3.0	2.0
LANGUAGE: DE, 4th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.05
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.84
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        12
         1.0       0.73      0.70      0.72        61
         2.0       0.67      0.58      0.62        66
         3.0       0.63      0.97      0.76        58
         4.0       0.00      0.00      0.00         8

    accuracy                           0.67       205
   macro avg       0.40      0.45      0.42       205
weighted avg       0.61      0.67      0.63       205

[[ 0 12  0  0  0]
 [ 0 43 17  1  0]
 [ 0  4 38 24  0]
 [ 0  0  2 56  0]
 [ 0  0  0  8  0]]
0.6277445398147362
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.78
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.74
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        12
         1.0       0.63      0.93      0.75        61
         2.0       0.67      0.53      0.59        66
         3.0       0.71      0.78      0.74        58
         4.0       0.00      0.00      0.00         8

    accuracy                           0.67       205
   macro avg       0.40      0.45      0.42       205
weighted avg       0.61      0.67      0.63       205

[[ 0 12  0  0  0]
 [ 0 57  4  0  0]
 [ 0 21 35 10  0]
 [ 0  0 13 45  0]
 [ 0  0  0  8  0]]
0.6260781384053673
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.65
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.71
              precision    recall  f1-score   support

         0.0       1.00      0.08      0.15        12
         1.0       0.73      0.74      0.73        61
         2.0       0.70      0.65      0.68        66
         3.0       0.69      0.97      0.81        58
         4.0       0.00      0.00      0.00         8

    accuracy                           0.71       205
   macro avg       0.62      0.49      0.47       205
weighted avg       0.70      0.71      0.67       205

[[ 1 11  0  0  0]
 [ 0 45 16  0  0]
 [ 0  6 43 17  0]
 [ 0  0  2 56  0]
 [ 0  0  0  8  0]]
0.6727172025516308
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.52
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.71
              precision    recall  f1-score   support

         0.0       1.00      0.42      0.59        12
         1.0       0.78      0.74      0.76        61
         2.0       0.70      0.65      0.68        66
         3.0       0.69      0.97      0.81        58
         4.0       0.00      0.00      0.00         8

    accuracy                           0.73       205
   macro avg       0.63      0.55      0.57       205
weighted avg       0.71      0.73      0.71       205

[[ 5  7  0  0  0]
 [ 0 45 16  0  0]
 [ 0  6 43 17  0]
 [ 0  0  2 56  0]
 [ 0  0  0  8  0]]
0.7054634324214216
205 205 205
Filename	True Label	Prediction
1023_0001423	2.0	2.0
1023_0101689	2.0	2.0
1023_0101701	2.0	3.0
1023_0101749	3.0	3.0
1023_0101751	3.0	3.0
1023_0101844	2.0	3.0
1023_0101846	4.0	3.0
1023_0101849	3.0	3.0
1023_0101898	3.0	3.0
1023_0102117	3.0	3.0
1023_0103834	3.0	3.0
1023_0103837	3.0	3.0
1023_0103840	3.0	3.0
1023_0103955	3.0	3.0
1023_0106816	3.0	3.0
1023_0107788	3.0	3.0
1023_0108307	3.0	3.0
1023_0108422	3.0	3.0
1023_0108426	2.0	3.0
1023_0108650	3.0	3.0
1023_0108812	2.0	3.0
1023_0108886	3.0	3.0
1023_0108993	3.0	3.0
1023_0109027	2.0	2.0
1023_0109247	3.0	3.0
1023_0109395	2.0	3.0
1023_0109520	2.0	3.0
1023_0109527	3.0	3.0
1023_0109609	2.0	2.0
1023_0109649	3.0	3.0
1023_0109674	3.0	3.0
1023_0109880	3.0	3.0
1023_0109891	3.0	3.0
1023_0109915	2.0	2.0
1023_0109951	2.0	3.0
1031_0001951	2.0	3.0
1031_0002036	4.0	3.0
1031_0002040	4.0	3.0
1031_0002184	3.0	3.0
1031_0002185	3.0	3.0
1031_0002195	3.0	3.0
1031_0002200	2.0	3.0
1031_0003012	3.0	3.0
1031_0003029	3.0	3.0
1031_0003035	3.0	3.0
1031_0003065	3.0	3.0
1031_0003073	4.0	3.0
1031_0003085	3.0	3.0
1031_0003090	3.0	3.0
1031_0003099	3.0	3.0
1031_0003121	3.0	3.0
1031_0003126	3.0	3.0
1031_0003128	4.0	3.0
1031_0003145	3.0	3.0
1031_0003150	3.0	3.0
1031_0003154	3.0	3.0
1031_0003163	3.0	3.0
1031_0003184	4.0	3.0
1031_0003203	2.0	3.0
1031_0003205	3.0	3.0
1031_0003217	4.0	3.0
1031_0003221	2.0	3.0
1031_0003224	3.0	3.0
1031_0003232	2.0	3.0
1031_0003235	3.0	3.0
1031_0003238	3.0	3.0
1031_0003243	3.0	3.0
1031_0003309	3.0	3.0
1031_0003313	3.0	3.0
1031_0003331	3.0	3.0
1031_0003337	3.0	3.0
1031_0003358	4.0	3.0
1031_0003366	3.0	3.0
1031_0003367	3.0	3.0
1031_0003384	2.0	3.0
1031_0003390	3.0	3.0
1031_0003392	3.0	3.0
1031_0003408	2.0	3.0
1031_0003410	3.0	3.0
1031_0003419	3.0	3.0
1061_0120299	2.0	2.0
1061_0120302	1.0	2.0
1061_0120307	2.0	2.0
1061_0120321	2.0	2.0
1061_0120324	2.0	2.0
1061_0120328	1.0	2.0
1061_0120334	2.0	2.0
1061_0120336	1.0	2.0
1061_0120338	1.0	2.0
1061_0120343	2.0	2.0
1061_0120349	1.0	1.0
1061_0120360	3.0	3.0
1061_0120366	3.0	2.0
1061_0120371	3.0	3.0
1061_0120374	2.0	2.0
1061_0120383	2.0	3.0
1061_0120407	3.0	2.0
1061_0120409	2.0	2.0
1061_0120415	2.0	1.0
1061_0120426	2.0	3.0
1061_0120429	2.0	2.0
1061_0120441	2.0	2.0
1061_0120460	2.0	2.0
1061_0120479	2.0	2.0
1061_0120480	2.0	2.0
1061_0120492	2.0	3.0
1061_0120874	2.0	2.0
1061_0120875	3.0	3.0
1061_0120884	2.0	2.0
1061_1029111	2.0	2.0
1061_1029117	1.0	2.0
1071_0024681	2.0	2.0
1071_0024682	2.0	2.0
1071_0024686	2.0	2.0
1071_0024692	2.0	2.0
1071_0024703	1.0	1.0
1071_0024706	1.0	2.0
1071_0024709	2.0	2.0
1071_0024710	1.0	1.0
1071_0024711	1.0	1.0
1071_0024713	1.0	1.0
1071_0024767	2.0	2.0
1071_0024772	0.0	0.0
1071_0024781	1.0	1.0
1071_0024800	1.0	1.0
1071_0024801	1.0	1.0
1071_0024806	1.0	1.0
1071_0024811	1.0	1.0
1071_0024819	1.0	2.0
1071_0024820	0.0	1.0
1071_0024821	1.0	1.0
1071_0024823	1.0	1.0
1071_0024825	1.0	1.0
1071_0024836	1.0	2.0
1071_0024841	0.0	1.0
1071_0024849	0.0	0.0
1071_0024850	1.0	1.0
1071_0024852	0.0	0.0
1071_0024856	1.0	1.0
1071_0024872	1.0	1.0
1071_0242023	1.0	1.0
1071_0242043	0.0	1.0
1071_0242071	0.0	1.0
1071_0242073	1.0	1.0
1071_0243502	1.0	1.0
1071_0243591	1.0	1.0
1071_0243622	1.0	1.0
1071_0248304	1.0	1.0
1071_0248316	1.0	1.0
1071_0248317	0.0	0.0
1071_0248318	0.0	0.0
1071_0248322	1.0	1.0
1071_0248325	0.0	1.0
1071_0248335	1.0	1.0
1071_0248336	1.0	1.0
1071_0248337	1.0	2.0
1071_0248339	1.0	1.0
1071_0248342	1.0	1.0
1071_0248348	1.0	1.0
1091_0000003	2.0	1.0
1091_0000008	2.0	2.0
1091_0000012	1.0	1.0
1091_0000013	1.0	1.0
1091_0000017	2.0	2.0
1091_0000019	1.0	1.0
1091_0000020	1.0	2.0
1091_0000024	1.0	1.0
1091_0000026	1.0	1.0
1091_0000027	2.0	1.0
1091_0000028	0.0	1.0
1091_0000032	2.0	2.0
1091_0000035	1.0	1.0
1091_0000042	0.0	1.0
1091_0000043	1.0	2.0
1091_0000046	1.0	1.0
1091_0000055	2.0	2.0
1091_0000056	2.0	2.0
1091_0000057	1.0	1.0
1091_0000058	2.0	2.0
1091_0000059	2.0	2.0
1091_0000060	2.0	2.0
1091_0000065	2.0	1.0
1091_0000066	1.0	1.0
1091_0000077	1.0	1.0
1091_0000113	2.0	2.0
1091_0000140	1.0	1.0
1091_0000146	1.0	1.0
1091_0000152	1.0	1.0
1091_0000160	1.0	2.0
1091_0000162	1.0	2.0
1091_0000170	2.0	1.0
1091_0000191	2.0	2.0
1091_0000199	2.0	2.0
1091_0000205	2.0	2.0
1091_0000209	1.0	1.0
1091_0000214	1.0	1.0
1091_0000226	1.0	2.0
1091_0000228	2.0	2.0
1091_0000232	2.0	1.0
1091_0000234	2.0	2.0
1091_0000241	1.0	1.0
1091_0000243	1.0	2.0
1091_0000258	2.0	2.0
1091_0000265	1.0	2.0
1091_0000274	2.0	2.0
Averaged weighted F1-scores 0.6970211913130911
130.68810916179336 82.83744971317162
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']
LANGUAGE: DE, 0th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.17
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.95
              precision    recall  f1-score   support

         0.0       0.47      0.39      0.42        18
         1.0       0.58      0.68      0.63        57
         2.0       0.65      0.43      0.52        70
         3.0       0.59      0.90      0.71        51
         4.0       0.00      0.00      0.00        10

    accuracy                           0.59       206
   macro avg       0.46      0.48      0.46       206
weighted avg       0.57      0.59      0.56       206

[[ 7 11  0  0  0]
 [ 5 39 13  0  0]
 [ 3 15 30 22  0]
 [ 0  2  3 46  0]
 [ 0  0  0 10  0]]
0.5634475335971894
206 206 206



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.97
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.91
              precision    recall  f1-score   support

         0.0       0.45      0.56      0.50        18
         1.0       0.61      0.47      0.53        57
         2.0       0.60      0.56      0.58        70
         3.0       0.61      0.90      0.73        51
         4.0       0.00      0.00      0.00        10

    accuracy                           0.59       206
   macro avg       0.46      0.50      0.47       206
weighted avg       0.57      0.59      0.57       206

[[10  7  1  0  0]
 [ 9 27 21  0  0]
 [ 3  9 39 19  0]
 [ 0  1  4 46  0]
 [ 0  0  0 10  0]]
0.5687271223654959
206 206 206



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.85
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.91
              precision    recall  f1-score   support

         0.0       1.00      0.06      0.11        18
         1.0       0.58      0.67      0.62        57
         2.0       0.62      0.59      0.60        70
         3.0       0.62      0.90      0.74        51
         4.0       0.00      0.00      0.00        10

    accuracy                           0.61       206
   macro avg       0.57      0.44      0.41       206
weighted avg       0.61      0.61      0.57       206

[[ 1 15  2  0  0]
 [ 0 38 19  0  0]
 [ 0 11 41 18  0]
 [ 0  1  4 46  0]
 [ 0  0  0 10  0]]
0.5686641549337763
206 206 206



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.74
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.87
              precision    recall  f1-score   support

         0.0       0.50      0.22      0.31        18
         1.0       0.61      0.75      0.67        57
         2.0       0.70      0.54      0.61        70
         3.0       0.63      0.90      0.74        51
         4.0       0.00      0.00      0.00        10

    accuracy                           0.64       206
   macro avg       0.49      0.48      0.47       206
weighted avg       0.61      0.64      0.60       206

[[ 4 14  0  0  0]
 [ 2 43 12  0  0]
 [ 2 13 38 17  0]
 [ 0  1  4 46  0]
 [ 0  0  0 10  0]]
0.6047440389433617
206 206 206
Filename	True Label	Prediction
1023_0001416	4.0	3.0
1023_0001423	2.0	3.0
1023_0101675	3.0	3.0
1023_0101693	3.0	3.0
1023_0101844	2.0	3.0
1023_0101853	2.0	3.0
1023_0101856	2.0	3.0
1023_0101901	3.0	3.0
1023_0102117	2.0	3.0
1023_0102118	3.0	3.0
1023_0103824	3.0	3.0
1023_0103834	4.0	3.0
1023_0103880	3.0	3.0
1023_0103955	3.0	3.0
1023_0107726	2.0	3.0
1023_0107780	3.0	3.0
1023_0108306	3.0	3.0
1023_0108520	2.0	2.0
1023_0108649	3.0	3.0
1023_0108751	2.0	3.0
1023_0108753	2.0	2.0
1023_0108887	2.0	2.0
1023_0108890	3.0	3.0
1023_0108908	2.0	3.0
1023_0108935	2.0	3.0
1023_0108993	3.0	3.0
1023_0109250	2.0	2.0
1023_0109422	3.0	3.0
1023_0109527	3.0	3.0
1023_0109609	2.0	2.0
1023_0109671	3.0	3.0
1023_0109721	2.0	3.0
1023_0109947	2.0	3.0
1031_0001949	3.0	3.0
1031_0001951	2.0	3.0
1031_0002004	3.0	3.0
1031_0002185	3.0	3.0
1031_0002195	3.0	3.0
1031_0002196	4.0	3.0
1031_0003029	3.0	3.0
1031_0003074	3.0	3.0
1031_0003076	4.0	3.0
1031_0003097	4.0	3.0
1031_0003098	4.0	3.0
1031_0003127	4.0	3.0
1031_0003129	3.0	3.0
1031_0003130	4.0	3.0
1031_0003145	3.0	3.0
1031_0003149	3.0	3.0
1031_0003157	4.0	3.0
1031_0003163	3.0	3.0
1031_0003165	2.0	3.0
1031_0003170	2.0	3.0
1031_0003179	3.0	3.0
1031_0003182	4.0	3.0
1031_0003203	2.0	3.0
1031_0003205	3.0	3.0
1031_0003211	2.0	3.0
1031_0003224	3.0	3.0
1031_0003226	3.0	3.0
1031_0003231	3.0	3.0
1031_0003233	2.0	3.0
1031_0003234	3.0	3.0
1031_0003236	3.0	3.0
1031_0003238	3.0	3.0
1031_0003242	3.0	3.0
1031_0003246	3.0	3.0
1031_0003274	3.0	3.0
1031_0003330	3.0	3.0
1031_0003354	3.0	3.0
1031_0003355	3.0	3.0
1031_0003359	3.0	3.0
1031_0003365	3.0	3.0
1031_0003366	3.0	3.0
1031_0003388	3.0	3.0
1031_0003393	3.0	3.0
1031_0003410	3.0	3.0
1031_0003419	3.0	3.0
1061_0120281	1.0	2.0
1061_0120282	0.0	1.0
1061_0120283	0.0	1.0
1061_0120296	1.0	1.0
1061_0120297	1.0	2.0
1061_0120303	0.0	1.0
1061_0120306	3.0	2.0
1061_0120307	2.0	2.0
1061_0120311	3.0	2.0
1061_0120313	1.0	1.0
1061_0120318	2.0	2.0
1061_0120321	2.0	2.0
1061_0120329	2.0	2.0
1061_0120336	1.0	2.0
1061_0120337	2.0	2.0
1061_0120345	2.0	2.0
1061_0120348	1.0	1.0
1061_0120351	2.0	2.0
1061_0120368	2.0	2.0
1061_0120371	3.0	2.0
1061_0120372	1.0	2.0
1061_0120384	1.0	1.0
1061_0120390	2.0	2.0
1061_0120408	2.0	2.0
1061_0120424	2.0	2.0
1061_0120431	2.0	2.0
1061_0120439	2.0	1.0
1061_0120479	2.0	2.0
1061_0120495	2.0	2.0
1061_0120498	2.0	2.0
1061_0120859	2.0	2.0
1061_0120877	2.0	2.0
1061_0120881	2.0	2.0
1061_0120886	2.0	2.0
1061_0120894	2.0	2.0
1061_1029117	1.0	2.0
1061_1029118	1.0	2.0
1061_1202914	1.0	1.0
1061_1202919	2.0	2.0
1071_0024702	1.0	1.0
1071_0024703	1.0	1.0
1071_0024704	1.0	1.0
1071_0024705	1.0	1.0
1071_0024706	1.0	1.0
1071_0024711	1.0	1.0
1071_0024757	2.0	2.0
1071_0024770	1.0	1.0
1071_0024772	0.0	0.0
1071_0024774	0.0	0.0
1071_0024784	1.0	0.0
1071_0024804	1.0	1.0
1071_0024806	1.0	1.0
1071_0024815	0.0	1.0
1071_0024821	1.0	0.0
1071_0024824	1.0	1.0
1071_0024826	1.0	1.0
1071_0024836	1.0	1.0
1071_0024837	0.0	0.0
1071_0024841	0.0	1.0
1071_0024848	1.0	1.0
1071_0024851	2.0	0.0
1071_0024875	1.0	1.0
1071_0241833	1.0	1.0
1071_0242042	0.0	1.0
1071_0242073	1.0	1.0
1071_0242091	1.0	1.0
1071_0242093	0.0	1.0
1071_0243501	2.0	1.0
1071_0243582	0.0	1.0
1071_0243623	1.0	1.0
1071_0248302	0.0	1.0
1071_0248303	0.0	1.0
1071_0248307	2.0	1.0
1071_0248311	1.0	1.0
1071_0248313	1.0	1.0
1071_0248319	0.0	0.0
1071_0248323	0.0	1.0
1071_0248329	1.0	1.0
1071_0248330	2.0	1.0
1071_0248335	1.0	1.0
1071_0248337	1.0	1.0
1071_0248338	1.0	1.0
1071_0248342	0.0	1.0
1071_0248345	1.0	2.0
1071_0248348	2.0	1.0
1071_0248349	1.0	1.0
1071_0248350	2.0	0.0
1091_0000007	3.0	2.0
1091_0000015	2.0	2.0
1091_0000026	1.0	1.0
1091_0000030	0.0	1.0
1091_0000035	1.0	1.0
1091_0000037	1.0	1.0
1091_0000039	1.0	1.0
1091_0000045	1.0	2.0
1091_0000046	2.0	1.0
1091_0000055	1.0	2.0
1091_0000058	2.0	2.0
1091_0000062	2.0	1.0
1091_0000063	1.0	1.0
1091_0000070	2.0	1.0
1091_0000073	3.0	1.0
1091_0000077	2.0	1.0
1091_0000125	2.0	2.0
1091_0000144	1.0	1.0
1091_0000148	1.0	1.0
1091_0000151	0.0	1.0
1091_0000158	2.0	2.0
1091_0000159	2.0	2.0
1091_0000165	1.0	1.0
1091_0000172	2.0	1.0
1091_0000192	1.0	1.0
1091_0000195	1.0	1.0
1091_0000203	1.0	1.0
1091_0000210	2.0	1.0
1091_0000222	2.0	1.0
1091_0000230	2.0	2.0
1091_0000232	2.0	1.0
1091_0000237	2.0	2.0
1091_0000242	1.0	1.0
1091_0000245	1.0	2.0
1091_0000251	2.0	2.0
1091_0000256	1.0	2.0
1091_0000261	1.0	1.0
1091_0000265	2.0	2.0
1091_0000266	2.0	2.0
1091_0000268	2.0	2.0
1091_0000269	1.0	2.0
LANGUAGE: DE, 1th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.24
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.07
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.51      0.89      0.65        57
         2.0       0.65      0.22      0.33        69
         3.0       0.59      0.92      0.72        52
         4.0       0.00      0.00      0.00        10

    accuracy                           0.56       205
   macro avg       0.35      0.41      0.34       205
weighted avg       0.51      0.56      0.47       205

[[ 0 17  0  0  0]
 [ 0 51  6  0  0]
 [ 0 30 15 24  0]
 [ 0  2  2 48  0]
 [ 0  0  0 10  0]]
0.4721247730587393
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.03
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.52      0.81      0.63        57
         2.0       0.59      0.38      0.46        69
         3.0       0.62      0.87      0.73        52
         4.0       0.00      0.00      0.00        10

    accuracy                           0.57       205
   macro avg       0.35      0.41      0.36       205
weighted avg       0.50      0.57      0.51       205

[[ 0 17  0  0  0]
 [ 0 46 11  0  0]
 [ 0 25 26 18  0]
 [ 0  1  6 45  0]
 [ 0  0  1  9  0]]
0.5142046638655101
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.92
  Training epoch took: 31
Running Validation...
  Average evaluation loss: 1.03
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.51      0.44      0.47        57
         2.0       0.48      0.57      0.52        69
         3.0       0.62      0.88      0.73        52
         4.0       0.00      0.00      0.00        10

    accuracy                           0.54       205
   macro avg       0.32      0.38      0.34       205
weighted avg       0.46      0.54      0.49       205

[[ 0 12  5  0  0]
 [ 0 25 32  0  0]
 [ 0 12 39 18  0]
 [ 0  0  6 46  0]
 [ 0  0  0 10  0]]
0.49023136842460696
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.82
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.98
              precision    recall  f1-score   support

         0.0       0.60      0.18      0.27        17
         1.0       0.62      0.68      0.65        57
         2.0       0.57      0.52      0.55        69
         3.0       0.62      0.88      0.73        52
         4.0       0.00      0.00      0.00        10

    accuracy                           0.60       205
   macro avg       0.48      0.45      0.44       205
weighted avg       0.57      0.60      0.57       205

[[ 3 10  4  0  0]
 [ 1 39 17  0  0]
 [ 1 14 36 18  0]
 [ 0  0  6 46  0]
 [ 0  0  0 10  0]]
0.5721511280047865
205 205 205
Filename	True Label	Prediction
1023_0001418	2.0	3.0
1023_0101688	3.0	3.0
1023_0101689	1.0	2.0
1023_0101691	3.0	3.0
1023_0101694	3.0	3.0
1023_0101753	3.0	3.0
1023_0101846	4.0	3.0
1023_0101854	2.0	2.0
1023_0103822	2.0	2.0
1023_0103823	3.0	3.0
1023_0103829	2.0	3.0
1023_0103837	3.0	3.0
1023_0103841	3.0	3.0
1023_0103883	2.0	3.0
1023_0107042	3.0	3.0
1023_0107074	3.0	3.0
1023_0107244	2.0	3.0
1023_0107682	2.0	2.0
1023_0107727	4.0	3.0
1023_0107729	3.0	3.0
1023_0107781	2.0	3.0
1023_0107787	2.0	3.0
1023_0107788	2.0	3.0
1023_0108304	3.0	3.0
1023_0108426	2.0	3.0
1023_0108648	3.0	3.0
1023_0108766	2.0	3.0
1023_0108811	3.0	3.0
1023_0108812	2.0	3.0
1023_0108886	3.0	3.0
1023_0109030	3.0	3.0
1023_0109033	3.0	3.0
1023_0109391	2.0	2.0
1023_0109392	2.0	3.0
1023_0109395	2.0	3.0
1023_0109400	3.0	2.0
1023_0109401	2.0	2.0
1023_0109500	2.0	3.0
1023_0109505	2.0	3.0
1023_0109524	3.0	3.0
1023_0109590	3.0	3.0
1023_0109651	3.0	3.0
1023_0109674	2.0	3.0
1023_0109716	3.0	3.0
1023_0109914	2.0	2.0
1023_0109945	4.0	3.0
1023_0109954	3.0	3.0
1023_0111896	2.0	2.0
1031_0002006	4.0	3.0
1031_0002010	2.0	3.0
1031_0002011	3.0	3.0
1031_0002032	3.0	3.0
1031_0002036	4.0	3.0
1031_0002084	3.0	3.0
1031_0002085	3.0	3.0
1031_0002087	3.0	3.0
1031_0002198	3.0	3.0
1031_0003035	3.0	3.0
1031_0003088	4.0	3.0
1031_0003106	3.0	3.0
1031_0003131	3.0	3.0
1031_0003132	3.0	3.0
1031_0003154	3.0	3.0
1031_0003156	3.0	3.0
1031_0003164	3.0	3.0
1031_0003172	3.0	3.0
1031_0003173	3.0	3.0
1031_0003174	4.0	3.0
1031_0003180	3.0	3.0
1031_0003206	3.0	3.0
1031_0003218	3.0	3.0
1031_0003219	3.0	3.0
1031_0003230	2.0	3.0
1031_0003232	2.0	3.0
1031_0003237	3.0	3.0
1031_0003313	4.0	3.0
1031_0003314	3.0	3.0
1031_0003337	3.0	3.0
1031_0003353	3.0	3.0
1031_0003367	4.0	3.0
1031_0003390	3.0	3.0
1031_0003407	3.0	3.0
1031_0003415	4.0	3.0
1061_0012029	3.0	2.0
1061_0120274	1.0	1.0
1061_0120280	1.0	1.0
1061_0120287	1.0	1.0
1061_0120295	0.0	2.0
1061_0120298	1.0	2.0
1061_0120308	3.0	2.0
1061_0120310	2.0	2.0
1061_0120316	2.0	2.0
1061_0120323	1.0	1.0
1061_0120325	2.0	2.0
1061_0120327	2.0	2.0
1061_0120331	1.0	1.0
1061_0120347	2.0	2.0
1061_0120349	1.0	1.0
1061_0120350	2.0	2.0
1061_0120354	1.0	1.0
1061_0120355	1.0	1.0
1061_0120358	1.0	1.0
1061_0120361	2.0	2.0
1061_0120373	2.0	2.0
1061_0120375	2.0	1.0
1061_0120386	0.0	2.0
1061_0120387	1.0	2.0
1061_0120388	1.0	2.0
1061_0120394	2.0	2.0
1061_0120405	2.0	2.0
1061_0120413	1.0	1.0
1061_0120415	2.0	2.0
1061_0120427	1.0	2.0
1061_0120428	2.0	2.0
1061_0120429	2.0	2.0
1061_0120433	1.0	1.0
1061_0120455	2.0	1.0
1061_0120458	3.0	2.0
1061_0120482	1.0	2.0
1061_0120486	2.0	2.0
1061_0120487	2.0	2.0
1061_0120488	2.0	2.0
1061_0120492	2.0	2.0
1061_0120493	1.0	2.0
1061_0120853	1.0	2.0
1061_0120855	1.0	2.0
1061_0120880	2.0	2.0
1061_1029114	1.0	1.0
1061_1029120	1.0	1.0
1061_1202911	0.0	2.0
1061_1202915	1.0	2.0
1071_0024689	1.0	1.0
1071_0024797	0.0	1.0
1071_0024798	1.0	1.0
1071_0024799	2.0	2.0
1071_0024800	1.0	1.0
1071_0024807	0.0	1.0
1071_0024811	1.0	1.0
1071_0024816	1.0	1.0
1071_0024819	1.0	2.0
1071_0024820	0.0	1.0
1071_0024823	1.0	1.0
1071_0024825	0.0	1.0
1071_0024833	1.0	1.0
1071_0024838	0.0	1.0
1071_0024840	1.0	1.0
1071_0024850	0.0	1.0
1071_0024852	0.0	0.0
1071_0024853	1.0	1.0
1071_0024876	1.0	1.0
1071_0242023	1.0	1.0
1071_0242043	0.0	1.0
1071_0242072	0.0	0.0
1071_0243581	0.0	1.0
1071_0243591	1.0	1.0
1071_0243592	1.0	1.0
1071_0243593	1.0	1.0
1071_0248317	0.0	0.0
1071_0248332	2.0	2.0
1071_0248339	2.0	1.0
1091_0000002	2.0	2.0
1091_0000010	3.0	2.0
1091_0000016	0.0	1.0
1091_0000019	1.0	1.0
1091_0000021	1.0	2.0
1091_0000031	1.0	1.0
1091_0000032	1.0	1.0
1091_0000036	1.0	1.0
1091_0000038	1.0	1.0
1091_0000042	1.0	1.0
1091_0000049	1.0	1.0
1091_0000057	2.0	1.0
1091_0000061	2.0	0.0
1091_0000066	2.0	1.0
1091_0000067	2.0	1.0
1091_0000076	2.0	2.0
1091_0000087	2.0	2.0
1091_0000116	2.0	2.0
1091_0000123	2.0	1.0
1091_0000146	1.0	0.0
1091_0000156	2.0	2.0
1091_0000162	1.0	2.0
1091_0000173	2.0	1.0
1091_0000185	2.0	1.0
1091_0000194	1.0	2.0
1091_0000199	2.0	1.0
1091_0000200	1.0	2.0
1091_0000204	2.0	1.0
1091_0000216	1.0	1.0
1091_0000218	2.0	1.0
1091_0000221	2.0	1.0
1091_0000224	1.0	1.0
1091_0000226	1.0	1.0
1091_0000227	0.0	2.0
1091_0000231	1.0	2.0
1091_0000234	3.0	2.0
1091_0000240	1.0	1.0
1091_0000244	2.0	2.0
1091_0000249	2.0	2.0
1091_0000250	1.0	1.0
1091_0000255	0.0	1.0
1091_0000258	2.0	2.0
1091_0000270	2.0	2.0
1091_0000271	2.0	1.0
1091_0000274	1.0	2.0
LANGUAGE: DE, 2th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.22
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.99
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.54      0.65      0.59        57
         2.0       0.59      0.46      0.52        70
         3.0       0.60      0.94      0.73        52
         4.0       0.00      0.00      0.00         9

    accuracy                           0.58       205
   macro avg       0.35      0.41      0.37       205
weighted avg       0.50      0.58      0.53       205

[[ 0 17  0  0  0]
 [ 0 37 19  1  0]
 [ 0 15 32 23  0]
 [ 0  0  3 49  0]
 [ 0  0  0  9  0]]
0.5250491389293837
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.99
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.91
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.51      0.70      0.59        57
         2.0       0.57      0.50      0.53        70
         3.0       0.65      0.83      0.73        52
         4.0       0.00      0.00      0.00         9

    accuracy                           0.58       205
   macro avg       0.35      0.41      0.37       205
weighted avg       0.50      0.58      0.53       205

[[ 0 17  0  0  0]
 [ 0 40 17  0  0]
 [ 0 21 35 14  0]
 [ 0  0  9 43  0]
 [ 0  0  0  9  0]]
0.5321007951960747
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.90
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.92
              precision    recall  f1-score   support

         0.0       0.67      0.12      0.20        17
         1.0       0.52      0.58      0.55        57
         2.0       0.56      0.49      0.52        70
         3.0       0.62      0.92      0.74        52
         4.0       0.00      0.00      0.00         9

    accuracy                           0.57       205
   macro avg       0.47      0.42      0.40       205
weighted avg       0.55      0.57      0.53       205

[[ 2 15  0  0  0]
 [ 1 33 23  0  0]
 [ 0 16 34 20  0]
 [ 0  0  4 48  0]
 [ 0  0  0  9  0]]
0.5342656652298008
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.79
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.91
              precision    recall  f1-score   support

         0.0       0.73      0.47      0.57        17
         1.0       0.57      0.60      0.58        57
         2.0       0.56      0.50      0.53        70
         3.0       0.62      0.85      0.72        52
         4.0       0.00      0.00      0.00         9

    accuracy                           0.59       205
   macro avg       0.49      0.48      0.48       205
weighted avg       0.56      0.59      0.57       205

[[ 8  9  0  0  0]
 [ 3 34 20  0  0]
 [ 0 17 35 18  0]
 [ 0  0  8 44  0]
 [ 0  0  0  9  0]]
0.5701846250642393
205 205 205
Filename	True Label	Prediction
1023_0001420	3.0	3.0
1023_0001575	3.0	2.0
1023_0101700	2.0	3.0
1023_0101841	2.0	3.0
1023_0101843	3.0	3.0
1023_0101845	2.0	2.0
1023_0101897	2.0	3.0
1023_0101906	2.0	2.0
1023_0103825	3.0	3.0
1023_0103826	3.0	3.0
1023_0103830	3.0	3.0
1023_0103838	3.0	3.0
1023_0103843	2.0	2.0
1023_0107672	2.0	3.0
1023_0107784	1.0	2.0
1023_0108650	3.0	3.0
1023_0108752	3.0	3.0
1023_0108810	3.0	3.0
1023_0108889	3.0	3.0
1023_0108934	2.0	3.0
1023_0108958	2.0	3.0
1023_0108992	3.0	2.0
1023_0109027	2.0	2.0
1023_0109039	3.0	3.0
1023_0109096	3.0	3.0
1023_0109267	2.0	2.0
1023_0109396	2.0	3.0
1023_0109496	3.0	3.0
1023_0109519	2.0	2.0
1023_0109520	2.0	3.0
1023_0109528	3.0	3.0
1023_0109591	3.0	2.0
1023_0109614	2.0	2.0
1023_0109649	2.0	3.0
1023_0109880	3.0	3.0
1023_0109915	2.0	2.0
1023_0109917	3.0	3.0
1023_0109951	2.0	3.0
1031_0001950	3.0	3.0
1031_0002040	4.0	3.0
1031_0002079	4.0	3.0
1031_0002083	2.0	3.0
1031_0002088	3.0	3.0
1031_0002091	3.0	3.0
1031_0002092	4.0	3.0
1031_0002187	3.0	3.0
1031_0002199	3.0	3.0
1031_0002200	2.0	3.0
1031_0003023	3.0	3.0
1031_0003042	3.0	3.0
1031_0003043	4.0	3.0
1031_0003052	3.0	3.0
1031_0003053	3.0	3.0
1031_0003054	3.0	3.0
1031_0003063	4.0	3.0
1031_0003078	3.0	3.0
1031_0003135	3.0	3.0
1031_0003141	3.0	3.0
1031_0003150	3.0	3.0
1031_0003155	3.0	3.0
1031_0003161	3.0	3.0
1031_0003183	4.0	3.0
1031_0003185	3.0	3.0
1031_0003186	3.0	3.0
1031_0003187	3.0	3.0
1031_0003191	3.0	3.0
1031_0003214	3.0	3.0
1031_0003220	2.0	3.0
1031_0003221	2.0	3.0
1031_0003244	4.0	3.0
1031_0003245	3.0	3.0
1031_0003249	3.0	3.0
1031_0003260	4.0	3.0
1031_0003272	3.0	3.0
1031_0003310	3.0	3.0
1031_0003327	2.0	3.0
1031_0003331	2.0	3.0
1031_0003356	3.0	3.0
1031_0003358	4.0	3.0
1031_0003368	3.0	3.0
1031_0003389	3.0	3.0
1031_0003408	2.0	3.0
1061_0120272	1.0	2.0
1061_0120273	2.0	2.0
1061_0120277	1.0	2.0
1061_0120278	1.0	2.0
1061_0120284	0.0	0.0
1061_0120286	0.0	1.0
1061_0120289	1.0	1.0
1061_0120291	1.0	1.0
1061_0120299	2.0	2.0
1061_0120300	2.0	1.0
1061_0120314	1.0	2.0
1061_0120315	2.0	1.0
1061_0120320	3.0	2.0
1061_0120326	2.0	2.0
1061_0120332	1.0	2.0
1061_0120346	2.0	2.0
1061_0120353	1.0	1.0
1061_0120356	2.0	2.0
1061_0120366	3.0	2.0
1061_0120383	2.0	3.0
1061_0120404	1.0	1.0
1061_0120411	3.0	2.0
1061_0120426	1.0	2.0
1061_0120441	2.0	2.0
1061_0120442	2.0	2.0
1061_0120448	3.0	2.0
1061_0120480	2.0	2.0
1061_0120481	2.0	2.0
1061_0120485	3.0	2.0
1061_0120490	2.0	2.0
1061_0120491	2.0	2.0
1061_0120494	1.0	2.0
1061_0120857	2.0	2.0
1061_0120874	2.0	2.0
1061_0120875	2.0	2.0
1061_0120883	1.0	1.0
1061_0120884	1.0	2.0
1061_0120885	2.0	2.0
1061_0120890	1.0	1.0
1061_1029116	1.0	2.0
1061_1202913	2.0	1.0
1071_0024681	1.0	2.0
1071_0024682	2.0	2.0
1071_0024683	0.0	1.0
1071_0024685	1.0	2.0
1071_0024686	2.0	1.0
1071_0024708	1.0	1.0
1071_0024713	1.0	1.0
1071_0024714	2.0	1.0
1071_0024716	1.0	1.0
1071_0024765	0.0	0.0
1071_0024766	1.0	0.0
1071_0024767	2.0	1.0
1071_0024769	0.0	1.0
1071_0024773	1.0	0.0
1071_0024776	0.0	0.0
1071_0024783	0.0	0.0
1071_0024810	2.0	1.0
1071_0024814	1.0	1.0
1071_0024844	1.0	1.0
1071_0024846	0.0	0.0
1071_0024854	0.0	0.0
1071_0024859	1.0	1.0
1071_0024862	1.0	1.0
1071_0024863	1.0	1.0
1071_0024867	1.0	1.0
1071_0024872	1.0	2.0
1071_0024878	1.0	1.0
1071_0024879	1.0	1.0
1071_0242013	1.0	1.0
1071_0242041	1.0	1.0
1071_0243621	2.0	2.0
1071_0248309	1.0	1.0
1071_0248310	0.0	1.0
1071_0248312	1.0	1.0
1071_0248320	0.0	0.0
1071_0248322	1.0	1.0
1071_0248328	1.0	1.0
1071_0248331	0.0	1.0
1071_0248336	0.0	1.0
1071_0248341	0.0	0.0
1071_0248343	2.0	1.0
1071_0248346	0.0	1.0
1071_0248347	1.0	0.0
1091_0000003	2.0	1.0
1091_0000005	2.0	2.0
1091_0000009	0.0	1.0
1091_0000011	1.0	1.0
1091_0000018	2.0	2.0
1091_0000025	1.0	1.0
1091_0000029	2.0	1.0
1091_0000043	1.0	1.0
1091_0000051	1.0	1.0
1091_0000052	0.0	1.0
1091_0000064	1.0	1.0
1091_0000065	2.0	1.0
1091_0000071	2.0	2.0
1091_0000075	2.0	2.0
1091_0000079	1.0	2.0
1091_0000095	1.0	1.0
1091_0000102	1.0	1.0
1091_0000113	1.0	2.0
1091_0000140	2.0	1.0
1091_0000145	1.0	1.0
1091_0000157	2.0	2.0
1091_0000161	2.0	2.0
1091_0000170	2.0	1.0
1091_0000193	2.0	1.0
1091_0000207	1.0	2.0
1091_0000209	2.0	2.0
1091_0000211	1.0	1.0
1091_0000214	2.0	1.0
1091_0000219	1.0	2.0
1091_0000220	1.0	1.0
1091_0000223	1.0	2.0
1091_0000228	1.0	1.0
1091_0000233	2.0	2.0
1091_0000238	1.0	2.0
1091_0000241	2.0	1.0
1091_0000248	2.0	2.0
1091_0000253	2.0	1.0
1091_0000257	1.0	2.0
1091_0000276	2.0	2.0
LANGUAGE: DE, 3th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.14
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.98
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        18
         1.0       0.46      0.62      0.53        56
         2.0       0.50      0.44      0.47        70
         3.0       0.63      0.81      0.71        52
         4.0       0.00      0.00      0.00         9

    accuracy                           0.53       205
   macro avg       0.32      0.38      0.34       205
weighted avg       0.46      0.53      0.48       205

[[ 0 18  0  0  0]
 [ 0 35 21  0  0]
 [ 0 23 31 16  0]
 [ 0  0 10 42  0]
 [ 0  0  0  9  0]]
0.4843006825790183
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.90
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.97
              precision    recall  f1-score   support

         0.0       0.75      0.33      0.46        18
         1.0       0.51      0.71      0.60        56
         2.0       0.49      0.30      0.37        70
         3.0       0.59      0.87      0.70        52
         4.0       0.00      0.00      0.00         9

    accuracy                           0.55       205
   macro avg       0.47      0.44      0.43       205
weighted avg       0.52      0.55      0.51       205

[[ 6 12  0  0  0]
 [ 0 40 16  0  0]
 [ 2 25 21 22  0]
 [ 0  1  6 45  0]
 [ 0  0  0  9  0]]
0.5088815963104003
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.82
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.95
              precision    recall  f1-score   support

         0.0       1.00      0.33      0.50        18
         1.0       0.52      0.57      0.55        56
         2.0       0.55      0.60      0.58        70
         3.0       0.68      0.81      0.74        52
         4.0       0.00      0.00      0.00         9

    accuracy                           0.60       205
   macro avg       0.55      0.46      0.47       205
weighted avg       0.59      0.60      0.58       205

[[ 6 12  0  0  0]
 [ 0 32 24  0  0]
 [ 0 17 42 11  0]
 [ 0  0 10 42  0]
 [ 0  0  0  9  0]]
0.5766938571165979
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.71
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.06
              precision    recall  f1-score   support

         0.0       1.00      0.33      0.50        18
         1.0       0.50      0.43      0.46        56
         2.0       0.49      0.54      0.51        70
         3.0       0.62      0.87      0.72        52
         4.0       0.00      0.00      0.00         9

    accuracy                           0.55       205
   macro avg       0.52      0.43      0.44       205
weighted avg       0.55      0.55      0.53       205

[[ 6 11  1  0  0]
 [ 0 24 32  0  0]
 [ 0 13 38 19  0]
 [ 0  0  7 45  0]
 [ 0  0  0  9  0]]
0.5279614624004867
205 205 205
Filename	True Label	Prediction
1023_0001419	3.0	3.0
1023_0001422	3.0	2.0
1023_0101683	3.0	3.0
1023_0101684	2.0	3.0
1023_0101690	2.0	3.0
1023_0101695	2.0	3.0
1023_0101848	2.0	3.0
1023_0101849	3.0	3.0
1023_0101851	3.0	3.0
1023_0101855	2.0	2.0
1023_0101894	2.0	3.0
1023_0101898	4.0	3.0
1023_0101900	3.0	3.0
1023_0101907	3.0	3.0
1023_0103831	3.0	3.0
1023_0103832	2.0	2.0
1023_0103840	3.0	3.0
1023_0103844	4.0	3.0
1023_0104206	3.0	2.0
1023_0104207	2.0	2.0
1023_0107075	2.0	3.0
1023_0107740	3.0	3.0
1023_0107773	2.0	2.0
1023_0107783	3.0	2.0
1023_0108305	3.0	3.0
1023_0108307	3.0	3.0
1023_0108422	3.0	3.0
1023_0108423	2.0	2.0
1023_0108510	2.0	3.0
1023_0108814	3.0	3.0
1023_0109026	2.0	3.0
1023_0109151	3.0	3.0
1023_0109247	3.0	3.0
1023_0109248	2.0	3.0
1023_0109495	2.0	3.0
1023_0109515	3.0	3.0
1023_0109522	3.0	3.0
1023_0109606	2.0	2.0
1023_0109717	3.0	3.0
1023_0109891	3.0	3.0
1031_0001703	4.0	3.0
1031_0001997	3.0	3.0
1031_0002002	2.0	3.0
1031_0002005	3.0	3.0
1031_0002042	3.0	3.0
1031_0002086	3.0	3.0
1031_0002197	4.0	3.0
1031_0003012	3.0	3.0
1031_0003048	4.0	3.0
1031_0003071	3.0	3.0
1031_0003072	3.0	3.0
1031_0003073	4.0	3.0
1031_0003085	3.0	3.0
1031_0003090	3.0	3.0
1031_0003091	2.0	3.0
1031_0003092	2.0	3.0
1031_0003095	2.0	3.0
1031_0003099	3.0	3.0
1031_0003121	4.0	3.0
1031_0003128	3.0	3.0
1031_0003133	4.0	3.0
1031_0003136	3.0	3.0
1031_0003144	3.0	3.0
1031_0003162	3.0	3.0
1031_0003167	3.0	3.0
1031_0003169	3.0	3.0
1031_0003189	3.0	3.0
1031_0003190	3.0	3.0
1031_0003212	2.0	3.0
1031_0003216	3.0	3.0
1031_0003239	4.0	3.0
1031_0003240	2.0	3.0
1031_0003243	3.0	3.0
1031_0003262	3.0	3.0
1031_0003273	3.0	3.0
1031_0003339	3.0	3.0
1031_0003369	3.0	3.0
1031_0003384	2.0	3.0
1031_0003386	2.0	3.0
1031_0003414	3.0	3.0
1061_0120271	2.0	2.0
1061_0120275	2.0	2.0
1061_0120276	2.0	2.0
1061_0120279	1.0	1.0
1061_0120288	1.0	2.0
1061_0120290	1.0	2.0
1061_0120301	2.0	1.0
1061_0120304	2.0	2.0
1061_0120309	1.0	1.0
1061_0120312	1.0	1.0
1061_0120324	2.0	2.0
1061_0120328	1.0	2.0
1061_0120333	3.0	3.0
1061_0120338	2.0	2.0
1061_0120341	1.0	2.0
1061_0120343	2.0	2.0
1061_0120352	1.0	1.0
1061_0120359	1.0	2.0
1061_0120367	2.0	2.0
1061_0120369	1.0	2.0
1061_0120376	2.0	2.0
1061_0120391	1.0	2.0
1061_0120407	3.0	2.0
1061_0120410	2.0	2.0
1061_0120414	2.0	2.0
1061_0120421	2.0	2.0
1061_0120423	2.0	2.0
1061_0120430	1.0	2.0
1061_0120432	1.0	2.0
1061_0120438	2.0	2.0
1061_0120449	3.0	2.0
1061_0120457	3.0	2.0
1061_0120460	2.0	2.0
1061_0120483	1.0	2.0
1061_0120496	1.0	2.0
1061_0120499	2.0	2.0
1061_0120876	2.0	2.0
1061_0120887	1.0	1.0
1061_1202917	1.0	2.0
1061_1202918	1.0	2.0
1071_0020001	1.0	1.0
1071_0024678	1.0	1.0
1071_0024690	1.0	2.0
1071_0024692	2.0	2.0
1071_0024699	1.0	1.0
1071_0024709	2.0	2.0
1071_0024710	1.0	1.0
1071_0024712	1.0	2.0
1071_0024758	2.0	1.0
1071_0024761	2.0	1.0
1071_0024763	1.0	1.0
1071_0024775	0.0	1.0
1071_0024779	1.0	1.0
1071_0024782	0.0	0.0
1071_0024801	1.0	1.0
1071_0024802	2.0	1.0
1071_0024818	2.0	1.0
1071_0024822	0.0	1.0
1071_0024831	0.0	1.0
1071_0024835	0.0	1.0
1071_0024843	0.0	1.0
1071_0024845	0.0	1.0
1071_0024847	1.0	2.0
1071_0024849	0.0	0.0
1071_0024855	1.0	1.0
1071_0024856	1.0	1.0
1071_0024860	1.0	1.0
1071_0024861	0.0	1.0
1071_0024864	0.0	0.0
1071_0024865	2.0	1.0
1071_0024866	2.0	2.0
1071_0024871	1.0	1.0
1071_0241831	1.0	1.0
1071_0241832	1.0	2.0
1071_0242012	1.0	2.0
1071_0242021	1.0	1.0
1071_0242022	0.0	1.0
1071_0242071	0.0	1.0
1071_0248305	0.0	0.0
1071_0248315	0.0	0.0
1071_0248318	0.0	0.0
1071_0248321	2.0	1.0
1071_0248325	0.0	1.0
1071_0248334	2.0	2.0
1091_0000001	1.0	1.0
1091_0000008	2.0	2.0
1091_0000027	0.0	1.0
1091_0000028	1.0	1.0
1091_0000034	2.0	1.0
1091_0000044	0.0	2.0
1091_0000047	2.0	1.0
1091_0000050	1.0	1.0
1091_0000059	1.0	2.0
1091_0000068	2.0	2.0
1091_0000069	2.0	1.0
1091_0000072	1.0	1.0
1091_0000086	1.0	2.0
1091_0000101	2.0	1.0
1091_0000126	2.0	2.0
1091_0000127	2.0	1.0
1091_0000152	1.0	1.0
1091_0000153	1.0	2.0
1091_0000160	2.0	3.0
1091_0000166	1.0	2.0
1091_0000168	2.0	2.0
1091_0000169	3.0	2.0
1091_0000171	1.0	2.0
1091_0000174	2.0	1.0
1091_0000190	1.0	2.0
1091_0000191	1.0	2.0
1091_0000196	2.0	2.0
1091_0000201	2.0	2.0
1091_0000202	1.0	2.0
1091_0000205	1.0	2.0
1091_0000206	1.0	1.0
1091_0000212	1.0	2.0
1091_0000215	2.0	2.0
1091_0000229	1.0	2.0
1091_0000236	2.0	2.0
1091_0000239	2.0	2.0
1091_0000243	1.0	2.0
1091_0000262	2.0	2.0
1091_0000267	1.0	2.0
1091_0000273	1.0	2.0
1091_0000275	2.0	2.0
LANGUAGE: DE, 4th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.20
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.02
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        18
         1.0       0.52      0.47      0.50        57
         2.0       0.54      0.59      0.56        70
         3.0       0.60      0.90      0.72        51
         4.0       0.00      0.00      0.00         9

    accuracy                           0.56       205
   macro avg       0.33      0.39      0.36       205
weighted avg       0.48      0.56      0.51       205

[[ 0 16  2  0  0]
 [ 0 27 29  1  0]
 [ 0  8 41 21  0]
 [ 0  1  4 46  0]
 [ 0  0  0  9  0]]
0.5083407346499631
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.97
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.12
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        18
         1.0       0.53      0.60      0.56        57
         2.0       0.51      0.31      0.39        70
         3.0       0.50      0.96      0.66        51
         4.0       0.00      0.00      0.00         9

    accuracy                           0.51       205
   macro avg       0.31      0.37      0.32       205
weighted avg       0.45      0.51      0.45       205

[[ 0 18  0  0  0]
 [ 0 34 20  3  0]
 [ 0 11 22 37  0]
 [ 0  1  1 49  0]
 [ 0  0  0  9  0]]
0.45284545941685406
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.88
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.90
              precision    recall  f1-score   support

         0.0       0.37      0.39      0.38        18
         1.0       0.56      0.54      0.55        57
         2.0       0.66      0.61      0.64        70
         3.0       0.67      0.86      0.75        51
         4.0       0.00      0.00      0.00         9

    accuracy                           0.61       205
   macro avg       0.45      0.48      0.46       205
weighted avg       0.58      0.61      0.59       205

[[ 7 11  0  0  0]
 [10 31 16  0  0]
 [ 2 12 43 13  0]
 [ 0  1  6 44  0]
 [ 0  0  0  9  0]]
0.5917851180046302
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.77
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.92
              precision    recall  f1-score   support

         0.0       0.44      0.22      0.30        18
         1.0       0.56      0.58      0.57        57
         2.0       0.63      0.63      0.63        70
         3.0       0.66      0.86      0.75        51
         4.0       0.00      0.00      0.00         9

    accuracy                           0.61       205
   macro avg       0.46      0.46      0.45       205
weighted avg       0.57      0.61      0.58       205

[[ 4 14  0  0  0]
 [ 4 33 20  0  0]
 [ 1 11 44 14  0]
 [ 0  1  6 44  0]
 [ 0  0  0  9  0]]
0.5843817859569594
205 205 205
Filename	True Label	Prediction
1023_0101701	2.0	3.0
1023_0101749	4.0	3.0
1023_0101751	3.0	3.0
1023_0101752	2.0	3.0
1023_0101847	3.0	3.0
1023_0101852	3.0	3.0
1023_0101893	3.0	3.0
1023_0101895	4.0	3.0
1023_0101896	2.0	2.0
1023_0101899	3.0	3.0
1023_0101904	2.0	2.0
1023_0101909	3.0	3.0
1023_0103821	3.0	3.0
1023_0103827	3.0	3.0
1023_0103828	1.0	1.0
1023_0103833	4.0	3.0
1023_0103836	3.0	3.0
1023_0103839	3.0	3.0
1023_0104203	2.0	3.0
1023_0104209	3.0	3.0
1023_0106816	3.0	3.0
1023_0107725	2.0	3.0
1023_0108518	3.0	2.0
1023_0108641	3.0	3.0
1023_0108813	3.0	3.0
1023_0108815	2.0	2.0
1023_0108885	2.0	2.0
1023_0108888	3.0	3.0
1023_0108931	3.0	2.0
1023_0108932	2.0	3.0
1023_0108933	2.0	2.0
1023_0108955	3.0	3.0
1023_0109022	2.0	3.0
1023_0109029	1.0	2.0
1023_0109038	3.0	3.0
1023_0109192	2.0	2.0
1023_0109249	2.0	3.0
1023_0109399	2.0	3.0
1023_0109402	2.0	2.0
1023_0109516	3.0	3.0
1023_0109518	2.0	2.0
1023_0109588	3.0	3.0
1023_0109878	2.0	3.0
1023_0109890	3.0	3.0
1023_0109946	2.0	2.0
1031_0001998	4.0	3.0
1031_0002003	2.0	3.0
1031_0002043	3.0	3.0
1031_0002061	3.0	3.0
1031_0002089	3.0	3.0
1031_0002131	3.0	3.0
1031_0002184	3.0	3.0
1031_0003013	4.0	3.0
1031_0003065	3.0	3.0
1031_0003077	3.0	3.0
1031_0003126	3.0	3.0
1031_0003140	3.0	3.0
1031_0003146	4.0	3.0
1031_0003160	3.0	3.0
1031_0003166	2.0	3.0
1031_0003181	3.0	3.0
1031_0003184	4.0	3.0
1031_0003207	4.0	3.0
1031_0003217	3.0	3.0
1031_0003225	3.0	3.0
1031_0003235	3.0	3.0
1031_0003261	3.0	3.0
1031_0003309	3.0	3.0
1031_0003315	3.0	3.0
1031_0003336	2.0	3.0
1031_0003338	3.0	3.0
1031_0003352	2.0	3.0
1031_0003357	3.0	3.0
1031_0003383	3.0	3.0
1031_0003387	3.0	3.0
1031_0003391	2.0	3.0
1031_0003392	3.0	3.0
1031_0003409	4.0	3.0
1061_0120285	1.0	2.0
1061_0120302	1.0	1.0
1061_0120317	2.0	2.0
1061_0120319	2.0	2.0
1061_0120330	2.0	2.0
1061_0120334	2.0	2.0
1061_0120335	3.0	3.0
1061_0120357	3.0	2.0
1061_0120360	3.0	2.0
1061_0120370	2.0	2.0
1061_0120374	2.0	2.0
1061_0120382	1.0	2.0
1061_0120389	2.0	2.0
1061_0120403	3.0	2.0
1061_0120406	2.0	2.0
1061_0120409	2.0	2.0
1061_0120425	2.0	2.0
1061_0120440	1.0	1.0
1061_0120443	0.0	0.0
1061_0120450	2.0	2.0
1061_0120453	2.0	2.0
1061_0120456	2.0	2.0
1061_0120459	2.0	2.0
1061_0120478	2.0	2.0
1061_0120484	1.0	2.0
1061_0120489	2.0	2.0
1061_0120497	2.0	2.0
1061_0120500	1.0	2.0
1061_0120856	1.0	2.0
1061_0120858	2.0	2.0
1061_0120878	1.0	1.0
1061_0120882	2.0	2.0
1061_0120888	1.0	2.0
1061_0120889	1.0	1.0
1061_1029111	2.0	2.0
1061_1029112	3.0	3.0
1061_1029113	2.0	1.0
1061_1029115	2.0	1.0
1061_1029119	1.0	2.0
1061_1202910	2.0	2.0
1061_1202912	2.0	2.0
1061_1202916	2.0	2.0
1071_0024680	2.0	2.0
1071_0024687	0.0	1.0
1071_0024688	1.0	1.0
1071_0024691	1.0	2.0
1071_0024693	1.0	1.0
1071_0024694	1.0	2.0
1071_0024701	2.0	2.0
1071_0024715	2.0	2.0
1071_0024756	1.0	1.0
1071_0024759	0.0	1.0
1071_0024762	0.0	1.0
1071_0024768	1.0	1.0
1071_0024777	1.0	1.0
1071_0024778	0.0	1.0
1071_0024781	0.0	1.0
1071_0024803	1.0	1.0
1071_0024808	0.0	1.0
1071_0024809	0.0	1.0
1071_0024812	1.0	1.0
1071_0024813	0.0	0.0
1071_0024817	1.0	1.0
1071_0024827	1.0	1.0
1071_0024834	2.0	2.0
1071_0024857	0.0	1.0
1071_0024873	0.0	1.0
1071_0024874	1.0	1.0
1071_0024877	1.0	1.0
1071_0024881	2.0	1.0
1071_0242011	2.0	1.0
1071_0242092	0.0	1.0
1071_0243502	1.0	0.0
1071_0243622	1.0	0.0
1071_0248301	2.0	1.0
1071_0248304	1.0	1.0
1071_0248308	1.0	1.0
1071_0248314	1.0	1.0
1071_0248316	1.0	0.0
1071_0248324	0.0	0.0
1071_0248326	1.0	1.0
1071_0248327	0.0	1.0
1071_0248333	2.0	1.0
1071_0248340	0.0	0.0
1071_0248344	1.0	1.0
1091_0000004	1.0	1.0
1091_0000006	1.0	1.0
1091_0000012	1.0	1.0
1091_0000013	1.0	1.0
1091_0000014	0.0	1.0
1091_0000017	2.0	2.0
1091_0000020	1.0	2.0
1091_0000022	1.0	2.0
1091_0000023	2.0	0.0
1091_0000024	3.0	1.0
1091_0000033	1.0	1.0
1091_0000041	1.0	0.0
1091_0000048	1.0	1.0
1091_0000053	0.0	1.0
1091_0000054	0.0	1.0
1091_0000056	1.0	2.0
1091_0000060	2.0	2.0
1091_0000074	1.0	2.0
1091_0000078	2.0	1.0
1091_0000092	1.0	1.0
1091_0000114	1.0	2.0
1091_0000154	1.0	2.0
1091_0000155	1.0	2.0
1091_0000163	1.0	1.0
1091_0000164	1.0	1.0
1091_0000167	2.0	2.0
1091_0000197	1.0	2.0
1091_0000198	2.0	2.0
1091_0000208	1.0	1.0
1091_0000213	2.0	2.0
1091_0000217	2.0	1.0
1091_0000225	2.0	1.0
1091_0000235	1.0	1.0
1091_0000246	2.0	2.0
1091_0000247	2.0	2.0
1091_0000252	1.0	2.0
1091_0000254	2.0	1.0
1091_0000259	2.0	2.0
1091_0000260	1.0	2.0
1091_0000263	3.0	2.0
1091_0000264	2.0	1.0
1091_0000272	1.0	1.0
Averaged weighted F1-scores 0.5718846080739668
130.68810916179336 82.83744971317162
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
LANGUAGE: DE, 0th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.36
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.09
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.45      0.80      0.58        30
         2.0       0.59      0.45      0.51        64
         3.0       0.48      0.75      0.58        67
         4.0       0.00      0.00      0.00        33
         5.0       0.00      0.00      0.00         8

    accuracy                           0.50       206
   macro avg       0.25      0.33      0.28       206
weighted avg       0.41      0.50      0.43       206

[[ 0  3  1  0  0  0]
 [ 0 24  6  0  0  0]
 [ 0 21 29 14  0  0]
 [ 0  5 12 50  0  0]
 [ 0  0  1 32  0  0]
 [ 0  0  0  8  0  0]]
0.43388466826307254
206 206 206



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.09
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.51      0.80      0.62        30
         2.0       0.66      0.69      0.67        64
         3.0       0.53      0.63      0.57        67
         4.0       0.42      0.15      0.22        33
         5.0       0.00      0.00      0.00         8

    accuracy                           0.56       206
   macro avg       0.35      0.38      0.35       206
weighted avg       0.52      0.56      0.52       206

[[ 0  3  1  0  0  0]
 [ 0 24  6  0  0  0]
 [ 0 15 44  5  0  0]
 [ 0  5 15 42  5  0]
 [ 0  0  1 27  5  0]
 [ 0  0  0  6  2  0]]
0.5209354987017478
206 206 206



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.98
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.99
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.50      0.53      0.52        30
         2.0       0.65      0.72      0.68        64
         3.0       0.67      0.45      0.54        67
         4.0       0.41      0.73      0.53        33
         5.0       0.00      0.00      0.00         8

    accuracy                           0.56       206
   macro avg       0.37      0.40      0.38       206
weighted avg       0.56      0.56      0.55       206

[[ 0  3  1  0  0  0]
 [ 0 16 14  0  0  0]
 [ 0  9 46  7  2  0]
 [ 0  4  9 30 24  0]
 [ 0  0  1  8 24  0]
 [ 0  0  0  0  8  0]]
0.5456220210291616
206 206 206



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.84
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.07
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.53      0.27      0.36        30
         2.0       0.62      0.77      0.69        64
         3.0       0.65      0.52      0.58        67
         4.0       0.47      0.82      0.59        33
         5.0       0.00      0.00      0.00         8

    accuracy                           0.58       206
   macro avg       0.38      0.40      0.37       206
weighted avg       0.56      0.58      0.55       206

[[ 0  3  0  1  0  0]
 [ 0  8 22  0  0  0]
 [ 0  1 49 12  2  0]
 [ 0  3  7 35 22  0]
 [ 0  0  1  5 27  0]
 [ 0  0  0  1  7  0]]
0.5479104596492063
206 206 206
Filename	True Label	Prediction
1023_0001420	3.0	4.0
1023_0101683	3.0	3.0
1023_0101693	3.0	4.0
1023_0101843	4.0	4.0
1023_0101845	3.0	3.0
1023_0101894	2.0	4.0
1023_0101897	3.0	3.0
1023_0101899	3.0	4.0
1023_0101907	4.0	4.0
1023_0101909	4.0	4.0
1023_0103823	5.0	4.0
1023_0103824	4.0	4.0
1023_0103840	4.0	4.0
1023_0103883	3.0	4.0
1023_0104203	4.0	4.0
1023_0107773	3.0	4.0
1023_0108304	4.0	4.0
1023_0108305	3.0	4.0
1023_0108520	3.0	3.0
1023_0108810	3.0	3.0
1023_0108811	3.0	3.0
1023_0108888	3.0	4.0
1023_0108889	3.0	3.0
1023_0108955	5.0	4.0
1023_0109192	4.0	4.0
1023_0109249	3.0	4.0
1023_0109250	3.0	4.0
1023_0109395	3.0	4.0
1023_0109495	3.0	3.0
1023_0109500	4.0	4.0
1023_0109505	4.0	4.0
1023_0109519	4.0	4.0
1023_0109527	5.0	4.0
1023_0109716	5.0	3.0
1023_0109717	3.0	4.0
1023_0109891	4.0	4.0
1023_0109915	2.0	3.0
1031_0002006	4.0	3.0
1031_0002036	5.0	4.0
1031_0002079	4.0	4.0
1031_0002085	4.0	4.0
1031_0002199	4.0	4.0
1031_0002200	3.0	4.0
1031_0003012	3.0	3.0
1031_0003023	3.0	4.0
1031_0003029	3.0	4.0
1031_0003048	4.0	4.0
1031_0003052	4.0	4.0
1031_0003072	5.0	4.0
1031_0003074	5.0	4.0
1031_0003076	4.0	4.0
1031_0003077	3.0	3.0
1031_0003090	4.0	4.0
1031_0003127	4.0	3.0
1031_0003144	3.0	3.0
1031_0003145	4.0	4.0
1031_0003160	3.0	4.0
1031_0003169	3.0	4.0
1031_0003172	3.0	4.0
1031_0003184	4.0	4.0
1031_0003191	3.0	3.0
1031_0003212	2.0	4.0
1031_0003216	4.0	4.0
1031_0003219	3.0	4.0
1031_0003224	3.0	4.0
1031_0003232	3.0	4.0
1031_0003235	4.0	4.0
1031_0003238	4.0	4.0
1031_0003336	5.0	4.0
1031_0003356	4.0	3.0
1031_0003357	3.0	4.0
1031_0003368	3.0	3.0
1031_0003387	3.0	4.0
1031_0003389	4.0	3.0
1031_0003391	3.0	3.0
1031_0003393	4.0	4.0
1031_0003409	4.0	3.0
1061_0120274	2.0	2.0
1061_0120278	2.0	3.0
1061_0120279	2.0	2.0
1061_0120284	1.0	1.0
1061_0120295	0.0	3.0
1061_0120297	3.0	3.0
1061_0120302	2.0	2.0
1061_0120311	3.0	3.0
1061_0120313	2.0	2.0
1061_0120316	2.0	3.0
1061_0120321	2.0	3.0
1061_0120326	3.0	3.0
1061_0120328	2.0	2.0
1061_0120332	2.0	2.0
1061_0120345	3.0	3.0
1061_0120346	3.0	3.0
1061_0120354	1.0	2.0
1061_0120356	2.0	2.0
1061_0120374	3.0	3.0
1061_0120382	2.0	2.0
1061_0120386	1.0	2.0
1061_0120391	1.0	2.0
1061_0120394	3.0	3.0
1061_0120410	3.0	2.0
1061_0120413	2.0	2.0
1061_0120423	3.0	3.0
1061_0120425	2.0	3.0
1061_0120428	3.0	3.0
1061_0120430	2.0	2.0
1061_0120439	2.0	2.0
1061_0120441	2.0	2.0
1061_0120443	1.0	2.0
1061_0120458	4.0	4.0
1061_0120491	4.0	2.0
1061_0120495	3.0	3.0
1061_0120497	4.0	4.0
1061_0120853	3.0	3.0
1061_0120855	2.0	2.0
1061_0120874	3.0	3.0
1061_0120877	3.0	3.0
1061_0120878	2.0	2.0
1061_0120881	3.0	3.0
1061_0120884	3.0	3.0
1061_0120886	2.0	3.0
1061_0120888	2.0	3.0
1061_0120889	2.0	2.0
1061_1029113	2.0	3.0
1061_1029118	2.0	2.0
1061_1202917	2.0	2.0
1071_0024686	2.0	2.0
1071_0024693	2.0	2.0
1071_0024716	3.0	1.0
1071_0024757	2.0	2.0
1071_0024763	1.0	2.0
1071_0024768	1.0	2.0
1071_0024769	1.0	2.0
1071_0024770	1.0	2.0
1071_0024781	1.0	2.0
1071_0024782	0.0	1.0
1071_0024784	1.0	1.0
1071_0024802	2.0	2.0
1071_0024809	1.0	1.0
1071_0024821	1.0	1.0
1071_0024822	1.0	2.0
1071_0024827	2.0	2.0
1071_0024836	2.0	2.0
1071_0024847	2.0	2.0
1071_0024850	1.0	2.0
1071_0024856	1.0	2.0
1071_0024859	2.0	2.0
1071_0024861	1.0	2.0
1071_0024865	2.0	2.0
1071_0024878	2.0	2.0
1071_0024881	3.0	2.0
1071_0242011	2.0	2.0
1071_0242022	1.0	2.0
1071_0242023	1.0	2.0
1071_0242043	1.0	1.0
1071_0242093	0.0	1.0
1071_0243502	2.0	1.0
1071_0248308	2.0	2.0
1071_0248311	2.0	2.0
1071_0248323	2.0	2.0
1071_0248327	0.0	1.0
1071_0248333	2.0	2.0
1071_0248344	1.0	1.0
1071_0248348	2.0	2.0
1071_0248349	1.0	1.0
1091_0000002	3.0	2.0
1091_0000004	1.0	2.0
1091_0000006	1.0	2.0
1091_0000007	3.0	3.0
1091_0000010	3.0	3.0
1091_0000016	1.0	2.0
1091_0000019	2.0	2.0
1091_0000020	3.0	2.0
1091_0000031	2.0	2.0
1091_0000041	1.0	1.0
1091_0000044	1.0	2.0
1091_0000053	1.0	2.0
1091_0000055	2.0	2.0
1091_0000057	3.0	1.0
1091_0000067	3.0	2.0
1091_0000069	3.0	1.0
1091_0000074	2.0	2.0
1091_0000095	2.0	2.0
1091_0000102	2.0	2.0
1091_0000123	3.0	3.0
1091_0000148	2.0	2.0
1091_0000153	2.0	3.0
1091_0000160	3.0	3.0
1091_0000164	2.0	2.0
1091_0000165	1.0	2.0
1091_0000172	2.0	2.0
1091_0000192	2.0	3.0
1091_0000201	3.0	3.0
1091_0000211	2.0	2.0
1091_0000216	2.0	2.0
1091_0000225	3.0	2.0
1091_0000226	3.0	2.0
1091_0000231	3.0	3.0
1091_0000234	2.0	3.0
1091_0000235	2.0	2.0
1091_0000241	2.0	2.0
1091_0000247	2.0	2.0
1091_0000253	2.0	2.0
1091_0000266	2.0	3.0
1091_0000270	2.0	2.0
1091_0000272	1.0	2.0
LANGUAGE: DE, 1th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.29
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.17
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         3
         1.0       0.00      0.00      0.00        30
         2.0       0.47      0.95      0.63        64
         3.0       0.53      0.59      0.56        68
         4.0       0.00      0.00      0.00        32
         5.0       0.00      0.00      0.00         8

    accuracy                           0.49       205
   macro avg       0.17      0.26      0.20       205
weighted avg       0.32      0.49      0.38       205

[[ 0  0  3  0  0  0]
 [ 0  0 30  0  0  0]
 [ 0  0 61  3  0  0]
 [ 0  0 28 40  0  0]
 [ 0  0  6 26  0  0]
 [ 0  0  1  7  0  0]]
0.38162798208292964
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.08
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.11
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         3
         1.0       0.58      0.23      0.33        30
         2.0       0.56      0.55      0.55        64
         3.0       0.51      0.87      0.64        68
         4.0       0.64      0.28      0.39        32
         5.0       0.00      0.00      0.00         8

    accuracy                           0.54       205
   macro avg       0.38      0.32      0.32       205
weighted avg       0.53      0.54      0.49       205

[[ 0  2  1  0  0  0]
 [ 0  7 22  1  0  0]
 [ 0  1 35 28  0  0]
 [ 0  1  5 59  3  0]
 [ 0  1  0 22  9  0]
 [ 0  0  0  6  2  0]]
0.4946635382136087
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.91
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.08
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         3
         1.0       0.58      0.37      0.45        30
         2.0       0.61      0.67      0.64        64
         3.0       0.53      0.38      0.44        68
         4.0       0.39      0.81      0.53        32
         5.0       0.00      0.00      0.00         8

    accuracy                           0.52       205
   macro avg       0.35      0.37      0.34       205
weighted avg       0.51      0.52      0.50       205

[[ 0  2  1  0  0  0]
 [ 0 11 19  0  0  0]
 [ 0  3 43 18  0  0]
 [ 0  2  6 26 34  0]
 [ 0  1  1  4 26  0]
 [ 0  0  0  1  7  0]]
0.49548447634671566
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.78
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.04
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         3
         1.0       0.61      0.37      0.46        30
         2.0       0.58      0.77      0.66        64
         3.0       0.64      0.40      0.49        68
         4.0       0.44      0.84      0.58        32
         5.0       0.00      0.00      0.00         8

    accuracy                           0.56       205
   macro avg       0.38      0.40      0.37       205
weighted avg       0.55      0.56      0.53       205

[[ 0  2  1  0  0  0]
 [ 0 11 19  0  0  0]
 [ 0  3 49 12  0  0]
 [ 0  1 13 27 27  0]
 [ 0  1  1  3 27  0]
 [ 0  0  1  0  7  0]]
0.5272723986413994
205 205 205
Filename	True Label	Prediction
1023_0001423	4.0	4.0
1023_0101675	4.0	4.0
1023_0101694	3.0	4.0
1023_0101700	4.0	4.0
1023_0101841	3.0	3.0
1023_0101844	3.0	4.0
1023_0101853	3.0	4.0
1023_0101854	2.0	2.0
1023_0101855	2.0	3.0
1023_0103821	4.0	4.0
1023_0103829	4.0	4.0
1023_0103831	4.0	4.0
1023_0103838	4.0	4.0
1023_0103841	5.0	4.0
1023_0103844	4.0	4.0
1023_0104209	3.0	4.0
1023_0106816	5.0	4.0
1023_0107672	3.0	4.0
1023_0107682	2.0	1.0
1023_0107780	4.0	4.0
1023_0107784	3.0	2.0
1023_0107787	3.0	4.0
1023_0107788	3.0	4.0
1023_0108307	5.0	4.0
1023_0108423	3.0	2.0
1023_0108648	4.0	4.0
1023_0108752	5.0	4.0
1023_0108753	4.0	4.0
1023_0108932	3.0	4.0
1023_0108933	4.0	4.0
1023_0108992	3.0	3.0
1023_0109027	2.0	2.0
1023_0109151	2.0	3.0
1023_0109396	3.0	4.0
1023_0109422	5.0	4.0
1023_0109518	3.0	3.0
1023_0109528	3.0	4.0
1023_0109588	3.0	4.0
1023_0109614	3.0	3.0
1023_0109649	3.0	4.0
1023_0109651	4.0	4.0
1023_0109721	3.0	3.0
1023_0109878	3.0	4.0
1023_0109880	4.0	4.0
1023_0109914	3.0	3.0
1023_0109951	3.0	3.0
1031_0002032	4.0	4.0
1031_0002042	4.0	4.0
1031_0002084	4.0	4.0
1031_0002086	3.0	4.0
1031_0002091	3.0	4.0
1031_0002092	4.0	4.0
1031_0002195	3.0	3.0
1031_0002196	4.0	4.0
1031_0003054	3.0	4.0
1031_0003071	3.0	4.0
1031_0003129	3.0	3.0
1031_0003130	5.0	4.0
1031_0003136	4.0	4.0
1031_0003146	4.0	4.0
1031_0003161	4.0	4.0
1031_0003165	3.0	4.0
1031_0003166	3.0	3.0
1031_0003174	3.0	3.0
1031_0003180	4.0	4.0
1031_0003183	4.0	4.0
1031_0003185	3.0	3.0
1031_0003187	3.0	4.0
1031_0003189	4.0	4.0
1031_0003211	3.0	4.0
1031_0003240	3.0	4.0
1031_0003262	3.0	4.0
1031_0003313	3.0	4.0
1031_0003315	3.0	3.0
1031_0003330	3.0	4.0
1031_0003338	4.0	4.0
1031_0003353	3.0	3.0
1031_0003355	3.0	3.0
1031_0003390	5.0	4.0
1061_0120272	2.0	2.0
1061_0120273	2.0	2.0
1061_0120286	1.0	2.0
1061_0120289	2.0	2.0
1061_0120299	3.0	3.0
1061_0120306	4.0	3.0
1061_0120310	2.0	2.0
1061_0120315	2.0	1.0
1061_0120319	2.0	2.0
1061_0120320	3.0	4.0
1061_0120324	2.0	2.0
1061_0120327	3.0	4.0
1061_0120330	4.0	3.0
1061_0120347	1.0	2.0
1061_0120349	2.0	2.0
1061_0120359	2.0	2.0
1061_0120360	3.0	4.0
1061_0120366	3.0	3.0
1061_0120367	3.0	3.0
1061_0120370	2.0	2.0
1061_0120376	2.0	3.0
1061_0120383	4.0	4.0
1061_0120387	2.0	3.0
1061_0120389	3.0	3.0
1061_0120406	3.0	3.0
1061_0120409	3.0	3.0
1061_0120460	2.0	2.0
1061_0120480	3.0	3.0
1061_0120489	3.0	3.0
1061_0120858	2.0	3.0
1061_0120883	3.0	2.0
1061_0120890	2.0	2.0
1061_1202910	3.0	3.0
1061_1202911	1.0	2.0
1061_1202913	2.0	2.0
1071_0020001	2.0	2.0
1071_0024683	1.0	1.0
1071_0024685	2.0	2.0
1071_0024688	1.0	2.0
1071_0024692	4.0	3.0
1071_0024704	2.0	2.0
1071_0024708	2.0	2.0
1071_0024711	1.0	1.0
1071_0024712	2.0	2.0
1071_0024714	2.0	2.0
1071_0024773	1.0	2.0
1071_0024775	0.0	2.0
1071_0024776	1.0	1.0
1071_0024778	0.0	1.0
1071_0024799	3.0	2.0
1071_0024823	2.0	2.0
1071_0024831	1.0	2.0
1071_0024835	2.0	2.0
1071_0024844	1.0	1.0
1071_0024848	1.0	2.0
1071_0024852	1.0	1.0
1071_0024862	2.0	2.0
1071_0024863	1.0	2.0
1071_0024866	5.0	2.0
1071_0024876	2.0	2.0
1071_0024877	2.0	2.0
1071_0242041	1.0	2.0
1071_0242072	0.0	1.0
1071_0243581	1.0	1.0
1071_0243582	1.0	2.0
1071_0243593	2.0	2.0
1071_0243623	2.0	2.0
1071_0248309	2.0	2.0
1071_0248313	2.0	2.0
1071_0248314	1.0	2.0
1071_0248316	1.0	1.0
1071_0248319	1.0	1.0
1071_0248321	3.0	2.0
1071_0248325	1.0	2.0
1071_0248329	2.0	2.0
1071_0248342	1.0	2.0
1071_0248345	2.0	2.0
1091_0000005	3.0	2.0
1091_0000008	3.0	2.0
1091_0000015	2.0	2.0
1091_0000017	3.0	3.0
1091_0000018	3.0	2.0
1091_0000022	2.0	3.0
1091_0000023	4.0	1.0
1091_0000028	1.0	1.0
1091_0000030	1.0	2.0
1091_0000033	2.0	2.0
1091_0000036	3.0	3.0
1091_0000038	2.0	2.0
1091_0000043	2.0	2.0
1091_0000045	2.0	3.0
1091_0000046	2.0	2.0
1091_0000052	1.0	1.0
1091_0000061	3.0	1.0
1091_0000062	3.0	2.0
1091_0000065	1.0	2.0
1091_0000066	1.0	1.0
1091_0000071	2.0	2.0
1091_0000072	1.0	2.0
1091_0000078	3.0	2.0
1091_0000092	2.0	2.0
1091_0000114	2.0	3.0
1091_0000126	3.0	2.0
1091_0000140	2.0	1.0
1091_0000145	1.0	2.0
1091_0000152	2.0	2.0
1091_0000155	3.0	3.0
1091_0000156	2.0	3.0
1091_0000158	2.0	3.0
1091_0000163	2.0	2.0
1091_0000166	2.0	2.0
1091_0000167	2.0	3.0
1091_0000190	1.0	2.0
1091_0000191	2.0	2.0
1091_0000206	1.0	2.0
1091_0000209	3.0	2.0
1091_0000210	2.0	2.0
1091_0000212	2.0	2.0
1091_0000214	3.0	2.0
1091_0000228	2.0	2.0
1091_0000238	2.0	2.0
1091_0000240	2.0	2.0
1091_0000252	2.0	2.0
1091_0000257	2.0	3.0
1091_0000261	2.0	2.0
1091_0000263	4.0	2.0
LANGUAGE: DE, 2th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.33
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.18
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         3
         1.0       0.49      0.80      0.61        30
         2.0       0.53      0.53      0.53        64
         3.0       0.46      0.62      0.52        68
         4.0       0.00      0.00      0.00        32
         5.0       0.00      0.00      0.00         8

    accuracy                           0.49       205
   macro avg       0.25      0.32      0.28       205
weighted avg       0.39      0.49      0.43       205

[[ 0  3  0  0  0  0]
 [ 0 24  6  0  0  0]
 [ 0 18 34 12  0  0]
 [ 0  4 22 42  0  0]
 [ 0  0  2 30  0  0]
 [ 0  0  0  8  0  0]]
0.4289163322012967
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.08
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         3
         1.0       0.73      0.53      0.62        30
         2.0       0.57      0.73      0.64        64
         3.0       0.46      0.68      0.55        68
         4.0       0.00      0.00      0.00        32
         5.0       0.00      0.00      0.00         8

    accuracy                           0.53       205
   macro avg       0.29      0.32      0.30       205
weighted avg       0.44      0.53      0.47       205

[[ 0  3  0  0  0  0]
 [ 0 16 14  0  0  0]
 [ 0  3 47 14  0  0]
 [ 0  0 21 46  1  0]
 [ 0  0  0 32  0  0]
 [ 0  0  0  7  1  0]]
0.47379558906294933
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.93
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.17
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         3
         1.0       0.65      0.37      0.47        30
         2.0       0.54      0.52      0.53        64
         3.0       0.49      0.47      0.48        68
         4.0       0.47      0.91      0.62        32
         5.0       0.00      0.00      0.00         8

    accuracy                           0.51       205
   macro avg       0.36      0.38      0.35       205
weighted avg       0.50      0.51      0.49       205

[[ 0  3  0  0  0  0]
 [ 0 11 17  2  0  0]
 [ 0  3 33 28  0  0]
 [ 0  0 11 32 25  0]
 [ 0  0  0  3 29  0]
 [ 0  0  0  0  8  0]]
0.4892733587991776
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.79
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.03
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         3
         1.0       0.70      0.53      0.60        30
         2.0       0.59      0.59      0.59        64
         3.0       0.58      0.54      0.56        68
         4.0       0.50      0.84      0.63        32
         5.0       0.00      0.00      0.00         8

    accuracy                           0.58       205
   macro avg       0.39      0.42      0.40       205
weighted avg       0.56      0.58      0.56       205

[[ 0  3  0  0  0  0]
 [ 0 16 14  0  0  0]
 [ 0  4 38 22  0  0]
 [ 0  0 12 37 19  0]
 [ 0  0  0  5 27  0]
 [ 0  0  0  0  8  0]]
0.5576948435326629
205 205 205
Filename	True Label	Prediction
1023_0001575	5.0	4.0
1023_0101690	2.0	3.0
1023_0101749	4.0	4.0
1023_0101751	3.0	4.0
1023_0101856	2.0	3.0
1023_0101898	3.0	3.0
1023_0101906	3.0	3.0
1023_0102118	3.0	4.0
1023_0103826	4.0	4.0
1023_0103827	3.0	3.0
1023_0103828	2.0	2.0
1023_0103833	5.0	4.0
1023_0103837	5.0	4.0
1023_0104207	3.0	4.0
1023_0107074	3.0	4.0
1023_0107783	3.0	3.0
1023_0108422	4.0	4.0
1023_0108510	4.0	4.0
1023_0108650	3.0	4.0
1023_0108751	3.0	3.0
1023_0108766	3.0	3.0
1023_0108813	4.0	3.0
1023_0108814	4.0	3.0
1023_0108890	4.0	3.0
1023_0108931	4.0	4.0
1023_0108958	3.0	4.0
1023_0108993	4.0	4.0
1023_0109022	3.0	4.0
1023_0109026	3.0	3.0
1023_0109030	3.0	4.0
1023_0109096	4.0	4.0
1023_0109248	3.0	4.0
1023_0109267	3.0	3.0
1023_0109401	3.0	3.0
1023_0109402	3.0	3.0
1023_0109496	3.0	4.0
1023_0109516	5.0	4.0
1023_0109609	3.0	3.0
1023_0109946	2.0	3.0
1031_0001997	3.0	3.0
1031_0002004	3.0	3.0
1031_0002011	5.0	4.0
1031_0002043	4.0	4.0
1031_0002061	4.0	4.0
1031_0002187	4.0	4.0
1031_0002198	4.0	4.0
1031_0003013	5.0	4.0
1031_0003043	5.0	4.0
1031_0003078	3.0	4.0
1031_0003085	4.0	4.0
1031_0003091	2.0	3.0
1031_0003098	4.0	4.0
1031_0003131	3.0	4.0
1031_0003132	3.0	4.0
1031_0003140	4.0	4.0
1031_0003150	4.0	4.0
1031_0003156	3.0	4.0
1031_0003157	4.0	4.0
1031_0003162	4.0	4.0
1031_0003167	3.0	4.0
1031_0003170	4.0	4.0
1031_0003173	3.0	4.0
1031_0003181	4.0	4.0
1031_0003190	3.0	4.0
1031_0003203	2.0	3.0
1031_0003205	4.0	4.0
1031_0003207	4.0	4.0
1031_0003218	4.0	4.0
1031_0003233	3.0	3.0
1031_0003236	4.0	4.0
1031_0003242	3.0	4.0
1031_0003244	4.0	4.0
1031_0003274	4.0	4.0
1031_0003327	2.0	3.0
1031_0003354	3.0	4.0
1031_0003358	5.0	4.0
1031_0003365	3.0	3.0
1031_0003384	2.0	3.0
1031_0003407	3.0	3.0
1031_0003408	2.0	3.0
1031_0003415	4.0	4.0
1061_0120277	2.0	3.0
1061_0120281	2.0	2.0
1061_0120282	1.0	2.0
1061_0120288	3.0	3.0
1061_0120296	2.0	2.0
1061_0120301	2.0	2.0
1061_0120304	2.0	2.0
1061_0120307	3.0	2.0
1061_0120312	2.0	2.0
1061_0120318	3.0	3.0
1061_0120333	2.0	3.0
1061_0120336	2.0	3.0
1061_0120343	3.0	3.0
1061_0120350	4.0	3.0
1061_0120357	3.0	3.0
1061_0120372	3.0	3.0
1061_0120388	2.0	2.0
1061_0120405	2.0	2.0
1061_0120407	3.0	3.0
1061_0120411	4.0	4.0
1061_0120414	4.0	3.0
1061_0120421	3.0	3.0
1061_0120424	3.0	3.0
1061_0120426	2.0	3.0
1061_0120427	3.0	3.0
1061_0120429	2.0	3.0
1061_0120431	3.0	3.0
1061_0120455	3.0	3.0
1061_0120457	3.0	2.0
1061_0120459	3.0	3.0
1061_0120478	3.0	3.0
1061_0120482	2.0	3.0
1061_0120486	2.0	3.0
1061_0120487	2.0	3.0
1061_0120490	3.0	3.0
1061_0120494	2.0	2.0
1061_0120856	1.0	2.0
1061_0120859	3.0	3.0
1061_0120885	3.0	3.0
1061_1029111	3.0	3.0
1061_1029117	2.0	3.0
1061_1029119	3.0	3.0
1061_1029120	1.0	2.0
1061_1202912	3.0	3.0
1061_1202914	2.0	2.0
1061_1202916	2.0	3.0
1071_0024680	2.0	2.0
1071_0024681	2.0	2.0
1071_0024682	3.0	3.0
1071_0024694	2.0	2.0
1071_0024702	2.0	2.0
1071_0024703	2.0	2.0
1071_0024705	2.0	2.0
1071_0024759	1.0	2.0
1071_0024761	2.0	2.0
1071_0024766	1.0	1.0
1071_0024777	1.0	2.0
1071_0024797	1.0	1.0
1071_0024798	1.0	1.0
1071_0024801	1.0	2.0
1071_0024803	1.0	2.0
1071_0024804	1.0	2.0
1071_0024810	2.0	1.0
1071_0024812	1.0	1.0
1071_0024817	1.0	1.0
1071_0024820	1.0	1.0
1071_0024824	2.0	1.0
1071_0024825	1.0	1.0
1071_0024838	1.0	1.0
1071_0024846	0.0	1.0
1071_0024854	1.0	2.0
1071_0024855	2.0	2.0
1071_0024857	1.0	1.0
1071_0024867	3.0	2.0
1071_0241831	1.0	2.0
1071_0241833	2.0	2.0
1071_0242013	2.0	2.0
1071_0242042	1.0	1.0
1071_0243501	2.0	2.0
1071_0243592	2.0	2.0
1071_0248301	2.0	2.0
1071_0248303	1.0	1.0
1071_0248304	1.0	2.0
1071_0248307	3.0	2.0
1071_0248315	0.0	1.0
1071_0248318	0.0	1.0
1071_0248326	2.0	2.0
1071_0248328	1.0	1.0
1071_0248337	2.0	2.0
1071_0248340	1.0	1.0
1091_0000014	1.0	1.0
1091_0000021	1.0	2.0
1091_0000025	1.0	1.0
1091_0000034	3.0	2.0
1091_0000047	3.0	2.0
1091_0000048	1.0	1.0
1091_0000049	2.0	2.0
1091_0000058	3.0	2.0
1091_0000064	2.0	2.0
1091_0000068	2.0	2.0
1091_0000070	3.0	2.0
1091_0000075	2.0	3.0
1091_0000076	2.0	3.0
1091_0000144	2.0	1.0
1091_0000169	3.0	2.0
1091_0000174	2.0	1.0
1091_0000194	2.0	2.0
1091_0000199	3.0	2.0
1091_0000200	3.0	3.0
1091_0000205	2.0	3.0
1091_0000208	1.0	2.0
1091_0000213	2.0	2.0
1091_0000217	3.0	2.0
1091_0000219	2.0	2.0
1091_0000236	3.0	2.0
1091_0000237	2.0	2.0
1091_0000242	1.0	2.0
1091_0000244	2.0	3.0
1091_0000245	2.0	2.0
1091_0000248	2.0	2.0
1091_0000264	2.0	2.0
1091_0000268	2.0	2.0
1091_0000269	2.0	2.0
1091_0000274	2.0	2.0
LANGUAGE: DE, 3th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.31
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.23
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.00      0.00      0.00        30
         2.0       0.44      0.42      0.43        64
         3.0       0.43      0.91      0.59        68
         4.0       0.00      0.00      0.00        32
         5.0       0.00      0.00      0.00         7

    accuracy                           0.43       205
   macro avg       0.14      0.22      0.17       205
weighted avg       0.28      0.43      0.33       205

[[ 0  0  4  0  0  0]
 [ 0  0 25  5  0  0]
 [ 0  0 27 37  0  0]
 [ 0  0  6 62  0  0]
 [ 0  0  0 32  0  0]
 [ 0  0  0  7  0  0]]
0.3287349109103819
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.04
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.25
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.55      0.37      0.44        30
         2.0       0.55      0.41      0.47        64
         3.0       0.42      0.46      0.44        68
         4.0       0.38      0.75      0.50        32
         5.0       0.00      0.00      0.00         7

    accuracy                           0.45       205
   macro avg       0.32      0.33      0.31       205
weighted avg       0.45      0.45      0.43       205

[[ 0  4  0  0  0  0]
 [ 0 11 15  4  0  0]
 [ 0  4 26 31  3  0]
 [ 0  1  6 31 30  0]
 [ 0  0  0  8 24  0]
 [ 0  0  0  0  7  0]]
0.4335225503758654
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.84
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.03
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.50      0.70      0.58        30
         2.0       0.65      0.58      0.61        64
         3.0       0.59      0.59      0.59        68
         4.0       0.42      0.50      0.46        32
         5.0       0.00      0.00      0.00         7

    accuracy                           0.56       205
   macro avg       0.36      0.39      0.37       205
weighted avg       0.54      0.56      0.54       205

[[ 0  4  0  0  0  0]
 [ 0 21  9  0  0  0]
 [ 0 14 37 10  3  0]
 [ 0  3 11 40 14  0]
 [ 0  0  0 16 16  0]
 [ 0  0  0  2  5  0]]
0.5427759380309269
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.71
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.09
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.51      0.67      0.58        30
         2.0       0.68      0.56      0.62        64
         3.0       0.64      0.54      0.59        68
         4.0       0.49      0.84      0.62        32
         5.0       0.00      0.00      0.00         7

    accuracy                           0.59       205
   macro avg       0.39      0.44      0.40       205
weighted avg       0.58      0.59      0.57       205

[[ 0  4  0  0  0  0]
 [ 0 20  9  1  0  0]
 [ 0 12 36 13  3  0]
 [ 0  3  8 37 20  0]
 [ 0  0  0  5 27  0]
 [ 0  0  0  2  5  0]]
0.5686560811437397
205 205 205
Filename	True Label	Prediction
1023_0001418	3.0	3.0
1023_0001422	2.0	2.0
1023_0101688	4.0	4.0
1023_0101701	3.0	3.0
1023_0101752	3.0	4.0
1023_0101753	4.0	4.0
1023_0101847	2.0	4.0
1023_0101848	3.0	3.0
1023_0101849	2.0	3.0
1023_0101852	4.0	4.0
1023_0101893	3.0	4.0
1023_0101895	3.0	4.0
1023_0101896	2.0	2.0
1023_0101900	3.0	3.0
1023_0103822	3.0	3.0
1023_0103832	3.0	3.0
1023_0103834	3.0	4.0
1023_0107042	3.0	3.0
1023_0107075	2.0	3.0
1023_0107244	4.0	4.0
1023_0107725	3.0	3.0
1023_0107726	3.0	3.0
1023_0107781	4.0	4.0
1023_0108426	3.0	4.0
1023_0108518	4.0	4.0
1023_0108641	5.0	4.0
1023_0108649	4.0	4.0
1023_0108812	3.0	4.0
1023_0108885	2.0	3.0
1023_0108886	4.0	4.0
1023_0108934	4.0	4.0
1023_0109033	4.0	4.0
1023_0109038	5.0	4.0
1023_0109247	4.0	4.0
1023_0109391	4.0	4.0
1023_0109515	4.0	4.0
1023_0109520	3.0	3.0
1023_0109522	3.0	4.0
1023_0109524	4.0	4.0
1023_0109590	3.0	3.0
1023_0109674	3.0	4.0
1023_0109954	3.0	3.0
1023_0111896	3.0	3.0
1031_0001703	4.0	4.0
1031_0002002	3.0	3.0
1031_0002005	3.0	3.0
1031_0002010	4.0	4.0
1031_0002040	5.0	4.0
1031_0002083	3.0	4.0
1031_0002089	4.0	4.0
1031_0002131	3.0	4.0
1031_0002197	3.0	4.0
1031_0003035	3.0	3.0
1031_0003065	3.0	3.0
1031_0003073	5.0	4.0
1031_0003088	4.0	4.0
1031_0003092	2.0	4.0
1031_0003095	3.0	3.0
1031_0003099	3.0	3.0
1031_0003121	5.0	3.0
1031_0003126	3.0	3.0
1031_0003135	4.0	4.0
1031_0003141	3.0	4.0
1031_0003163	3.0	3.0
1031_0003164	4.0	3.0
1031_0003179	4.0	4.0
1031_0003186	5.0	3.0
1031_0003206	3.0	3.0
1031_0003217	4.0	4.0
1031_0003220	4.0	4.0
1031_0003221	2.0	4.0
1031_0003226	5.0	4.0
1031_0003261	3.0	3.0
1031_0003272	3.0	3.0
1031_0003309	3.0	4.0
1031_0003331	3.0	4.0
1031_0003337	3.0	4.0
1031_0003366	3.0	4.0
1031_0003367	4.0	4.0
1031_0003369	4.0	4.0
1031_0003386	3.0	3.0
1031_0003392	3.0	4.0
1031_0003419	4.0	4.0
1061_0120275	2.0	2.0
1061_0120285	2.0	3.0
1061_0120287	2.0	2.0
1061_0120291	2.0	2.0
1061_0120298	2.0	2.0
1061_0120303	1.0	2.0
1061_0120308	3.0	3.0
1061_0120309	2.0	1.0
1061_0120314	3.0	2.0
1061_0120331	1.0	2.0
1061_0120334	3.0	3.0
1061_0120337	3.0	3.0
1061_0120351	2.0	3.0
1061_0120353	1.0	1.0
1061_0120355	2.0	1.0
1061_0120361	3.0	3.0
1061_0120368	3.0	2.0
1061_0120369	2.0	2.0
1061_0120371	3.0	3.0
1061_0120375	2.0	1.0
1061_0120403	4.0	3.0
1061_0120415	2.0	2.0
1061_0120438	4.0	4.0
1061_0120442	2.0	3.0
1061_0120448	4.0	3.0
1061_0120449	4.0	4.0
1061_0120450	2.0	3.0
1061_0120453	3.0	3.0
1061_0120479	3.0	3.0
1061_0120481	4.0	3.0
1061_0120484	3.0	4.0
1061_0120492	3.0	4.0
1061_0120498	3.0	4.0
1061_0120857	3.0	3.0
1061_0120876	2.0	3.0
1061_0120882	4.0	3.0
1061_0120887	2.0	2.0
1061_1029112	3.0	3.0
1061_1202915	2.0	2.0
1071_0024678	2.0	1.0
1071_0024762	1.0	1.0
1071_0024765	1.0	1.0
1071_0024774	0.0	1.0
1071_0024779	2.0	2.0
1071_0024783	0.0	1.0
1071_0024806	1.0	1.0
1071_0024808	1.0	1.0
1071_0024811	1.0	1.0
1071_0024813	1.0	1.0
1071_0024814	1.0	2.0
1071_0024815	1.0	1.0
1071_0024826	2.0	1.0
1071_0024833	2.0	2.0
1071_0024834	3.0	3.0
1071_0024843	1.0	1.0
1071_0024851	1.0	1.0
1071_0024853	2.0	1.0
1071_0024860	2.0	1.0
1071_0024873	2.0	1.0
1071_0024874	2.0	1.0
1071_0024875	3.0	2.0
1071_0024879	3.0	1.0
1071_0241832	1.0	2.0
1071_0242012	2.0	2.0
1071_0242021	2.0	2.0
1071_0242071	0.0	1.0
1071_0242073	1.0	2.0
1071_0242091	1.0	1.0
1071_0243591	2.0	2.0
1071_0243621	2.0	2.0
1071_0243622	1.0	1.0
1071_0248302	1.0	1.0
1071_0248305	0.0	1.0
1071_0248310	1.0	1.0
1071_0248332	3.0	3.0
1071_0248335	2.0	2.0
1071_0248336	2.0	1.0
1071_0248338	2.0	2.0
1071_0248341	1.0	1.0
1071_0248343	1.0	1.0
1071_0248346	1.0	1.0
1071_0248350	1.0	1.0
1091_0000003	2.0	2.0
1091_0000011	2.0	2.0
1091_0000013	1.0	1.0
1091_0000024	3.0	1.0
1091_0000027	1.0	2.0
1091_0000032	2.0	3.0
1091_0000037	2.0	1.0
1091_0000051	1.0	1.0
1091_0000056	2.0	2.0
1091_0000059	2.0	3.0
1091_0000060	3.0	3.0
1091_0000063	2.0	2.0
1091_0000073	3.0	2.0
1091_0000077	3.0	1.0
1091_0000086	2.0	2.0
1091_0000113	2.0	3.0
1091_0000116	3.0	2.0
1091_0000125	3.0	2.0
1091_0000161	2.0	2.0
1091_0000168	2.0	3.0
1091_0000185	2.0	1.0
1091_0000193	2.0	2.0
1091_0000195	2.0	2.0
1091_0000197	1.0	3.0
1091_0000198	2.0	2.0
1091_0000218	3.0	2.0
1091_0000222	2.0	2.0
1091_0000223	2.0	2.0
1091_0000232	2.0	2.0
1091_0000243	1.0	2.0
1091_0000246	3.0	2.0
1091_0000249	2.0	2.0
1091_0000250	2.0	2.0
1091_0000251	2.0	2.0
1091_0000255	1.0	2.0
1091_0000256	2.0	2.0
1091_0000259	2.0	2.0
1091_0000260	2.0	3.0
1091_0000262	2.0	2.0
1091_0000273	1.0	2.0
LANGUAGE: DE, 4th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.32
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.14
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.00      0.00      0.00        30
         2.0       0.47      0.68      0.56        65
         3.0       0.44      0.58      0.50        67
         4.0       0.35      0.25      0.29        32
         5.0       0.00      0.00      0.00         7

    accuracy                           0.44       205
   macro avg       0.21      0.25      0.23       205
weighted avg       0.35      0.44      0.39       205

[[ 0  1  3  0  0  0]
 [ 0  0 29  1  0  0]
 [ 0  0 44 21  0  0]
 [ 0  0 17 39 11  0]
 [ 0  0  0 24  8  0]
 [ 0  0  0  3  4  0]]
0.38647683700935903
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.10
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.07
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.56      0.50      0.53        30
         2.0       0.56      0.63      0.59        65
         3.0       0.49      0.51      0.50        67
         4.0       0.46      0.50      0.48        32
         5.0       0.00      0.00      0.00         7

    accuracy                           0.52       205
   macro avg       0.34      0.36      0.35       205
weighted avg       0.49      0.52      0.50       205

[[ 0  4  0  0  0  0]
 [ 0 15 15  0  0  0]
 [ 0  7 41 17  0  0]
 [ 0  1 17 34 15  0]
 [ 0  0  0 16 16  0]
 [ 0  0  0  3  4  0]]
0.5022035055225104
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.92
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.13
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.58      0.37      0.45        30
         2.0       0.54      0.57      0.55        65
         3.0       0.47      0.46      0.47        67
         4.0       0.45      0.72      0.55        32
         5.0       0.00      0.00      0.00         7

    accuracy                           0.50       205
   macro avg       0.34      0.35      0.34       205
weighted avg       0.48      0.50      0.48       205

[[ 0  4  0  0  0  0]
 [ 0 11 18  1  0  0]
 [ 0  3 37 24  1  0]
 [ 0  1 14 31 21  0]
 [ 0  0  0  9 23  0]
 [ 0  0  0  1  6  0]]
0.47967284199495025
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.76
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.14
              precision    recall  f1-score   support

         0.0       1.00      0.25      0.40         4
         1.0       0.62      0.50      0.56        30
         2.0       0.57      0.57      0.57        65
         3.0       0.47      0.42      0.44        67
         4.0       0.41      0.72      0.52        32
         5.0       0.00      0.00      0.00         7

    accuracy                           0.51       205
   macro avg       0.51      0.41      0.42       205
weighted avg       0.51      0.51      0.50       205

[[ 1  3  0  0  0  0]
 [ 0 15 14  1  0  0]
 [ 0  5 37 20  3  0]
 [ 0  1 14 28 24  0]
 [ 0  0  0  9 23  0]
 [ 0  0  0  1  6  0]]
0.4964474008376448
205 205 205
Filename	True Label	Prediction
1023_0001416	4.0	3.0
1023_0001419	4.0	4.0
1023_0101684	3.0	4.0
1023_0101689	2.0	2.0
1023_0101691	3.0	4.0
1023_0101695	4.0	3.0
1023_0101846	4.0	4.0
1023_0101851	3.0	3.0
1023_0101901	4.0	4.0
1023_0101904	2.0	3.0
1023_0102117	4.0	3.0
1023_0103825	4.0	4.0
1023_0103830	3.0	3.0
1023_0103836	5.0	4.0
1023_0103839	3.0	4.0
1023_0103843	3.0	4.0
1023_0103880	4.0	4.0
1023_0103955	3.0	4.0
1023_0104206	3.0	3.0
1023_0107727	5.0	4.0
1023_0107729	3.0	4.0
1023_0107740	3.0	3.0
1023_0108306	4.0	4.0
1023_0108815	4.0	4.0
1023_0108887	3.0	3.0
1023_0108908	4.0	3.0
1023_0108935	4.0	4.0
1023_0109029	2.0	3.0
1023_0109039	4.0	4.0
1023_0109392	3.0	3.0
1023_0109399	3.0	3.0
1023_0109400	3.0	4.0
1023_0109591	3.0	3.0
1023_0109606	5.0	4.0
1023_0109671	3.0	3.0
1023_0109890	4.0	4.0
1023_0109917	4.0	4.0
1023_0109945	4.0	4.0
1023_0109947	3.0	3.0
1031_0001949	4.0	4.0
1031_0001950	4.0	3.0
1031_0001951	3.0	4.0
1031_0001998	4.0	4.0
1031_0002003	3.0	3.0
1031_0002087	3.0	3.0
1031_0002088	3.0	4.0
1031_0002184	4.0	4.0
1031_0002185	4.0	4.0
1031_0003042	3.0	4.0
1031_0003053	3.0	4.0
1031_0003063	4.0	3.0
1031_0003097	3.0	4.0
1031_0003106	3.0	3.0
1031_0003128	4.0	4.0
1031_0003133	4.0	4.0
1031_0003149	4.0	4.0
1031_0003154	3.0	4.0
1031_0003155	3.0	4.0
1031_0003182	5.0	4.0
1031_0003214	3.0	4.0
1031_0003225	5.0	4.0
1031_0003230	4.0	4.0
1031_0003231	3.0	4.0
1031_0003234	3.0	3.0
1031_0003237	4.0	4.0
1031_0003239	5.0	4.0
1031_0003243	3.0	4.0
1031_0003245	3.0	4.0
1031_0003246	3.0	3.0
1031_0003249	3.0	4.0
1031_0003260	3.0	3.0
1031_0003273	3.0	3.0
1031_0003310	4.0	3.0
1031_0003314	3.0	4.0
1031_0003339	5.0	3.0
1031_0003352	3.0	3.0
1031_0003359	2.0	4.0
1031_0003383	3.0	3.0
1031_0003388	3.0	4.0
1031_0003410	4.0	4.0
1031_0003414	3.0	4.0
1061_0012029	4.0	3.0
1061_0120271	3.0	3.0
1061_0120276	3.0	3.0
1061_0120280	2.0	2.0
1061_0120283	1.0	2.0
1061_0120290	2.0	3.0
1061_0120300	3.0	2.0
1061_0120317	3.0	4.0
1061_0120323	2.0	2.0
1061_0120325	2.0	3.0
1061_0120329	2.0	3.0
1061_0120335	3.0	4.0
1061_0120338	2.0	3.0
1061_0120341	2.0	2.0
1061_0120348	1.0	2.0
1061_0120352	1.0	1.0
1061_0120358	2.0	2.0
1061_0120373	3.0	3.0
1061_0120384	2.0	2.0
1061_0120390	2.0	3.0
1061_0120404	1.0	2.0
1061_0120408	3.0	3.0
1061_0120432	3.0	3.0
1061_0120433	1.0	2.0
1061_0120440	2.0	2.0
1061_0120456	2.0	3.0
1061_0120483	2.0	2.0
1061_0120485	3.0	3.0
1061_0120488	2.0	3.0
1061_0120493	3.0	2.0
1061_0120496	2.0	2.0
1061_0120499	4.0	4.0
1061_0120500	2.0	3.0
1061_0120875	3.0	3.0
1061_0120880	4.0	3.0
1061_0120894	2.0	3.0
1061_1029114	2.0	2.0
1061_1029115	3.0	3.0
1061_1029116	2.0	3.0
1061_1202918	2.0	3.0
1061_1202919	2.0	2.0
1071_0024687	1.0	2.0
1071_0024689	1.0	2.0
1071_0024690	2.0	3.0
1071_0024691	2.0	3.0
1071_0024699	1.0	2.0
1071_0024701	3.0	2.0
1071_0024706	2.0	2.0
1071_0024709	3.0	3.0
1071_0024710	1.0	2.0
1071_0024713	2.0	2.0
1071_0024715	1.0	2.0
1071_0024756	2.0	2.0
1071_0024758	3.0	2.0
1071_0024767	2.0	2.0
1071_0024772	1.0	1.0
1071_0024800	1.0	1.0
1071_0024807	1.0	1.0
1071_0024816	1.0	1.0
1071_0024818	3.0	1.0
1071_0024819	1.0	2.0
1071_0024837	1.0	1.0
1071_0024840	2.0	1.0
1071_0024841	1.0	1.0
1071_0024845	1.0	2.0
1071_0024849	0.0	1.0
1071_0024864	1.0	1.0
1071_0024871	3.0	2.0
1071_0024872	2.0	2.0
1071_0242092	0.0	1.0
1071_0248312	1.0	2.0
1071_0248317	1.0	1.0
1071_0248320	0.0	0.0
1071_0248322	2.0	2.0
1071_0248324	0.0	1.0
1071_0248330	2.0	2.0
1071_0248331	2.0	2.0
1071_0248334	2.0	2.0
1071_0248339	2.0	1.0
1071_0248347	1.0	1.0
1091_0000001	2.0	2.0
1091_0000009	1.0	1.0
1091_0000012	2.0	1.0
1091_0000026	2.0	2.0
1091_0000029	3.0	2.0
1091_0000035	2.0	2.0
1091_0000039	2.0	1.0
1091_0000042	1.0	1.0
1091_0000050	1.0	1.0
1091_0000054	1.0	2.0
1091_0000079	1.0	3.0
1091_0000087	3.0	2.0
1091_0000101	2.0	2.0
1091_0000127	2.0	2.0
1091_0000146	1.0	1.0
1091_0000151	1.0	1.0
1091_0000154	2.0	2.0
1091_0000157	3.0	2.0
1091_0000159	2.0	2.0
1091_0000162	2.0	2.0
1091_0000170	3.0	2.0
1091_0000171	2.0	3.0
1091_0000173	2.0	2.0
1091_0000196	3.0	2.0
1091_0000202	2.0	3.0
1091_0000203	2.0	2.0
1091_0000204	2.0	3.0
1091_0000207	2.0	4.0
1091_0000215	2.0	4.0
1091_0000220	2.0	2.0
1091_0000221	3.0	2.0
1091_0000224	2.0	1.0
1091_0000227	1.0	2.0
1091_0000229	2.0	2.0
1091_0000230	2.0	2.0
1091_0000233	2.0	3.0
1091_0000239	3.0	2.0
1091_0000254	3.0	2.0
1091_0000258	2.0	3.0
1091_0000265	3.0	2.0
1091_0000267	2.0	2.0
1091_0000271	2.0	2.0
1091_0000275	2.0	2.0
1091_0000276	2.0	2.0
Averaged weighted F1-scores 0.5395962367609306
130.68810916179336 82.83744971317162
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
LANGUAGE: DE, 0th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.14
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.93
              precision    recall  f1-score   support

         0.0       0.40      0.40      0.40        10
         1.0       0.49      0.84      0.62        43
         2.0       0.76      0.54      0.63        69
         3.0       0.82      0.56      0.67        55
         4.0       0.58      0.75      0.66        28
         5.0       0.00      0.00      0.00         1

    accuracy                           0.63       206
   macro avg       0.51      0.51      0.50       206
weighted avg       0.67      0.63      0.63       206

[[ 4  5  1  0  0  0]
 [ 4 36  3  0  0  0]
 [ 1 31 37  0  0  0]
 [ 1  1  8 31 14  0]
 [ 0  0  0  7 21  0]
 [ 0  0  0  0  1  0]]
0.6262257683483419
206 206 206



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.82
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.80
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.52      0.53      0.53        43
         2.0       0.67      0.86      0.75        69
         3.0       0.82      0.58      0.68        55
         4.0       0.60      0.75      0.67        28
         5.0       0.00      0.00      0.00         1

    accuracy                           0.66       206
   macro avg       0.44      0.45      0.44       206
weighted avg       0.63      0.66      0.63       206

[[ 0  9  1  0  0  0]
 [ 0 23 20  0  0  0]
 [ 0 10 59  0  0  0]
 [ 0  2  8 32 13  0]
 [ 0  0  0  7 21  0]
 [ 0  0  0  0  1  0]]
0.6345096115178542
206 206 206



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.71
  Training epoch took: 31
Running Validation...
  Average evaluation loss: 0.82
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.54      0.91      0.68        43
         2.0       0.75      0.65      0.70        69
         3.0       0.80      0.58      0.67        55
         4.0       0.62      0.71      0.67        28
         5.0       0.00      0.00      0.00         1

    accuracy                           0.66       206
   macro avg       0.45      0.48      0.45       206
weighted avg       0.66      0.66      0.65       206

[[ 0  9  1  0  0  0]
 [ 0 39  4  0  0  0]
 [ 1 23 45  0  0  0]
 [ 1  1 10 32 11  0]
 [ 0  0  0  8 20  0]
 [ 0  0  0  0  1  0]]
0.645747818061356
206 206 206



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.59
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.77
              precision    recall  f1-score   support

         0.0       0.57      0.40      0.47        10
         1.0       0.60      0.63      0.61        43
         2.0       0.72      0.78      0.75        69
         3.0       0.85      0.60      0.70        55
         4.0       0.60      0.86      0.71        28
         5.0       0.00      0.00      0.00         1

    accuracy                           0.69       206
   macro avg       0.56      0.54      0.54       206
weighted avg       0.70      0.69      0.69       206

[[ 4  5  1  0  0  0]
 [ 1 27 15  0  0  0]
 [ 1 12 54  2  0  0]
 [ 1  1  5 33 15  0]
 [ 0  0  0  4 24  0]
 [ 0  0  0  0  1  0]]
0.6855532677099684
206 206 206
Filename	True Label	Prediction
1023_0001420	3.0	3.0
1023_0101683	3.0	3.0
1023_0101693	4.0	3.0
1023_0101845	3.0	3.0
1023_0101856	3.0	3.0
1023_0101896	3.0	2.0
1023_0101897	3.0	3.0
1023_0103822	2.0	2.0
1023_0103824	3.0	4.0
1023_0103825	3.0	3.0
1023_0103828	2.0	2.0
1023_0103833	4.0	4.0
1023_0103836	3.0	4.0
1023_0103838	3.0	3.0
1023_0103883	3.0	3.0
1023_0104209	3.0	3.0
1023_0107074	4.0	3.0
1023_0107740	3.0	3.0
1023_0107781	3.0	3.0
1023_0107788	3.0	3.0
1023_0108304	3.0	3.0
1023_0108423	3.0	3.0
1023_0108751	3.0	3.0
1023_0108812	3.0	3.0
1023_0108890	3.0	3.0
1023_0108908	3.0	3.0
1023_0108932	3.0	3.0
1023_0108958	3.0	3.0
1023_0108993	3.0	3.0
1023_0109249	3.0	3.0
1023_0109395	3.0	3.0
1023_0109496	3.0	3.0
1023_0109516	3.0	3.0
1023_0109528	3.0	3.0
1023_0109609	3.0	3.0
1023_0109651	3.0	3.0
1023_0109880	4.0	3.0
1023_0109891	3.0	3.0
1031_0001949	4.0	4.0
1031_0001951	3.0	4.0
1031_0002084	4.0	4.0
1031_0002086	4.0	4.0
1031_0002131	3.0	4.0
1031_0002197	4.0	4.0
1031_0002199	4.0	4.0
1031_0003048	4.0	4.0
1031_0003063	5.0	4.0
1031_0003077	4.0	4.0
1031_0003085	3.0	4.0
1031_0003097	4.0	4.0
1031_0003126	4.0	4.0
1031_0003135	4.0	4.0
1031_0003146	4.0	4.0
1031_0003155	4.0	4.0
1031_0003160	3.0	4.0
1031_0003161	4.0	4.0
1031_0003174	4.0	4.0
1031_0003179	4.0	4.0
1031_0003182	4.0	4.0
1031_0003184	4.0	4.0
1031_0003212	3.0	4.0
1031_0003214	3.0	4.0
1031_0003216	3.0	4.0
1031_0003231	4.0	4.0
1031_0003236	3.0	4.0
1031_0003245	4.0	4.0
1031_0003246	3.0	4.0
1031_0003261	3.0	4.0
1031_0003327	3.0	4.0
1031_0003331	3.0	4.0
1031_0003336	3.0	3.0
1031_0003337	4.0	4.0
1031_0003338	4.0	4.0
1031_0003357	4.0	4.0
1031_0003365	4.0	3.0
1031_0003386	3.0	4.0
1031_0003409	4.0	4.0
1061_0120273	1.0	2.0
1061_0120290	2.0	2.0
1061_0120300	2.0	2.0
1061_0120307	2.0	2.0
1061_0120310	3.0	2.0
1061_0120314	2.0	2.0
1061_0120323	2.0	2.0
1061_0120329	2.0	2.0
1061_0120347	2.0	2.0
1061_0120360	3.0	3.0
1061_0120361	3.0	2.0
1061_0120366	3.0	2.0
1061_0120372	2.0	2.0
1061_0120384	2.0	2.0
1061_0120389	2.0	2.0
1061_0120421	2.0	3.0
1061_0120423	3.0	3.0
1061_0120429	3.0	3.0
1061_0120450	2.0	2.0
1061_0120453	2.0	2.0
1061_0120459	2.0	2.0
1061_0120485	2.0	2.0
1061_0120491	2.0	2.0
1061_0120497	3.0	2.0
1061_0120877	2.0	2.0
1061_0120887	2.0	2.0
1061_1029111	2.0	2.0
1061_1202911	1.0	2.0
1061_1202912	2.0	2.0
1061_1202914	2.0	2.0
1061_1202916	2.0	2.0
1071_0024680	2.0	2.0
1071_0024681	2.0	2.0
1071_0024683	0.0	1.0
1071_0024685	2.0	3.0
1071_0024689	1.0	2.0
1071_0024693	1.0	2.0
1071_0024699	2.0	1.0
1071_0024702	2.0	2.0
1071_0024704	1.0	1.0
1071_0024708	1.0	1.0
1071_0024709	2.0	2.0
1071_0024714	2.0	2.0
1071_0024761	2.0	1.0
1071_0024763	1.0	1.0
1071_0024772	0.0	0.0
1071_0024781	1.0	1.0
1071_0024797	0.0	1.0
1071_0024799	2.0	2.0
1071_0024806	1.0	1.0
1071_0024808	1.0	2.0
1071_0024812	1.0	1.0
1071_0024816	1.0	1.0
1071_0024820	1.0	1.0
1071_0024831	1.0	1.0
1071_0024841	1.0	1.0
1071_0024847	2.0	2.0
1071_0024849	0.0	0.0
1071_0024851	2.0	1.0
1071_0024854	0.0	1.0
1071_0024863	2.0	1.0
1071_0024872	1.0	2.0
1071_0024874	1.0	1.0
1071_0242011	1.0	1.0
1071_0242012	2.0	2.0
1071_0242022	0.0	0.0
1071_0242043	0.0	1.0
1071_0242071	0.0	1.0
1071_0242093	0.0	0.0
1071_0243501	1.0	1.0
1071_0243591	1.0	1.0
1071_0243593	1.0	1.0
1071_0243622	1.0	0.0
1071_0248302	1.0	1.0
1071_0248314	1.0	1.0
1071_0248326	1.0	1.0
1071_0248329	1.0	1.0
1071_0248334	2.0	2.0
1071_0248335	1.0	1.0
1071_0248337	1.0	2.0
1071_0248338	1.0	1.0
1071_0248339	1.0	1.0
1071_0248342	1.0	1.0
1071_0248345	2.0	2.0
1091_0000003	2.0	2.0
1091_0000008	2.0	2.0
1091_0000022	2.0	2.0
1091_0000031	1.0	1.0
1091_0000037	1.0	1.0
1091_0000052	1.0	1.0
1091_0000053	1.0	1.0
1091_0000067	2.0	1.0
1091_0000070	2.0	1.0
1091_0000073	2.0	1.0
1091_0000076	2.0	2.0
1091_0000077	2.0	0.0
1091_0000078	3.0	0.0
1091_0000102	2.0	2.0
1091_0000123	2.0	2.0
1091_0000140	2.0	1.0
1091_0000144	2.0	1.0
1091_0000162	2.0	2.0
1091_0000163	2.0	2.0
1091_0000170	3.0	1.0
1091_0000172	2.0	1.0
1091_0000185	2.0	1.0
1091_0000190	1.0	2.0
1091_0000192	2.0	2.0
1091_0000206	1.0	2.0
1091_0000207	2.0	2.0
1091_0000212	1.0	2.0
1091_0000215	2.0	2.0
1091_0000221	2.0	2.0
1091_0000223	2.0	2.0
1091_0000229	2.0	2.0
1091_0000231	2.0	2.0
1091_0000237	1.0	2.0
1091_0000240	2.0	2.0
1091_0000241	2.0	1.0
1091_0000242	1.0	2.0
1091_0000243	1.0	2.0
1091_0000247	2.0	2.0
1091_0000249	2.0	2.0
1091_0000255	0.0	2.0
1091_0000256	1.0	2.0
1091_0000258	2.0	2.0
1091_0000271	2.0	2.0
1091_0000272	1.0	2.0
1091_0000275	2.0	2.0
LANGUAGE: DE, 1th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.08
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.94
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.46      0.14      0.22        42
         2.0       0.55      0.79      0.65        70
         3.0       0.65      0.63      0.64        54
         4.0       0.55      0.79      0.65        28
         5.0       0.00      0.00      0.00         1

    accuracy                           0.57       205
   macro avg       0.37      0.39      0.36       205
weighted avg       0.53      0.57      0.52       205

[[ 0  5  5  0  0  0]
 [ 0  6 36  0  0  0]
 [ 0  2 55 12  1  0]
 [ 0  0  4 34 16  0]
 [ 0  0  0  6 22  0]
 [ 0  0  0  0  1  0]]
0.5230093195291755
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.83
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.81
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.58      0.76      0.66        42
         2.0       0.75      0.74      0.75        70
         3.0       0.74      0.72      0.73        54
         4.0       0.64      0.64      0.64        28
         5.0       0.00      0.00      0.00         1

    accuracy                           0.69       205
   macro avg       0.45      0.48      0.46       205
weighted avg       0.66      0.69      0.67       205

[[ 0  9  1  0  0  0]
 [ 0 32 10  0  0  0]
 [ 0 14 52  4  0  0]
 [ 0  0  6 39  9  0]
 [ 0  0  0 10 18  0]
 [ 0  0  0  0  1  0]]
0.6704874483264315
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.72
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.81
              precision    recall  f1-score   support

         0.0       1.00      0.20      0.33        10
         1.0       0.59      0.81      0.68        42
         2.0       0.76      0.69      0.72        70
         3.0       0.75      0.61      0.67        54
         4.0       0.58      0.79      0.67        28
         5.0       0.00      0.00      0.00         1

    accuracy                           0.68       205
   macro avg       0.61      0.52      0.51       205
weighted avg       0.71      0.68      0.67       205

[[ 2  7  1  0  0  0]
 [ 0 34  8  0  0  0]
 [ 0 17 48  5  0  0]
 [ 0  0  6 33 15  0]
 [ 0  0  0  6 22  0]
 [ 0  0  0  0  1  0]]
0.670505671845118
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.59
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       1.00      0.20      0.33        10
         1.0       0.62      0.76      0.68        42
         2.0       0.74      0.71      0.72        70
         3.0       0.73      0.61      0.67        54
         4.0       0.58      0.79      0.67        28
         5.0       0.00      0.00      0.00         1

    accuracy                           0.68       205
   macro avg       0.61      0.51      0.51       205
weighted avg       0.70      0.68      0.67       205

[[ 2  6  2  0  0  0]
 [ 0 32 10  0  0  0]
 [ 0 14 50  6  0  0]
 [ 0  0  6 33 15  0]
 [ 0  0  0  6 22  0]
 [ 0  0  0  0  1  0]]
0.6698555237171242
205 205 205
Filename	True Label	Prediction
1023_0001416	3.0	3.0
1023_0001419	3.0	3.0
1023_0101684	3.0	2.0
1023_0101690	2.0	3.0
1023_0101694	3.0	3.0
1023_0101701	3.0	3.0
1023_0101749	4.0	3.0
1023_0101753	3.0	3.0
1023_0101841	3.0	3.0
1023_0101852	3.0	3.0
1023_0101853	3.0	3.0
1023_0101895	3.0	3.0
1023_0103830	3.0	3.0
1023_0103837	3.0	3.0
1023_0103839	3.0	3.0
1023_0103844	4.0	3.0
1023_0103880	3.0	3.0
1023_0106816	3.0	3.0
1023_0107244	3.0	3.0
1023_0107682	2.0	2.0
1023_0107727	3.0	3.0
1023_0108305	3.0	3.0
1023_0108510	3.0	3.0
1023_0108648	3.0	3.0
1023_0108649	3.0	3.0
1023_0108650	3.0	3.0
1023_0108753	3.0	3.0
1023_0108814	3.0	3.0
1023_0108887	2.0	3.0
1023_0108931	3.0	3.0
1023_0109033	4.0	3.0
1023_0109399	2.0	3.0
1023_0109400	3.0	2.0
1023_0109422	3.0	3.0
1023_0109606	3.0	3.0
1023_0109649	3.0	3.0
1023_0109671	3.0	3.0
1023_0109890	4.0	3.0
1023_0109915	2.0	2.0
1023_0109951	3.0	3.0
1023_0111896	3.0	3.0
1031_0001703	4.0	4.0
1031_0001998	4.0	4.0
1031_0002005	4.0	4.0
1031_0002040	4.0	4.0
1031_0002043	4.0	4.0
1031_0002198	4.0	4.0
1031_0003073	4.0	4.0
1031_0003074	4.0	4.0
1031_0003088	4.0	3.0
1031_0003090	4.0	4.0
1031_0003099	3.0	4.0
1031_0003129	4.0	4.0
1031_0003130	5.0	4.0
1031_0003132	4.0	4.0
1031_0003133	4.0	4.0
1031_0003141	3.0	4.0
1031_0003149	4.0	4.0
1031_0003163	3.0	4.0
1031_0003166	3.0	4.0
1031_0003170	3.0	3.0
1031_0003172	3.0	4.0
1031_0003180	4.0	4.0
1031_0003187	4.0	4.0
1031_0003189	4.0	4.0
1031_0003190	4.0	4.0
1031_0003203	2.0	3.0
1031_0003205	4.0	4.0
1031_0003206	3.0	4.0
1031_0003211	3.0	4.0
1031_0003243	3.0	4.0
1031_0003272	3.0	4.0
1031_0003274	3.0	4.0
1031_0003315	3.0	4.0
1031_0003330	4.0	4.0
1031_0003339	4.0	4.0
1031_0003352	3.0	3.0
1031_0003353	3.0	4.0
1031_0003355	4.0	3.0
1031_0003368	3.0	4.0
1031_0003389	3.0	4.0
1031_0003391	3.0	4.0
1031_0003392	4.0	4.0
1031_0003410	4.0	4.0
1061_0012029	3.0	2.0
1061_0120276	2.0	2.0
1061_0120282	0.0	2.0
1061_0120285	2.0	2.0
1061_0120295	0.0	2.0
1061_0120301	2.0	2.0
1061_0120302	1.0	2.0
1061_0120313	2.0	1.0
1061_0120317	3.0	2.0
1061_0120324	2.0	2.0
1061_0120332	2.0	2.0
1061_0120338	2.0	2.0
1061_0120343	2.0	2.0
1061_0120345	2.0	2.0
1061_0120348	1.0	1.0
1061_0120358	1.0	2.0
1061_0120367	3.0	2.0
1061_0120371	3.0	3.0
1061_0120373	2.0	2.0
1061_0120375	2.0	2.0
1061_0120394	2.0	2.0
1061_0120409	2.0	2.0
1061_0120424	2.0	2.0
1061_0120426	2.0	3.0
1061_0120428	2.0	2.0
1061_0120448	3.0	2.0
1061_0120455	2.0	2.0
1061_0120457	2.0	2.0
1061_0120479	2.0	2.0
1061_0120492	2.0	2.0
1061_0120494	2.0	2.0
1061_0120495	2.0	2.0
1061_0120499	2.0	2.0
1061_0120858	2.0	2.0
1061_0120878	1.0	2.0
1061_0120890	1.0	1.0
1061_0120894	2.0	2.0
1061_1029113	2.0	2.0
1061_1029115	2.0	2.0
1061_1029116	1.0	2.0
1061_1029119	2.0	2.0
1061_1029120	2.0	2.0
1071_0020001	1.0	1.0
1071_0024682	2.0	2.0
1071_0024686	2.0	2.0
1071_0024688	2.0	2.0
1071_0024692	2.0	2.0
1071_0024705	2.0	2.0
1071_0024711	2.0	1.0
1071_0024712	1.0	2.0
1071_0024713	2.0	1.0
1071_0024715	2.0	2.0
1071_0024716	1.0	1.0
1071_0024765	0.0	1.0
1071_0024767	2.0	1.0
1071_0024773	1.0	1.0
1071_0024774	0.0	0.0
1071_0024775	0.0	1.0
1071_0024778	1.0	1.0
1071_0024801	1.0	1.0
1071_0024811	1.0	1.0
1071_0024817	1.0	1.0
1071_0024827	1.0	1.0
1071_0024845	0.0	1.0
1071_0024859	2.0	2.0
1071_0242041	1.0	1.0
1071_0243581	1.0	1.0
1071_0243582	1.0	1.0
1071_0248301	2.0	1.0
1071_0248303	1.0	1.0
1071_0248304	1.0	1.0
1071_0248309	2.0	1.0
1071_0248318	0.0	0.0
1071_0248327	1.0	1.0
1071_0248330	2.0	1.0
1071_0248332	2.0	1.0
1071_0248333	2.0	1.0
1071_0248336	1.0	1.0
1071_0248341	1.0	1.0
1071_0248343	1.0	1.0
1071_0248348	1.0	1.0
1091_0000006	1.0	1.0
1091_0000009	0.0	1.0
1091_0000011	2.0	2.0
1091_0000012	1.0	1.0
1091_0000014	0.0	1.0
1091_0000017	2.0	2.0
1091_0000019	1.0	1.0
1091_0000025	1.0	1.0
1091_0000027	0.0	1.0
1091_0000028	1.0	1.0
1091_0000029	2.0	1.0
1091_0000032	1.0	2.0
1091_0000039	1.0	1.0
1091_0000041	1.0	1.0
1091_0000055	1.0	2.0
1091_0000057	2.0	1.0
1091_0000071	1.0	2.0
1091_0000095	2.0	2.0
1091_0000113	1.0	2.0
1091_0000125	2.0	2.0
1091_0000127	2.0	1.0
1091_0000145	1.0	1.0
1091_0000146	1.0	1.0
1091_0000151	1.0	1.0
1091_0000152	1.0	1.0
1091_0000154	2.0	3.0
1091_0000193	2.0	1.0
1091_0000195	1.0	1.0
1091_0000200	2.0	2.0
1091_0000201	2.0	2.0
1091_0000203	2.0	2.0
1091_0000204	2.0	2.0
1091_0000205	2.0	2.0
1091_0000224	2.0	1.0
1091_0000236	2.0	2.0
1091_0000246	2.0	2.0
1091_0000251	2.0	2.0
1091_0000260	2.0	2.0
1091_0000264	2.0	2.0
1091_0000274	1.0	2.0
LANGUAGE: DE, 2th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.17
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.86
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.59      0.81      0.68        42
         2.0       0.72      0.69      0.70        70
         3.0       0.66      0.65      0.65        54
         4.0       0.63      0.61      0.62        28
         5.0       0.00      0.00      0.00         1

    accuracy                           0.65       205
   macro avg       0.43      0.46      0.44       205
weighted avg       0.62      0.65      0.64       205

[[ 0  9  1  0  0  0]
 [ 0 34  8  0  0  0]
 [ 0 15 48  7  0  0]
 [ 0  0 10 35  9  0]
 [ 0  0  0 11 17  0]
 [ 0  0  0  0  1  0]]
0.6353526273333666
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.85
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.82
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.57      0.74      0.65        42
         2.0       0.73      0.70      0.72        70
         3.0       0.69      0.69      0.69        54
         4.0       0.63      0.68      0.66        28
         5.0       0.00      0.00      0.00         1

    accuracy                           0.66       205
   macro avg       0.44      0.47      0.45       205
weighted avg       0.63      0.66      0.65       205

[[ 0 10  0  0  0  0]
 [ 0 31 11  0  0  0]
 [ 0 13 49  8  0  0]
 [ 0  0  7 37 10  0]
 [ 0  0  0  9 19  0]
 [ 0  0  0  0  1  0]]
0.6465503428631065
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.75
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.78
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.58      0.60      0.59        42
         2.0       0.65      0.76      0.70        70
         3.0       0.74      0.63      0.68        54
         4.0       0.68      0.82      0.74        28
         5.0       0.00      0.00      0.00         1

    accuracy                           0.66       205
   macro avg       0.44      0.47      0.45       205
weighted avg       0.63      0.66      0.64       205

[[ 0  8  2  0  0  0]
 [ 0 25 17  0  0  0]
 [ 0 10 53  7  0  0]
 [ 0  0 10 34 10  0]
 [ 0  0  0  5 23  0]
 [ 0  0  0  0  1  0]]
0.6391017823171341
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.64
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.77
              precision    recall  f1-score   support

         0.0       1.00      0.20      0.33        10
         1.0       0.60      0.71      0.65        42
         2.0       0.72      0.71      0.72        70
         3.0       0.75      0.70      0.72        54
         4.0       0.70      0.82      0.75        28
         5.0       0.00      0.00      0.00         1

    accuracy                           0.70       205
   macro avg       0.63      0.53      0.53       205
weighted avg       0.71      0.70      0.69       205

[[ 2  8  0  0  0  0]
 [ 0 30 12  0  0  0]
 [ 0 12 50  8  0  0]
 [ 0  0  7 38  9  0]
 [ 0  0  0  5 23  0]
 [ 0  0  0  0  1  0]]
0.689194235587562
205 205 205
Filename	True Label	Prediction
1023_0101675	3.0	3.0
1023_0101695	3.0	3.0
1023_0101700	3.0	3.0
1023_0101751	3.0	3.0
1023_0101752	3.0	3.0
1023_0101844	3.0	3.0
1023_0101846	4.0	3.0
1023_0101848	2.0	3.0
1023_0101854	2.0	2.0
1023_0101855	3.0	3.0
1023_0101893	3.0	3.0
1023_0101894	3.0	3.0
1023_0101900	4.0	3.0
1023_0101906	3.0	3.0
1023_0101907	4.0	3.0
1023_0102118	3.0	3.0
1023_0103834	3.0	3.0
1023_0103840	3.0	3.0
1023_0103843	3.0	3.0
1023_0107672	2.0	3.0
1023_0107729	3.0	3.0
1023_0107780	3.0	3.0
1023_0107784	2.0	2.0
1023_0108810	3.0	3.0
1023_0108888	3.0	3.0
1023_0108935	2.0	3.0
1023_0108992	3.0	3.0
1023_0109029	2.0	2.0
1023_0109096	3.0	3.0
1023_0109250	3.0	3.0
1023_0109267	3.0	3.0
1023_0109391	2.0	3.0
1023_0109396	3.0	3.0
1023_0109505	3.0	3.0
1023_0109519	2.0	3.0
1023_0109522	3.0	3.0
1023_0109614	2.0	3.0
1023_0109674	3.0	3.0
1023_0109717	3.0	3.0
1023_0109721	3.0	3.0
1023_0109914	2.0	3.0
1023_0109946	3.0	3.0
1023_0109947	3.0	3.0
1023_0109954	3.0	3.0
1031_0001950	4.0	4.0
1031_0002004	4.0	4.0
1031_0002006	4.0	4.0
1031_0002079	5.0	4.0
1031_0002083	3.0	4.0
1031_0002087	4.0	4.0
1031_0002092	4.0	4.0
1031_0002185	4.0	4.0
1031_0002187	4.0	4.0
1031_0003013	4.0	4.0
1031_0003071	4.0	4.0
1031_0003072	3.0	4.0
1031_0003076	4.0	4.0
1031_0003078	4.0	4.0
1031_0003092	3.0	4.0
1031_0003095	3.0	3.0
1031_0003106	4.0	3.0
1031_0003131	4.0	4.0
1031_0003140	4.0	4.0
1031_0003144	3.0	4.0
1031_0003145	4.0	3.0
1031_0003156	4.0	4.0
1031_0003169	3.0	4.0
1031_0003186	4.0	4.0
1031_0003217	4.0	4.0
1031_0003224	3.0	4.0
1031_0003232	3.0	4.0
1031_0003234	3.0	3.0
1031_0003237	3.0	4.0
1031_0003239	4.0	4.0
1031_0003314	4.0	4.0
1031_0003354	3.0	4.0
1031_0003356	3.0	3.0
1031_0003383	4.0	4.0
1031_0003384	3.0	3.0
1031_0003387	4.0	4.0
1031_0003388	4.0	4.0
1031_0003393	4.0	4.0
1031_0003407	3.0	3.0
1031_0003415	4.0	4.0
1061_0120272	2.0	1.0
1061_0120275	2.0	2.0
1061_0120277	1.0	2.0
1061_0120286	1.0	1.0
1061_0120287	1.0	2.0
1061_0120288	2.0	2.0
1061_0120291	1.0	1.0
1061_0120306	2.0	2.0
1061_0120318	2.0	2.0
1061_0120319	3.0	2.0
1061_0120321	2.0	2.0
1061_0120325	2.0	2.0
1061_0120335	2.0	3.0
1061_0120337	2.0	2.0
1061_0120349	1.0	1.0
1061_0120351	2.0	2.0
1061_0120357	3.0	3.0
1061_0120383	3.0	3.0
1061_0120386	1.0	2.0
1061_0120387	2.0	2.0
1061_0120391	1.0	2.0
1061_0120404	2.0	2.0
1061_0120405	3.0	2.0
1061_0120406	2.0	2.0
1061_0120413	1.0	1.0
1061_0120414	2.0	2.0
1061_0120425	2.0	2.0
1061_0120438	2.0	2.0
1061_0120439	2.0	1.0
1061_0120440	1.0	2.0
1061_0120460	2.0	2.0
1061_0120487	2.0	2.0
1061_0120493	2.0	2.0
1061_0120498	2.0	2.0
1061_0120853	2.0	2.0
1061_0120856	2.0	2.0
1061_0120874	2.0	2.0
1061_0120880	3.0	2.0
1061_0120882	3.0	2.0
1061_0120884	2.0	2.0
1061_0120886	2.0	2.0
1061_1029118	2.0	2.0
1061_1202918	2.0	2.0
1061_1202919	2.0	2.0
1071_0024687	1.0	1.0
1071_0024701	2.0	2.0
1071_0024756	2.0	1.0
1071_0024757	2.0	2.0
1071_0024803	1.0	1.0
1071_0024804	1.0	1.0
1071_0024807	1.0	1.0
1071_0024810	1.0	1.0
1071_0024818	2.0	1.0
1071_0024824	1.0	1.0
1071_0024825	1.0	1.0
1071_0024826	2.0	2.0
1071_0024834	2.0	2.0
1071_0024837	0.0	1.0
1071_0024843	1.0	1.0
1071_0024844	1.0	1.0
1071_0024846	1.0	1.0
1071_0024853	1.0	1.0
1071_0024855	1.0	1.0
1071_0024867	2.0	2.0
1071_0024871	1.0	1.0
1071_0024873	0.0	1.0
1071_0242072	0.0	0.0
1071_0242092	0.0	1.0
1071_0243592	1.0	1.0
1071_0248313	2.0	1.0
1071_0248315	0.0	1.0
1071_0248316	1.0	1.0
1071_0248319	1.0	1.0
1071_0248320	0.0	0.0
1071_0248321	1.0	1.0
1071_0248322	1.0	1.0
1071_0248340	0.0	1.0
1071_0248347	1.0	1.0
1071_0248350	2.0	1.0
1091_0000001	1.0	1.0
1091_0000002	2.0	2.0
1091_0000004	1.0	1.0
1091_0000005	2.0	2.0
1091_0000007	2.0	2.0
1091_0000013	1.0	1.0
1091_0000015	1.0	2.0
1091_0000016	0.0	1.0
1091_0000030	0.0	1.0
1091_0000035	2.0	1.0
1091_0000036	1.0	2.0
1091_0000042	1.0	1.0
1091_0000046	1.0	2.0
1091_0000050	1.0	1.0
1091_0000051	1.0	1.0
1091_0000054	0.0	1.0
1091_0000072	1.0	2.0
1091_0000086	1.0	2.0
1091_0000116	3.0	2.0
1091_0000148	1.0	1.0
1091_0000157	3.0	2.0
1091_0000158	2.0	2.0
1091_0000160	3.0	2.0
1091_0000164	2.0	1.0
1091_0000165	2.0	1.0
1091_0000173	2.0	2.0
1091_0000202	2.0	2.0
1091_0000208	2.0	2.0
1091_0000216	1.0	2.0
1091_0000222	2.0	2.0
1091_0000225	2.0	1.0
1091_0000228	2.0	2.0
1091_0000232	2.0	2.0
1091_0000233	2.0	2.0
1091_0000239	2.0	1.0
1091_0000244	2.0	2.0
1091_0000248	2.0	2.0
1091_0000253	2.0	1.0
1091_0000259	2.0	2.0
1091_0000261	2.0	2.0
1091_0000269	1.0	2.0
1091_0000270	2.0	2.0
LANGUAGE: DE, 3th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 11
Elapsed time 23

  Average training loss: 1.20
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.96
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.00      0.00      0.00        43
         2.0       0.50      0.96      0.66        70
         3.0       0.62      0.69      0.65        54
         4.0       0.70      0.26      0.38        27
         5.0       0.00      0.00      0.00         1

    accuracy                           0.54       205
   macro avg       0.30      0.32      0.28       205
weighted avg       0.43      0.54      0.45       205

[[ 0  2  8  0  0  0]
 [ 0  0 43  0  0  0]
 [ 0  0 67  3  0  0]
 [ 0  0 15 37  2  0]
 [ 0  0  0 20  7  0]
 [ 0  0  0  0  1  0]]
0.4462231431555498
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.85
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.89
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.53      0.37      0.44        43
         2.0       0.62      0.76      0.68        70
         3.0       0.67      0.67      0.67        54
         4.0       0.63      0.81      0.71        27
         5.0       0.00      0.00      0.00         1

    accuracy                           0.62       205
   macro avg       0.41      0.44      0.42       205
weighted avg       0.58      0.62      0.59       205

[[ 0  9  1  0  0  0]
 [ 0 16 26  1  0  0]
 [ 0  5 53 12  0  0]
 [ 0  0  6 36 12  0]
 [ 0  0  0  5 22  0]
 [ 0  0  0  0  1  0]]
0.593047355878909
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.72
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.92
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.48      0.28      0.35        43
         2.0       0.60      0.80      0.68        70
         3.0       0.71      0.67      0.69        54
         4.0       0.63      0.81      0.71        27
         5.0       0.00      0.00      0.00         1

    accuracy                           0.61       205
   macro avg       0.40      0.43      0.41       205
weighted avg       0.57      0.61      0.58       205

[[ 0  9  1  0  0  0]
 [ 0 12 31  0  0  0]
 [ 0  4 56 10  0  0]
 [ 0  0  6 36 12  0]
 [ 0  0  0  5 22  0]
 [ 0  0  0  0  1  0]]
0.5813229775032578
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.62
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.81
              precision    recall  f1-score   support

         0.0       1.00      0.30      0.46        10
         1.0       0.58      0.49      0.53        43
         2.0       0.64      0.76      0.69        70
         3.0       0.69      0.70      0.70        54
         4.0       0.68      0.70      0.69        27
         5.0       0.00      0.00      0.00         1

    accuracy                           0.65       205
   macro avg       0.60      0.49      0.51       205
weighted avg       0.66      0.65      0.65       205

[[ 3  7  0  0  0  0]
 [ 0 21 22  0  0  0]
 [ 0  8 53  9  0  0]
 [ 0  0  8 38  8  0]
 [ 0  0  0  8 19  0]
 [ 0  0  0  0  1  0]]
0.6452624279830912
205 205 205
Filename	True Label	Prediction
1023_0001422	3.0	2.0
1023_0001423	2.0	3.0
1023_0001575	3.0	3.0
1023_0101688	3.0	3.0
1023_0101691	3.0	3.0
1023_0101843	3.0	3.0
1023_0101901	4.0	3.0
1023_0103829	2.0	3.0
1023_0103832	3.0	2.0
1023_0103841	3.0	3.0
1023_0104203	3.0	3.0
1023_0104207	3.0	3.0
1023_0107042	3.0	3.0
1023_0107075	3.0	3.0
1023_0107726	3.0	3.0
1023_0107783	3.0	3.0
1023_0108306	3.0	3.0
1023_0108307	3.0	3.0
1023_0108518	3.0	3.0
1023_0108520	3.0	3.0
1023_0108641	4.0	3.0
1023_0108766	3.0	3.0
1023_0108811	3.0	3.0
1023_0108815	3.0	3.0
1023_0108885	3.0	3.0
1023_0108933	3.0	3.0
1023_0108934	3.0	3.0
1023_0108955	3.0	3.0
1023_0109027	3.0	3.0
1023_0109038	3.0	3.0
1023_0109151	4.0	3.0
1023_0109192	3.0	3.0
1023_0109248	2.0	3.0
1023_0109392	3.0	3.0
1023_0109495	3.0	3.0
1023_0109500	2.0	3.0
1023_0109518	2.0	3.0
1023_0109527	3.0	3.0
1023_0109591	3.0	3.0
1023_0109878	3.0	3.0
1023_0109945	4.0	3.0
1031_0001997	4.0	4.0
1031_0002003	3.0	4.0
1031_0002011	4.0	4.0
1031_0002042	4.0	3.0
1031_0002088	4.0	4.0
1031_0002089	4.0	4.0
1031_0002196	4.0	4.0
1031_0002200	3.0	3.0
1031_0003042	3.0	3.0
1031_0003043	5.0	4.0
1031_0003091	3.0	3.0
1031_0003121	4.0	3.0
1031_0003127	4.0	3.0
1031_0003128	4.0	4.0
1031_0003150	4.0	4.0
1031_0003154	4.0	4.0
1031_0003157	4.0	4.0
1031_0003162	4.0	4.0
1031_0003167	3.0	4.0
1031_0003181	4.0	4.0
1031_0003218	4.0	4.0
1031_0003221	3.0	3.0
1031_0003225	3.0	4.0
1031_0003226	4.0	4.0
1031_0003235	4.0	4.0
1031_0003238	4.0	4.0
1031_0003244	4.0	4.0
1031_0003260	4.0	3.0
1031_0003262	3.0	4.0
1031_0003273	3.0	4.0
1031_0003309	3.0	4.0
1031_0003310	4.0	4.0
1031_0003358	4.0	4.0
1031_0003359	3.0	4.0
1031_0003367	4.0	4.0
1031_0003408	3.0	4.0
1061_0120271	2.0	2.0
1061_0120279	2.0	2.0
1061_0120284	0.0	1.0
1061_0120289	2.0	2.0
1061_0120299	2.0	2.0
1061_0120304	2.0	2.0
1061_0120308	3.0	2.0
1061_0120309	2.0	1.0
1061_0120311	3.0	2.0
1061_0120316	2.0	2.0
1061_0120320	3.0	3.0
1061_0120327	2.0	2.0
1061_0120330	2.0	3.0
1061_0120333	3.0	3.0
1061_0120334	2.0	3.0
1061_0120336	1.0	2.0
1061_0120350	2.0	2.0
1061_0120353	1.0	1.0
1061_0120355	1.0	1.0
1061_0120359	2.0	2.0
1061_0120368	2.0	2.0
1061_0120369	2.0	2.0
1061_0120370	2.0	3.0
1061_0120390	2.0	2.0
1061_0120408	3.0	2.0
1061_0120410	2.0	2.0
1061_0120411	3.0	3.0
1061_0120427	2.0	2.0
1061_0120432	2.0	2.0
1061_0120443	0.0	1.0
1061_0120456	2.0	2.0
1061_0120480	2.0	2.0
1061_0120483	2.0	2.0
1061_0120486	2.0	2.0
1061_0120490	2.0	2.0
1061_0120496	2.0	2.0
1061_0120500	2.0	2.0
1061_0120855	2.0	2.0
1061_0120859	2.0	3.0
1061_0120876	2.0	2.0
1061_0120883	2.0	2.0
1061_0120885	2.0	2.0
1061_0120889	1.0	2.0
1061_1029112	3.0	3.0
1061_1202910	2.0	2.0
1061_1202915	1.0	2.0
1061_1202917	2.0	2.0
1071_0024691	2.0	2.0
1071_0024694	2.0	2.0
1071_0024703	1.0	2.0
1071_0024710	1.0	1.0
1071_0024768	1.0	1.0
1071_0024769	0.0	1.0
1071_0024770	1.0	1.0
1071_0024776	0.0	0.0
1071_0024777	1.0	2.0
1071_0024779	2.0	1.0
1071_0024782	0.0	1.0
1071_0024783	0.0	0.0
1071_0024798	0.0	1.0
1071_0024800	0.0	1.0
1071_0024814	1.0	1.0
1071_0024819	2.0	2.0
1071_0024821	1.0	1.0
1071_0024822	1.0	2.0
1071_0024833	2.0	2.0
1071_0024838	0.0	0.0
1071_0024848	1.0	1.0
1071_0024850	1.0	1.0
1071_0024856	1.0	1.0
1071_0024862	2.0	2.0
1071_0024875	1.0	1.0
1071_0024876	1.0	1.0
1071_0024877	1.0	1.0
1071_0241832	1.0	1.0
1071_0241833	1.0	1.0
1071_0242023	1.0	1.0
1071_0242091	1.0	1.0
1071_0243621	2.0	2.0
1071_0243623	1.0	1.0
1071_0248307	2.0	1.0
1071_0248308	1.0	1.0
1071_0248310	1.0	1.0
1071_0248323	1.0	2.0
1071_0248325	0.0	1.0
1071_0248344	2.0	1.0
1091_0000010	2.0	2.0
1091_0000021	2.0	2.0
1091_0000024	2.0	1.0
1091_0000033	1.0	2.0
1091_0000038	2.0	1.0
1091_0000043	1.0	2.0
1091_0000048	1.0	1.0
1091_0000056	1.0	2.0
1091_0000059	1.0	2.0
1091_0000064	1.0	2.0
1091_0000069	2.0	1.0
1091_0000074	2.0	2.0
1091_0000075	1.0	2.0
1091_0000114	2.0	2.0
1091_0000156	3.0	2.0
1091_0000159	2.0	2.0
1091_0000166	1.0	2.0
1091_0000169	3.0	2.0
1091_0000171	2.0	2.0
1091_0000194	1.0	2.0
1091_0000197	1.0	2.0
1091_0000198	2.0	2.0
1091_0000199	2.0	2.0
1091_0000209	2.0	2.0
1091_0000211	1.0	2.0
1091_0000213	2.0	2.0
1091_0000214	2.0	2.0
1091_0000219	1.0	2.0
1091_0000220	1.0	2.0
1091_0000226	1.0	2.0
1091_0000227	1.0	2.0
1091_0000230	2.0	2.0
1091_0000234	3.0	2.0
1091_0000252	2.0	2.0
1091_0000254	2.0	1.0
1091_0000257	2.0	2.0
1091_0000262	2.0	2.0
1091_0000265	2.0	2.0
1091_0000266	2.0	2.0
1091_0000267	2.0	2.0
1091_0000268	2.0	2.0
1091_0000273	1.0	2.0
LANGUAGE: DE, 4th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.15
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.83
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.55      0.65      0.60        43
         2.0       0.74      0.77      0.76        70
         3.0       0.72      0.80      0.75        54
         4.0       0.62      0.48      0.54        27
         5.0       0.00      0.00      0.00         1

    accuracy                           0.67       205
   macro avg       0.44      0.45      0.44       205
weighted avg       0.64      0.67      0.65       205

[[ 0 10  0  0  0  0]
 [ 0 28 15  0  0  0]
 [ 0 13 54  3  0  0]
 [ 0  0  4 43  7  0]
 [ 0  0  0 14 13  0]
 [ 0  0  0  0  1  0]]
0.6529072987755695
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.84
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.50      0.49      0.49        43
         2.0       0.67      0.80      0.73        70
         3.0       0.77      0.74      0.75        54
         4.0       0.64      0.67      0.65        27
         5.0       0.00      0.00      0.00         1

    accuracy                           0.66       205
   macro avg       0.43      0.45      0.44       205
weighted avg       0.62      0.66      0.64       205

[[ 0 10  0  0  0  0]
 [ 0 21 22  0  0  0]
 [ 0 11 56  3  0  0]
 [ 0  0  5 40  9  0]
 [ 0  0  0  9 18  0]
 [ 0  0  0  0  1  0]]
0.6386162592329181
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.72
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.76
              precision    recall  f1-score   support

         0.0       1.00      0.30      0.46        10
         1.0       0.59      0.67      0.63        43
         2.0       0.72      0.77      0.74        70
         3.0       0.79      0.70      0.75        54
         4.0       0.67      0.74      0.70        27
         5.0       0.00      0.00      0.00         1

    accuracy                           0.70       205
   macro avg       0.63      0.53      0.55       205
weighted avg       0.72      0.70      0.70       205

[[ 3  7  0  0  0  0]
 [ 0 29 14  0  0  0]
 [ 0 13 54  3  0  0]
 [ 0  0  7 38  9  0]
 [ 0  0  0  7 20  0]
 [ 0  0  0  0  1  0]]
0.6977788967841023
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.61
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.76
              precision    recall  f1-score   support

         0.0       1.00      0.30      0.46        10
         1.0       0.60      0.65      0.62        43
         2.0       0.73      0.79      0.76        70
         3.0       0.79      0.76      0.77        54
         4.0       0.68      0.70      0.69        27
         5.0       0.00      0.00      0.00         1

    accuracy                           0.71       205
   macro avg       0.63      0.53      0.55       205
weighted avg       0.72      0.71      0.71       205

[[ 3  7  0  0  0  0]
 [ 0 28 15  0  0  0]
 [ 0 12 55  3  0  0]
 [ 0  0  5 41  8  0]
 [ 0  0  0  8 19  0]
 [ 0  0  0  0  1  0]]
0.7068415551561368
205 205 205
Filename	True Label	Prediction
1023_0001418	3.0	3.0
1023_0101689	2.0	2.0
1023_0101847	3.0	3.0
1023_0101849	3.0	3.0
1023_0101851	3.0	3.0
1023_0101898	4.0	3.0
1023_0101899	3.0	3.0
1023_0101904	2.0	3.0
1023_0101909	4.0	3.0
1023_0102117	3.0	3.0
1023_0103821	3.0	3.0
1023_0103823	4.0	3.0
1023_0103826	3.0	3.0
1023_0103827	3.0	3.0
1023_0103831	3.0	3.0
1023_0103955	4.0	3.0
1023_0104206	3.0	3.0
1023_0107725	3.0	3.0
1023_0107773	3.0	3.0
1023_0107787	2.0	3.0
1023_0108422	3.0	3.0
1023_0108426	3.0	3.0
1023_0108752	3.0	3.0
1023_0108813	3.0	3.0
1023_0108886	3.0	3.0
1023_0108889	3.0	3.0
1023_0109022	3.0	3.0
1023_0109026	2.0	3.0
1023_0109030	3.0	3.0
1023_0109039	3.0	3.0
1023_0109247	3.0	3.0
1023_0109401	3.0	3.0
1023_0109402	3.0	3.0
1023_0109515	3.0	3.0
1023_0109520	3.0	3.0
1023_0109524	3.0	3.0
1023_0109588	3.0	3.0
1023_0109590	3.0	3.0
1023_0109716	3.0	3.0
1023_0109917	3.0	3.0
1031_0002002	3.0	4.0
1031_0002010	3.0	4.0
1031_0002032	3.0	4.0
1031_0002036	4.0	4.0
1031_0002061	3.0	4.0
1031_0002085	4.0	4.0
1031_0002091	4.0	4.0
1031_0002184	4.0	4.0
1031_0002195	4.0	3.0
1031_0003012	4.0	3.0
1031_0003023	4.0	4.0
1031_0003029	4.0	4.0
1031_0003035	4.0	4.0
1031_0003052	4.0	4.0
1031_0003053	4.0	4.0
1031_0003054	4.0	4.0
1031_0003065	3.0	3.0
1031_0003098	5.0	4.0
1031_0003136	4.0	3.0
1031_0003164	4.0	4.0
1031_0003165	3.0	3.0
1031_0003173	4.0	4.0
1031_0003183	4.0	4.0
1031_0003185	3.0	3.0
1031_0003191	4.0	4.0
1031_0003207	4.0	4.0
1031_0003219	3.0	3.0
1031_0003220	3.0	3.0
1031_0003230	4.0	4.0
1031_0003233	3.0	3.0
1031_0003240	3.0	4.0
1031_0003242	3.0	4.0
1031_0003249	3.0	4.0
1031_0003313	4.0	4.0
1031_0003366	3.0	3.0
1031_0003369	4.0	4.0
1031_0003390	4.0	4.0
1031_0003414	3.0	4.0
1031_0003419	4.0	3.0
1061_0120274	1.0	2.0
1061_0120278	2.0	2.0
1061_0120280	1.0	2.0
1061_0120281	2.0	2.0
1061_0120283	1.0	1.0
1061_0120296	2.0	2.0
1061_0120297	2.0	2.0
1061_0120298	2.0	2.0
1061_0120303	1.0	1.0
1061_0120312	1.0	1.0
1061_0120315	2.0	1.0
1061_0120326	2.0	2.0
1061_0120328	2.0	2.0
1061_0120331	1.0	1.0
1061_0120341	2.0	2.0
1061_0120346	2.0	2.0
1061_0120352	1.0	1.0
1061_0120354	2.0	2.0
1061_0120356	2.0	2.0
1061_0120374	3.0	2.0
1061_0120376	2.0	2.0
1061_0120382	2.0	2.0
1061_0120388	2.0	2.0
1061_0120403	2.0	2.0
1061_0120407	3.0	2.0
1061_0120415	2.0	2.0
1061_0120430	2.0	2.0
1061_0120431	2.0	2.0
1061_0120433	1.0	2.0
1061_0120441	2.0	2.0
1061_0120442	2.0	2.0
1061_0120449	2.0	2.0
1061_0120458	3.0	2.0
1061_0120478	2.0	2.0
1061_0120481	3.0	3.0
1061_0120482	2.0	2.0
1061_0120484	2.0	2.0
1061_0120488	2.0	2.0
1061_0120489	2.0	2.0
1061_0120857	2.0	2.0
1061_0120875	3.0	3.0
1061_0120881	3.0	2.0
1061_0120888	2.0	2.0
1061_1029114	1.0	2.0
1061_1029117	2.0	2.0
1061_1202913	2.0	2.0
1071_0024678	2.0	1.0
1071_0024690	2.0	2.0
1071_0024706	1.0	2.0
1071_0024758	2.0	2.0
1071_0024759	0.0	1.0
1071_0024762	1.0	1.0
1071_0024766	1.0	1.0
1071_0024784	1.0	1.0
1071_0024802	2.0	2.0
1071_0024809	1.0	1.0
1071_0024813	0.0	1.0
1071_0024815	1.0	2.0
1071_0024823	1.0	1.0
1071_0024835	1.0	1.0
1071_0024836	2.0	2.0
1071_0024840	1.0	1.0
1071_0024852	0.0	0.0
1071_0024857	1.0	2.0
1071_0024860	1.0	1.0
1071_0024861	0.0	1.0
1071_0024864	0.0	0.0
1071_0024865	2.0	2.0
1071_0024866	2.0	2.0
1071_0024878	2.0	2.0
1071_0024879	1.0	1.0
1071_0024881	2.0	2.0
1071_0241831	1.0	2.0
1071_0242013	1.0	1.0
1071_0242021	1.0	1.0
1071_0242042	1.0	1.0
1071_0242073	1.0	1.0
1071_0243502	1.0	1.0
1071_0248305	0.0	1.0
1071_0248311	2.0	1.0
1071_0248312	1.0	1.0
1071_0248317	0.0	0.0
1071_0248324	0.0	1.0
1071_0248328	0.0	1.0
1071_0248331	1.0	1.0
1071_0248346	1.0	1.0
1071_0248349	0.0	1.0
1091_0000018	2.0	2.0
1091_0000020	1.0	2.0
1091_0000023	2.0	1.0
1091_0000026	1.0	1.0
1091_0000034	2.0	1.0
1091_0000044	1.0	1.0
1091_0000045	2.0	2.0
1091_0000047	2.0	1.0
1091_0000049	1.0	1.0
1091_0000058	2.0	2.0
1091_0000060	2.0	2.0
1091_0000061	2.0	1.0
1091_0000062	2.0	1.0
1091_0000063	2.0	1.0
1091_0000065	1.0	1.0
1091_0000066	2.0	1.0
1091_0000068	1.0	2.0
1091_0000079	1.0	2.0
1091_0000087	2.0	2.0
1091_0000092	1.0	2.0
1091_0000101	2.0	2.0
1091_0000126	3.0	2.0
1091_0000153	1.0	2.0
1091_0000155	2.0	2.0
1091_0000161	2.0	2.0
1091_0000167	1.0	2.0
1091_0000168	2.0	2.0
1091_0000174	2.0	1.0
1091_0000191	1.0	2.0
1091_0000196	2.0	2.0
1091_0000210	2.0	2.0
1091_0000217	2.0	1.0
1091_0000218	2.0	2.0
1091_0000235	1.0	1.0
1091_0000238	2.0	2.0
1091_0000245	1.0	1.0
1091_0000250	2.0	2.0
1091_0000263	2.0	2.0
1091_0000276	2.0	2.0
Averaged weighted F1-scores 0.6793414020307764
130.68810916179336 82.83744971317162
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
LANGUAGE: DE, 0th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 1.31
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.03
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        16
         1.0       0.56      0.73      0.63        52
         2.0       0.65      0.65      0.65        63
         3.0       0.63      0.84      0.72        56
         4.0       0.00      0.00      0.00        16
         5.0       0.00      0.00      0.00         3

    accuracy                           0.61       206
   macro avg       0.31      0.37      0.33       206
weighted avg       0.51      0.61      0.55       206

[[ 0 15  1  0  0  0]
 [ 0 38 14  0  0  0]
 [ 0 13 41  9  0  0]
 [ 0  2  7 47  0  0]
 [ 0  0  0 16  0  0]
 [ 0  0  0  3  0  0]]
0.5539637836903086
206 206 206



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.05
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.01
              precision    recall  f1-score   support

         0.0       0.43      0.19      0.26        16
         1.0       0.49      0.75      0.60        52
         2.0       0.55      0.51      0.53        63
         3.0       0.63      0.70      0.66        56
         4.0       0.00      0.00      0.00        16
         5.0       0.00      0.00      0.00         3

    accuracy                           0.55       206
   macro avg       0.35      0.36      0.34       206
weighted avg       0.50      0.55      0.51       206

[[ 3 13  0  0  0  0]
 [ 3 39 10  0  0  0]
 [ 1 25 32  5  0  0]
 [ 0  2 15 39  0  0]
 [ 0  0  0 16  0  0]
 [ 0  0  1  2  0  0]]
0.5120146034304666
206 206 206



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.91
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.06
              precision    recall  f1-score   support

         0.0       0.50      0.19      0.27        16
         1.0       0.47      0.83      0.60        52
         2.0       0.50      0.41      0.45        63
         3.0       0.60      0.61      0.60        56
         4.0       0.00      0.00      0.00        16
         5.0       0.00      0.00      0.00         3

    accuracy                           0.51       206
   macro avg       0.34      0.34      0.32       206
weighted avg       0.47      0.51      0.47       206

[[ 3 13  0  0  0  0]
 [ 3 43  6  0  0  0]
 [ 0 32 26  5  0  0]
 [ 0  3 19 34  0  0]
 [ 0  0  0 16  0  0]
 [ 0  0  1  2  0  0]]
0.47486619030267263
206 206 206



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.78
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.02
              precision    recall  f1-score   support

         0.0       0.44      0.25      0.32        16
         1.0       0.49      0.73      0.59        52
         2.0       0.55      0.46      0.50        63
         3.0       0.61      0.73      0.67        56
         4.0       0.00      0.00      0.00        16
         5.0       0.00      0.00      0.00         3

    accuracy                           0.54       206
   macro avg       0.35      0.36      0.35       206
weighted avg       0.49      0.54      0.51       206

[[ 4 12  0  0  0  0]
 [ 3 38 11  0  0  0]
 [ 1 26 29  7  0  0]
 [ 1  1 13 41  0  0]
 [ 0  0  0 16  0  0]
 [ 0  0  0  3  0  0]]
0.5077135546022429
206 206 206
Filename	True Label	Prediction
1023_0001422	3.0	3.0
1023_0001575	3.0	3.0
1023_0101683	3.0	2.0
1023_0101695	2.0	2.0
1023_0101700	2.0	3.0
1023_0101843	3.0	2.0
1023_0101851	3.0	3.0
1023_0101852	5.0	3.0
1023_0101901	3.0	3.0
1023_0103844	5.0	3.0
1023_0103955	4.0	3.0
1023_0104206	3.0	3.0
1023_0107725	3.0	2.0
1023_0107727	5.0	3.0
1023_0107784	2.0	2.0
1023_0108650	3.0	2.0
1023_0108751	3.0	3.0
1023_0108766	2.0	3.0
1023_0108813	3.0	3.0
1023_0108814	3.0	3.0
1023_0108958	2.0	3.0
1023_0109151	3.0	3.0
1023_0109392	3.0	3.0
1023_0109401	3.0	2.0
1023_0109402	3.0	3.0
1023_0109505	3.0	3.0
1023_0109515	4.0	3.0
1023_0109516	3.0	3.0
1023_0109606	3.0	3.0
1023_0109614	2.0	2.0
1023_0109649	3.0	3.0
1023_0109671	2.0	3.0
1023_0109716	3.0	2.0
1023_0109717	3.0	3.0
1031_0001950	4.0	3.0
1031_0001997	3.0	3.0
1031_0002006	4.0	3.0
1031_0002086	3.0	3.0
1031_0002185	4.0	3.0
1031_0002196	4.0	3.0
1031_0002200	3.0	3.0
1031_0003052	4.0	3.0
1031_0003071	3.0	3.0
1031_0003076	4.0	3.0
1031_0003077	3.0	3.0
1031_0003121	3.0	3.0
1031_0003130	4.0	3.0
1031_0003131	3.0	3.0
1031_0003133	4.0	3.0
1031_0003157	4.0	3.0
1031_0003160	3.0	3.0
1031_0003164	4.0	3.0
1031_0003166	3.0	3.0
1031_0003167	3.0	3.0
1031_0003173	3.0	3.0
1031_0003181	4.0	3.0
1031_0003182	4.0	3.0
1031_0003184	4.0	3.0
1031_0003186	4.0	3.0
1031_0003221	2.0	3.0
1031_0003238	3.0	3.0
1031_0003244	3.0	3.0
1031_0003245	3.0	3.0
1031_0003262	3.0	3.0
1031_0003272	3.0	3.0
1031_0003274	3.0	3.0
1031_0003313	3.0	3.0
1031_0003337	3.0	3.0
1031_0003338	3.0	3.0
1031_0003354	3.0	3.0
1031_0003356	3.0	3.0
1031_0003388	3.0	3.0
1031_0003419	3.0	3.0
1061_0120276	2.0	2.0
1061_0120279	1.0	2.0
1061_0120282	0.0	1.0
1061_0120298	2.0	2.0
1061_0120300	3.0	2.0
1061_0120303	1.0	1.0
1061_0120313	2.0	1.0
1061_0120317	3.0	2.0
1061_0120326	2.0	2.0
1061_0120328	1.0	2.0
1061_0120330	2.0	2.0
1061_0120331	1.0	1.0
1061_0120333	2.0	3.0
1061_0120358	1.0	2.0
1061_0120366	3.0	2.0
1061_0120367	3.0	2.0
1061_0120370	2.0	2.0
1061_0120383	2.0	3.0
1061_0120390	2.0	2.0
1061_0120409	3.0	2.0
1061_0120415	2.0	1.0
1061_0120431	2.0	2.0
1061_0120443	0.0	1.0
1061_0120448	3.0	2.0
1061_0120459	2.0	2.0
1061_0120460	2.0	2.0
1061_0120480	2.0	2.0
1061_0120485	3.0	2.0
1061_0120487	2.0	2.0
1061_0120491	2.0	2.0
1061_0120494	2.0	2.0
1061_0120855	2.0	2.0
1061_0120883	2.0	2.0
1061_0120887	1.0	2.0
1061_1029111	2.0	2.0
1061_1029112	3.0	3.0
1061_1029113	2.0	1.0
1061_1029119	1.0	2.0
1061_1202912	2.0	2.0
1071_0024680	2.0	2.0
1071_0024683	0.0	1.0
1071_0024693	1.0	1.0
1071_0024701	2.0	2.0
1071_0024708	1.0	1.0
1071_0024711	1.0	1.0
1071_0024712	1.0	1.0
1071_0024713	1.0	1.0
1071_0024756	1.0	1.0
1071_0024759	0.0	1.0
1071_0024761	2.0	1.0
1071_0024775	0.0	0.0
1071_0024800	0.0	1.0
1071_0024814	1.0	1.0
1071_0024816	1.0	1.0
1071_0024817	1.0	1.0
1071_0024833	1.0	1.0
1071_0024834	2.0	2.0
1071_0024835	1.0	1.0
1071_0024836	2.0	1.0
1071_0024837	0.0	0.0
1071_0024846	0.0	1.0
1071_0024851	2.0	1.0
1071_0024877	1.0	1.0
1071_0024878	2.0	1.0
1071_0241833	1.0	1.0
1071_0242011	1.0	1.0
1071_0242043	0.0	1.0
1071_0243502	1.0	0.0
1071_0243622	1.0	0.0
1071_0248303	1.0	1.0
1071_0248308	1.0	1.0
1071_0248313	1.0	1.0
1071_0248314	1.0	1.0
1071_0248316	1.0	0.0
1071_0248318	0.0	0.0
1071_0248320	0.0	0.0
1071_0248321	1.0	1.0
1071_0248323	0.0	1.0
1071_0248324	0.0	1.0
1071_0248326	1.0	1.0
1071_0248331	1.0	1.0
1071_0248333	2.0	1.0
1071_0248338	2.0	1.0
1071_0248349	0.0	1.0
1071_0248350	2.0	1.0
1091_0000001	1.0	1.0
1091_0000013	1.0	1.0
1091_0000016	1.0	1.0
1091_0000018	2.0	2.0
1091_0000020	1.0	2.0
1091_0000028	1.0	1.0
1091_0000030	0.0	1.0
1091_0000036	1.0	1.0
1091_0000044	0.0	1.0
1091_0000052	1.0	1.0
1091_0000059	1.0	2.0
1091_0000063	2.0	1.0
1091_0000077	2.0	0.0
1091_0000078	3.0	0.0
1091_0000086	1.0	1.0
1091_0000087	2.0	1.0
1091_0000101	2.0	1.0
1091_0000113	1.0	2.0
1091_0000116	2.0	2.0
1091_0000123	2.0	2.0
1091_0000126	2.0	1.0
1091_0000145	1.0	1.0
1091_0000152	1.0	1.0
1091_0000166	2.0	1.0
1091_0000170	3.0	1.0
1091_0000172	2.0	1.0
1091_0000195	1.0	1.0
1091_0000196	2.0	1.0
1091_0000211	1.0	2.0
1091_0000214	2.0	1.0
1091_0000218	2.0	1.0
1091_0000222	2.0	1.0
1091_0000226	1.0	1.0
1091_0000228	2.0	1.0
1091_0000229	2.0	2.0
1091_0000234	2.0	2.0
1091_0000236	2.0	1.0
1091_0000239	2.0	1.0
1091_0000242	1.0	1.0
1091_0000247	1.0	1.0
1091_0000248	2.0	1.0
1091_0000249	2.0	1.0
1091_0000252	1.0	2.0
1091_0000253	1.0	1.0
1091_0000264	2.0	1.0
1091_0000268	2.0	2.0
1091_0000269	1.0	2.0
1091_0000272	1.0	1.0
LANGUAGE: DE, 1th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.29
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.05
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        15
         1.0       0.55      0.56      0.55        52
         2.0       0.58      0.59      0.58        64
         3.0       0.58      0.91      0.71        55
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         2

    accuracy                           0.57       205
   macro avg       0.28      0.34      0.31       205
weighted avg       0.47      0.57      0.51       205

[[ 0 15  0  0  0  0]
 [ 0 29 23  0  0  0]
 [ 0  8 38 18  0  0]
 [ 0  1  4 50  0  0]
 [ 0  0  1 16  0  0]
 [ 0  0  0  2  0  0]]
0.5129087138428083
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.03
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.12
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        15
         1.0       0.47      0.37      0.41        52
         2.0       0.50      0.59      0.54        64
         3.0       0.57      0.93      0.71        55
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         2

    accuracy                           0.53       205
   macro avg       0.26      0.31      0.28       205
weighted avg       0.43      0.53      0.46       205

[[ 0 14  1  0  0  0]
 [ 0 19 32  1  0  0]
 [ 0  7 38 19  0  0]
 [ 0  0  4 51  0  0]
 [ 0  0  1 16  0  0]
 [ 0  0  0  2  0  0]]
0.4642900065646619
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.95
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.93
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        15
         1.0       0.56      0.67      0.61        52
         2.0       0.63      0.66      0.64        64
         3.0       0.63      0.85      0.72        55
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         2

    accuracy                           0.60       205
   macro avg       0.30      0.36      0.33       205
weighted avg       0.50      0.60      0.55       205

[[ 0 15  0  0  0  0]
 [ 0 35 17  0  0  0]
 [ 0 12 42 10  0  0]
 [ 0  1  7 47  0  0]
 [ 0  0  1 16  0  0]
 [ 0  0  0  2  0  0]]
0.5485832810790515
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.84
  Training epoch took: 31
Running Validation...
  Average evaluation loss: 0.92
              precision    recall  f1-score   support

         0.0       0.71      0.33      0.45        15
         1.0       0.57      0.58      0.57        52
         2.0       0.61      0.64      0.63        64
         3.0       0.62      0.87      0.73        55
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         2

    accuracy                           0.60       205
   macro avg       0.42      0.40      0.40       205
weighted avg       0.55      0.60      0.57       205

[[ 5 10  0  0  0  0]
 [ 2 30 20  0  0  0]
 [ 0 12 41 11  0  0]
 [ 0  1  5 48  1  0]
 [ 0  0  1 16  0  0]
 [ 0  0  0  2  0  0]]
0.56874895724272
205 205 205
Filename	True Label	Prediction
1023_0101690	2.0	3.0
1023_0101693	3.0	3.0
1023_0101845	3.0	3.0
1023_0101847	3.0	3.0
1023_0101849	3.0	3.0
1023_0101855	2.0	3.0
1023_0101856	3.0	3.0
1023_0101909	4.0	3.0
1023_0103829	2.0	3.0
1023_0103830	3.0	3.0
1023_0103834	3.0	3.0
1023_0103838	3.0	3.0
1023_0103839	3.0	3.0
1023_0107244	3.0	3.0
1023_0107682	2.0	2.0
1023_0107729	3.0	3.0
1023_0107740	3.0	3.0
1023_0107783	3.0	3.0
1023_0108306	3.0	3.0
1023_0108307	3.0	3.0
1023_0108520	3.0	3.0
1023_0108648	2.0	3.0
1023_0108649	3.0	3.0
1023_0108885	2.0	3.0
1023_0108889	3.0	3.0
1023_0108992	3.0	3.0
1023_0108993	3.0	3.0
1023_0109030	2.0	3.0
1023_0109033	4.0	3.0
1023_0109039	3.0	3.0
1023_0109248	2.0	3.0
1023_0109400	3.0	2.0
1023_0109524	2.0	3.0
1023_0109527	4.0	3.0
1023_0109528	3.0	3.0
1023_0109588	3.0	3.0
1023_0109651	3.0	3.0
1023_0109945	5.0	3.0
1031_0001949	3.0	3.0
1031_0002003	3.0	3.0
1031_0002087	4.0	3.0
1031_0002088	4.0	3.0
1031_0002092	4.0	3.0
1031_0002197	4.0	3.0
1031_0002198	3.0	3.0
1031_0002199	4.0	3.0
1031_0003013	4.0	3.0
1031_0003042	3.0	3.0
1031_0003048	4.0	3.0
1031_0003063	4.0	3.0
1031_0003073	3.0	3.0
1031_0003085	3.0	4.0
1031_0003088	4.0	3.0
1031_0003098	4.0	3.0
1031_0003141	3.0	3.0
1031_0003145	4.0	3.0
1031_0003150	3.0	3.0
1031_0003155	3.0	3.0
1031_0003156	3.0	3.0
1031_0003169	3.0	3.0
1031_0003170	3.0	3.0
1031_0003174	3.0	3.0
1031_0003180	4.0	3.0
1031_0003189	3.0	3.0
1031_0003211	2.0	3.0
1031_0003327	3.0	3.0
1031_0003336	3.0	3.0
1031_0003359	3.0	3.0
1031_0003368	3.0	3.0
1031_0003387	3.0	3.0
1031_0003407	3.0	3.0
1031_0003409	5.0	3.0
1031_0003410	3.0	3.0
1031_0003415	4.0	3.0
1061_0012029	4.0	2.0
1061_0120275	2.0	2.0
1061_0120283	1.0	1.0
1061_0120289	1.0	2.0
1061_0120296	2.0	2.0
1061_0120307	2.0	2.0
1061_0120310	2.0	2.0
1061_0120311	3.0	2.0
1061_0120314	2.0	2.0
1061_0120315	2.0	1.0
1061_0120318	1.0	2.0
1061_0120324	2.0	2.0
1061_0120325	2.0	2.0
1061_0120332	2.0	2.0
1061_0120334	2.0	2.0
1061_0120335	3.0	3.0
1061_0120341	2.0	1.0
1061_0120343	2.0	2.0
1061_0120346	2.0	2.0
1061_0120348	1.0	1.0
1061_0120349	1.0	1.0
1061_0120353	1.0	1.0
1061_0120354	1.0	2.0
1061_0120361	3.0	2.0
1061_0120369	2.0	2.0
1061_0120373	2.0	2.0
1061_0120376	2.0	2.0
1061_0120388	2.0	2.0
1061_0120389	2.0	2.0
1061_0120403	3.0	2.0
1061_0120423	2.0	3.0
1061_0120432	2.0	2.0
1061_0120441	2.0	2.0
1061_0120442	3.0	2.0
1061_0120458	3.0	3.0
1061_0120479	2.0	2.0
1061_0120492	3.0	3.0
1061_0120493	2.0	2.0
1061_0120495	2.0	2.0
1061_0120497	2.0	3.0
1061_0120498	2.0	2.0
1061_0120853	2.0	2.0
1061_0120857	2.0	2.0
1061_0120859	2.0	2.0
1061_0120874	1.0	2.0
1061_0120876	2.0	2.0
1061_0120882	3.0	3.0
1061_0120888	1.0	2.0
1061_0120889	1.0	1.0
1061_1029115	1.0	2.0
1061_1029116	1.0	2.0
1061_1029120	2.0	2.0
1061_1202913	2.0	1.0
1071_0024685	2.0	2.0
1071_0024690	1.0	2.0
1071_0024691	1.0	2.0
1071_0024699	1.0	1.0
1071_0024706	1.0	1.0
1071_0024758	2.0	1.0
1071_0024762	0.0	1.0
1071_0024769	0.0	1.0
1071_0024782	0.0	0.0
1071_0024797	0.0	1.0
1071_0024806	1.0	1.0
1071_0024808	1.0	1.0
1071_0024809	0.0	1.0
1071_0024811	1.0	1.0
1071_0024815	1.0	1.0
1071_0024821	0.0	1.0
1071_0024824	1.0	1.0
1071_0024826	1.0	1.0
1071_0024831	0.0	0.0
1071_0024840	1.0	0.0
1071_0024841	0.0	1.0
1071_0024845	0.0	0.0
1071_0024850	0.0	1.0
1071_0024879	1.0	1.0
1071_0241831	1.0	1.0
1071_0241832	1.0	1.0
1071_0242013	1.0	1.0
1071_0242091	1.0	0.0
1071_0243581	1.0	1.0
1071_0243592	1.0	1.0
1071_0243593	1.0	1.0
1071_0243621	2.0	1.0
1071_0248301	2.0	1.0
1071_0248305	0.0	1.0
1071_0248311	2.0	1.0
1071_0248315	0.0	0.0
1071_0248317	0.0	0.0
1071_0248325	0.0	1.0
1071_0248327	0.0	1.0
1071_0248339	2.0	1.0
1071_0248345	1.0	2.0
1091_0000002	2.0	2.0
1091_0000004	1.0	1.0
1091_0000012	1.0	1.0
1091_0000017	2.0	1.0
1091_0000019	1.0	1.0
1091_0000025	1.0	1.0
1091_0000032	1.0	1.0
1091_0000039	1.0	1.0
1091_0000056	1.0	2.0
1091_0000057	2.0	1.0
1091_0000062	2.0	1.0
1091_0000068	1.0	2.0
1091_0000073	3.0	1.0
1091_0000102	2.0	2.0
1091_0000144	1.0	1.0
1091_0000157	2.0	2.0
1091_0000159	2.0	2.0
1091_0000160	2.0	2.0
1091_0000161	2.0	2.0
1091_0000167	1.0	2.0
1091_0000185	2.0	1.0
1091_0000190	1.0	1.0
1091_0000191	1.0	2.0
1091_0000200	2.0	2.0
1091_0000201	2.0	2.0
1091_0000208	1.0	2.0
1091_0000212	1.0	2.0
1091_0000219	1.0	1.0
1091_0000220	1.0	2.0
1091_0000223	2.0	2.0
1091_0000233	2.0	2.0
1091_0000238	1.0	2.0
1091_0000240	1.0	1.0
1091_0000244	2.0	2.0
1091_0000246	2.0	2.0
1091_0000257	1.0	2.0
1091_0000274	1.0	2.0
LANGUAGE: DE, 2th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.30
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.04
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        15
         1.0       0.57      0.83      0.67        52
         2.0       0.65      0.53      0.59        64
         3.0       0.57      0.80      0.67        55
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         2

    accuracy                           0.59       205
   macro avg       0.30      0.36      0.32       205
weighted avg       0.50      0.59      0.53       205

[[ 0 15  0  0  0  0]
 [ 0 43  9  0  0  0]
 [ 0 16 34 14  0  0]
 [ 0  2  9 44  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  2  0  0]]
0.5322995514437903
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.01
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.06
              precision    recall  f1-score   support

         0.0       1.00      0.07      0.12        15
         1.0       0.62      0.69      0.65        52
         2.0       0.63      0.52      0.57        64
         3.0       0.53      0.91      0.67        55
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         2

    accuracy                           0.59       205
   macro avg       0.46      0.36      0.34       205
weighted avg       0.57      0.59      0.53       205

[[ 1 13  1  0  0  0]
 [ 0 36 15  1  0  0]
 [ 0  7 33 24  0  0]
 [ 0  2  3 50  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  2  0  0]]
0.5328678459398256
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.86
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.06
              precision    recall  f1-score   support

         0.0       0.64      0.47      0.54        15
         1.0       0.68      0.50      0.58        52
         2.0       0.57      0.58      0.57        64
         3.0       0.54      0.89      0.67        55
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         2

    accuracy                           0.58       205
   macro avg       0.40      0.41      0.39       205
weighted avg       0.54      0.58      0.55       205

[[ 7  5  3  0  0  0]
 [ 4 26 21  1  0  0]
 [ 0  5 37 22  0  0]
 [ 0  2  4 49  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  2  0  0]]
0.5451334343176549
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.73
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.03
              precision    recall  f1-score   support

         0.0       0.64      0.47      0.54        15
         1.0       0.63      0.63      0.63        52
         2.0       0.56      0.50      0.53        64
         3.0       0.52      0.80      0.63        55
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         2

    accuracy                           0.57       205
   macro avg       0.39      0.40      0.39       205
weighted avg       0.52      0.57      0.53       205

[[ 7  7  1  0  0  0]
 [ 4 33 15  0  0  0]
 [ 0 10 32 22  0  0]
 [ 0  2  9 44  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  2  0  0]]
0.5341443478915763
205 205 205
Filename	True Label	Prediction
1023_0001419	3.0	3.0
1023_0001420	4.0	3.0
1023_0101675	3.0	3.0
1023_0101688	4.0	3.0
1023_0101689	2.0	2.0
1023_0101701	2.0	3.0
1023_0101751	3.0	3.0
1023_0101846	4.0	3.0
1023_0101854	2.0	2.0
1023_0101895	3.0	3.0
1023_0101899	2.0	3.0
1023_0101906	2.0	3.0
1023_0102118	3.0	3.0
1023_0103822	2.0	2.0
1023_0103825	3.0	3.0
1023_0103832	3.0	3.0
1023_0103883	3.0	3.0
1023_0104203	2.0	3.0
1023_0104207	2.0	3.0
1023_0104209	3.0	3.0
1023_0107042	4.0	3.0
1023_0107074	4.0	3.0
1023_0107672	2.0	3.0
1023_0107773	3.0	3.0
1023_0107781	3.0	3.0
1023_0108518	3.0	2.0
1023_0108811	3.0	3.0
1023_0108812	3.0	3.0
1023_0108887	2.0	3.0
1023_0108888	2.0	3.0
1023_0108890	3.0	3.0
1023_0108932	2.0	3.0
1023_0108934	3.0	3.0
1023_0109022	3.0	3.0
1023_0109026	2.0	3.0
1023_0109096	3.0	2.0
1023_0109247	3.0	2.0
1023_0109249	2.0	3.0
1023_0109250	2.0	3.0
1023_0109267	2.0	3.0
1023_0109422	4.0	3.0
1023_0109495	3.0	3.0
1023_0109591	4.0	3.0
1023_0109946	3.0	3.0
1023_0109947	2.0	3.0
1023_0109951	3.0	3.0
1023_0109954	3.0	3.0
1023_0111896	2.0	3.0
1031_0001703	4.0	3.0
1031_0002002	2.0	3.0
1031_0002036	4.0	3.0
1031_0002042	4.0	3.0
1031_0002079	5.0	3.0
1031_0002083	3.0	3.0
1031_0002131	3.0	3.0
1031_0002187	3.0	3.0
1031_0003043	5.0	3.0
1031_0003072	3.0	3.0
1031_0003074	3.0	3.0
1031_0003099	3.0	3.0
1031_0003126	3.0	3.0
1031_0003127	4.0	3.0
1031_0003129	4.0	3.0
1031_0003136	3.0	3.0
1031_0003140	3.0	3.0
1031_0003162	3.0	3.0
1031_0003183	3.0	3.0
1031_0003191	3.0	3.0
1031_0003207	4.0	3.0
1031_0003214	2.0	3.0
1031_0003217	4.0	3.0
1031_0003218	3.0	3.0
1031_0003231	4.0	3.0
1031_0003234	3.0	3.0
1031_0003235	3.0	3.0
1031_0003246	4.0	3.0
1031_0003315	3.0	3.0
1031_0003331	3.0	3.0
1031_0003339	3.0	3.0
1031_0003365	3.0	3.0
1031_0003366	3.0	3.0
1031_0003369	3.0	3.0
1031_0003384	2.0	3.0
1031_0003391	2.0	3.0
1031_0003392	4.0	3.0
1031_0003393	3.0	3.0
1031_0003408	3.0	3.0
1061_0120272	2.0	2.0
1061_0120278	2.0	2.0
1061_0120290	1.0	2.0
1061_0120291	1.0	1.0
1061_0120308	2.0	2.0
1061_0120309	1.0	1.0
1061_0120329	2.0	2.0
1061_0120336	1.0	2.0
1061_0120337	2.0	2.0
1061_0120345	2.0	2.0
1061_0120351	2.0	2.0
1061_0120352	1.0	1.0
1061_0120355	1.0	1.0
1061_0120357	3.0	3.0
1061_0120375	2.0	1.0
1061_0120382	1.0	2.0
1061_0120410	2.0	1.0
1061_0120413	1.0	1.0
1061_0120414	2.0	2.0
1061_0120424	2.0	3.0
1061_0120429	3.0	2.0
1061_0120430	2.0	2.0
1061_0120433	1.0	1.0
1061_0120438	3.0	2.0
1061_0120440	1.0	1.0
1061_0120449	2.0	2.0
1061_0120450	2.0	3.0
1061_0120455	2.0	2.0
1061_0120478	2.0	3.0
1061_0120483	2.0	2.0
1061_0120484	2.0	2.0
1061_0120489	2.0	2.0
1061_0120490	2.0	2.0
1061_0120880	3.0	2.0
1061_1029118	1.0	2.0
1061_1202914	1.0	2.0
1061_1202917	2.0	2.0
1061_1202918	1.0	2.0
1061_1202919	2.0	2.0
1071_0024678	2.0	1.0
1071_0024682	2.0	2.0
1071_0024688	1.0	2.0
1071_0024694	1.0	2.0
1071_0024704	1.0	1.0
1071_0024715	1.0	1.0
1071_0024765	0.0	0.0
1071_0024766	1.0	0.0
1071_0024773	1.0	1.0
1071_0024777	1.0	1.0
1071_0024778	0.0	1.0
1071_0024784	1.0	0.0
1071_0024799	2.0	2.0
1071_0024807	1.0	1.0
1071_0024812	0.0	1.0
1071_0024820	0.0	1.0
1071_0024825	1.0	1.0
1071_0024827	1.0	1.0
1071_0024847	1.0	1.0
1071_0024848	1.0	1.0
1071_0024849	0.0	0.0
1071_0024859	1.0	2.0
1071_0024862	2.0	1.0
1071_0024864	0.0	0.0
1071_0024865	2.0	2.0
1071_0024871	1.0	2.0
1071_0024874	0.0	1.0
1071_0242012	1.0	2.0
1071_0242021	1.0	1.0
1071_0242041	1.0	1.0
1071_0242042	0.0	1.0
1071_0242072	0.0	0.0
1071_0243501	1.0	1.0
1071_0243591	1.0	1.0
1071_0248304	1.0	1.0
1071_0248312	1.0	1.0
1071_0248319	0.0	0.0
1071_0248328	0.0	0.0
1071_0248329	1.0	1.0
1071_0248340	0.0	0.0
1071_0248341	1.0	0.0
1071_0248347	1.0	1.0
1071_0248348	1.0	1.0
1091_0000010	3.0	2.0
1091_0000024	3.0	1.0
1091_0000026	1.0	1.0
1091_0000029	2.0	1.0
1091_0000034	2.0	1.0
1091_0000035	1.0	1.0
1091_0000038	2.0	1.0
1091_0000041	1.0	0.0
1091_0000049	1.0	1.0
1091_0000050	1.0	1.0
1091_0000051	1.0	1.0
1091_0000053	0.0	1.0
1091_0000058	2.0	2.0
1091_0000070	2.0	1.0
1091_0000072	0.0	2.0
1091_0000154	1.0	2.0
1091_0000158	2.0	2.0
1091_0000164	2.0	1.0
1091_0000165	1.0	1.0
1091_0000168	2.0	2.0
1091_0000169	3.0	2.0
1091_0000171	2.0	2.0
1091_0000199	2.0	2.0
1091_0000204	3.0	2.0
1091_0000205	1.0	2.0
1091_0000213	1.0	1.0
1091_0000215	1.0	2.0
1091_0000217	3.0	1.0
1091_0000224	1.0	1.0
1091_0000232	2.0	2.0
1091_0000255	0.0	1.0
1091_0000256	1.0	2.0
1091_0000258	2.0	2.0
1091_0000260	2.0	2.0
1091_0000262	2.0	1.0
1091_0000276	2.0	2.0
LANGUAGE: DE, 3th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.32
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.09
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        15
         1.0       0.55      0.52      0.53        52
         2.0       0.55      0.59      0.57        64
         3.0       0.59      0.91      0.71        56
         4.0       0.00      0.00      0.00        16
         5.0       0.00      0.00      0.00         2

    accuracy                           0.57       205
   macro avg       0.28      0.34      0.30       205
weighted avg       0.47      0.57      0.51       205

[[ 0 13  2  0  0  0]
 [ 0 27 25  0  0  0]
 [ 0  8 38 18  0  0]
 [ 0  1  4 51  0  0]
 [ 0  0  0 16  0  0]
 [ 0  0  0  2  0  0]]
0.5088656815292893
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.04
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.99
              precision    recall  f1-score   support

         0.0       0.75      0.20      0.32        15
         1.0       0.48      0.67      0.56        52
         2.0       0.50      0.41      0.45        64
         3.0       0.59      0.80      0.68        56
         4.0       0.00      0.00      0.00        16
         5.0       0.00      0.00      0.00         2

    accuracy                           0.53       205
   macro avg       0.39      0.35      0.33       205
weighted avg       0.49      0.53      0.49       205

[[ 3 11  1  0  0  0]
 [ 1 35 16  0  0  0]
 [ 0 25 26 13  0  0]
 [ 0  2  9 45  0  0]
 [ 0  0  0 16  0  0]
 [ 0  0  0  2  0  0]]
0.4913576363877811
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.91
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.75      0.20      0.32        15
         1.0       0.51      0.48      0.50        52
         2.0       0.53      0.61      0.57        64
         3.0       0.60      0.84      0.70        56
         4.0       0.00      0.00      0.00        16
         5.0       0.00      0.00      0.00         2

    accuracy                           0.56       205
   macro avg       0.40      0.35      0.35       205
weighted avg       0.51      0.56      0.52       205

[[ 3 11  1  0  0  0]
 [ 1 25 26  0  0  0]
 [ 0 12 39 13  0  0]
 [ 0  1  8 47  0  0]
 [ 0  0  0 16  0  0]
 [ 0  0  0  2  0  0]]
0.5167654219303386
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.80
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.98
              precision    recall  f1-score   support

         0.0       0.62      0.33      0.43        15
         1.0       0.50      0.60      0.54        52
         2.0       0.52      0.52      0.52        64
         3.0       0.62      0.80      0.70        56
         4.0       0.00      0.00      0.00        16
         5.0       0.00      0.00      0.00         2

    accuracy                           0.56       205
   macro avg       0.38      0.37      0.37       205
weighted avg       0.51      0.56      0.52       205

[[ 5  9  1  0  0  0]
 [ 2 31 19  0  0  0]
 [ 1 21 33  9  0  0]
 [ 0  1 10 45  0  0]
 [ 0  0  0 16  0  0]
 [ 0  0  0  2  0  0]]
0.5240843092902722
205 205 205
Filename	True Label	Prediction
1023_0001416	5.0	3.0
1023_0001418	4.0	3.0
1023_0101749	4.0	3.0
1023_0101752	3.0	3.0
1023_0101848	2.0	2.0
1023_0101893	3.0	3.0
1023_0101894	2.0	3.0
1023_0101907	4.0	3.0
1023_0102117	3.0	3.0
1023_0103821	3.0	3.0
1023_0103826	3.0	3.0
1023_0103827	3.0	3.0
1023_0103831	3.0	3.0
1023_0103836	4.0	3.0
1023_0103837	3.0	3.0
1023_0103840	3.0	3.0
1023_0103843	3.0	2.0
1023_0106816	3.0	3.0
1023_0107780	3.0	3.0
1023_0108422	3.0	3.0
1023_0108423	3.0	2.0
1023_0108426	2.0	3.0
1023_0108510	3.0	3.0
1023_0108753	2.0	2.0
1023_0108815	2.0	3.0
1023_0108933	3.0	2.0
1023_0108935	2.0	2.0
1023_0109027	3.0	3.0
1023_0109391	2.0	2.0
1023_0109396	2.0	3.0
1023_0109399	2.0	3.0
1023_0109496	3.0	3.0
1023_0109500	2.0	3.0
1023_0109519	2.0	2.0
1023_0109522	3.0	3.0
1023_0109721	2.0	3.0
1023_0109878	2.0	3.0
1023_0109880	3.0	3.0
1023_0109891	3.0	3.0
1023_0109915	2.0	2.0
1023_0109917	5.0	3.0
1031_0002005	4.0	3.0
1031_0002010	3.0	3.0
1031_0002011	4.0	3.0
1031_0002032	3.0	3.0
1031_0002043	4.0	3.0
1031_0002084	4.0	3.0
1031_0002089	4.0	3.0
1031_0003012	3.0	3.0
1031_0003023	4.0	3.0
1031_0003035	3.0	3.0
1031_0003053	3.0	3.0
1031_0003054	4.0	3.0
1031_0003090	4.0	3.0
1031_0003091	2.0	3.0
1031_0003092	3.0	3.0
1031_0003132	4.0	3.0
1031_0003135	3.0	3.0
1031_0003144	3.0	3.0
1031_0003149	4.0	3.0
1031_0003161	3.0	3.0
1031_0003216	3.0	3.0
1031_0003219	3.0	3.0
1031_0003224	3.0	3.0
1031_0003226	3.0	3.0
1031_0003230	3.0	3.0
1031_0003233	3.0	3.0
1031_0003236	3.0	3.0
1031_0003237	3.0	3.0
1031_0003239	4.0	3.0
1031_0003242	3.0	3.0
1031_0003243	3.0	3.0
1031_0003249	3.0	3.0
1031_0003273	3.0	3.0
1031_0003330	4.0	3.0
1031_0003353	3.0	3.0
1031_0003386	3.0	3.0
1031_0003390	3.0	3.0
1031_0003414	3.0	3.0
1061_0120274	1.0	1.0
1061_0120277	1.0	2.0
1061_0120280	1.0	1.0
1061_0120284	0.0	1.0
1061_0120286	1.0	1.0
1061_0120297	1.0	2.0
1061_0120299	2.0	2.0
1061_0120301	2.0	1.0
1061_0120304	1.0	2.0
1061_0120312	1.0	1.0
1061_0120316	2.0	2.0
1061_0120319	2.0	2.0
1061_0120320	3.0	2.0
1061_0120338	2.0	2.0
1061_0120347	2.0	2.0
1061_0120350	2.0	2.0
1061_0120356	2.0	2.0
1061_0120359	2.0	1.0
1061_0120360	3.0	2.0
1061_0120371	3.0	3.0
1061_0120372	1.0	2.0
1061_0120384	1.0	2.0
1061_0120386	1.0	2.0
1061_0120391	1.0	2.0
1061_0120407	3.0	2.0
1061_0120411	3.0	2.0
1061_0120421	2.0	2.0
1061_0120425	2.0	2.0
1061_0120426	2.0	2.0
1061_0120453	2.0	2.0
1061_0120456	2.0	2.0
1061_0120457	3.0	2.0
1061_0120482	2.0	2.0
1061_0120488	3.0	2.0
1061_0120496	2.0	2.0
1061_0120858	2.0	2.0
1061_0120878	2.0	2.0
1061_0120885	2.0	2.0
1061_0120890	1.0	1.0
1061_0120894	2.0	2.0
1061_1029114	1.0	2.0
1061_1202911	0.0	2.0
1061_1202915	1.0	1.0
1071_0020001	1.0	1.0
1071_0024681	2.0	2.0
1071_0024687	1.0	1.0
1071_0024689	1.0	1.0
1071_0024692	2.0	2.0
1071_0024702	1.0	2.0
1071_0024705	1.0	1.0
1071_0024710	0.0	1.0
1071_0024714	2.0	1.0
1071_0024716	1.0	1.0
1071_0024763	1.0	1.0
1071_0024770	1.0	1.0
1071_0024772	0.0	0.0
1071_0024774	0.0	0.0
1071_0024798	0.0	1.0
1071_0024803	1.0	1.0
1071_0024810	1.0	1.0
1071_0024819	1.0	1.0
1071_0024852	0.0	0.0
1071_0024854	0.0	0.0
1071_0024855	1.0	1.0
1071_0024861	0.0	1.0
1071_0024866	2.0	2.0
1071_0024867	1.0	1.0
1071_0024872	1.0	1.0
1071_0242023	1.0	1.0
1071_0242071	0.0	1.0
1071_0242093	0.0	0.0
1071_0243582	1.0	1.0
1071_0243623	1.0	1.0
1071_0248307	2.0	1.0
1071_0248332	2.0	2.0
1071_0248334	2.0	1.0
1071_0248335	1.0	1.0
1071_0248337	1.0	1.0
1071_0248344	1.0	0.0
1091_0000003	2.0	1.0
1091_0000005	2.0	1.0
1091_0000007	3.0	2.0
1091_0000008	2.0	1.0
1091_0000011	2.0	1.0
1091_0000014	0.0	1.0
1091_0000022	2.0	2.0
1091_0000023	2.0	1.0
1091_0000027	0.0	1.0
1091_0000031	1.0	1.0
1091_0000033	1.0	1.0
1091_0000045	1.0	2.0
1091_0000047	2.0	1.0
1091_0000054	0.0	1.0
1091_0000055	1.0	2.0
1091_0000060	2.0	2.0
1091_0000064	2.0	1.0
1091_0000065	1.0	1.0
1091_0000066	1.0	0.0
1091_0000074	2.0	2.0
1091_0000076	2.0	2.0
1091_0000092	1.0	1.0
1091_0000095	1.0	1.0
1091_0000127	2.0	1.0
1091_0000148	1.0	1.0
1091_0000151	0.0	1.0
1091_0000155	3.0	3.0
1091_0000163	2.0	1.0
1091_0000173	2.0	1.0
1091_0000174	2.0	0.0
1091_0000193	2.0	1.0
1091_0000194	1.0	2.0
1091_0000197	1.0	2.0
1091_0000198	2.0	2.0
1091_0000202	1.0	2.0
1091_0000216	1.0	2.0
1091_0000221	2.0	1.0
1091_0000225	2.0	1.0
1091_0000230	1.0	2.0
1091_0000235	1.0	1.0
1091_0000237	2.0	1.0
1091_0000251	1.0	2.0
1091_0000254	3.0	1.0
1091_0000259	2.0	1.0
1091_0000271	1.0	2.0
1091_0000273	1.0	2.0
1091_0000275	2.0	1.0
LANGUAGE: DE, 4th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.24
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.04
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        16
         1.0       0.56      0.57      0.56        51
         2.0       0.54      0.58      0.56        64
         3.0       0.56      0.86      0.68        56
         4.0       0.00      0.00      0.00        16
         5.0       0.00      0.00      0.00         2

    accuracy                           0.56       205
   macro avg       0.28      0.33      0.30       205
weighted avg       0.46      0.56      0.50       205

[[ 0 15  1  0  0  0]
 [ 0 29 22  0  0  0]
 [ 0  8 37 19  0  0]
 [ 0  0  8 48  0  0]
 [ 0  0  0 16  0  0]
 [ 0  0  0  2  0  0]]
0.5010970441716961
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.99
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.02
              precision    recall  f1-score   support

         0.0       0.57      0.50      0.53        16
         1.0       0.57      0.86      0.69        51
         2.0       0.70      0.36      0.47        64
         3.0       0.58      0.84      0.69        56
         4.0       0.00      0.00      0.00        16
         5.0       0.00      0.00      0.00         2

    accuracy                           0.60       205
   macro avg       0.40      0.43      0.40       205
weighted avg       0.56      0.60      0.55       205

[[ 8  8  0  0  0  0]
 [ 3 44  4  0  0  0]
 [ 3 22 23 16  0  0]
 [ 0  3  6 47  0  0]
 [ 0  0  0 16  0  0]
 [ 0  0  0  2  0  0]]
0.5481449095682168
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.87
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.06
              precision    recall  f1-score   support

         0.0       0.75      0.19      0.30        16
         1.0       0.57      0.55      0.56        51
         2.0       0.55      0.56      0.55        64
         3.0       0.57      0.88      0.69        56
         4.0       0.00      0.00      0.00        16
         5.0       0.00      0.00      0.00         2

    accuracy                           0.57       205
   macro avg       0.41      0.36      0.35       205
weighted avg       0.53      0.57      0.52       205

[[ 3 12  1  0  0  0]
 [ 0 28 23  0  0  0]
 [ 1  8 36 19  0  0]
 [ 0  1  6 49  0  0]
 [ 0  0  0 16  0  0]
 [ 0  0  0  2  0  0]]
0.5241660544882806
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.75
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.04
              precision    recall  f1-score   support

         0.0       0.57      0.25      0.35        16
         1.0       0.59      0.59      0.59        51
         2.0       0.56      0.55      0.56        64
         3.0       0.56      0.86      0.68        56
         4.0       0.00      0.00      0.00        16
         5.0       0.00      0.00      0.00         2

    accuracy                           0.57       205
   macro avg       0.38      0.37      0.36       205
weighted avg       0.52      0.57      0.53       205

[[ 4 11  1  0  0  0]
 [ 2 30 19  0  0  0]
 [ 1  9 35 19  0  0]
 [ 0  1  7 48  0  0]
 [ 0  0  0 16  0  0]
 [ 0  0  0  2  0  0]]
0.5329191830308683
205 205 205
Filename	True Label	Prediction
1023_0001423	2.0	3.0
1023_0101684	2.0	3.0
1023_0101691	4.0	3.0
1023_0101694	2.0	3.0
1023_0101753	3.0	3.0
1023_0101841	3.0	3.0
1023_0101844	2.0	3.0
1023_0101853	2.0	3.0
1023_0101896	3.0	2.0
1023_0101897	3.0	3.0
1023_0101898	4.0	3.0
1023_0101900	3.0	3.0
1023_0101904	2.0	3.0
1023_0103823	4.0	3.0
1023_0103824	3.0	3.0
1023_0103828	1.0	2.0
1023_0103833	5.0	3.0
1023_0103841	5.0	3.0
1023_0103880	3.0	3.0
1023_0107075	2.0	3.0
1023_0107726	3.0	3.0
1023_0107787	2.0	3.0
1023_0107788	3.0	3.0
1023_0108304	3.0	3.0
1023_0108305	3.0	3.0
1023_0108641	3.0	3.0
1023_0108752	3.0	3.0
1023_0108810	3.0	3.0
1023_0108886	3.0	3.0
1023_0108908	2.0	3.0
1023_0108931	3.0	3.0
1023_0108955	3.0	3.0
1023_0109029	1.0	2.0
1023_0109038	3.0	3.0
1023_0109192	3.0	3.0
1023_0109395	2.0	2.0
1023_0109518	2.0	2.0
1023_0109520	2.0	3.0
1023_0109590	2.0	3.0
1023_0109609	3.0	3.0
1023_0109674	3.0	3.0
1023_0109890	3.0	3.0
1023_0109914	2.0	2.0
1031_0001951	2.0	3.0
1031_0001998	4.0	3.0
1031_0002004	4.0	3.0
1031_0002040	4.0	3.0
1031_0002061	3.0	3.0
1031_0002085	3.0	3.0
1031_0002091	4.0	3.0
1031_0002184	3.0	3.0
1031_0002195	3.0	3.0
1031_0003029	4.0	3.0
1031_0003065	3.0	3.0
1031_0003078	3.0	3.0
1031_0003095	3.0	3.0
1031_0003097	3.0	3.0
1031_0003106	3.0	3.0
1031_0003128	4.0	3.0
1031_0003146	4.0	3.0
1031_0003154	4.0	3.0
1031_0003163	3.0	3.0
1031_0003165	2.0	3.0
1031_0003172	3.0	3.0
1031_0003179	3.0	3.0
1031_0003185	3.0	3.0
1031_0003187	4.0	3.0
1031_0003190	3.0	3.0
1031_0003203	2.0	3.0
1031_0003205	3.0	3.0
1031_0003206	3.0	3.0
1031_0003212	3.0	3.0
1031_0003220	3.0	3.0
1031_0003225	3.0	3.0
1031_0003232	2.0	3.0
1031_0003240	2.0	3.0
1031_0003260	4.0	3.0
1031_0003261	3.0	3.0
1031_0003309	3.0	3.0
1031_0003310	3.0	3.0
1031_0003314	4.0	3.0
1031_0003352	2.0	3.0
1031_0003355	3.0	3.0
1031_0003357	3.0	3.0
1031_0003358	4.0	3.0
1031_0003367	4.0	3.0
1031_0003383	3.0	3.0
1031_0003389	3.0	3.0
1061_0120271	2.0	2.0
1061_0120273	1.0	2.0
1061_0120281	1.0	2.0
1061_0120285	2.0	2.0
1061_0120287	1.0	2.0
1061_0120288	2.0	2.0
1061_0120295	0.0	2.0
1061_0120302	1.0	2.0
1061_0120306	3.0	2.0
1061_0120321	2.0	2.0
1061_0120323	1.0	2.0
1061_0120327	2.0	2.0
1061_0120368	2.0	2.0
1061_0120374	2.0	2.0
1061_0120387	1.0	2.0
1061_0120394	2.0	2.0
1061_0120404	2.0	2.0
1061_0120405	2.0	2.0
1061_0120406	2.0	2.0
1061_0120408	2.0	2.0
1061_0120427	2.0	2.0
1061_0120428	2.0	2.0
1061_0120439	1.0	1.0
1061_0120481	3.0	3.0
1061_0120486	2.0	2.0
1061_0120499	2.0	3.0
1061_0120500	2.0	2.0
1061_0120856	2.0	2.0
1061_0120875	3.0	2.0
1061_0120877	2.0	2.0
1061_0120881	2.0	3.0
1061_0120884	2.0	2.0
1061_0120886	2.0	2.0
1061_1029117	2.0	2.0
1061_1202910	3.0	2.0
1061_1202916	2.0	2.0
1071_0024686	3.0	1.0
1071_0024703	1.0	1.0
1071_0024709	2.0	2.0
1071_0024757	2.0	2.0
1071_0024767	2.0	1.0
1071_0024768	1.0	1.0
1071_0024776	0.0	0.0
1071_0024779	1.0	1.0
1071_0024781	0.0	1.0
1071_0024783	0.0	0.0
1071_0024801	1.0	1.0
1071_0024802	1.0	1.0
1071_0024804	0.0	1.0
1071_0024813	0.0	1.0
1071_0024818	2.0	1.0
1071_0024822	0.0	1.0
1071_0024823	1.0	1.0
1071_0024838	0.0	0.0
1071_0024843	1.0	1.0
1071_0024844	1.0	1.0
1071_0024853	0.0	0.0
1071_0024856	1.0	1.0
1071_0024857	0.0	1.0
1071_0024860	1.0	0.0
1071_0024863	1.0	1.0
1071_0024873	0.0	1.0
1071_0024875	1.0	1.0
1071_0024876	1.0	1.0
1071_0024881	1.0	2.0
1071_0242022	1.0	1.0
1071_0242073	1.0	1.0
1071_0242092	0.0	1.0
1071_0248302	1.0	1.0
1071_0248309	2.0	1.0
1071_0248310	1.0	1.0
1071_0248322	1.0	1.0
1071_0248330	2.0	1.0
1071_0248336	1.0	1.0
1071_0248342	0.0	1.0
1071_0248343	1.0	1.0
1071_0248346	1.0	1.0
1091_0000006	0.0	1.0
1091_0000009	0.0	1.0
1091_0000015	1.0	2.0
1091_0000021	1.0	2.0
1091_0000037	1.0	1.0
1091_0000042	0.0	1.0
1091_0000043	1.0	1.0
1091_0000046	1.0	1.0
1091_0000048	1.0	1.0
1091_0000061	2.0	0.0
1091_0000067	1.0	1.0
1091_0000069	2.0	1.0
1091_0000071	1.0	2.0
1091_0000075	2.0	2.0
1091_0000079	1.0	2.0
1091_0000114	2.0	2.0
1091_0000125	2.0	2.0
1091_0000140	2.0	1.0
1091_0000146	1.0	0.0
1091_0000153	1.0	2.0
1091_0000156	3.0	2.0
1091_0000162	1.0	2.0
1091_0000192	1.0	1.0
1091_0000203	2.0	1.0
1091_0000206	1.0	1.0
1091_0000207	2.0	2.0
1091_0000209	3.0	2.0
1091_0000210	2.0	1.0
1091_0000227	1.0	2.0
1091_0000231	2.0	2.0
1091_0000241	2.0	2.0
1091_0000243	1.0	1.0
1091_0000245	1.0	2.0
1091_0000250	1.0	2.0
1091_0000261	2.0	1.0
1091_0000263	3.0	2.0
1091_0000265	2.0	2.0
1091_0000266	1.0	2.0
1091_0000267	1.0	1.0
1091_0000270	2.0	2.0
Averaged weighted F1-scores 0.533522070411536
130.68810916179336 82.83744971317162
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
LANGUAGE: DE, 0th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.20
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.01
              precision    recall  f1-score   support

         0.0       0.75      0.53      0.62        17
         1.0       0.61      0.54      0.57        46
         2.0       0.57      0.82      0.67        71
         3.0       0.65      0.63      0.64        54
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.61       206
   macro avg       0.43      0.42      0.42       206
weighted avg       0.57      0.61      0.58       206

[[ 9  6  2  0  0  0]
 [ 0 25 21  0  0  0]
 [ 3  9 58  1  0  0]
 [ 0  1 19 34  0  0]
 [ 0  0  1 16  0  0]
 [ 0  0  0  1  0  0]]
0.5801637675306858
206 206 206



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.91
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.95
              precision    recall  f1-score   support

         0.0       1.00      0.12      0.21        17
         1.0       0.50      0.41      0.45        46
         2.0       0.62      0.75      0.68        71
         3.0       0.64      0.94      0.76        54
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.61       206
   macro avg       0.46      0.37      0.35       206
weighted avg       0.57      0.61      0.55       206

[[ 2 12  3  0  0  0]
 [ 0 19 27  0  0  0]
 [ 0  7 53 11  0  0]
 [ 0  0  3 51  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
0.5506274491377148
206 206 206



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.81
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.91
              precision    recall  f1-score   support

         0.0       0.86      0.35      0.50        17
         1.0       0.47      0.50      0.48        46
         2.0       0.59      0.68      0.63        71
         3.0       0.64      0.81      0.72        54
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.59       206
   macro avg       0.43      0.39      0.39       206
weighted avg       0.55      0.59      0.55       206

[[ 6  9  2  0  0  0]
 [ 1 23 22  0  0  0]
 [ 0 16 48  7  0  0]
 [ 0  1  9 44  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
0.5546113389084836
206 206 206



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.66
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.92
              precision    recall  f1-score   support

         0.0       0.83      0.29      0.43        17
         1.0       0.48      0.52      0.50        46
         2.0       0.63      0.66      0.64        71
         3.0       0.66      0.80      0.72        54
         4.0       0.40      0.24      0.30        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.60       206
   macro avg       0.50      0.42      0.43       206
weighted avg       0.60      0.60      0.58       206

[[ 5 10  2  0  0  0]
 [ 1 24 21  0  0  0]
 [ 0 15 47  8  1  0]
 [ 0  1  5 43  5  0]
 [ 0  0  0 13  4  0]
 [ 0  0  0  1  0  0]]
0.5833295157088351
206 206 206
Filename	True Label	Prediction
1023_0001416	3.0	3.0
1023_0101675	3.0	2.0
1023_0101846	4.0	3.0
1023_0101907	3.0	3.0
1023_0101909	4.0	3.0
1023_0103823	3.0	3.0
1023_0103830	3.0	3.0
1023_0103831	3.0	2.0
1023_0103833	4.0	3.0
1023_0103880	3.0	3.0
1023_0107074	3.0	3.0
1023_0107075	2.0	3.0
1023_0107672	3.0	3.0
1023_0107740	3.0	3.0
1023_0107781	3.0	3.0
1023_0107784	1.0	2.0
1023_0108648	3.0	3.0
1023_0108650	3.0	3.0
1023_0108753	3.0	2.0
1023_0108811	3.0	3.0
1023_0108812	3.0	3.0
1023_0108958	3.0	3.0
1023_0109039	3.0	3.0
1023_0109250	2.0	2.0
1023_0109399	2.0	3.0
1023_0109422	3.0	3.0
1023_0109495	3.0	2.0
1023_0109505	3.0	3.0
1023_0109522	3.0	3.0
1023_0109527	3.0	3.0
1023_0109614	2.0	2.0
1023_0109717	3.0	3.0
1023_0109878	2.0	2.0
1023_0109891	2.0	3.0
1023_0109946	2.0	3.0
1023_0109954	3.0	3.0
1023_0111896	2.0	3.0
1031_0001703	4.0	3.0
1031_0001949	4.0	4.0
1031_0002004	4.0	3.0
1031_0002006	5.0	3.0
1031_0002036	4.0	4.0
1031_0002085	3.0	4.0
1031_0002091	3.0	4.0
1031_0002092	4.0	4.0
1031_0002196	3.0	3.0
1031_0003013	4.0	3.0
1031_0003023	3.0	4.0
1031_0003042	3.0	3.0
1031_0003048	4.0	3.0
1031_0003085	3.0	3.0
1031_0003092	2.0	4.0
1031_0003121	3.0	3.0
1031_0003136	4.0	3.0
1031_0003144	3.0	3.0
1031_0003156	3.0	3.0
1031_0003169	3.0	4.0
1031_0003182	4.0	3.0
1031_0003184	4.0	3.0
1031_0003205	3.0	4.0
1031_0003206	3.0	3.0
1031_0003207	4.0	3.0
1031_0003211	3.0	3.0
1031_0003218	4.0	4.0
1031_0003225	3.0	3.0
1031_0003233	3.0	3.0
1031_0003236	3.0	3.0
1031_0003246	3.0	3.0
1031_0003260	3.0	3.0
1031_0003261	3.0	3.0
1031_0003336	3.0	3.0
1031_0003355	4.0	3.0
1031_0003366	3.0	3.0
1031_0003387	4.0	3.0
1031_0003390	3.0	3.0
1031_0003410	3.0	3.0
1031_0003419	3.0	3.0
1061_0120274	2.0	2.0
1061_0120277	1.0	2.0
1061_0120287	1.0	2.0
1061_0120291	1.0	1.0
1061_0120300	2.0	2.0
1061_0120301	2.0	1.0
1061_0120313	2.0	1.0
1061_0120316	2.0	2.0
1061_0120331	1.0	2.0
1061_0120335	2.0	3.0
1061_0120341	1.0	2.0
1061_0120355	1.0	1.0
1061_0120360	3.0	3.0
1061_0120366	3.0	3.0
1061_0120373	2.0	2.0
1061_0120374	3.0	3.0
1061_0120375	2.0	2.0
1061_0120387	2.0	2.0
1061_0120394	2.0	2.0
1061_0120408	2.0	2.0
1061_0120448	2.0	2.0
1061_0120456	2.0	2.0
1061_0120460	2.0	2.0
1061_0120487	2.0	2.0
1061_0120489	2.0	2.0
1061_0120490	2.0	2.0
1061_0120495	2.0	2.0
1061_0120496	2.0	2.0
1061_0120499	2.0	2.0
1061_0120859	2.0	3.0
1061_0120881	2.0	3.0
1061_0120882	3.0	2.0
1061_0120884	2.0	2.0
1061_0120887	2.0	2.0
1061_1029113	1.0	2.0
1061_1029115	2.0	2.0
1061_1029117	1.0	2.0
1061_1202910	2.0	2.0
1071_0024682	2.0	2.0
1071_0024685	2.0	2.0
1071_0024692	2.0	2.0
1071_0024715	2.0	2.0
1071_0024765	0.0	1.0
1071_0024766	0.0	0.0
1071_0024768	0.0	1.0
1071_0024772	0.0	0.0
1071_0024774	0.0	0.0
1071_0024781	1.0	1.0
1071_0024806	1.0	1.0
1071_0024812	0.0	1.0
1071_0024823	1.0	1.0
1071_0024835	1.0	1.0
1071_0024836	1.0	1.0
1071_0024837	0.0	0.0
1071_0024840	1.0	1.0
1071_0024841	0.0	1.0
1071_0024844	1.0	1.0
1071_0024848	1.0	1.0
1071_0024857	1.0	1.0
1071_0024865	1.0	1.0
1071_0024874	1.0	1.0
1071_0024881	2.0	1.0
1071_0241831	1.0	1.0
1071_0242043	0.0	1.0
1071_0242093	0.0	1.0
1071_0243581	0.0	1.0
1071_0243582	1.0	1.0
1071_0243591	1.0	1.0
1071_0243593	0.0	1.0
1071_0243621	1.0	1.0
1071_0248301	1.0	1.0
1071_0248309	2.0	1.0
1071_0248311	1.0	1.0
1071_0248317	0.0	1.0
1071_0248320	0.0	0.0
1071_0248333	2.0	1.0
1071_0248335	1.0	1.0
1071_0248339	1.0	1.0
1071_0248344	2.0	1.0
1091_0000006	0.0	1.0
1091_0000008	2.0	2.0
1091_0000016	1.0	1.0
1091_0000033	1.0	2.0
1091_0000039	1.0	0.0
1091_0000042	1.0	1.0
1091_0000046	2.0	1.0
1091_0000053	0.0	2.0
1091_0000055	2.0	2.0
1091_0000057	2.0	1.0
1091_0000059	1.0	2.0
1091_0000060	2.0	2.0
1091_0000061	2.0	1.0
1091_0000063	1.0	2.0
1091_0000064	1.0	2.0
1091_0000065	1.0	2.0
1091_0000070	2.0	2.0
1091_0000072	1.0	2.0
1091_0000073	2.0	1.0
1091_0000077	2.0	1.0
1091_0000087	2.0	2.0
1091_0000123	2.0	2.0
1091_0000140	2.0	1.0
1091_0000148	1.0	1.0
1091_0000153	1.0	2.0
1091_0000158	2.0	2.0
1091_0000161	2.0	2.0
1091_0000162	2.0	2.0
1091_0000166	1.0	2.0
1091_0000170	3.0	1.0
1091_0000194	1.0	2.0
1091_0000197	1.0	2.0
1091_0000198	2.0	2.0
1091_0000203	1.0	2.0
1091_0000205	1.0	2.0
1091_0000209	2.0	2.0
1091_0000211	1.0	2.0
1091_0000214	2.0	2.0
1091_0000217	2.0	1.0
1091_0000223	2.0	2.0
1091_0000225	2.0	1.0
1091_0000234	2.0	2.0
1091_0000235	2.0	1.0
1091_0000238	2.0	2.0
1091_0000240	2.0	2.0
1091_0000252	2.0	2.0
1091_0000255	0.0	2.0
1091_0000269	1.0	2.0
1091_0000270	2.0	2.0
1091_0000276	2.0	2.0
LANGUAGE: DE, 1th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.20
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.92
              precision    recall  f1-score   support

         0.0       0.54      0.44      0.48        16
         1.0       0.53      0.65      0.58        46
         2.0       0.67      0.52      0.59        71
         3.0       0.61      0.91      0.73        54
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.60       205
   macro avg       0.39      0.42      0.40       205
weighted avg       0.55      0.60      0.56       205

[[ 7  8  1  0  0  0]
 [ 5 30 11  0  0  0]
 [ 1 19 37 14  0  0]
 [ 0  0  5 49  0  0]
 [ 0  0  1 16  0  0]
 [ 0  0  0  1  0  0]]
0.564444899745046
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.89
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.04
              precision    recall  f1-score   support

         0.0       0.41      0.88      0.56        16
         1.0       0.43      0.41      0.42        46
         2.0       0.58      0.61      0.59        71
         3.0       0.62      0.61      0.62        54
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.53       205
   macro avg       0.34      0.42      0.37       205
weighted avg       0.49      0.53      0.51       205

[[14  2  0  0  0  0]
 [18 19  9  0  0  0]
 [ 2 23 43  3  0  0]
 [ 0  0 21 33  0  0]
 [ 0  0  1 16  0  0]
 [ 0  0  0  1  0  0]]
0.506346235437862
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.79
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.85
              precision    recall  f1-score   support

         0.0       0.70      0.44      0.54        16
         1.0       0.57      0.63      0.60        46
         2.0       0.68      0.62      0.65        71
         3.0       0.62      0.89      0.73        54
         4.0       0.50      0.06      0.11        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.63       205
   macro avg       0.51      0.44      0.44       205
weighted avg       0.62      0.63      0.60       205

[[ 7  8  1  0  0  0]
 [ 3 29 14  0  0  0]
 [ 0 14 44 13  0  0]
 [ 0  0  6 48  0  0]
 [ 0  0  0 16  1  0]
 [ 0  0  0  0  1  0]]
0.6020668706991614
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.68
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.85
              precision    recall  f1-score   support

         0.0       0.88      0.44      0.58        16
         1.0       0.59      0.65      0.62        46
         2.0       0.68      0.66      0.67        71
         3.0       0.65      0.89      0.75        54
         4.0       0.67      0.12      0.20        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.65       205
   macro avg       0.58      0.46      0.47       205
weighted avg       0.66      0.65      0.63       205

[[ 7  8  1  0  0  0]
 [ 1 30 15  0  0  0]
 [ 0 13 47 11  0  0]
 [ 0  0  6 48  0  0]
 [ 0  0  0 15  2  0]
 [ 0  0  0  0  1  0]]
0.6310164397667541
205 205 205
Filename	True Label	Prediction
1023_0001418	3.0	3.0
1023_0101749	3.0	3.0
1023_0101844	2.0	2.0
1023_0101845	2.0	3.0
1023_0101848	2.0	2.0
1023_0101855	2.0	2.0
1023_0101896	3.0	2.0
1023_0103826	3.0	3.0
1023_0103829	2.0	3.0
1023_0103836	3.0	3.0
1023_0103840	3.0	3.0
1023_0103883	3.0	3.0
1023_0104206	3.0	3.0
1023_0104207	2.0	2.0
1023_0106816	3.0	3.0
1023_0107042	3.0	3.0
1023_0107244	3.0	3.0
1023_0107727	3.0	2.0
1023_0107729	3.0	3.0
1023_0107783	3.0	2.0
1023_0107787	2.0	2.0
1023_0108518	3.0	3.0
1023_0108520	3.0	3.0
1023_0108641	4.0	3.0
1023_0108813	3.0	3.0
1023_0108886	3.0	3.0
1023_0108888	3.0	3.0
1023_0108889	3.0	3.0
1023_0108908	3.0	3.0
1023_0108992	2.0	3.0
1023_0109022	3.0	3.0
1023_0109151	4.0	3.0
1023_0109192	3.0	3.0
1023_0109267	2.0	3.0
1023_0109496	3.0	3.0
1023_0109721	2.0	3.0
1031_0002002	2.0	3.0
1031_0002003	3.0	3.0
1031_0002040	5.0	4.0
1031_0002042	4.0	3.0
1031_0002061	3.0	3.0
1031_0002079	4.0	4.0
1031_0002198	4.0	3.0
1031_0003012	3.0	3.0
1031_0003029	3.0	3.0
1031_0003052	4.0	3.0
1031_0003054	3.0	3.0
1031_0003088	4.0	3.0
1031_0003091	3.0	3.0
1031_0003106	3.0	3.0
1031_0003126	4.0	3.0
1031_0003127	4.0	3.0
1031_0003164	3.0	3.0
1031_0003165	3.0	3.0
1031_0003170	3.0	3.0
1031_0003173	3.0	3.0
1031_0003181	4.0	3.0
1031_0003183	4.0	3.0
1031_0003186	3.0	3.0
1031_0003187	3.0	3.0
1031_0003189	4.0	4.0
1031_0003191	3.0	3.0
1031_0003203	3.0	3.0
1031_0003232	3.0	3.0
1031_0003238	3.0	3.0
1031_0003239	4.0	3.0
1031_0003242	3.0	3.0
1031_0003245	3.0	3.0
1031_0003262	3.0	3.0
1031_0003272	3.0	3.0
1031_0003330	3.0	3.0
1031_0003337	4.0	3.0
1031_0003339	3.0	3.0
1031_0003352	3.0	3.0
1031_0003359	3.0	3.0
1031_0003368	3.0	3.0
1031_0003391	2.0	3.0
1031_0003392	4.0	3.0
1031_0003407	2.0	3.0
1031_0003414	4.0	3.0
1061_0012029	2.0	2.0
1061_0120279	2.0	2.0
1061_0120284	0.0	1.0
1061_0120286	1.0	1.0
1061_0120307	2.0	2.0
1061_0120310	3.0	2.0
1061_0120312	1.0	1.0
1061_0120314	2.0	2.0
1061_0120315	2.0	1.0
1061_0120337	2.0	2.0
1061_0120345	2.0	2.0
1061_0120348	1.0	1.0
1061_0120350	2.0	2.0
1061_0120352	1.0	1.0
1061_0120357	2.0	3.0
1061_0120367	3.0	2.0
1061_0120376	2.0	2.0
1061_0120384	2.0	2.0
1061_0120386	0.0	2.0
1061_0120409	2.0	2.0
1061_0120411	4.0	3.0
1061_0120413	2.0	1.0
1061_0120415	2.0	2.0
1061_0120425	2.0	2.0
1061_0120429	3.0	3.0
1061_0120431	3.0	2.0
1061_0120438	2.0	2.0
1061_0120459	2.0	2.0
1061_0120478	2.0	2.0
1061_0120479	2.0	2.0
1061_0120483	2.0	2.0
1061_0120484	2.0	2.0
1061_0120498	2.0	3.0
1061_0120500	2.0	2.0
1061_0120853	2.0	3.0
1061_0120857	2.0	2.0
1061_0120883	2.0	2.0
1061_0120885	2.0	2.0
1061_0120886	2.0	2.0
1061_0120890	2.0	2.0
1061_1029116	1.0	2.0
1061_1029119	2.0	2.0
1061_1202911	1.0	2.0
1061_1202919	1.0	2.0
1071_0024686	2.0	2.0
1071_0024691	1.0	2.0
1071_0024693	1.0	1.0
1071_0024702	1.0	2.0
1071_0024712	1.0	1.0
1071_0024767	2.0	1.0
1071_0024769	1.0	1.0
1071_0024775	0.0	1.0
1071_0024777	1.0	1.0
1071_0024782	0.0	1.0
1071_0024784	1.0	0.0
1071_0024798	0.0	1.0
1071_0024800	0.0	1.0
1071_0024808	1.0	1.0
1071_0024820	1.0	1.0
1071_0024822	0.0	1.0
1071_0024826	2.0	1.0
1071_0024833	1.0	1.0
1071_0024838	0.0	0.0
1071_0024852	0.0	0.0
1071_0024853	0.0	0.0
1071_0024866	2.0	2.0
1071_0024867	2.0	1.0
1071_0024878	2.0	2.0
1071_0241833	1.0	1.0
1071_0242011	1.0	1.0
1071_0242071	0.0	0.0
1071_0242073	1.0	1.0
1071_0242091	0.0	1.0
1071_0243501	1.0	1.0
1071_0243502	0.0	0.0
1071_0243592	1.0	1.0
1071_0243622	0.0	0.0
1071_0243623	1.0	1.0
1071_0248302	1.0	1.0
1071_0248312	1.0	1.0
1071_0248315	0.0	0.0
1071_0248337	2.0	1.0
1071_0248341	1.0	1.0
1071_0248349	1.0	1.0
1091_0000002	2.0	2.0
1091_0000003	2.0	2.0
1091_0000012	1.0	1.0
1091_0000017	2.0	2.0
1091_0000023	2.0	1.0
1091_0000025	1.0	1.0
1091_0000028	1.0	1.0
1091_0000031	1.0	1.0
1091_0000037	1.0	1.0
1091_0000050	0.0	1.0
1091_0000067	2.0	1.0
1091_0000068	1.0	2.0
1091_0000074	2.0	1.0
1091_0000076	2.0	2.0
1091_0000079	1.0	2.0
1091_0000095	2.0	1.0
1091_0000113	1.0	2.0
1091_0000152	1.0	1.0
1091_0000163	2.0	1.0
1091_0000171	1.0	2.0
1091_0000173	2.0	2.0
1091_0000174	2.0	1.0
1091_0000192	2.0	2.0
1091_0000193	2.0	1.0
1091_0000195	1.0	1.0
1091_0000196	2.0	2.0
1091_0000199	2.0	2.0
1091_0000202	1.0	2.0
1091_0000206	1.0	2.0
1091_0000212	1.0	2.0
1091_0000216	1.0	2.0
1091_0000218	2.0	2.0
1091_0000226	1.0	1.0
1091_0000227	1.0	2.0
1091_0000231	2.0	2.0
1091_0000237	2.0	2.0
1091_0000241	1.0	1.0
1091_0000247	2.0	2.0
1091_0000254	2.0	2.0
1091_0000256	1.0	2.0
1091_0000262	2.0	2.0
LANGUAGE: DE, 2th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.17
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.09
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.44      0.27      0.33        45
         2.0       0.55      0.79      0.65        71
         3.0       0.58      0.81      0.68        54
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.55       205
   macro avg       0.26      0.31      0.28       205
weighted avg       0.44      0.55      0.48       205

[[ 0 11  6  0  0  0]
 [ 0 12 32  1  0  0]
 [ 0  2 56 13  0  0]
 [ 0  2  8 44  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
0.4757032393801039
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.91
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.55      0.78      0.64        45
         2.0       0.71      0.59      0.65        71
         3.0       0.55      0.83      0.66        54
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.60       205
   macro avg       0.30      0.37      0.33       205
weighted avg       0.51      0.60      0.54       205

[[ 0 16  1  0  0  0]
 [ 0 35  9  1  0  0]
 [ 0 11 42 18  0  0]
 [ 0  2  7 45  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
0.5390795110408657
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.80
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.33      0.12      0.17        17
         1.0       0.45      0.44      0.45        45
         2.0       0.62      0.69      0.65        71
         3.0       0.62      0.72      0.67        54
         4.0       0.54      0.41      0.47        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.57       205
   macro avg       0.43      0.40      0.40       205
weighted avg       0.55      0.57      0.55       205

[[ 2 13  2  0  0  0]
 [ 4 20 20  1  0  0]
 [ 0  9 49 13  0  0]
 [ 0  2  8 39  5  0]
 [ 0  0  0 10  7  0]
 [ 0  0  0  0  1  0]]
0.5536645894646537
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.70
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.02
              precision    recall  f1-score   support

         0.0       0.50      0.12      0.19        17
         1.0       0.49      0.53      0.51        45
         2.0       0.64      0.66      0.65        71
         3.0       0.58      0.81      0.68        54
         4.0       0.50      0.06      0.11        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.58       205
   macro avg       0.45      0.36      0.36       205
weighted avg       0.56      0.58      0.54       205

[[ 2 13  2  0  0  0]
 [ 2 24 18  1  0  0]
 [ 0 10 47 14  0  0]
 [ 0  2  7 44  1  0]
 [ 0  0  0 16  1  0]
 [ 0  0  0  1  0  0]]
0.5394523155478232
205 205 205
Filename	True Label	Prediction
1023_0001575	3.0	2.0
1023_0101689	2.0	2.0
1023_0101690	2.0	3.0
1023_0101701	2.0	3.0
1023_0101753	3.0	3.0
1023_0101841	2.0	3.0
1023_0101851	2.0	3.0
1023_0101853	2.0	3.0
1023_0101895	4.0	3.0
1023_0101897	2.0	3.0
1023_0101898	3.0	3.0
1023_0101901	3.0	3.0
1023_0103825	2.0	3.0
1023_0103827	3.0	3.0
1023_0103828	1.0	2.0
1023_0103834	3.0	3.0
1023_0103841	3.0	3.0
1023_0103843	2.0	2.0
1023_0107682	3.0	3.0
1023_0107725	2.0	2.0
1023_0108306	4.0	3.0
1023_0108649	3.0	3.0
1023_0108751	3.0	3.0
1023_0108752	3.0	3.0
1023_0108810	3.0	3.0
1023_0108931	2.0	3.0
1023_0108993	3.0	3.0
1023_0109033	4.0	3.0
1023_0109249	3.0	3.0
1023_0109395	2.0	2.0
1023_0109400	3.0	2.0
1023_0109401	3.0	3.0
1023_0109402	2.0	3.0
1023_0109518	2.0	2.0
1023_0109520	3.0	3.0
1023_0109524	3.0	3.0
1023_0109528	3.0	3.0
1023_0109671	3.0	3.0
1023_0109890	4.0	3.0
1023_0109917	2.0	3.0
1023_0109951	2.0	3.0
1031_0001950	4.0	3.0
1031_0002043	4.0	3.0
1031_0002083	3.0	3.0
1031_0002086	3.0	3.0
1031_0002089	4.0	3.0
1031_0002184	3.0	3.0
1031_0002197	3.0	3.0
1031_0003035	4.0	3.0
1031_0003043	4.0	3.0
1031_0003065	3.0	3.0
1031_0003072	3.0	3.0
1031_0003073	4.0	4.0
1031_0003074	4.0	3.0
1031_0003077	3.0	4.0
1031_0003090	3.0	3.0
1031_0003095	3.0	3.0
1031_0003141	3.0	3.0
1031_0003145	4.0	3.0
1031_0003155	3.0	3.0
1031_0003162	3.0	3.0
1031_0003163	3.0	3.0
1031_0003166	2.0	3.0
1031_0003167	3.0	3.0
1031_0003185	3.0	2.0
1031_0003219	3.0	3.0
1031_0003221	3.0	3.0
1031_0003224	3.0	3.0
1031_0003226	3.0	3.0
1031_0003249	4.0	3.0
1031_0003273	3.0	3.0
1031_0003309	3.0	3.0
1031_0003313	4.0	3.0
1031_0003314	4.0	3.0
1031_0003331	3.0	3.0
1031_0003354	3.0	3.0
1031_0003383	4.0	3.0
1031_0003384	3.0	3.0
1031_0003389	3.0	3.0
1031_0003393	4.0	3.0
1031_0003415	5.0	3.0
1061_0120278	2.0	2.0
1061_0120281	1.0	2.0
1061_0120282	0.0	1.0
1061_0120285	1.0	2.0
1061_0120288	2.0	2.0
1061_0120290	1.0	2.0
1061_0120295	0.0	2.0
1061_0120296	2.0	2.0
1061_0120302	1.0	2.0
1061_0120306	2.0	2.0
1061_0120317	3.0	2.0
1061_0120319	3.0	2.0
1061_0120325	2.0	2.0
1061_0120326	2.0	2.0
1061_0120333	2.0	3.0
1061_0120343	2.0	2.0
1061_0120347	1.0	2.0
1061_0120351	2.0	2.0
1061_0120359	2.0	2.0
1061_0120361	2.0	2.0
1061_0120382	2.0	2.0
1061_0120388	2.0	2.0
1061_0120403	2.0	2.0
1061_0120404	2.0	2.0
1061_0120410	2.0	2.0
1061_0120414	3.0	2.0
1061_0120424	2.0	2.0
1061_0120430	2.0	2.0
1061_0120433	1.0	2.0
1061_0120440	1.0	1.0
1061_0120441	2.0	2.0
1061_0120450	2.0	2.0
1061_0120455	2.0	2.0
1061_0120481	3.0	3.0
1061_0120485	2.0	2.0
1061_0120492	2.0	2.0
1061_0120497	3.0	3.0
1061_0120876	2.0	2.0
1061_0120877	3.0	2.0
1061_0120878	1.0	1.0
1061_0120894	2.0	2.0
1061_1029111	2.0	2.0
1061_1029112	3.0	3.0
1061_1202913	2.0	2.0
1061_1202914	1.0	1.0
1061_1202917	1.0	2.0
1071_0020001	2.0	1.0
1071_0024680	2.0	2.0
1071_0024687	0.0	2.0
1071_0024688	1.0	2.0
1071_0024703	1.0	1.0
1071_0024704	1.0	1.0
1071_0024705	2.0	2.0
1071_0024708	1.0	1.0
1071_0024710	0.0	1.0
1071_0024758	1.0	2.0
1071_0024759	0.0	1.0
1071_0024762	0.0	1.0
1071_0024763	1.0	1.0
1071_0024778	0.0	1.0
1071_0024779	1.0	1.0
1071_0024783	0.0	1.0
1071_0024802	1.0	1.0
1071_0024807	0.0	1.0
1071_0024814	1.0	1.0
1071_0024818	1.0	1.0
1071_0024849	0.0	0.0
1071_0024855	1.0	1.0
1071_0024862	2.0	1.0
1071_0024875	1.0	1.0
1071_0024876	1.0	1.0
1071_0024879	1.0	0.0
1071_0242012	1.0	1.0
1071_0242021	1.0	1.0
1071_0242042	1.0	1.0
1071_0248319	0.0	0.0
1071_0248323	1.0	1.0
1071_0248326	1.0	1.0
1071_0248327	0.0	1.0
1071_0248328	0.0	1.0
1071_0248330	2.0	1.0
1071_0248332	2.0	2.0
1071_0248334	1.0	2.0
1071_0248338	2.0	1.0
1071_0248340	0.0	1.0
1091_0000004	1.0	1.0
1091_0000014	0.0	1.0
1091_0000019	2.0	1.0
1091_0000020	2.0	2.0
1091_0000027	1.0	2.0
1091_0000030	0.0	1.0
1091_0000041	1.0	1.0
1091_0000043	1.0	1.0
1091_0000062	3.0	1.0
1091_0000078	3.0	1.0
1091_0000092	1.0	2.0
1091_0000102	2.0	2.0
1091_0000127	1.0	1.0
1091_0000144	2.0	1.0
1091_0000145	1.0	0.0
1091_0000146	0.0	1.0
1091_0000154	1.0	3.0
1091_0000155	2.0	3.0
1091_0000159	2.0	2.0
1091_0000165	1.0	1.0
1091_0000185	2.0	1.0
1091_0000190	1.0	2.0
1091_0000200	2.0	2.0
1091_0000215	2.0	2.0
1091_0000220	1.0	2.0
1091_0000221	2.0	1.0
1091_0000222	2.0	2.0
1091_0000229	2.0	2.0
1091_0000230	2.0	2.0
1091_0000233	2.0	2.0
1091_0000236	2.0	2.0
1091_0000239	2.0	1.0
1091_0000242	1.0	2.0
1091_0000249	2.0	2.0
1091_0000253	2.0	1.0
1091_0000257	2.0	2.0
1091_0000268	2.0	2.0
1091_0000273	1.0	2.0
1091_0000274	1.0	2.0
LANGUAGE: DE, 3th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.19
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.88
              precision    recall  f1-score   support

         0.0       0.40      0.71      0.51        17
         1.0       0.62      0.29      0.39        45
         2.0       0.72      0.82      0.77        71
         3.0       0.65      0.89      0.75        54
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.64       205
   macro avg       0.40      0.45      0.40       205
weighted avg       0.59      0.64      0.59       205

[[12  5  0  0  0  0]
 [15 13 17  0  0  0]
 [ 3  2 58  8  0  0]
 [ 0  1  5 48  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
0.5924447324327727
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.93
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.83
              precision    recall  f1-score   support

         0.0       0.45      0.29      0.36        17
         1.0       0.55      0.69      0.61        45
         2.0       0.78      0.63      0.70        71
         3.0       0.61      0.91      0.73        54
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.63       205
   macro avg       0.40      0.42      0.40       205
weighted avg       0.59      0.63      0.60       205

[[ 5 12  0  0  0  0]
 [ 4 31 10  0  0  0]
 [ 2 11 45 13  0  0]
 [ 0  2  3 49  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
0.598646887717216
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.80
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.85
              precision    recall  f1-score   support

         0.0       0.44      0.24      0.31        17
         1.0       0.52      0.51      0.52        45
         2.0       0.64      0.85      0.73        71
         3.0       0.64      0.69      0.66        54
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.60       205
   macro avg       0.37      0.38      0.37       205
weighted avg       0.54      0.60      0.56       205

[[ 4 13  0  0  0  0]
 [ 4 23 18  0  0  0]
 [ 1  7 60  3  0  0]
 [ 0  1 16 37  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
0.5648982012780314
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.68
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.83
              precision    recall  f1-score   support

         0.0       0.50      0.24      0.32        17
         1.0       0.52      0.51      0.52        45
         2.0       0.70      0.75      0.72        71
         3.0       0.69      0.76      0.73        54
         4.0       0.56      0.59      0.57        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.64       205
   macro avg       0.50      0.47      0.48       205
weighted avg       0.63      0.64      0.63       205

[[ 4 13  0  0  0  0]
 [ 3 23 19  0  0  0]
 [ 1  7 53 10  0  0]
 [ 0  1  4 41  8  0]
 [ 0  0  0  7 10  0]
 [ 0  0  0  1  0  0]]
0.628272352683449
205 205 205
Filename	True Label	Prediction
1023_0001419	3.0	3.0
1023_0001420	3.0	3.0
1023_0001422	3.0	3.0
1023_0001423	2.0	2.0
1023_0101683	2.0	2.0
1023_0101688	3.0	3.0
1023_0101694	3.0	3.0
1023_0101847	3.0	3.0
1023_0101849	2.0	2.0
1023_0101856	2.0	3.0
1023_0101893	3.0	3.0
1023_0101894	3.0	3.0
1023_0101899	2.0	3.0
1023_0101900	3.0	3.0
1023_0102118	2.0	3.0
1023_0103824	3.0	3.0
1023_0103839	3.0	3.0
1023_0103844	4.0	3.0
1023_0104209	3.0	3.0
1023_0108422	3.0	3.0
1023_0108510	3.0	3.0
1023_0108814	3.0	3.0
1023_0108885	3.0	3.0
1023_0108887	2.0	2.0
1023_0108890	3.0	3.0
1023_0108933	3.0	3.0
1023_0108934	2.0	3.0
1023_0108935	2.0	3.0
1023_0108955	4.0	3.0
1023_0109027	3.0	3.0
1023_0109029	2.0	2.0
1023_0109038	3.0	3.0
1023_0109096	3.0	3.0
1023_0109247	4.0	3.0
1023_0109248	2.0	3.0
1023_0109392	3.0	3.0
1023_0109396	2.0	3.0
1023_0109500	2.0	3.0
1023_0109515	3.0	3.0
1023_0109516	3.0	3.0
1023_0109588	3.0	3.0
1023_0109590	3.0	3.0
1023_0109591	2.0	3.0
1023_0109606	2.0	2.0
1023_0109609	2.0	2.0
1023_0109674	3.0	3.0
1023_0109945	3.0	3.0
1023_0109947	3.0	3.0
1031_0001997	4.0	4.0
1031_0002010	3.0	4.0
1031_0002032	3.0	4.0
1031_0002084	3.0	3.0
1031_0002185	4.0	4.0
1031_0002195	3.0	3.0
1031_0003053	4.0	4.0
1031_0003078	3.0	3.0
1031_0003098	4.0	4.0
1031_0003099	4.0	3.0
1031_0003130	4.0	4.0
1031_0003131	3.0	4.0
1031_0003132	4.0	3.0
1031_0003149	3.0	3.0
1031_0003150	3.0	3.0
1031_0003161	4.0	3.0
1031_0003179	4.0	4.0
1031_0003180	4.0	4.0
1031_0003190	3.0	4.0
1031_0003217	4.0	4.0
1031_0003237	3.0	4.0
1031_0003243	3.0	3.0
1031_0003244	4.0	4.0
1031_0003274	4.0	4.0
1031_0003310	3.0	4.0
1031_0003315	4.0	3.0
1031_0003356	3.0	3.0
1031_0003365	3.0	3.0
1031_0003367	3.0	4.0
1031_0003386	3.0	3.0
1031_0003388	3.0	4.0
1031_0003409	5.0	3.0
1061_0120272	1.0	2.0
1061_0120283	0.0	1.0
1061_0120289	2.0	2.0
1061_0120298	2.0	2.0
1061_0120299	2.0	2.0
1061_0120304	2.0	2.0
1061_0120318	2.0	2.0
1061_0120324	2.0	2.0
1061_0120327	2.0	2.0
1061_0120329	2.0	2.0
1061_0120330	3.0	2.0
1061_0120332	2.0	2.0
1061_0120334	3.0	2.0
1061_0120336	1.0	2.0
1061_0120338	1.0	2.0
1061_0120346	2.0	2.0
1061_0120353	1.0	1.0
1061_0120356	2.0	2.0
1061_0120358	1.0	2.0
1061_0120368	2.0	2.0
1061_0120369	2.0	2.0
1061_0120383	3.0	3.0
1061_0120391	1.0	2.0
1061_0120406	2.0	2.0
1061_0120421	2.0	2.0
1061_0120423	3.0	3.0
1061_0120426	2.0	3.0
1061_0120428	2.0	2.0
1061_0120439	2.0	1.0
1061_0120443	1.0	1.0
1061_0120449	2.0	2.0
1061_0120453	2.0	2.0
1061_0120482	2.0	2.0
1061_0120488	2.0	2.0
1061_0120491	2.0	2.0
1061_0120855	2.0	2.0
1061_0120856	2.0	2.0
1061_0120858	2.0	2.0
1061_0120874	1.0	2.0
1061_0120880	3.0	3.0
1061_0120889	1.0	1.0
1061_1202916	1.0	2.0
1061_1202918	2.0	2.0
1071_0024678	2.0	0.0
1071_0024683	1.0	1.0
1071_0024690	2.0	2.0
1071_0024713	1.0	1.0
1071_0024716	1.0	1.0
1071_0024757	2.0	2.0
1071_0024761	1.0	1.0
1071_0024770	1.0	0.0
1071_0024773	1.0	1.0
1071_0024799	2.0	2.0
1071_0024803	0.0	1.0
1071_0024816	1.0	1.0
1071_0024821	0.0	1.0
1071_0024824	1.0	1.0
1071_0024825	0.0	1.0
1071_0024843	0.0	1.0
1071_0024845	0.0	1.0
1071_0024846	1.0	0.0
1071_0024850	0.0	1.0
1071_0024856	1.0	1.0
1071_0024860	1.0	1.0
1071_0024863	1.0	1.0
1071_0024864	0.0	0.0
1071_0241832	0.0	1.0
1071_0242013	1.0	1.0
1071_0242023	0.0	1.0
1071_0242072	0.0	0.0
1071_0248304	0.0	1.0
1071_0248305	0.0	1.0
1071_0248307	2.0	1.0
1071_0248316	1.0	0.0
1071_0248318	0.0	0.0
1071_0248321	1.0	1.0
1071_0248324	0.0	0.0
1071_0248329	1.0	1.0
1071_0248342	1.0	1.0
1071_0248343	1.0	1.0
1071_0248345	2.0	2.0
1071_0248346	1.0	1.0
1071_0248348	1.0	1.0
1071_0248350	2.0	1.0
1091_0000007	2.0	2.0
1091_0000009	1.0	1.0
1091_0000021	2.0	2.0
1091_0000024	3.0	1.0
1091_0000032	1.0	2.0
1091_0000038	1.0	2.0
1091_0000047	2.0	1.0
1091_0000048	1.0	1.0
1091_0000049	1.0	2.0
1091_0000051	1.0	2.0
1091_0000052	0.0	1.0
1091_0000054	0.0	1.0
1091_0000056	1.0	2.0
1091_0000058	2.0	2.0
1091_0000071	2.0	2.0
1091_0000075	1.0	2.0
1091_0000114	2.0	2.0
1091_0000125	3.0	2.0
1091_0000126	2.0	1.0
1091_0000157	2.0	2.0
1091_0000164	1.0	1.0
1091_0000167	1.0	2.0
1091_0000169	2.0	2.0
1091_0000172	2.0	2.0
1091_0000204	2.0	2.0
1091_0000207	2.0	2.0
1091_0000208	1.0	2.0
1091_0000219	1.0	2.0
1091_0000224	2.0	1.0
1091_0000248	2.0	1.0
1091_0000250	2.0	2.0
1091_0000251	2.0	2.0
1091_0000258	1.0	2.0
1091_0000259	2.0	2.0
1091_0000260	3.0	2.0
1091_0000263	2.0	2.0
1091_0000264	1.0	2.0
1091_0000265	2.0	2.0
1091_0000266	2.0	2.0
1091_0000267	2.0	2.0
1091_0000272	1.0	2.0
LANGUAGE: DE, 4th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.19
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.43      0.98      0.60        45
         2.0       0.62      0.40      0.49        72
         3.0       0.62      0.66      0.64        53
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.53       205
   macro avg       0.28      0.34      0.29       205
weighted avg       0.47      0.53      0.47       205

[[ 0 17  0  0  0  0]
 [ 0 44  1  0  0  0]
 [ 0 39 29  4  0  0]
 [ 0  2 16 35  0  0]
 [ 0  0  1 16  0  0]
 [ 0  0  0  1  0  0]]
0.4686239499156118
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.94
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.90
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.56      0.73      0.63        45
         2.0       0.73      0.60      0.66        72
         3.0       0.56      0.92      0.70        53
         4.0       0.00      0.00      0.00        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.61       205
   macro avg       0.31      0.38      0.33       205
weighted avg       0.52      0.61      0.55       205

[[ 0 17  0  0  0  0]
 [ 0 33 12  0  0  0]
 [ 0  9 43 20  0  0]
 [ 0  0  4 49  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
0.5508530140498117
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.81
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.83
              precision    recall  f1-score   support

         0.0       0.75      0.18      0.29        17
         1.0       0.61      0.76      0.67        45
         2.0       0.74      0.75      0.74        72
         3.0       0.62      0.75      0.68        53
         4.0       0.29      0.12      0.17        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.65       205
   macro avg       0.50      0.43      0.42       205
weighted avg       0.64      0.65      0.62       205

[[ 3 14  0  0  0  0]
 [ 0 34 11  0  0  0]
 [ 1  8 54  9  0  0]
 [ 0  0  8 40  5  0]
 [ 0  0  0 15  2  0]
 [ 0  0  0  1  0  0]]
0.6221819292203588
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.69
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.82
              precision    recall  f1-score   support

         0.0       0.80      0.24      0.36        17
         1.0       0.61      0.67      0.64        45
         2.0       0.70      0.76      0.73        72
         3.0       0.63      0.74      0.68        53
         4.0       0.40      0.24      0.30        17
         5.0       0.00      0.00      0.00         1

    accuracy                           0.64       205
   macro avg       0.52      0.44      0.45       205
weighted avg       0.64      0.64      0.63       205

[[ 4 12  1  0  0  0]
 [ 0 30 15  0  0  0]
 [ 1  7 55  9  0  0]
 [ 0  0  8 39  6  0]
 [ 0  0  0 13  4  0]
 [ 0  0  0  1  0  0]]
0.6260508131071155
205 205 205
Filename	True Label	Prediction
1023_0101684	2.0	2.0
1023_0101691	4.0	3.0
1023_0101693	4.0	3.0
1023_0101695	2.0	2.0
1023_0101700	3.0	3.0
1023_0101751	3.0	3.0
1023_0101752	3.0	3.0
1023_0101843	2.0	2.0
1023_0101852	2.0	2.0
1023_0101854	2.0	2.0
1023_0101904	2.0	2.0
1023_0101906	2.0	2.0
1023_0102117	2.0	3.0
1023_0103821	3.0	3.0
1023_0103822	2.0	2.0
1023_0103832	2.0	2.0
1023_0103837	3.0	3.0
1023_0103838	3.0	3.0
1023_0103955	4.0	3.0
1023_0104203	3.0	2.0
1023_0107726	3.0	3.0
1023_0107773	2.0	3.0
1023_0107780	3.0	3.0
1023_0107788	3.0	3.0
1023_0108304	3.0	3.0
1023_0108305	3.0	3.0
1023_0108307	3.0	3.0
1023_0108423	3.0	2.0
1023_0108426	2.0	3.0
1023_0108766	2.0	3.0
1023_0108815	3.0	3.0
1023_0108932	3.0	3.0
1023_0109026	2.0	3.0
1023_0109030	3.0	3.0
1023_0109391	2.0	2.0
1023_0109519	2.0	2.0
1023_0109649	3.0	3.0
1023_0109651	3.0	3.0
1023_0109716	3.0	3.0
1023_0109880	3.0	3.0
1023_0109914	2.0	2.0
1023_0109915	2.0	2.0
1031_0001951	3.0	3.0
1031_0001998	4.0	3.0
1031_0002005	4.0	3.0
1031_0002011	4.0	3.0
1031_0002087	4.0	3.0
1031_0002088	3.0	3.0
1031_0002131	3.0	3.0
1031_0002187	3.0	4.0
1031_0002199	3.0	3.0
1031_0002200	3.0	3.0
1031_0003063	5.0	3.0
1031_0003071	3.0	4.0
1031_0003076	4.0	4.0
1031_0003097	4.0	3.0
1031_0003128	3.0	3.0
1031_0003129	3.0	3.0
1031_0003133	4.0	4.0
1031_0003135	3.0	3.0
1031_0003140	3.0	4.0
1031_0003146	4.0	4.0
1031_0003154	3.0	3.0
1031_0003157	4.0	4.0
1031_0003160	3.0	3.0
1031_0003172	3.0	4.0
1031_0003174	4.0	3.0
1031_0003212	3.0	3.0
1031_0003214	3.0	4.0
1031_0003216	3.0	3.0
1031_0003220	3.0	3.0
1031_0003230	3.0	3.0
1031_0003231	3.0	3.0
1031_0003234	3.0	3.0
1031_0003235	4.0	3.0
1031_0003240	3.0	4.0
1031_0003327	3.0	3.0
1031_0003338	4.0	3.0
1031_0003353	2.0	3.0
1031_0003357	3.0	3.0
1031_0003358	4.0	3.0
1031_0003369	4.0	3.0
1031_0003408	2.0	3.0
1061_0120271	2.0	2.0
1061_0120273	2.0	2.0
1061_0120275	2.0	2.0
1061_0120276	2.0	2.0
1061_0120280	1.0	1.0
1061_0120297	2.0	2.0
1061_0120303	0.0	1.0
1061_0120308	3.0	2.0
1061_0120309	1.0	1.0
1061_0120311	3.0	2.0
1061_0120320	3.0	3.0
1061_0120321	2.0	2.0
1061_0120323	1.0	2.0
1061_0120328	1.0	2.0
1061_0120349	1.0	2.0
1061_0120354	1.0	2.0
1061_0120370	2.0	3.0
1061_0120371	3.0	3.0
1061_0120372	2.0	2.0
1061_0120389	2.0	2.0
1061_0120390	2.0	2.0
1061_0120405	2.0	2.0
1061_0120407	3.0	2.0
1061_0120427	2.0	2.0
1061_0120432	2.0	2.0
1061_0120442	2.0	2.0
1061_0120457	2.0	2.0
1061_0120458	3.0	2.0
1061_0120480	2.0	2.0
1061_0120486	1.0	2.0
1061_0120493	2.0	2.0
1061_0120494	2.0	2.0
1061_0120875	3.0	2.0
1061_0120888	1.0	2.0
1061_1029114	2.0	2.0
1061_1029118	1.0	2.0
1061_1029120	2.0	2.0
1061_1202912	2.0	2.0
1061_1202915	1.0	1.0
1071_0024681	2.0	2.0
1071_0024689	1.0	1.0
1071_0024694	2.0	2.0
1071_0024699	1.0	1.0
1071_0024701	2.0	2.0
1071_0024706	1.0	1.0
1071_0024709	2.0	2.0
1071_0024711	1.0	1.0
1071_0024714	2.0	1.0
1071_0024756	1.0	1.0
1071_0024776	0.0	0.0
1071_0024797	0.0	1.0
1071_0024801	1.0	1.0
1071_0024804	0.0	1.0
1071_0024809	1.0	1.0
1071_0024810	1.0	1.0
1071_0024811	1.0	1.0
1071_0024813	1.0	1.0
1071_0024815	0.0	1.0
1071_0024817	1.0	1.0
1071_0024819	1.0	2.0
1071_0024827	1.0	1.0
1071_0024831	0.0	1.0
1071_0024834	2.0	2.0
1071_0024847	1.0	1.0
1071_0024851	1.0	1.0
1071_0024854	0.0	0.0
1071_0024859	1.0	2.0
1071_0024861	0.0	1.0
1071_0024871	1.0	1.0
1071_0024872	1.0	2.0
1071_0024873	1.0	1.0
1071_0024877	1.0	1.0
1071_0242022	0.0	0.0
1071_0242041	1.0	1.0
1071_0242092	0.0	0.0
1071_0248303	0.0	1.0
1071_0248308	1.0	1.0
1071_0248310	1.0	1.0
1071_0248313	1.0	2.0
1071_0248314	1.0	1.0
1071_0248322	0.0	1.0
1071_0248325	0.0	1.0
1071_0248331	1.0	1.0
1071_0248336	0.0	1.0
1071_0248347	1.0	1.0
1091_0000001	1.0	1.0
1091_0000005	2.0	2.0
1091_0000010	3.0	2.0
1091_0000011	2.0	2.0
1091_0000013	1.0	1.0
1091_0000015	2.0	2.0
1091_0000018	2.0	2.0
1091_0000022	2.0	2.0
1091_0000026	1.0	1.0
1091_0000029	2.0	1.0
1091_0000034	2.0	1.0
1091_0000035	2.0	1.0
1091_0000036	1.0	2.0
1091_0000044	0.0	1.0
1091_0000045	2.0	2.0
1091_0000066	2.0	0.0
1091_0000069	2.0	1.0
1091_0000086	1.0	2.0
1091_0000101	2.0	2.0
1091_0000116	2.0	2.0
1091_0000151	0.0	1.0
1091_0000156	2.0	2.0
1091_0000160	2.0	3.0
1091_0000168	2.0	2.0
1091_0000191	1.0	2.0
1091_0000201	2.0	1.0
1091_0000210	2.0	1.0
1091_0000213	2.0	2.0
1091_0000228	1.0	2.0
1091_0000232	2.0	2.0
1091_0000243	0.0	2.0
1091_0000244	2.0	2.0
1091_0000245	1.0	1.0
1091_0000246	2.0	2.0
1091_0000261	2.0	2.0
1091_0000271	2.0	2.0
1091_0000275	2.0	2.0
Averaged weighted F1-scores 0.6016242873627954
130.68810916179336 82.83744971317162
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']
LANGUAGE: DE, 0th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.20
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.99
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.55      0.73      0.63        52
         2.0       0.52      0.51      0.51        59
         3.0       0.70      0.79      0.74        70
         4.0       0.00      0.00      0.00        14

    accuracy                           0.60       206
   macro avg       0.35      0.40      0.38       206
weighted avg       0.52      0.60      0.56       206

[[ 0 10  1  0  0]
 [ 0 38 14  0  0]
 [ 0 19 30 10  0]
 [ 0  2 13 55  0]
 [ 0  0  0 14  0]]
0.5562884448067813
206 206 206



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.99
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.03
              precision    recall  f1-score   support

         0.0       1.00      0.09      0.17        11
         1.0       0.58      0.29      0.38        52
         2.0       0.42      0.71      0.53        59
         3.0       0.69      0.79      0.73        70
         4.0       0.00      0.00      0.00        14

    accuracy                           0.55       206
   macro avg       0.54      0.38      0.36       206
weighted avg       0.55      0.55      0.51       206

[[ 1  4  6  0  0]
 [ 0 15 37  0  0]
 [ 0  6 42 11  0]
 [ 0  1 14 55  0]
 [ 0  0  0 14  0]]
0.5074454139527262
206 206 206



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.87
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.94
              precision    recall  f1-score   support

         0.0       0.33      0.18      0.24        11
         1.0       0.56      0.79      0.66        52
         2.0       0.52      0.44      0.48        59
         3.0       0.69      0.76      0.72        70
         4.0       0.00      0.00      0.00        14

    accuracy                           0.59       206
   macro avg       0.42      0.43      0.42       206
weighted avg       0.54      0.59      0.56       206

[[ 2  9  0  0  0]
 [ 3 41  8  0  0]
 [ 1 22 26 10  0]
 [ 0  1 16 53  0]
 [ 0  0  0 14  0]]
0.5598214308792904
206 206 206



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24

  Average training loss: 0.78
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.93
              precision    recall  f1-score   support

         0.0       1.00      0.09      0.17        11
         1.0       0.61      0.75      0.67        52
         2.0       0.54      0.53      0.53        59
         3.0       0.67      0.79      0.72        70
         4.0       0.00      0.00      0.00        14

    accuracy                           0.61       206
   macro avg       0.56      0.43      0.42       206
weighted avg       0.59      0.61      0.58       206

[[ 1  9  1  0  0]
 [ 0 39 13  0  0]
 [ 0 15 31 13  0]
 [ 0  1 12 55  2]
 [ 0  0  0 14  0]]
0.577627320729007
206 206 206
Filename	True Label	Prediction
1023_0001420	3.0	3.0
1023_0001575	3.0	3.0
1023_0101694	3.0	3.0
1023_0101700	3.0	3.0
1023_0101851	3.0	3.0
1023_0102117	3.0	3.0
1023_0103837	3.0	3.0
1023_0103838	3.0	3.0
1023_0103840	4.0	3.0
1023_0103843	3.0	2.0
1023_0103844	4.0	3.0
1023_0103883	3.0	3.0
1023_0107729	3.0	3.0
1023_0107740	3.0	3.0
1023_0107781	3.0	3.0
1023_0108304	4.0	3.0
1023_0108306	4.0	3.0
1023_0108520	3.0	3.0
1023_0108752	4.0	3.0
1023_0108931	3.0	3.0
1023_0108934	3.0	3.0
1023_0108958	3.0	3.0
1023_0108992	3.0	3.0
1023_0109033	4.0	3.0
1023_0109038	4.0	3.0
1023_0109192	3.0	3.0
1023_0109392	3.0	3.0
1023_0109400	3.0	3.0
1023_0109505	3.0	3.0
1023_0109527	3.0	3.0
1023_0109591	2.0	3.0
1023_0109651	3.0	3.0
1023_0109671	3.0	3.0
1023_0111896	3.0	3.0
1031_0001949	3.0	3.0
1031_0002006	4.0	3.0
1031_0002061	3.0	3.0
1031_0002091	3.0	3.0
1031_0002131	3.0	3.0
1031_0002195	3.0	3.0
1031_0002198	3.0	3.0
1031_0003029	3.0	3.0
1031_0003035	3.0	3.0
1031_0003126	4.0	3.0
1031_0003131	3.0	3.0
1031_0003133	4.0	3.0
1031_0003140	3.0	4.0
1031_0003145	3.0	3.0
1031_0003149	3.0	3.0
1031_0003160	3.0	3.0
1031_0003163	3.0	3.0
1031_0003170	3.0	3.0
1031_0003185	3.0	3.0
1031_0003190	3.0	4.0
1031_0003207	4.0	3.0
1031_0003211	2.0	3.0
1031_0003224	2.0	3.0
1031_0003226	3.0	3.0
1031_0003237	3.0	3.0
1031_0003239	4.0	3.0
1031_0003242	2.0	3.0
1031_0003243	3.0	3.0
1031_0003260	3.0	3.0
1031_0003310	3.0	3.0
1031_0003315	4.0	3.0
1031_0003336	3.0	3.0
1031_0003354	3.0	3.0
1031_0003356	3.0	3.0
1031_0003365	3.0	3.0
1031_0003386	2.0	3.0
1031_0003387	4.0	3.0
1031_0003407	3.0	3.0
1061_0120272	1.0	1.0
1061_0120285	2.0	2.0
1061_0120291	1.0	1.0
1061_0120307	3.0	2.0
1061_0120315	2.0	1.0
1061_0120317	2.0	3.0
1061_0120319	3.0	2.0
1061_0120326	3.0	3.0
1061_0120329	2.0	3.0
1061_0120331	1.0	1.0
1061_0120332	1.0	2.0
1061_0120351	2.0	3.0
1061_0120357	3.0	3.0
1061_0120383	2.0	3.0
1061_0120386	1.0	2.0
1061_0120390	3.0	2.0
1061_0120403	3.0	3.0
1061_0120405	3.0	2.0
1061_0120409	3.0	2.0
1061_0120413	1.0	1.0
1061_0120430	2.0	2.0
1061_0120431	3.0	2.0
1061_0120432	2.0	2.0
1061_0120443	0.0	1.0
1061_0120457	3.0	2.0
1061_0120458	3.0	3.0
1061_0120459	3.0	2.0
1061_0120479	2.0	3.0
1061_0120483	2.0	2.0
1061_0120486	2.0	3.0
1061_0120876	3.0	3.0
1061_0120884	2.0	2.0
1061_0120890	1.0	1.0
1061_1029111	3.0	2.0
1061_1029114	1.0	2.0
1061_1029115	2.0	2.0
1061_1029117	2.0	3.0
1061_1029120	2.0	2.0
1061_1202918	2.0	2.0
1071_0024682	2.0	2.0
1071_0024683	0.0	1.0
1071_0024688	2.0	2.0
1071_0024694	2.0	2.0
1071_0024701	2.0	2.0
1071_0024702	2.0	2.0
1071_0024711	2.0	1.0
1071_0024774	0.0	0.0
1071_0024775	0.0	1.0
1071_0024778	0.0	1.0
1071_0024798	1.0	1.0
1071_0024803	1.0	1.0
1071_0024818	2.0	1.0
1071_0024821	1.0	1.0
1071_0024822	0.0	2.0
1071_0024825	0.0	1.0
1071_0024827	1.0	1.0
1071_0024834	2.0	2.0
1071_0024835	1.0	1.0
1071_0024837	1.0	1.0
1071_0024838	1.0	1.0
1071_0024843	1.0	1.0
1071_0024847	2.0	2.0
1071_0024856	1.0	2.0
1071_0024861	0.0	1.0
1071_0024866	2.0	2.0
1071_0024871	1.0	1.0
1071_0024876	1.0	1.0
1071_0024877	1.0	1.0
1071_0242022	1.0	1.0
1071_0242041	1.0	1.0
1071_0242042	1.0	1.0
1071_0243501	1.0	1.0
1071_0243591	1.0	1.0
1071_0248312	1.0	1.0
1071_0248313	1.0	1.0
1071_0248329	1.0	1.0
1071_0248339	1.0	1.0
1071_0248349	1.0	1.0
1091_0000002	2.0	2.0
1091_0000003	2.0	1.0
1091_0000007	2.0	2.0
1091_0000009	0.0	1.0
1091_0000011	1.0	1.0
1091_0000013	1.0	1.0
1091_0000018	3.0	2.0
1091_0000025	1.0	1.0
1091_0000026	1.0	1.0
1091_0000027	1.0	1.0
1091_0000028	1.0	1.0
1091_0000031	1.0	1.0
1091_0000033	1.0	1.0
1091_0000034	2.0	1.0
1091_0000035	2.0	1.0
1091_0000044	0.0	1.0
1091_0000046	2.0	1.0
1091_0000054	0.0	1.0
1091_0000056	1.0	2.0
1091_0000068	1.0	1.0
1091_0000072	1.0	2.0
1091_0000077	2.0	1.0
1091_0000079	1.0	2.0
1091_0000086	1.0	2.0
1091_0000095	2.0	1.0
1091_0000113	1.0	2.0
1091_0000123	2.0	2.0
1091_0000125	3.0	2.0
1091_0000148	1.0	1.0
1091_0000151	1.0	1.0
1091_0000155	2.0	3.0
1091_0000170	2.0	1.0
1091_0000172	2.0	1.0
1091_0000192	1.0	2.0
1091_0000202	2.0	2.0
1091_0000203	1.0	1.0
1091_0000204	2.0	2.0
1091_0000209	2.0	2.0
1091_0000213	2.0	2.0
1091_0000214	2.0	1.0
1091_0000221	2.0	1.0
1091_0000222	2.0	2.0
1091_0000223	1.0	2.0
1091_0000230	2.0	2.0
1091_0000233	2.0	2.0
1091_0000236	2.0	2.0
1091_0000237	1.0	2.0
1091_0000240	1.0	1.0
1091_0000250	1.0	2.0
1091_0000251	2.0	2.0
1091_0000253	2.0	1.0
1091_0000254	3.0	1.0
1091_0000262	2.0	2.0
1091_0000268	2.0	2.0
1091_0000271	2.0	1.0
1091_0000274	2.0	2.0
LANGUAGE: DE, 1th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.22
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.02
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.64      0.54      0.58        52
         2.0       0.59      0.55      0.57        60
         3.0       0.62      0.94      0.75        69
         4.0       0.00      0.00      0.00        14

    accuracy                           0.61       205
   macro avg       0.37      0.41      0.38       205
weighted avg       0.54      0.61      0.57       205

[[ 0  9  1  0  0]
 [ 0 28 19  5  0]
 [ 0  6 33 21  0]
 [ 0  1  3 65  0]
 [ 0  0  0 14  0]]
0.5659657975890104
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.95
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.94
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.56      0.79      0.66        52
         2.0       0.69      0.40      0.51        60
         3.0       0.66      0.93      0.77        69
         4.0       0.00      0.00      0.00        14

    accuracy                           0.63       205
   macro avg       0.38      0.42      0.39       205
weighted avg       0.57      0.63      0.57       205

[[ 0 10  0  0  0]
 [ 0 41  8  3  0]
 [ 0 20 24 16  0]
 [ 0  2  3 64  0]
 [ 0  0  0 14  0]]
0.5738176036624031
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 11
Elapsed time 23

  Average training loss: 0.83
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.94
              precision    recall  f1-score   support

         0.0       0.40      0.20      0.27        10
         1.0       0.57      0.56      0.56        52
         2.0       0.53      0.57      0.55        60
         3.0       0.68      0.84      0.75        69
         4.0       0.00      0.00      0.00        14

    accuracy                           0.60       205
   macro avg       0.44      0.43      0.43       205
weighted avg       0.55      0.60      0.57       205

[[ 2  8  0  0  0]
 [ 3 29 20  0  0]
 [ 0 13 34 13  0]
 [ 0  1 10 58  0]
 [ 0  0  0 14  0]]
0.5698803504644024
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.71
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.97
              precision    recall  f1-score   support

         0.0       0.50      0.10      0.17        10
         1.0       0.57      0.58      0.57        52
         2.0       0.53      0.52      0.53        60
         3.0       0.66      0.88      0.76        69
         4.0       0.00      0.00      0.00        14

    accuracy                           0.60       205
   macro avg       0.45      0.42      0.40       205
weighted avg       0.55      0.60      0.56       205

[[ 1  9  0  0  0]
 [ 1 30 20  1  0]
 [ 0 13 31 16  0]
 [ 0  1  7 61  0]
 [ 0  0  0 14  0]]
0.5619126360755132
205 205 205
Filename	True Label	Prediction
1023_0001419	3.0	3.0
1023_0101689	2.0	2.0
1023_0101845	2.0	3.0
1023_0101846	4.0	3.0
1023_0101849	3.0	3.0
1023_0101854	2.0	2.0
1023_0101893	3.0	3.0
1023_0101897	2.0	3.0
1023_0101900	3.0	3.0
1023_0101907	3.0	3.0
1023_0103822	2.0	2.0
1023_0103828	1.0	1.0
1023_0103829	2.0	3.0
1023_0103830	3.0	3.0
1023_0103834	3.0	3.0
1023_0103839	3.0	3.0
1023_0103841	3.0	3.0
1023_0104209	3.0	3.0
1023_0106816	3.0	3.0
1023_0107244	3.0	3.0
1023_0107672	3.0	3.0
1023_0107727	3.0	3.0
1023_0108510	3.0	3.0
1023_0108650	3.0	3.0
1023_0108766	2.0	3.0
1023_0108885	3.0	3.0
1023_0108886	3.0	3.0
1023_0108888	3.0	3.0
1023_0108908	3.0	3.0
1023_0109026	2.0	3.0
1023_0109096	3.0	3.0
1023_0109248	3.0	3.0
1023_0109267	3.0	3.0
1023_0109396	2.0	3.0
1023_0109496	3.0	3.0
1023_0109515	3.0	3.0
1023_0109522	3.0	3.0
1023_0109606	3.0	3.0
1023_0109649	3.0	3.0
1023_0109717	3.0	3.0
1023_0109880	3.0	3.0
1023_0109915	2.0	2.0
1023_0109945	3.0	3.0
1023_0109954	3.0	3.0
1031_0001998	4.0	3.0
1031_0002010	3.0	3.0
1031_0002036	4.0	3.0
1031_0002084	3.0	3.0
1031_0002085	3.0	3.0
1031_0002086	3.0	3.0
1031_0002087	4.0	3.0
1031_0002089	3.0	3.0
1031_0002197	3.0	3.0
1031_0002200	2.0	3.0
1031_0003043	4.0	3.0
1031_0003088	4.0	3.0
1031_0003098	4.0	3.0
1031_0003121	3.0	3.0
1031_0003130	4.0	3.0
1031_0003165	2.0	3.0
1031_0003169	3.0	3.0
1031_0003181	4.0	3.0
1031_0003183	4.0	3.0
1031_0003184	4.0	3.0
1031_0003218	3.0	3.0
1031_0003221	2.0	3.0
1031_0003233	3.0	3.0
1031_0003246	3.0	3.0
1031_0003249	4.0	3.0
1031_0003272	3.0	3.0
1031_0003330	3.0	3.0
1031_0003352	3.0	3.0
1031_0003353	2.0	3.0
1031_0003359	2.0	3.0
1031_0003367	3.0	3.0
1031_0003388	4.0	3.0
1031_0003390	3.0	3.0
1031_0003391	2.0	3.0
1031_0003393	3.0	3.0
1031_0003415	4.0	3.0
1061_0120271	2.0	2.0
1061_0120273	2.0	2.0
1061_0120276	3.0	2.0
1061_0120282	0.0	1.0
1061_0120289	2.0	2.0
1061_0120296	2.0	2.0
1061_0120299	2.0	3.0
1061_0120303	1.0	1.0
1061_0120316	2.0	2.0
1061_0120327	3.0	3.0
1061_0120333	3.0	3.0
1061_0120336	1.0	2.0
1061_0120337	3.0	2.0
1061_0120345	3.0	2.0
1061_0120349	1.0	2.0
1061_0120353	1.0	1.0
1061_0120358	1.0	2.0
1061_0120371	3.0	3.0
1061_0120374	3.0	3.0
1061_0120376	2.0	2.0
1061_0120384	2.0	2.0
1061_0120388	2.0	2.0
1061_0120394	2.0	3.0
1061_0120415	1.0	1.0
1061_0120423	3.0	3.0
1061_0120429	3.0	3.0
1061_0120438	2.0	2.0
1061_0120441	2.0	2.0
1061_0120448	3.0	2.0
1061_0120456	1.0	2.0
1061_0120478	3.0	3.0
1061_0120481	3.0	3.0
1061_0120488	3.0	2.0
1061_0120490	3.0	3.0
1061_0120491	2.0	2.0
1061_0120492	3.0	3.0
1061_0120494	2.0	2.0
1061_0120495	2.0	3.0
1061_0120498	3.0	2.0
1061_0120500	2.0	2.0
1061_0120874	2.0	3.0
1061_0120877	2.0	2.0
1061_0120880	3.0	3.0
1061_1029116	1.0	2.0
1061_1029118	2.0	2.0
1061_1202910	3.0	3.0
1061_1202914	1.0	2.0
1061_1202916	2.0	2.0
1071_0024678	2.0	1.0
1071_0024681	1.0	2.0
1071_0024686	2.0	2.0
1071_0024703	1.0	2.0
1071_0024704	1.0	2.0
1071_0024705	1.0	2.0
1071_0024712	1.0	2.0
1071_0024713	2.0	1.0
1071_0024714	2.0	1.0
1071_0024758	2.0	2.0
1071_0024763	1.0	1.0
1071_0024772	0.0	0.0
1071_0024781	1.0	1.0
1071_0024800	1.0	1.0
1071_0024802	1.0	2.0
1071_0024817	0.0	1.0
1071_0024826	1.0	1.0
1071_0024831	0.0	1.0
1071_0024836	2.0	2.0
1071_0024849	0.0	1.0
1071_0024851	2.0	1.0
1071_0024853	0.0	1.0
1071_0024865	2.0	2.0
1071_0024872	1.0	2.0
1071_0024873	1.0	1.0
1071_0024875	1.0	1.0
1071_0024879	1.0	1.0
1071_0241833	1.0	1.0
1071_0242021	1.0	1.0
1071_0242023	1.0	1.0
1071_0242091	1.0	1.0
1071_0242093	0.0	1.0
1071_0243502	1.0	1.0
1071_0243592	1.0	1.0
1071_0243621	2.0	1.0
1071_0248311	2.0	1.0
1071_0248314	1.0	1.0
1071_0248324	0.0	1.0
1071_0248336	1.0	1.0
1071_0248341	1.0	0.0
1071_0248344	1.0	1.0
1071_0248347	1.0	1.0
1091_0000008	3.0	2.0
1091_0000015	2.0	1.0
1091_0000029	3.0	1.0
1091_0000030	1.0	1.0
1091_0000037	1.0	1.0
1091_0000043	1.0	1.0
1091_0000049	1.0	1.0
1091_0000055	2.0	2.0
1091_0000063	1.0	1.0
1091_0000064	1.0	1.0
1091_0000067	2.0	1.0
1091_0000069	1.0	1.0
1091_0000071	2.0	2.0
1091_0000074	2.0	2.0
1091_0000075	1.0	2.0
1091_0000076	2.0	2.0
1091_0000092	1.0	2.0
1091_0000140	1.0	1.0
1091_0000146	0.0	1.0
1091_0000153	1.0	3.0
1091_0000160	1.0	2.0
1091_0000162	2.0	2.0
1091_0000185	2.0	1.0
1091_0000190	0.0	1.0
1091_0000193	2.0	1.0
1091_0000196	2.0	1.0
1091_0000197	1.0	2.0
1091_0000210	2.0	1.0
1091_0000215	1.0	2.0
1091_0000219	2.0	1.0
1091_0000220	1.0	2.0
1091_0000231	2.0	2.0
1091_0000238	1.0	2.0
1091_0000243	1.0	1.0
1091_0000275	2.0	2.0
LANGUAGE: DE, 2th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.19
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.13
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.61      0.69      0.65        51
         2.0       0.52      0.20      0.29        60
         3.0       0.50      0.91      0.65        69
         4.0       0.00      0.00      0.00        14

    accuracy                           0.54       205
   macro avg       0.33      0.36      0.32       205
weighted avg       0.48      0.54      0.46       205

[[ 0 10  1  0  0]
 [ 0 35  7  9  0]
 [ 0  9 12 39  0]
 [ 0  3  3 63  0]
 [ 0  0  0 14  0]]
0.46448481041759865
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.96
  Training epoch took: 29
Running Validation...
  Average evaluation loss: 0.95
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.60      0.75      0.67        51
         2.0       0.60      0.53      0.57        60
         3.0       0.63      0.81      0.71        69
         4.0       0.00      0.00      0.00        14

    accuracy                           0.61       205
   macro avg       0.37      0.42      0.39       205
weighted avg       0.54      0.61      0.57       205

[[ 0 11  0  0  0]
 [ 0 38 12  1  0]
 [ 0 10 32 18  0]
 [ 0  4  9 56  0]
 [ 0  0  0 14  0]]
0.5702131380000929
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 11
Elapsed time 23

  Average training loss: 0.84
  Training epoch took: 29
Running Validation...
  Average evaluation loss: 0.99
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.62      0.55      0.58        51
         2.0       0.52      0.57      0.54        60
         3.0       0.61      0.83      0.70        69
         4.0       0.00      0.00      0.00        14

    accuracy                           0.58       205
   macro avg       0.35      0.39      0.36       205
weighted avg       0.51      0.58      0.54       205

[[ 0  9  2  0  0]
 [ 0 28 20  3  0]
 [ 0  6 34 20  0]
 [ 0  2 10 57  0]
 [ 0  0  0 14  0]]
0.5384810785005308
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.72
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.94
              precision    recall  f1-score   support

         0.0       0.50      0.09      0.15        11
         1.0       0.63      0.65      0.64        51
         2.0       0.56      0.55      0.55        60
         3.0       0.62      0.83      0.71        69
         4.0       0.00      0.00      0.00        14

    accuracy                           0.60       205
   macro avg       0.46      0.42      0.41       205
weighted avg       0.56      0.60      0.57       205

[[ 1  9  1  0  0]
 [ 0 33 15  3  0]
 [ 1  8 33 18  0]
 [ 0  2 10 57  0]
 [ 0  0  0 14  0]]
0.5683237713380767
205 205 205
Filename	True Label	Prediction
1023_0001416	3.0	3.0
1023_0001423	2.0	3.0
1023_0101684	2.0	3.0
1023_0101843	2.0	3.0
1023_0101895	3.0	3.0
1023_0101896	2.0	2.0
1023_0101901	3.0	3.0
1023_0101904	2.0	2.0
1023_0102118	3.0	3.0
1023_0103823	3.0	3.0
1023_0103824	3.0	3.0
1023_0103826	3.0	3.0
1023_0103955	3.0	3.0
1023_0107074	4.0	3.0
1023_0107682	2.0	1.0
1023_0107726	2.0	3.0
1023_0107780	3.0	3.0
1023_0107784	2.0	2.0
1023_0108648	3.0	3.0
1023_0108649	4.0	3.0
1023_0108810	3.0	3.0
1023_0108814	3.0	3.0
1023_0108889	3.0	3.0
1023_0108933	3.0	3.0
1023_0108935	3.0	3.0
1023_0108955	4.0	3.0
1023_0109029	2.0	2.0
1023_0109039	3.0	3.0
1023_0109247	4.0	3.0
1023_0109391	3.0	3.0
1023_0109399	2.0	3.0
1023_0109422	3.0	3.0
1023_0109495	3.0	3.0
1023_0109500	2.0	3.0
1023_0109518	2.0	3.0
1023_0109590	2.0	3.0
1023_0109609	2.0	3.0
1023_0109674	3.0	3.0
1023_0109716	3.0	3.0
1023_0109878	3.0	3.0
1023_0109917	3.0	3.0
1023_0109946	2.0	3.0
1031_0001950	3.0	3.0
1031_0002011	3.0	3.0
1031_0002040	4.0	3.0
1031_0002043	4.0	3.0
1031_0002079	4.0	3.0
1031_0002184	3.0	3.0
1031_0002185	3.0	3.0
1031_0002199	4.0	3.0
1031_0003012	4.0	3.0
1031_0003013	4.0	3.0
1031_0003071	3.0	3.0
1031_0003074	3.0	3.0
1031_0003076	3.0	3.0
1031_0003077	3.0	3.0
1031_0003097	3.0	3.0
1031_0003106	3.0	3.0
1031_0003128	3.0	3.0
1031_0003129	3.0	3.0
1031_0003132	3.0	3.0
1031_0003141	3.0	3.0
1031_0003150	3.0	3.0
1031_0003162	4.0	3.0
1031_0003167	3.0	3.0
1031_0003173	3.0	3.0
1031_0003179	3.0	3.0
1031_0003182	4.0	3.0
1031_0003206	3.0	3.0
1031_0003212	2.0	3.0
1031_0003214	3.0	3.0
1031_0003217	4.0	3.0
1031_0003219	3.0	3.0
1031_0003220	3.0	3.0
1031_0003234	2.0	3.0
1031_0003238	3.0	3.0
1031_0003337	3.0	3.0
1031_0003339	3.0	3.0
1031_0003389	3.0	3.0
1031_0003409	4.0	3.0
1061_0012029	2.0	2.0
1061_0120275	3.0	2.0
1061_0120281	1.0	2.0
1061_0120283	1.0	2.0
1061_0120284	0.0	1.0
1061_0120286	1.0	1.0
1061_0120288	2.0	2.0
1061_0120290	1.0	2.0
1061_0120297	2.0	2.0
1061_0120300	2.0	2.0
1061_0120306	3.0	2.0
1061_0120308	2.0	3.0
1061_0120312	1.0	1.0
1061_0120314	2.0	2.0
1061_0120320	3.0	3.0
1061_0120325	3.0	2.0
1061_0120334	3.0	3.0
1061_0120346	2.0	2.0
1061_0120347	2.0	2.0
1061_0120350	3.0	3.0
1061_0120352	1.0	1.0
1061_0120356	2.0	3.0
1061_0120359	1.0	2.0
1061_0120361	2.0	3.0
1061_0120366	3.0	3.0
1061_0120369	2.0	2.0
1061_0120382	2.0	2.0
1061_0120387	2.0	2.0
1061_0120404	2.0	2.0
1061_0120407	3.0	3.0
1061_0120425	3.0	3.0
1061_0120426	2.0	3.0
1061_0120427	2.0	2.0
1061_0120428	2.0	2.0
1061_0120439	1.0	2.0
1061_0120440	1.0	1.0
1061_0120450	1.0	3.0
1061_0120480	2.0	2.0
1061_0120487	3.0	2.0
1061_0120489	3.0	3.0
1061_0120499	2.0	3.0
1061_0120887	2.0	2.0
1061_1029119	1.0	2.0
1061_1202912	3.0	2.0
1061_1202917	2.0	2.0
1061_1202919	2.0	2.0
1071_0020001	2.0	1.0
1071_0024687	0.0	1.0
1071_0024690	2.0	3.0
1071_0024708	2.0	1.0
1071_0024709	3.0	2.0
1071_0024716	1.0	1.0
1071_0024759	0.0	1.0
1071_0024761	1.0	1.0
1071_0024762	1.0	1.0
1071_0024766	1.0	1.0
1071_0024768	1.0	2.0
1071_0024769	1.0	1.0
1071_0024779	1.0	2.0
1071_0024784	1.0	1.0
1071_0024804	1.0	1.0
1071_0024813	0.0	1.0
1071_0024815	1.0	1.0
1071_0024820	0.0	1.0
1071_0024840	1.0	1.0
1071_0024845	1.0	1.0
1071_0024846	1.0	1.0
1071_0024855	1.0	1.0
1071_0024860	0.0	1.0
1071_0024864	0.0	1.0
1071_0024878	2.0	2.0
1071_0024881	2.0	2.0
1071_0241832	1.0	1.0
1071_0242011	2.0	1.0
1071_0243581	1.0	1.0
1071_0243623	1.0	1.0
1071_0248302	1.0	1.0
1071_0248303	1.0	1.0
1071_0248307	2.0	1.0
1071_0248310	1.0	1.0
1071_0248322	1.0	1.0
1071_0248323	1.0	1.0
1071_0248331	1.0	1.0
1071_0248334	2.0	2.0
1071_0248340	0.0	0.0
1071_0248343	1.0	1.0
1071_0248346	0.0	1.0
1091_0000006	0.0	1.0
1091_0000012	1.0	1.0
1091_0000019	1.0	1.0
1091_0000021	1.0	2.0
1091_0000032	1.0	2.0
1091_0000042	1.0	1.0
1091_0000045	2.0	2.0
1091_0000051	1.0	1.0
1091_0000053	1.0	1.0
1091_0000058	2.0	2.0
1091_0000066	2.0	0.0
1091_0000070	2.0	1.0
1091_0000073	3.0	2.0
1091_0000078	3.0	1.0
1091_0000101	1.0	2.0
1091_0000126	2.0	2.0
1091_0000127	2.0	2.0
1091_0000145	1.0	1.0
1091_0000154	1.0	3.0
1091_0000156	3.0	2.0
1091_0000163	2.0	1.0
1091_0000168	1.0	3.0
1091_0000171	1.0	2.0
1091_0000174	1.0	1.0
1091_0000194	2.0	2.0
1091_0000198	2.0	2.0
1091_0000216	1.0	2.0
1091_0000225	3.0	1.0
1091_0000227	0.0	2.0
1091_0000232	2.0	2.0
1091_0000234	3.0	3.0
1091_0000239	3.0	2.0
1091_0000241	2.0	1.0
1091_0000248	3.0	2.0
1091_0000259	2.0	2.0
1091_0000266	1.0	2.0
1091_0000267	2.0	2.0
1091_0000272	1.0	2.0
LANGUAGE: DE, 3th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.19
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.03
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.56      0.76      0.64        51
         2.0       0.54      0.25      0.34        60
         3.0       0.59      0.91      0.72        69
         4.0       0.00      0.00      0.00        14

    accuracy                           0.57       205
   macro avg       0.34      0.39      0.34       205
weighted avg       0.49      0.57      0.50       205

[[ 0 11  0  0  0]
 [ 0 39 11  1  0]
 [ 0 16 15 29  0]
 [ 0  4  2 63  0]
 [ 0  0  0 14  0]]
0.5011136867567022
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.98
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.03
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.65      0.55      0.60        51
         2.0       0.52      0.57      0.54        60
         3.0       0.62      0.87      0.73        69
         4.0       0.00      0.00      0.00        14

    accuracy                           0.60       205
   macro avg       0.36      0.40      0.37       205
weighted avg       0.52      0.60      0.55       205

[[ 0  9  2  0  0]
 [ 0 28 23  0  0]
 [ 0  4 34 22  0]
 [ 0  2  7 60  0]
 [ 0  0  0 14  0]]
0.5509548745667064
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.88
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.91
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.60      0.67      0.63        51
         2.0       0.56      0.52      0.54        60
         3.0       0.65      0.87      0.74        69
         4.0       0.00      0.00      0.00        14

    accuracy                           0.61       205
   macro avg       0.36      0.41      0.38       205
weighted avg       0.53      0.61      0.56       205

[[ 0  9  2  0  0]
 [ 0 34 17  0  0]
 [ 0 10 31 19  0]
 [ 0  4  5 60  0]
 [ 0  0  0 14  0]]
0.5637563332155061
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.77
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 0.93
              precision    recall  f1-score   support

         0.0       1.00      0.09      0.17        11
         1.0       0.65      0.65      0.65        51
         2.0       0.54      0.53      0.54        60
         3.0       0.63      0.86      0.72        69
         4.0       0.00      0.00      0.00        14

    accuracy                           0.61       205
   macro avg       0.56      0.43      0.42       205
weighted avg       0.58      0.61      0.57       205

[[ 1  8  2  0  0]
 [ 0 33 18  0  0]
 [ 0  7 32 21  0]
 [ 0  3  7 59  0]
 [ 0  0  0 14  0]]
0.5709910299597919
205 205 205
Filename	True Label	Prediction
1023_0001418	3.0	3.0
1023_0101675	3.0	3.0
1023_0101683	3.0	3.0
1023_0101691	3.0	2.0
1023_0101752	3.0	3.0
1023_0101753	3.0	3.0
1023_0101847	3.0	3.0
1023_0101855	2.0	3.0
1023_0101898	3.0	3.0
1023_0101899	3.0	2.0
1023_0101909	4.0	3.0
1023_0103825	2.0	3.0
1023_0103827	3.0	3.0
1023_0103832	2.0	3.0
1023_0103833	3.0	3.0
1023_0103836	2.0	3.0
1023_0104203	3.0	3.0
1023_0104206	2.0	2.0
1023_0107725	3.0	3.0
1023_0107788	3.0	3.0
1023_0108305	3.0	3.0
1023_0108307	2.0	3.0
1023_0108426	2.0	3.0
1023_0108641	4.0	3.0
1023_0108812	2.0	3.0
1023_0108815	3.0	3.0
1023_0108932	3.0	3.0
1023_0108993	4.0	3.0
1023_0109022	3.0	3.0
1023_0109151	4.0	3.0
1023_0109402	3.0	3.0
1023_0109520	3.0	3.0
1023_0109524	3.0	3.0
1023_0109528	3.0	3.0
1023_0109614	2.0	3.0
1023_0109890	4.0	3.0
1023_0109891	3.0	3.0
1031_0002004	4.0	3.0
1031_0002083	3.0	3.0
1031_0002088	3.0	3.0
1031_0002092	3.0	3.0
1031_0002187	3.0	3.0
1031_0003023	3.0	3.0
1031_0003042	3.0	3.0
1031_0003053	3.0	3.0
1031_0003065	3.0	3.0
1031_0003072	3.0	3.0
1031_0003073	4.0	3.0
1031_0003091	2.0	3.0
1031_0003144	3.0	3.0
1031_0003146	4.0	3.0
1031_0003155	3.0	3.0
1031_0003156	3.0	3.0
1031_0003157	4.0	3.0
1031_0003161	3.0	3.0
1031_0003166	2.0	3.0
1031_0003174	4.0	3.0
1031_0003186	4.0	3.0
1031_0003187	3.0	3.0
1031_0003189	4.0	3.0
1031_0003191	3.0	3.0
1031_0003230	3.0	3.0
1031_0003245	3.0	3.0
1031_0003261	3.0	3.0
1031_0003313	3.0	3.0
1031_0003327	2.0	3.0
1031_0003331	3.0	3.0
1031_0003338	3.0	3.0
1031_0003355	3.0	3.0
1031_0003357	3.0	3.0
1031_0003358	4.0	3.0
1031_0003368	3.0	3.0
1031_0003383	3.0	3.0
1031_0003392	4.0	3.0
1031_0003410	3.0	3.0
1031_0003419	3.0	3.0
1061_0120277	1.0	2.0
1061_0120278	2.0	2.0
1061_0120280	1.0	1.0
1061_0120295	0.0	2.0
1061_0120301	2.0	2.0
1061_0120309	1.0	1.0
1061_0120310	2.0	2.0
1061_0120313	2.0	1.0
1061_0120318	2.0	3.0
1061_0120321	2.0	2.0
1061_0120323	1.0	2.0
1061_0120324	3.0	2.0
1061_0120328	1.0	2.0
1061_0120330	3.0	3.0
1061_0120335	3.0	3.0
1061_0120341	1.0	2.0
1061_0120343	3.0	3.0
1061_0120348	1.0	1.0
1061_0120367	3.0	2.0
1061_0120370	2.0	3.0
1061_0120372	2.0	2.0
1061_0120373	2.0	3.0
1061_0120375	2.0	2.0
1061_0120389	2.0	3.0
1061_0120391	1.0	2.0
1061_0120408	3.0	3.0
1061_0120410	2.0	2.0
1061_0120411	3.0	3.0
1061_0120424	2.0	3.0
1061_0120453	2.0	3.0
1061_0120482	2.0	3.0
1061_0120484	2.0	3.0
1061_0120485	3.0	3.0
1061_0120493	2.0	2.0
1061_0120497	3.0	3.0
1061_0120855	1.0	2.0
1061_0120856	2.0	2.0
1061_0120857	2.0	2.0
1061_0120858	2.0	3.0
1061_0120875	3.0	3.0
1061_0120881	2.0	3.0
1061_0120882	3.0	3.0
1061_0120883	2.0	2.0
1061_0120885	3.0	3.0
1061_1202911	1.0	2.0
1061_1202913	1.0	2.0
1061_1202915	1.0	2.0
1071_0024680	2.0	2.0
1071_0024685	2.0	2.0
1071_0024691	2.0	2.0
1071_0024692	3.0	3.0
1071_0024706	1.0	2.0
1071_0024710	1.0	1.0
1071_0024715	2.0	2.0
1071_0024756	1.0	1.0
1071_0024773	1.0	1.0
1071_0024806	1.0	1.0
1071_0024808	1.0	2.0
1071_0024809	1.0	1.0
1071_0024810	1.0	1.0
1071_0024811	1.0	1.0
1071_0024816	1.0	1.0
1071_0024833	2.0	1.0
1071_0024841	1.0	1.0
1071_0024844	1.0	1.0
1071_0024854	1.0	1.0
1071_0024857	1.0	1.0
1071_0024863	2.0	1.0
1071_0024874	1.0	1.0
1071_0242012	2.0	2.0
1071_0242043	0.0	1.0
1071_0242071	0.0	1.0
1071_0242072	0.0	1.0
1071_0243593	1.0	1.0
1071_0243622	1.0	1.0
1071_0248301	1.0	1.0
1071_0248304	1.0	1.0
1071_0248305	0.0	1.0
1071_0248308	1.0	1.0
1071_0248318	0.0	0.0
1071_0248320	0.0	1.0
1071_0248325	1.0	1.0
1071_0248326	1.0	1.0
1071_0248327	0.0	1.0
1071_0248328	0.0	1.0
1071_0248332	2.0	2.0
1071_0248333	2.0	1.0
1071_0248335	1.0	2.0
1071_0248337	1.0	1.0
1071_0248345	2.0	2.0
1091_0000004	1.0	1.0
1091_0000010	3.0	2.0
1091_0000016	1.0	1.0
1091_0000017	3.0	1.0
1091_0000023	3.0	1.0
1091_0000041	0.0	1.0
1091_0000047	2.0	1.0
1091_0000059	1.0	2.0
1091_0000062	3.0	1.0
1091_0000065	1.0	1.0
1091_0000102	2.0	2.0
1091_0000116	2.0	2.0
1091_0000144	1.0	1.0
1091_0000152	1.0	1.0
1091_0000157	3.0	2.0
1091_0000158	1.0	2.0
1091_0000159	2.0	2.0
1091_0000161	2.0	2.0
1091_0000164	1.0	1.0
1091_0000166	1.0	1.0
1091_0000169	2.0	2.0
1091_0000173	2.0	2.0
1091_0000195	1.0	1.0
1091_0000205	1.0	2.0
1091_0000207	2.0	2.0
1091_0000212	2.0	2.0
1091_0000217	2.0	2.0
1091_0000224	2.0	1.0
1091_0000229	1.0	2.0
1091_0000235	1.0	1.0
1091_0000245	1.0	2.0
1091_0000246	2.0	2.0
1091_0000249	2.0	1.0
1091_0000256	0.0	2.0
1091_0000258	2.0	2.0
1091_0000260	2.0	2.0
1091_0000261	2.0	2.0
1091_0000265	3.0	2.0
1091_0000269	1.0	2.0
LANGUAGE: DE, 4th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.18
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.54      0.79      0.64        52
         2.0       0.49      0.32      0.38        60
         3.0       0.62      0.81      0.70        69
         4.0       0.00      0.00      0.00        13

    accuracy                           0.57       205
   macro avg       0.33      0.38      0.35       205
weighted avg       0.49      0.57      0.51       205

[[ 0 11  0  0  0]
 [ 0 41 11  0  0]
 [ 0 20 19 21  0]
 [ 0  4  9 56  0]
 [ 0  0  0 13  0]]
0.5119345200741887
205 205 205



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 1.00
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.01
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.55      0.79      0.65        52
         2.0       0.52      0.40      0.45        60
         3.0       0.63      0.77      0.69        69
         4.0       0.00      0.00      0.00        13

    accuracy                           0.58       205
   macro avg       0.34      0.39      0.36       205
weighted avg       0.50      0.58      0.53       205

[[ 0 11  0  0  0]
 [ 0 41 11  0  0]
 [ 0 18 24 18  0]
 [ 0  5 11 53  0]
 [ 0  0  0 13  0]]
0.5295050538487241
205 205 205



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.90
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.06
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.57      0.50      0.53        52
         2.0       0.42      0.40      0.41        60
         3.0       0.60      0.88      0.71        69
         4.0       0.00      0.00      0.00        13

    accuracy                           0.54       205
   macro avg       0.32      0.36      0.33       205
weighted avg       0.47      0.54      0.49       205

[[ 0 10  1  0  0]
 [ 0 26 25  1  0]
 [ 0  9 24 27  0]
 [ 0  1  7 61  0]
 [ 0  0  0 13  0]]
0.4948063001244737
205 205 205



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23

  Average training loss: 0.81
  Training epoch took: 30
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.59      0.52      0.55        52
         2.0       0.43      0.43      0.43        60
         3.0       0.60      0.86      0.71        69
         4.0       0.00      0.00      0.00        13

    accuracy                           0.55       205
   macro avg       0.32      0.36      0.34       205
weighted avg       0.48      0.55      0.50       205

[[ 0 10  1  0  0]
 [ 0 27 24  1  0]
 [ 1  8 26 25  0]
 [ 0  1  9 59  0]
 [ 0  0  0 13  0]]
0.5044270841095311
205 205 205
Filename	True Label	Prediction
1023_0001422	2.0	2.0
1023_0101688	3.0	3.0
1023_0101690	2.0	3.0
1023_0101693	4.0	3.0
1023_0101695	2.0	3.0
1023_0101701	2.0	3.0
1023_0101749	3.0	3.0
1023_0101751	3.0	3.0
1023_0101841	3.0	3.0
1023_0101844	2.0	3.0
1023_0101848	2.0	2.0
1023_0101852	3.0	3.0
1023_0101853	2.0	3.0
1023_0101856	2.0	3.0
1023_0101894	2.0	3.0
1023_0101906	2.0	3.0
1023_0103821	3.0	3.0
1023_0103831	3.0	3.0
1023_0103880	3.0	3.0
1023_0104207	3.0	3.0
1023_0107042	3.0	3.0
1023_0107075	3.0	3.0
1023_0107773	3.0	3.0
1023_0107783	3.0	3.0
1023_0107787	3.0	3.0
1023_0108422	4.0	3.0
1023_0108423	3.0	3.0
1023_0108518	3.0	3.0
1023_0108751	3.0	3.0
1023_0108753	2.0	3.0
1023_0108811	4.0	3.0
1023_0108813	2.0	3.0
1023_0108887	2.0	3.0
1023_0108890	3.0	3.0
1023_0109027	3.0	3.0
1023_0109030	3.0	3.0
1023_0109249	3.0	3.0
1023_0109250	2.0	3.0
1023_0109395	2.0	3.0
1023_0109401	3.0	3.0
1023_0109516	3.0	3.0
1023_0109519	2.0	3.0
1023_0109588	2.0	3.0
1023_0109721	2.0	3.0
1023_0109914	3.0	3.0
1023_0109947	3.0	3.0
1023_0109951	3.0	3.0
1031_0001703	3.0	3.0
1031_0001951	2.0	3.0
1031_0001997	3.0	3.0
1031_0002002	2.0	3.0
1031_0002003	3.0	3.0
1031_0002005	4.0	3.0
1031_0002032	3.0	3.0
1031_0002042	3.0	3.0
1031_0002196	3.0	3.0
1031_0003048	4.0	3.0
1031_0003052	3.0	3.0
1031_0003054	3.0	3.0
1031_0003063	4.0	3.0
1031_0003078	3.0	3.0
1031_0003085	3.0	3.0
1031_0003090	4.0	3.0
1031_0003092	2.0	3.0
1031_0003095	3.0	3.0
1031_0003099	3.0	3.0
1031_0003127	4.0	3.0
1031_0003135	3.0	3.0
1031_0003136	3.0	3.0
1031_0003154	3.0	3.0
1031_0003164	4.0	3.0
1031_0003172	3.0	3.0
1031_0003180	4.0	3.0
1031_0003203	2.0	3.0
1031_0003205	3.0	3.0
1031_0003216	2.0	3.0
1031_0003225	3.0	3.0
1031_0003231	3.0	3.0
1031_0003232	2.0	3.0
1031_0003235	3.0	3.0
1031_0003236	3.0	3.0
1031_0003240	3.0	3.0
1031_0003244	4.0	3.0
1031_0003262	3.0	3.0
1031_0003273	3.0	3.0
1031_0003274	3.0	3.0
1031_0003309	3.0	3.0
1031_0003314	4.0	3.0
1031_0003366	3.0	3.0
1031_0003369	4.0	3.0
1031_0003384	3.0	3.0
1031_0003408	3.0	3.0
1031_0003414	3.0	3.0
1061_0120274	2.0	2.0
1061_0120279	1.0	2.0
1061_0120287	1.0	2.0
1061_0120298	2.0	2.0
1061_0120302	1.0	2.0
1061_0120304	2.0	2.0
1061_0120311	2.0	2.0
1061_0120338	1.0	3.0
1061_0120354	2.0	2.0
1061_0120355	1.0	1.0
1061_0120360	3.0	3.0
1061_0120368	2.0	2.0
1061_0120406	2.0	3.0
1061_0120414	3.0	2.0
1061_0120421	2.0	3.0
1061_0120433	1.0	2.0
1061_0120442	3.0	2.0
1061_0120449	3.0	2.0
1061_0120455	2.0	2.0
1061_0120460	2.0	2.0
1061_0120496	2.0	2.0
1061_0120853	2.0	3.0
1061_0120859	3.0	3.0
1061_0120878	2.0	1.0
1061_0120886	2.0	2.0
1061_0120888	2.0	2.0
1061_0120889	1.0	2.0
1061_0120894	2.0	2.0
1061_1029112	3.0	3.0
1061_1029113	1.0	2.0
1071_0024689	2.0	2.0
1071_0024693	1.0	1.0
1071_0024699	1.0	2.0
1071_0024757	2.0	2.0
1071_0024765	1.0	1.0
1071_0024767	2.0	1.0
1071_0024770	1.0	1.0
1071_0024776	0.0	1.0
1071_0024777	1.0	2.0
1071_0024782	0.0	1.0
1071_0024783	0.0	1.0
1071_0024797	0.0	1.0
1071_0024799	3.0	2.0
1071_0024801	1.0	1.0
1071_0024807	1.0	1.0
1071_0024812	1.0	1.0
1071_0024814	0.0	1.0
1071_0024819	1.0	2.0
1071_0024823	1.0	1.0
1071_0024824	1.0	1.0
1071_0024848	1.0	1.0
1071_0024850	1.0	1.0
1071_0024852	0.0	1.0
1071_0024859	1.0	2.0
1071_0024862	1.0	1.0
1071_0024867	2.0	2.0
1071_0241831	1.0	2.0
1071_0242013	1.0	1.0
1071_0242073	1.0	1.0
1071_0242092	0.0	1.0
1071_0243582	1.0	1.0
1071_0248309	2.0	1.0
1071_0248315	0.0	1.0
1071_0248316	1.0	1.0
1071_0248317	0.0	1.0
1071_0248319	0.0	1.0
1071_0248321	2.0	1.0
1071_0248330	1.0	1.0
1071_0248338	2.0	1.0
1071_0248342	1.0	1.0
1071_0248348	1.0	1.0
1071_0248350	1.0	1.0
1091_0000001	1.0	1.0
1091_0000005	3.0	2.0
1091_0000014	1.0	1.0
1091_0000020	1.0	2.0
1091_0000022	2.0	2.0
1091_0000024	3.0	1.0
1091_0000036	1.0	2.0
1091_0000038	2.0	1.0
1091_0000039	1.0	1.0
1091_0000048	1.0	1.0
1091_0000050	1.0	1.0
1091_0000052	1.0	1.0
1091_0000057	2.0	1.0
1091_0000060	3.0	2.0
1091_0000061	2.0	0.0
1091_0000087	2.0	2.0
1091_0000114	1.0	2.0
1091_0000165	2.0	1.0
1091_0000167	1.0	2.0
1091_0000191	1.0	2.0
1091_0000199	2.0	2.0
1091_0000200	2.0	2.0
1091_0000201	2.0	2.0
1091_0000206	1.0	2.0
1091_0000208	1.0	2.0
1091_0000211	1.0	2.0
1091_0000218	3.0	2.0
1091_0000226	1.0	1.0
1091_0000228	1.0	2.0
1091_0000242	1.0	2.0
1091_0000244	2.0	2.0
1091_0000247	2.0	2.0
1091_0000252	2.0	2.0
1091_0000255	0.0	2.0
1091_0000257	1.0	2.0
1091_0000263	3.0	2.0
1091_0000264	1.0	2.0
1091_0000270	3.0	2.0
1091_0000273	1.0	2.0
1091_0000276	2.0	2.0
Averaged weighted F1-scores 0.556656368442384
MONOLINGUAL Experiments with:  CZ
144.90552995391704 65.35717405024758
LABEL SET ['A2', 'B1', 'B2']
LANGUAGE: CZ, 0th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.05
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.95
              precision    recall  f1-score   support

         0.0       0.52      0.87      0.65        38
         1.0       0.62      0.45      0.53        33
         2.0       0.00      0.00      0.00        16

    accuracy                           0.55        87
   macro avg       0.38      0.44      0.39        87
weighted avg       0.47      0.55      0.49        87

[[33  5  0]
 [18 15  0]
 [12  4  0]]
0.4850586692063036
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.82
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.62
              precision    recall  f1-score   support

         0.0       0.90      0.68      0.78        38
         1.0       0.57      0.48      0.52        33
         2.0       0.53      1.00      0.70        16

    accuracy                           0.67        87
   macro avg       0.67      0.72      0.67        87
weighted avg       0.71      0.67      0.67        87

[[26 12  0]
 [ 3 16 14]
 [ 0  0 16]]
0.6659131897227311
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.59
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.50
              precision    recall  f1-score   support

         0.0       0.78      0.95      0.86        38
         1.0       0.79      0.58      0.67        33
         2.0       0.76      0.81      0.79        16

    accuracy                           0.78        87
   macro avg       0.78      0.78      0.77        87
weighted avg       0.78      0.78      0.77        87

[[36  2  0]
 [10 19  4]
 [ 0  3 13]]
0.772155048017117
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.50
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.49
              precision    recall  f1-score   support

         0.0       0.79      0.97      0.87        38
         1.0       0.85      0.52      0.64        33
         2.0       0.70      0.88      0.78        16

    accuracy                           0.78        87
   macro avg       0.78      0.79      0.76        87
weighted avg       0.79      0.78      0.77        87

[[37  1  0]
 [10 17  6]
 [ 0  2 14]]
0.7666276862801796
87 87 87
Filename	True Label	Prediction
0607	1.0	0.0
0614	1.0	1.0
0615	0.0	0.0
0616	0.0	0.0
0619	1.0	0.0
0633	1.0	1.0
0634	1.0	1.0
0635	0.0	0.0
0641	0.0	0.0
0720	0.0	0.0
0722	1.0	0.0
0801	0.0	0.0
0811	1.0	1.0
0822	0.0	0.0
0827	0.0	0.0
0829	0.0	1.0
0901	1.0	1.0
0902	1.0	0.0
0915	1.0	0.0
0918	0.0	0.0
0929	0.0	0.0
1005	0.0	0.0
1006	1.0	0.0
1007	1.0	0.0
1016	0.0	0.0
1021	0.0	0.0
1022	1.0	0.0
KYJ0611004A	0.0	0.0
KYJ0611005B	0.0	0.0
KYJ0611009A	0.0	0.0
LIB0611003A	0.0	0.0
LON0610002A	0.0	0.0
LON0611003	2.0	2.0
MOS0509004	1.0	1.0
MOS0611013	1.0	2.0
PAR1011009A	0.0	0.0
PAR1011009B	0.0	0.0
PAR1011017	2.0	2.0
PHA0111003A	0.0	0.0
PHA0112006B	1.0	0.0
PHA0112012A	0.0	0.0
PHA0112012B	0.0	0.0
PHA0209001	0.0	0.0
PHA0209013	0.0	0.0
PHA0411009B	0.0	0.0
PHA0411012B	0.0	0.0
PHA0411028	1.0	1.0
PHA0411029	1.0	1.0
PHA0411030	2.0	2.0
PHA0411033	1.0	1.0
PHA0411036	2.0	2.0
PHA0411041	2.0	2.0
PHA0411042	1.0	2.0
PHA0411055	2.0	2.0
PHA0411062	1.0	1.0
PHA0509021	1.0	1.0
PHA0509034	1.0	1.0
PHA0509035	1.0	1.0
PHA0509043	2.0	1.0
PHA0510003A	0.0	0.0
PHA0510010B	0.0	0.0
PHA0510032	2.0	2.0
PHA0510036	2.0	2.0
PHA0510047	1.0	1.0
PHA0510049	1.0	1.0
PHA0510050	1.0	2.0
PHA0610005B	0.0	0.0
PHA0610006A	0.0	0.0
PHA0610006B	0.0	0.0
PHA0610016	2.0	2.0
PHA0710012	2.0	1.0
PHA0710017	2.0	2.0
PHA0810003	1.0	2.0
PHA0810010	1.0	2.0
PHA0811014	1.0	1.0
PHA0811019	2.0	2.0
PHA1109001	0.0	0.0
PHA1109004	2.0	2.0
PHA1109008	0.0	0.0
PHA1109026	2.0	2.0
PHA1110002A	1.0	0.0
PHA1110015	2.0	2.0
PHA1111001B	0.0	0.0
PHA1111003B	0.0	0.0
PHA1111004B	0.0	0.0
VAR0910005	1.0	1.0
VAR0910006	1.0	2.0
LANGUAGE: CZ, 1th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.93
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.96
              precision    recall  f1-score   support

         0.0       1.00      0.24      0.38        38
         1.0       0.42      1.00      0.59        33
         2.0       0.00      0.00      0.00        16

    accuracy                           0.48        87
   macro avg       0.47      0.41      0.33        87
weighted avg       0.60      0.48      0.39        87

[[ 9 29  0]
 [ 0 33  0]
 [ 0 16  0]]
0.39281394380440593
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.72
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.54
              precision    recall  f1-score   support

         0.0       0.81      0.76      0.78        38
         1.0       0.66      0.70      0.68        33
         2.0       0.81      0.81      0.81        16

    accuracy                           0.75        87
   macro avg       0.76      0.76      0.76        87
weighted avg       0.75      0.75      0.75        87

[[29  9  0]
 [ 7 23  3]
 [ 0  3 13]]
0.7483599217879137
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.51
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.58
              precision    recall  f1-score   support

         0.0       0.80      0.87      0.84        38
         1.0       0.68      0.39      0.50        33
         2.0       0.56      0.94      0.70        16

    accuracy                           0.70        87
   macro avg       0.68      0.73      0.68        87
weighted avg       0.71      0.70      0.68        87

[[33  5  0]
 [ 8 13 12]
 [ 0  1 15]]
0.6828692659851999
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.45
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.62
              precision    recall  f1-score   support

         0.0       0.96      0.68      0.80        38
         1.0       0.59      0.58      0.58        33
         2.0       0.54      0.94      0.68        16

    accuracy                           0.69        87
   macro avg       0.70      0.73      0.69        87
weighted avg       0.74      0.69      0.70        87

[[26 12  0]
 [ 1 19 13]
 [ 0  1 15]]
0.6965678000160759
87 87 87
Filename	True Label	Prediction
0602	0.0	1.0
0608	0.0	1.0
0611	1.0	1.0
0621	1.0	1.0
0623	0.0	1.0
0627	1.0	1.0
0631	1.0	1.0
0632	0.0	1.0
0644	0.0	1.0
0715	1.0	1.0
0721	1.0	1.0
0725	0.0	1.0
0812	0.0	0.0
0813	0.0	0.0
0814	0.0	0.0
0815	1.0	1.0
0820	0.0	0.0
0906	1.0	1.0
0913	1.0	0.0
0919	0.0	0.0
0922	0.0	0.0
0923	1.0	1.0
0930	0.0	1.0
1001	0.0	0.0
1018	0.0	1.0
1111	0.0	0.0
1112	0.0	1.0
1113	0.0	1.0
1114	1.0	1.0
BER0609003	1.0	1.0
BER0611006	1.0	2.0
KYJ0611005A	0.0	0.0
KYJ0611006B	0.0	0.0
LIB0611002B	0.0	0.0
LON0611002B	0.0	0.0
LON0611004A	0.0	0.0
MOS0509001	1.0	1.0
MOS0611014	0.0	1.0
PAR1011008A	0.0	0.0
PAR1011014	1.0	2.0
PAR1011015	1.0	1.0
PAR1011018	2.0	2.0
PHA0111005A	0.0	0.0
PHA0111018	1.0	2.0
PHA0112003A	0.0	0.0
PHA0209034	1.0	2.0
PHA0411011A	0.0	0.0
PHA0411011B	0.0	0.0
PHA0411012A	0.0	0.0
PHA0411034	0.0	1.0
PHA0411035	2.0	1.0
PHA0411039	1.0	2.0
PHA0411047	1.0	2.0
PHA0411053	2.0	2.0
PHA0411060	1.0	2.0
PHA0509022	2.0	2.0
PHA0509027	1.0	2.0
PHA0509030	2.0	2.0
PHA0509036	2.0	2.0
PHA0509038	1.0	1.0
PHA0509039	2.0	2.0
PHA0509041	1.0	2.0
PHA0510010A	0.0	0.0
PHA0510034	2.0	2.0
PHA0510035	2.0	2.0
PHA0510037	1.0	1.0
PHA0610007B	0.0	0.0
PHA0610026	2.0	2.0
PHA0710013	2.0	2.0
PHA0710014	2.0	2.0
PHA0810001	2.0	2.0
PHA0810002	1.0	2.0
PHA0811017	2.0	2.0
PHA0811020	1.0	1.0
PHA1109005	1.0	1.0
PHA1109025	0.0	0.0
PHA1109027	2.0	2.0
PHA1110003A	0.0	0.0
PHA1110013	1.0	2.0
PHA1110022	2.0	2.0
PHA1111009A	0.0	0.0
ST071122B	0.0	0.0
TI071122B	0.0	0.0
VAR0909003	1.0	1.0
VAR0909005	1.0	1.0
VAR0909007	1.0	2.0
VAR0910011	1.0	2.0
LANGUAGE: CZ, 2th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.94
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.83
              precision    recall  f1-score   support

         0.0       0.67      1.00      0.80        38
         1.0       0.50      0.45      0.48        33
         2.0       0.00      0.00      0.00        16

    accuracy                           0.61        87
   macro avg       0.39      0.48      0.43        87
weighted avg       0.48      0.61      0.53        87

[[38  0  0]
 [18 15  0]
 [ 1 15  0]]
0.5300492610837438
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.71
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.60
              precision    recall  f1-score   support

         0.0       0.76      0.89      0.82        38
         1.0       0.58      0.64      0.61        33
         2.0       0.83      0.31      0.45        16

    accuracy                           0.69        87
   macro avg       0.72      0.61      0.63        87
weighted avg       0.70      0.69      0.67        87

[[34  4  0]
 [11 21  1]
 [ 0 11  5]]
0.6723242978729693
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.58
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.55
              precision    recall  f1-score   support

         0.0       0.89      0.82      0.85        38
         1.0       0.66      0.70      0.68        33
         2.0       0.65      0.69      0.67        16

    accuracy                           0.75        87
   macro avg       0.73      0.73      0.73        87
weighted avg       0.76      0.75      0.75        87

[[31  7  0]
 [ 4 23  6]
 [ 0  5 11]]
0.7501628584042653
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.52
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.56
              precision    recall  f1-score   support

         0.0       0.89      0.84      0.86        38
         1.0       0.69      0.67      0.68        33
         2.0       0.63      0.75      0.69        16

    accuracy                           0.76        87
   macro avg       0.74      0.75      0.74        87
weighted avg       0.77      0.76      0.76        87

[[32  6  0]
 [ 4 22  7]
 [ 0  4 12]]
0.7606293675259195
87 87 87
Filename	True Label	Prediction
0601	0.0	0.0
0604	1.0	1.0
0622	0.0	0.0
0624	1.0	1.0
0626	1.0	0.0
0628	1.0	1.0
0629	1.0	1.0
0718	0.0	0.0
0719	1.0	0.0
0802	0.0	0.0
0806	0.0	1.0
0807	1.0	1.0
0808	0.0	0.0
0816	1.0	1.0
0817	0.0	0.0
0826	0.0	0.0
0903	0.0	1.0
0905	1.0	1.0
0910	0.0	0.0
0912	1.0	1.0
0916	0.0	0.0
0920	1.0	1.0
0924	0.0	0.0
0927	1.0	1.0
1002	1.0	1.0
1008	0.0	1.0
1010	0.0	1.0
1017	0.0	0.0
1019	0.0	1.0
1115	0.0	1.0
BER0611003	1.0	1.0
BER0611005	1.0	1.0
KYJ0611006A	0.0	0.0
KYJ0611009B	0.0	0.0
PHA0111001A	0.0	0.0
PHA0111001B	0.0	0.0
PHA0111002B	1.0	0.0
PHA0111003B	0.0	0.0
PHA0111015	2.0	1.0
PHA0111016	2.0	2.0
PHA0112002A	0.0	0.0
PHA0112007A	0.0	0.0
PHA0112007B	0.0	0.0
PHA0112009A	1.0	0.0
PHA0209026	2.0	2.0
PHA0209039	1.0	2.0
PHA0411008A	0.0	0.0
PHA0411009A	0.0	0.0
PHA0411010B	0.0	0.0
PHA0411038	2.0	2.0
PHA0411044	2.0	2.0
PHA0411045	1.0	1.0
PHA0509002	0.0	0.0
PHA0509007	0.0	0.0
PHA0509013	0.0	0.0
PHA0509017	1.0	2.0
PHA0509019	1.0	1.0
PHA0509025	2.0	2.0
PHA0509031	1.0	1.0
PHA0509032	1.0	2.0
PHA0509040	1.0	2.0
PHA0510002A	0.0	0.0
PHA0510003B	0.0	0.0
PHA0510004A	0.0	0.0
PHA0510013A	0.0	0.0
PHA0510030	1.0	1.0
PHA0510039	1.0	2.0
PHA0709008	2.0	1.0
PHA0710011	2.0	2.0
PHA0710016	2.0	1.0
PHA0710018	2.0	2.0
PHA0710019	2.0	2.0
PHA0809010	1.0	1.0
PHA0810008	1.0	2.0
PHA0810012	1.0	1.0
PHA0810015	2.0	2.0
PHA0811012	2.0	2.0
PHA0811016	1.0	1.0
PHA1109006	1.0	1.0
PHA1109023	0.0	0.0
PHA1110001B	0.0	0.0
PHA1110003B	0.0	0.0
PHA1110019	1.0	2.0
VAR0209036	1.0	1.0
VAR0909006	2.0	2.0
VAR0910004	2.0	2.0
VAR0910010	2.0	1.0
LANGUAGE: CZ, 3th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.99
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.83
              precision    recall  f1-score   support

         0.0       0.67      0.59      0.63        37
         1.0       0.52      0.85      0.64        33
         2.0       0.00      0.00      0.00        17

    accuracy                           0.57        87
   macro avg       0.40      0.48      0.42        87
weighted avg       0.48      0.57      0.51        87

[[22 15  0]
 [ 5 28  0]
 [ 6 11  0]]
0.5114772662929619
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.74
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.69
              precision    recall  f1-score   support

         0.0       0.75      0.97      0.85        37
         1.0       0.71      0.52      0.60        33
         2.0       0.73      0.65      0.69        17

    accuracy                           0.74        87
   macro avg       0.73      0.71      0.71        87
weighted avg       0.73      0.74      0.72        87

[[36  1  0]
 [12 17  4]
 [ 0  6 11]]
0.7208377815736094
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.54
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.61
              precision    recall  f1-score   support

         0.0       0.85      0.78      0.82        37
         1.0       0.64      0.76      0.69        33
         2.0       0.79      0.65      0.71        17

    accuracy                           0.75        87
   macro avg       0.76      0.73      0.74        87
weighted avg       0.76      0.75      0.75        87

[[29  8  0]
 [ 5 25  3]
 [ 0  6 11]]
0.7495004012456894
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.42
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.59
              precision    recall  f1-score   support

         0.0       0.83      0.78      0.81        37
         1.0       0.64      0.70      0.67        33
         2.0       0.75      0.71      0.73        17

    accuracy                           0.74        87
   macro avg       0.74      0.73      0.73        87
weighted avg       0.74      0.74      0.74        87

[[29  8  0]
 [ 6 23  4]
 [ 0  5 12]]
0.7375769186114013
87 87 87
Filename	True Label	Prediction
0603	1.0	1.0
0605	1.0	1.0
0606	1.0	1.0
0610	0.0	1.0
0612	0.0	0.0
0613	0.0	0.0
0617	0.0	0.0
0618	0.0	1.0
0630	0.0	0.0
0636	1.0	1.0
0638	0.0	1.0
0645	1.0	1.0
0714	1.0	0.0
0717	0.0	1.0
0723	0.0	0.0
0724	1.0	1.0
0803	0.0	1.0
0804	0.0	0.0
0805	1.0	0.0
0809	1.0	0.0
0818	0.0	0.0
0821	1.0	1.0
0824	0.0	0.0
0825	0.0	0.0
0904	0.0	0.0
0914	0.0	1.0
0925	1.0	1.0
0928	1.0	1.0
1003	0.0	0.0
1004	0.0	0.0
1009	1.0	1.0
1014	1.0	0.0
1015	0.0	1.0
1020	1.0	0.0
1116	0.0	1.0
9999	0.0	0.0
BER0611007	1.0	1.0
KYJ0611003A	0.0	0.0
LON0611002A	0.0	0.0
LON0611004B	0.0	0.0
MOS0611012	1.0	1.0
PAR1011013	1.0	1.0
PAR1011016	2.0	2.0
PHA0111004A	0.0	0.0
PHA0111010	2.0	1.0
PHA0112009B	1.0	0.0
PHA0210001	0.0	0.0
PHA0210007	0.0	0.0
PHA0411031	2.0	2.0
PHA0411051	2.0	2.0
PHA0411056	2.0	1.0
PHA0411058	2.0	2.0
PHA0411061	2.0	2.0
PHA0509015	2.0	1.0
PHA0509024	1.0	2.0
PHA0509028	2.0	1.0
PHA0509045	1.0	1.0
PHA0510013B	0.0	0.0
PHA0510031	1.0	1.0
PHA0510040	1.0	2.0
PHA0510048	1.0	1.0
PHA0610005A	0.0	0.0
PHA0610015	1.0	2.0
PHA0610017	2.0	2.0
PHA0610018	2.0	2.0
PHA0610019A	0.0	0.0
PHA0610019B	0.0	0.0
PHA0610025	2.0	2.0
PHA0710009	1.0	1.0
PHA0710015	2.0	2.0
PHA0810004	1.0	1.0
PHA0810009	2.0	2.0
PHA0811010	1.0	2.0
PHA0811013	2.0	2.0
PHA1109002	2.0	1.0
PHA1109007	1.0	1.0
PHA1109024	2.0	2.0
PHA1110001A	0.0	0.0
PHA1110004A	0.0	0.0
PHA1110017	1.0	1.0
PHA1111001A	0.0	0.0
PHA1111002A	0.0	0.0
PHA1111006B	0.0	0.0
PHA1111008B	0.0	0.0
VAR0909004	1.0	1.0
VAR0909010	1.0	1.0
VAR0910007	1.0	1.0
LANGUAGE: CZ, 4th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.91
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.85
              precision    recall  f1-score   support

         0.0       0.71      0.81      0.76        37
         1.0       0.50      0.67      0.57        33
         2.0       0.00      0.00      0.00        16

    accuracy                           0.60        86
   macro avg       0.40      0.49      0.44        86
weighted avg       0.50      0.60      0.55        86

[[30  7  0]
 [11 22  0]
 [ 1 15  0]]
0.5460280079061357
86 86 86



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.77
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.78
              precision    recall  f1-score   support

         0.0       0.78      0.86      0.82        37
         1.0       0.53      0.73      0.62        33
         2.0       0.00      0.00      0.00        16

    accuracy                           0.65        86
   macro avg       0.44      0.53      0.48        86
weighted avg       0.54      0.65      0.59        86

[[32  5  0]
 [ 9 24  0]
 [ 0 16  0]]
0.5891472868217055
86 86 86



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.67
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.70
              precision    recall  f1-score   support

         0.0       0.72      0.89      0.80        37
         1.0       0.54      0.64      0.58        33
         2.0       1.00      0.06      0.12        16

    accuracy                           0.64        86
   macro avg       0.75      0.53      0.50        86
weighted avg       0.70      0.64      0.59        86

[[33  4  0]
 [12 21  0]
 [ 1 14  1]]
0.5878376707926096
86 86 86



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.58
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.66
              precision    recall  f1-score   support

         0.0       0.82      0.84      0.83        37
         1.0       0.61      0.67      0.64        33
         2.0       0.67      0.50      0.57        16

    accuracy                           0.71        86
   macro avg       0.70      0.67      0.68        86
weighted avg       0.71      0.71      0.71        86

[[31  6  0]
 [ 7 22  4]
 [ 0  8  8]]
0.706662814772016
86 86 86
Filename	True Label	Prediction
0609	0.0	1.0
0620	0.0	1.0
0625	0.0	0.0
0637	1.0	1.0
0639	0.0	0.0
0640	1.0	1.0
0642	0.0	0.0
0643	1.0	1.0
0716	1.0	0.0
0810	0.0	0.0
0819	2.0	1.0
0823	1.0	0.0
0828	1.0	0.0
0907	1.0	1.0
0911	0.0	0.0
0917	0.0	0.0
0921	0.0	0.0
0926	1.0	0.0
1023	0.0	1.0
1117	0.0	0.0
LIB0611001A	0.0	0.0
LIB0611001B	0.0	0.0
LIB0611002A	0.0	0.0
LIB0611004A	0.0	0.0
LIB0611004B	0.0	0.0
LIB0611011	0.0	1.0
LON0610002B	0.0	0.0
MOS0611015	1.0	2.0
PHA0111002A	1.0	0.0
PHA0111004B	0.0	0.0
PHA0111005B	0.0	0.0
PHA0111011	1.0	1.0
PHA0111012	1.0	1.0
PHA0111014	0.0	1.0
PHA0112002B	0.0	0.0
PHA0112003B	0.0	0.0
PHA0112006A	1.0	0.0
PHA0209008	0.0	0.0
PHA0209024	1.0	1.0
PHA0209028	1.0	1.0
PHA0209031	2.0	1.0
PHA0209038	2.0	2.0
PHA0210004	0.0	0.0
PHA0210008	0.0	0.0
PHA0411008B	0.0	0.0
PHA0411010A	0.0	0.0
PHA0411027	1.0	1.0
PHA0411032	1.0	1.0
PHA0411037	1.0	1.0
PHA0411043	1.0	1.0
PHA0411054	2.0	1.0
PHA0411059	2.0	1.0
PHA0509018	2.0	1.0
PHA0509020	2.0	2.0
PHA0509026	2.0	2.0
PHA0509033	0.0	1.0
PHA0509037	2.0	1.0
PHA0509042	2.0	2.0
PHA0509044	1.0	1.0
PHA0510002B	0.0	0.0
PHA0510004B	0.0	0.0
PHA0510023	2.0	2.0
PHA0510027	1.0	1.0
PHA0510029	2.0	2.0
PHA0510038	2.0	2.0
PHA0510046	1.0	1.0
PHA0610007A	0.0	0.0
PHA0710010	1.0	2.0
PHA0710021	2.0	2.0
PHA0809009	1.0	1.0
PHA0810006	1.0	1.0
PHA0810011	1.0	1.0
PHA1109003	1.0	1.0
PHA1109028	1.0	2.0
PHA1110002B	1.0	0.0
PHA1110014	1.0	2.0
PHA1110016	1.0	1.0
PHA1110021	1.0	1.0
PHA1111002B	0.0	0.0
PHA1111003A	0.0	0.0
PHA1111004A	0.0	0.0
PHA1111006A	0.0	0.0
PHA1111008A	0.0	0.0
VAR0909008	1.0	1.0
VAR0909009	2.0	1.0
VAR0910009	2.0	1.0
Averaged weighted F1-scores 0.7336129174411186
144.90552995391704 65.35717405024758
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']
LANGUAGE: CZ, 0th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.26
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.71      0.46      0.56        37
         2.0       0.41      0.81      0.55        32
         3.0       0.00      0.00      0.00        16
         4.0       0.00      0.00      0.00         1

    accuracy                           0.49        87
   macro avg       0.22      0.25      0.22        87
weighted avg       0.45      0.49      0.44        87

[[ 0  1  0  0  0]
 [ 0 17 20  0  0]
 [ 0  6 26  0  0]
 [ 0  0 16  0  0]
 [ 0  0  1  0  0]]
0.4383763252109925
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.94
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.87
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.67      0.65      0.66        37
         2.0       0.57      0.53      0.55        32
         3.0       0.71      0.94      0.81        16
         4.0       0.00      0.00      0.00         1

    accuracy                           0.64        87
   macro avg       0.39      0.42      0.40        87
weighted avg       0.62      0.64      0.63        87

[[ 0  1  0  0  0]
 [ 0 24 12  1  0]
 [ 0 11 17  4  0]
 [ 0  0  1 15  0]
 [ 0  0  0  1  0]]
0.6304612321038487
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.79
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.87
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.61      0.73      0.67        37
         2.0       0.43      0.41      0.42        32
         3.0       0.62      0.50      0.55        16
         4.0       0.00      0.00      0.00         1

    accuracy                           0.55        87
   macro avg       0.33      0.33      0.33        87
weighted avg       0.53      0.55      0.54        87

[[ 0  1  0  0  0]
 [ 0 27 10  0  0]
 [ 0 15 13  4  0]
 [ 0  1  7  8  0]
 [ 0  0  0  1  0]]
0.5392368702560103
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.69
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.86
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.75      0.65      0.70        37
         2.0       0.60      0.66      0.63        32
         3.0       0.70      0.88      0.78        16
         4.0       0.00      0.00      0.00         1

    accuracy                           0.68        87
   macro avg       0.41      0.44      0.42        87
weighted avg       0.67      0.68      0.67        87

[[ 0  1  0  0  0]
 [ 0 24 12  1  0]
 [ 0  7 21  4  0]
 [ 0  0  2 14  0]
 [ 0  0  0  1  0]]
0.6694629468018892
87 87 87
Filename	True Label	Prediction
0603	2.0	2.0
0621	2.0	2.0
0622	1.0	1.0
0626	2.0	2.0
0639	1.0	2.0
0715	2.0	1.0
0721	2.0	2.0
0723	2.0	2.0
0806	1.0	2.0
0807	2.0	2.0
0813	1.0	2.0
0816	2.0	2.0
0817	1.0	2.0
0821	2.0	2.0
0904	1.0	1.0
0914	1.0	2.0
0915	2.0	2.0
0920	2.0	2.0
0921	1.0	1.0
0923	2.0	2.0
0928	1.0	2.0
0930	2.0	2.0
1005	1.0	1.0
1010	1.0	1.0
1111	1.0	1.0
BER0609003	2.0	2.0
KYJ0611006A	1.0	1.0
KYJ0611009A	1.0	1.0
LIB0611001A	1.0	1.0
LIB0611002B	2.0	1.0
LIB0611011	1.0	2.0
LON0610002A	2.0	1.0
LON0611004A	1.0	1.0
MOS0611012	2.0	2.0
PAR1011009A	2.0	2.0
PAR1011015	2.0	2.0
PHA0111001B	1.0	1.0
PHA0111003A	1.0	1.0
PHA0111003B	2.0	1.0
PHA0111005A	1.0	2.0
PHA0111010	3.0	3.0
PHA0111015	3.0	3.0
PHA0112006B	2.0	2.0
PHA0112009A	2.0	1.0
PHA0209008	1.0	1.0
PHA0209013	1.0	1.0
PHA0209024	1.0	1.0
PHA0209031	4.0	3.0
PHA0210004	1.0	1.0
PHA0411010A	0.0	1.0
PHA0411011B	1.0	1.0
PHA0411029	2.0	2.0
PHA0411030	3.0	3.0
PHA0411031	3.0	3.0
PHA0411032	1.0	3.0
PHA0411036	3.0	3.0
PHA0411037	2.0	3.0
PHA0411039	2.0	2.0
PHA0411045	3.0	2.0
PHA0509013	1.0	1.0
PHA0509028	2.0	2.0
PHA0509036	3.0	3.0
PHA0509038	1.0	1.0
PHA0509041	2.0	3.0
PHA0509042	3.0	3.0
PHA0510029	2.0	3.0
PHA0510034	3.0	3.0
PHA0510037	1.0	2.0
PHA0610006B	1.0	1.0
PHA0610019A	2.0	2.0
PHA0610026	3.0	3.0
PHA0710012	3.0	3.0
PHA0810004	1.0	1.0
PHA0810008	3.0	3.0
PHA0811012	3.0	3.0
PHA0811014	1.0	2.0
PHA0811016	1.0	2.0
PHA0811017	3.0	3.0
PHA1109002	3.0	2.0
PHA1109007	1.0	1.0
PHA1110013	2.0	3.0
PHA1111001A	1.0	1.0
PHA1111002A	2.0	1.0
PHA1111004A	1.0	1.0
PHA1111006A	1.0	2.0
VAR0909008	2.0	1.0
VAR0910009	3.0	3.0
LANGUAGE: CZ, 1th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.23
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.14
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.53      0.73      0.61        37
         2.0       0.00      0.00      0.00        31
         3.0       0.39      0.82      0.53        17
         4.0       0.00      0.00      0.00         1

    accuracy                           0.47        87
   macro avg       0.18      0.31      0.23        87
weighted avg       0.30      0.47      0.36        87

[[ 0  1  0  0  0]
 [ 0 27  0 10  0]
 [ 0 20  0 11  0]
 [ 0  3  0 14  0]
 [ 0  0  0  1  0]]
0.36420319000019713
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.95
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.93
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.61      0.84      0.70        37
         2.0       0.47      0.29      0.36        31
         3.0       0.65      0.65      0.65        17
         4.0       0.00      0.00      0.00         1

    accuracy                           0.59        87
   macro avg       0.35      0.36      0.34        87
weighted avg       0.55      0.59      0.55        87

[[ 0  1  0  0  0]
 [ 0 31  6  0  0]
 [ 0 17  9  5  0]
 [ 0  2  4 11  0]
 [ 0  0  0  1  0]]
0.5543469174503657
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.85
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.88
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.67      0.84      0.75        37
         2.0       0.48      0.32      0.38        31
         3.0       0.60      0.71      0.65        17
         4.0       0.00      0.00      0.00         1

    accuracy                           0.61        87
   macro avg       0.35      0.37      0.36        87
weighted avg       0.57      0.61      0.58        87

[[ 0  1  0  0  0]
 [ 0 31  6  0  0]
 [ 0 14 10  7  0]
 [ 0  0  5 12  0]
 [ 0  0  0  1  0]]
0.5814788295054186
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.78
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.89
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.77      0.54      0.63        37
         2.0       0.48      0.52      0.50        31
         3.0       0.54      0.88      0.67        17
         4.0       0.00      0.00      0.00         1

    accuracy                           0.59        87
   macro avg       0.36      0.39      0.36        87
weighted avg       0.60      0.59      0.58        87

[[ 0  1  0  0  0]
 [ 0 20 15  2  0]
 [ 0  5 16 10  0]
 [ 0  0  2 15  0]
 [ 0  0  0  1  0]]
0.5784528370735268
87 87 87
Filename	True Label	Prediction
0604	2.0	2.0
0607	2.0	2.0
0617	1.0	2.0
0627	2.0	2.0
0628	2.0	2.0
0631	2.0	2.0
0635	1.0	1.0
0641	1.0	2.0
0717	1.0	2.0
0718	1.0	2.0
0719	2.0	2.0
0803	2.0	2.0
0804	1.0	2.0
0808	1.0	2.0
0810	2.0	1.0
0812	2.0	1.0
0814	1.0	1.0
0815	2.0	2.0
0826	1.0	2.0
0829	1.0	1.0
0903	1.0	1.0
0907	2.0	2.0
0913	2.0	2.0
0922	1.0	2.0
1007	2.0	2.0
1009	2.0	1.0
1018	1.0	2.0
1019	1.0	2.0
1113	1.0	1.0
1114	2.0	2.0
1115	1.0	1.0
BER0611007	2.0	2.0
KYJ0611006B	1.0	1.0
KYJ0611009B	1.0	1.0
LIB0611004B	1.0	1.0
MOS0509004	1.0	1.0
MOS0611014	1.0	2.0
MOS0611015	2.0	3.0
PAR1011009B	1.0	1.0
PAR1011013	2.0	3.0
PAR1011014	2.0	3.0
PAR1011016	3.0	3.0
PAR1011018	3.0	3.0
PHA0111001A	1.0	1.0
PHA0111011	2.0	2.0
PHA0111018	1.0	3.0
PHA0112007B	1.0	1.0
PHA0411009A	2.0	1.0
PHA0411012A	1.0	1.0
PHA0411035	3.0	2.0
PHA0411061	3.0	3.0
PHA0411062	3.0	3.0
PHA0509017	2.0	3.0
PHA0509018	3.0	3.0
PHA0509024	2.0	3.0
PHA0509025	3.0	3.0
PHA0509031	1.0	2.0
PHA0509033	1.0	1.0
PHA0509034	1.0	3.0
PHA0509045	1.0	2.0
PHA0510010B	0.0	1.0
PHA0510023	3.0	3.0
PHA0510032	3.0	3.0
PHA0510039	3.0	3.0
PHA0510040	2.0	3.0
PHA0510050	2.0	3.0
PHA0610006A	1.0	1.0
PHA0610025	2.0	3.0
PHA0709008	3.0	3.0
PHA0710009	3.0	2.0
PHA0710013	4.0	3.0
PHA0710018	3.0	3.0
PHA0810003	3.0	3.0
PHA0810010	2.0	3.0
PHA0810015	3.0	3.0
PHA0811010	2.0	2.0
PHA0811019	3.0	3.0
PHA1109004	3.0	3.0
PHA1109006	2.0	2.0
PHA1109008	1.0	1.0
PHA1109025	1.0	1.0
PHA1110002B	2.0	1.0
PHA1110003B	1.0	1.0
PHA1110019	2.0	3.0
TI071122B	1.0	1.0
VAR0909005	1.0	2.0
VAR0909010	1.0	2.0
LANGUAGE: CZ, 2th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.24
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.04
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.76      0.68      0.71        37
         2.0       0.52      0.35      0.42        31
         3.0       0.42      0.82      0.56        17
         4.0       0.00      0.00      0.00         1

    accuracy                           0.57        87
   macro avg       0.34      0.37      0.34        87
weighted avg       0.59      0.57      0.56        87

[[ 0  0  1  0  0]
 [ 0 25  6  6  0]
 [ 0  8 11 12  0]
 [ 0  0  3 14  0]
 [ 0  0  0  1  0]]
0.5639535177466212
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.98
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.05
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.81      0.35      0.49        37
         2.0       0.42      0.52      0.46        31
         3.0       0.45      0.88      0.60        17
         4.0       0.00      0.00      0.00         1

    accuracy                           0.51        87
   macro avg       0.34      0.35      0.31        87
weighted avg       0.58      0.51      0.49        87

[[ 0  0  1  0  0]
 [ 0 13 19  5  0]
 [ 0  3 16 12  0]
 [ 0  0  2 15  0]
 [ 0  0  0  1  0]]
0.4911236205796473
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.88
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.97
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.78      0.57      0.66        37
         2.0       0.47      0.55      0.51        31
         3.0       0.50      0.71      0.59        17
         4.0       0.00      0.00      0.00         1

    accuracy                           0.57        87
   macro avg       0.35      0.36      0.35        87
weighted avg       0.60      0.57      0.57        87

[[ 0  0  1  0  0]
 [ 0 21 14  2  0]
 [ 0  5 17  9  0]
 [ 0  1  4 12  0]
 [ 0  0  0  1  0]]
0.5742966988020369
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.78
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.94
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.69      0.73      0.71        37
         2.0       0.52      0.45      0.48        31
         3.0       0.52      0.65      0.58        17
         4.0       0.00      0.00      0.00         1

    accuracy                           0.60        87
   macro avg       0.35      0.37      0.35        87
weighted avg       0.58      0.60      0.59        87

[[ 0  1  0  0  0]
 [ 0 27  8  2  0]
 [ 0 10 14  7  0]
 [ 0  1  5 11  0]
 [ 0  0  0  1  0]]
0.5873229446982499
87 87 87
Filename	True Label	Prediction
0616	1.0	1.0
0618	1.0	1.0
0619	2.0	1.0
0624	2.0	1.0
0630	1.0	1.0
0633	2.0	1.0
0634	2.0	2.0
0636	2.0	2.0
0643	2.0	2.0
0645	2.0	2.0
0724	2.0	2.0
0818	1.0	1.0
0819	3.0	2.0
0822	1.0	2.0
0823	2.0	2.0
0824	2.0	1.0
0827	1.0	1.0
0901	2.0	2.0
0906	2.0	2.0
0912	2.0	2.0
0917	1.0	1.0
0918	1.0	2.0
0919	1.0	1.0
0925	2.0	1.0
0929	0.0	1.0
1003	1.0	1.0
1016	1.0	1.0
1021	1.0	2.0
1022	2.0	1.0
1023	1.0	2.0
1112	1.0	2.0
9999	1.0	1.0
LIB0611001B	1.0	1.0
LIB0611002A	1.0	1.0
MOS0611013	2.0	3.0
PHA0111002A	2.0	1.0
PHA0111002B	3.0	1.0
PHA0111004A	1.0	1.0
PHA0111004B	1.0	1.0
PHA0111014	1.0	2.0
PHA0112002A	2.0	1.0
PHA0112003B	1.0	1.0
PHA0112012B	1.0	1.0
PHA0209001	1.0	1.0
PHA0209026	2.0	3.0
PHA0209039	2.0	3.0
PHA0411011A	1.0	1.0
PHA0411012B	1.0	1.0
PHA0411027	2.0	2.0
PHA0411043	1.0	2.0
PHA0411044	3.0	3.0
PHA0411053	3.0	3.0
PHA0411060	2.0	3.0
PHA0509007	1.0	1.0
PHA0509015	3.0	2.0
PHA0509027	1.0	3.0
PHA0509040	2.0	2.0
PHA0510002B	2.0	1.0
PHA0510003A	1.0	1.0
PHA0510003B	1.0	1.0
PHA0510013B	1.0	1.0
PHA0510030	2.0	3.0
PHA0510031	2.0	3.0
PHA0510047	1.0	2.0
PHA0510048	1.0	3.0
PHA0610007A	1.0	1.0
PHA0610015	2.0	3.0
PHA0710011	3.0	2.0
PHA0710014	3.0	3.0
PHA0710019	3.0	3.0
PHA0710021	3.0	3.0
PHA0809009	2.0	2.0
PHA1109001	1.0	1.0
PHA1109023	1.0	1.0
PHA1109024	4.0	3.0
PHA1109026	3.0	3.0
PHA1109027	3.0	3.0
PHA1110003A	1.0	1.0
PHA1110015	3.0	3.0
PHA1110021	2.0	2.0
PHA1111003A	2.0	1.0
VAR0909007	2.0	2.0
VAR0909009	3.0	2.0
VAR0910004	3.0	3.0
VAR0910005	3.0	2.0
VAR0910006	3.0	3.0
VAR0910011	3.0	3.0
LANGUAGE: CZ, 3th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.18
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.05
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.56      0.97      0.71        37
         2.0       0.39      0.29      0.33        31
         3.0       0.00      0.00      0.00        16
         4.0       0.00      0.00      0.00         1

    accuracy                           0.52        87
   macro avg       0.19      0.25      0.21        87
weighted avg       0.38      0.52      0.42        87

[[ 0  2  0  0  0]
 [ 0 36  1  0  0]
 [ 0 22  9  0  0]
 [ 0  4 12  0  0]
 [ 0  0  1  0  0]]
0.4219490914608703
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.97
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.98
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.74      0.54      0.62        37
         2.0       0.44      0.45      0.44        31
         3.0       0.46      0.81      0.59        16
         4.0       0.00      0.00      0.00         1

    accuracy                           0.54        87
   macro avg       0.33      0.36      0.33        87
weighted avg       0.56      0.54      0.53        87

[[ 0  2  0  0  0]
 [ 0 20 15  2  0]
 [ 0  5 14 12  0]
 [ 0  0  3 13  0]
 [ 0  0  0  1  0]]
0.5328427957738302
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.86
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.97
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.81      0.46      0.59        37
         2.0       0.46      0.58      0.51        31
         3.0       0.48      0.81      0.60        16
         4.0       0.00      0.00      0.00         1

    accuracy                           0.55        87
   macro avg       0.35      0.37      0.34        87
weighted avg       0.60      0.55      0.54        87

[[ 0  2  0  0  0]
 [ 0 17 18  2  0]
 [ 0  2 18 11  0]
 [ 0  0  3 13  0]
 [ 0  0  0  1  0]]
0.5437578266657712
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.78
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.87
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.67      0.86      0.75        37
         2.0       0.61      0.35      0.45        31
         3.0       0.57      0.75      0.65        16
         4.0       0.00      0.00      0.00         1

    accuracy                           0.63        87
   macro avg       0.37      0.39      0.37        87
weighted avg       0.61      0.63      0.60        87

[[ 0  2  0  0  0]
 [ 0 32  5  0  0]
 [ 0 12 11  8  0]
 [ 0  2  2 12  0]
 [ 0  0  0  1  0]]
0.5994893017784934
87 87 87
Filename	True Label	Prediction
0602	2.0	2.0
0606	1.0	1.0
0608	1.0	1.0
0609	1.0	2.0
0610	2.0	2.0
0614	1.0	1.0
0620	1.0	2.0
0625	2.0	1.0
0629	2.0	2.0
0637	2.0	2.0
0638	2.0	1.0
0640	2.0	2.0
0642	1.0	1.0
0714	2.0	1.0
0801	1.0	1.0
0805	2.0	1.0
0809	2.0	1.0
0811	2.0	2.0
0820	1.0	1.0
0825	1.0	1.0
0911	1.0	1.0
0916	1.0	1.0
0927	1.0	1.0
1001	1.0	1.0
1002	2.0	1.0
1004	1.0	1.0
1006	2.0	1.0
1014	2.0	1.0
1015	1.0	2.0
1020	2.0	1.0
BER0611003	2.0	3.0
KYJ0611003A	1.0	1.0
KYJ0611005A	1.0	1.0
KYJ0611005B	1.0	1.0
LON0611004B	1.0	1.0
MOS0509001	1.0	2.0
PAR1011008A	1.0	1.0
PHA0111005B	2.0	1.0
PHA0111016	3.0	3.0
PHA0112006A	3.0	1.0
PHA0112012A	1.0	1.0
PHA0209028	2.0	2.0
PHA0209034	2.0	3.0
PHA0209038	4.0	3.0
PHA0210007	1.0	1.0
PHA0411008A	1.0	1.0
PHA0411009B	1.0	1.0
PHA0411010B	0.0	1.0
PHA0411033	2.0	2.0
PHA0411038	3.0	3.0
PHA0411041	3.0	3.0
PHA0411042	2.0	3.0
PHA0411051	3.0	3.0
PHA0411054	3.0	1.0
PHA0411055	3.0	3.0
PHA0411056	3.0	3.0
PHA0509019	3.0	2.0
PHA0509030	2.0	3.0
PHA0509035	2.0	2.0
PHA0509044	2.0	3.0
PHA0510002A	2.0	1.0
PHA0510004A	1.0	1.0
PHA0510004B	0.0	1.0
PHA0510038	3.0	3.0
PHA0610007B	1.0	1.0
PHA0710015	2.0	3.0
PHA0710017	3.0	3.0
PHA0810001	3.0	3.0
PHA0810002	1.0	1.0
PHA0810006	2.0	2.0
PHA0810009	3.0	3.0
PHA0810011	2.0	3.0
PHA0810012	3.0	2.0
PHA0811013	3.0	3.0
PHA1109003	1.0	1.0
PHA1109005	1.0	1.0
PHA1110001B	1.0	1.0
PHA1110002A	2.0	1.0
PHA1110004A	1.0	1.0
PHA1110014	2.0	3.0
PHA1110016	1.0	1.0
PHA1110017	1.0	2.0
PHA1110022	3.0	3.0
PHA1111001B	1.0	1.0
PHA1111008B	1.0	1.0
ST071122B	1.0	1.0
VAR0909004	2.0	2.0
LANGUAGE: CZ, 4th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.20
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.96
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.58      0.97      0.73        37
         2.0       0.45      0.16      0.24        31
         3.0       0.62      0.50      0.55        16
         4.0       0.00      0.00      0.00         1

    accuracy                           0.57        86
   macro avg       0.33      0.33      0.30        86
weighted avg       0.53      0.57      0.50        86

[[ 0  1  0  0  0]
 [ 0 36  1  0  0]
 [ 0 22  5  4  0]
 [ 0  3  5  8  0]
 [ 0  0  0  1  0]]
0.5013677848481377
86 86 86



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.99
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.86
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.62      0.89      0.73        37
         2.0       0.33      0.13      0.19        31
         3.0       0.57      0.75      0.65        16
         4.0       0.00      0.00      0.00         1

    accuracy                           0.57        86
   macro avg       0.31      0.35      0.31        86
weighted avg       0.49      0.57      0.50        86

[[ 0  1  0  0  0]
 [ 0 33  4  0  0]
 [ 0 19  4  8  0]
 [ 0  0  4 12  0]
 [ 0  0  0  1  0]]
0.503245971769498
86 86 86



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.85
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.87
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.77      0.54      0.63        37
         2.0       0.45      0.58      0.51        31
         3.0       0.55      0.69      0.61        16
         4.0       0.00      0.00      0.00         1

    accuracy                           0.57        86
   macro avg       0.35      0.36      0.35        86
weighted avg       0.60      0.57      0.57        86

[[ 0  1  0  0  0]
 [ 0 20 17  0  0]
 [ 0  5 18  8  0]
 [ 0  0  5 11  0]
 [ 0  0  0  1  0]]
0.5696296642906535
86 86 86



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.78
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.84
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.69      0.78      0.73        37
         2.0       0.50      0.35      0.42        31
         3.0       0.59      0.81      0.68        16
         4.0       0.00      0.00      0.00         1

    accuracy                           0.62        86
   macro avg       0.36      0.39      0.37        86
weighted avg       0.59      0.62      0.59        86

[[ 0  1  0  0  0]
 [ 0 29  8  0  0]
 [ 0 12 11  8  0]
 [ 0  0  3 13  0]
 [ 0  0  0  1  0]]
0.5927889524579052
86 86 86
Filename	True Label	Prediction
0601	1.0	1.0
0605	2.0	2.0
0611	2.0	1.0
0612	1.0	1.0
0613	1.0	1.0
0615	1.0	1.0
0623	2.0	2.0
0632	1.0	1.0
0644	1.0	2.0
0716	2.0	2.0
0720	1.0	2.0
0722	2.0	2.0
0725	1.0	2.0
0802	2.0	1.0
0828	2.0	1.0
0902	2.0	1.0
0905	2.0	2.0
0910	1.0	1.0
0924	1.0	1.0
0926	2.0	2.0
1008	2.0	1.0
1017	1.0	1.0
1116	1.0	2.0
1117	2.0	1.0
BER0611005	2.0	2.0
BER0611006	2.0	3.0
KYJ0611004A	1.0	1.0
LIB0611003A	1.0	1.0
LIB0611004A	1.0	1.0
LON0610002B	1.0	1.0
LON0611002A	1.0	1.0
LON0611002B	1.0	1.0
LON0611003	2.0	3.0
PAR1011017	3.0	3.0
PHA0111012	1.0	2.0
PHA0112002B	1.0	1.0
PHA0112003A	1.0	1.0
PHA0112007A	1.0	1.0
PHA0112009B	2.0	1.0
PHA0210001	1.0	1.0
PHA0210008	1.0	1.0
PHA0411008B	1.0	1.0
PHA0411028	2.0	1.0
PHA0411034	1.0	1.0
PHA0411047	2.0	3.0
PHA0411058	3.0	3.0
PHA0411059	3.0	3.0
PHA0509002	1.0	1.0
PHA0509020	3.0	3.0
PHA0509021	1.0	2.0
PHA0509022	4.0	3.0
PHA0509026	3.0	3.0
PHA0509032	2.0	3.0
PHA0509037	3.0	2.0
PHA0509039	3.0	2.0
PHA0509043	2.0	2.0
PHA0510010A	1.0	1.0
PHA0510013A	2.0	1.0
PHA0510027	1.0	2.0
PHA0510035	3.0	3.0
PHA0510036	3.0	3.0
PHA0510046	2.0	2.0
PHA0510049	3.0	3.0
PHA0610005A	1.0	1.0
PHA0610005B	0.0	1.0
PHA0610016	2.0	3.0
PHA0610017	3.0	3.0
PHA0610018	2.0	3.0
PHA0610019B	2.0	1.0
PHA0710010	2.0	3.0
PHA0710016	3.0	3.0
PHA0809010	2.0	2.0
PHA0811020	1.0	2.0
PHA1109028	3.0	3.0
PHA1110001A	2.0	1.0
PHA1111002B	1.0	1.0
PHA1111003B	1.0	1.0
PHA1111004B	1.0	1.0
PHA1111006B	1.0	1.0
PHA1111008A	2.0	1.0
PHA1111009A	1.0	1.0
VAR0209036	3.0	2.0
VAR0909003	2.0	2.0
VAR0909006	3.0	3.0
VAR0910007	2.0	3.0
VAR0910010	3.0	3.0
Averaged weighted F1-scores 0.6055033965620129
144.90552995391704 65.35717405024758
LABEL SET ['A2', 'B1', 'B2', 'C1']
LANGUAGE: CZ, 0th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.01
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.84
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         9
         1.0       0.62      1.00      0.77        54
         2.0       0.00      0.00      0.00        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.62        87
   macro avg       0.16      0.25      0.19        87
weighted avg       0.39      0.62      0.48        87

[[ 0  9  0  0]
 [ 0 54  0  0]
 [ 0 23  0  0]
 [ 0  1  0  0]]
0.47542186353631705
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.82
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.73
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         9
         1.0       0.74      0.96      0.84        54
         2.0       0.82      0.61      0.70        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.76        87
   macro avg       0.39      0.39      0.38        87
weighted avg       0.68      0.76      0.71        87

[[ 0  9  0  0]
 [ 0 52  2  0]
 [ 0  9 14  0]
 [ 0  0  1  0]]
0.7056358917315536
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.71
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.71
              precision    recall  f1-score   support

         0.0       1.00      0.11      0.20         9
         1.0       0.73      0.96      0.83        54
         2.0       0.80      0.52      0.63        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.75        87
   macro avg       0.63      0.40      0.42        87
weighted avg       0.77      0.75      0.70        87

[[ 1  8  0  0]
 [ 0 52  2  0]
 [ 0 11 12  0]
 [ 0  0  1  0]]
0.7040725952813066
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.60
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.69
              precision    recall  f1-score   support

         0.0       0.50      0.11      0.18         9
         1.0       0.75      0.89      0.81        54
         2.0       0.71      0.65      0.68        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.74        87
   macro avg       0.49      0.41      0.42        87
weighted avg       0.71      0.74      0.70        87

[[ 1  8  0  0]
 [ 1 48  5  0]
 [ 0  8 15  0]
 [ 0  0  1  0]]
0.7040274161840496
87 87 87
Filename	True Label	Prediction
0601	1.0	1.0
0609	1.0	1.0
0625	1.0	1.0
0626	2.0	1.0
0628	1.0	1.0
0632	1.0	1.0
0635	1.0	1.0
0636	2.0	1.0
0714	1.0	1.0
0721	2.0	1.0
0722	1.0	1.0
0725	1.0	1.0
0805	1.0	1.0
0808	1.0	1.0
0825	1.0	1.0
0827	1.0	1.0
0903	1.0	1.0
0905	1.0	1.0
0906	2.0	1.0
0907	1.0	1.0
0912	2.0	1.0
0923	1.0	1.0
1004	1.0	1.0
1008	0.0	1.0
1016	1.0	1.0
1017	1.0	1.0
1019	1.0	1.0
1023	1.0	1.0
1112	1.0	1.0
9999	0.0	1.0
KYJ0611005B	0.0	1.0
KYJ0611006B	0.0	1.0
KYJ0611009A	1.0	1.0
LIB0611002B	1.0	1.0
LIB0611004A	2.0	1.0
LON0611003	2.0	2.0
PAR1011008A	1.0	1.0
PAR1011014	1.0	2.0
PAR1011017	2.0	2.0
PHA0111003B	1.0	1.0
PHA0111005A	2.0	1.0
PHA0111010	2.0	2.0
PHA0111014	0.0	1.0
PHA0111018	1.0	2.0
PHA0112002B	1.0	1.0
PHA0209001	1.0	1.0
PHA0210001	1.0	1.0
PHA0411010B	1.0	0.0
PHA0411028	1.0	1.0
PHA0411041	1.0	2.0
PHA0411044	3.0	2.0
PHA0411053	2.0	2.0
PHA0411058	2.0	2.0
PHA0509013	0.0	1.0
PHA0509020	2.0	2.0
PHA0509026	2.0	2.0
PHA0509027	1.0	1.0
PHA0509031	1.0	2.0
PHA0509033	1.0	1.0
PHA0509039	2.0	2.0
PHA0509042	2.0	2.0
PHA0510002A	1.0	1.0
PHA0510010B	0.0	0.0
PHA0510013B	1.0	1.0
PHA0510031	1.0	1.0
PHA0510032	2.0	2.0
PHA0510035	2.0	2.0
PHA0610006B	0.0	1.0
PHA0610007A	1.0	1.0
PHA0610007B	1.0	1.0
PHA0610018	2.0	2.0
PHA0610019A	0.0	1.0
PHA0610019B	1.0	1.0
PHA0710013	2.0	2.0
PHA0710018	2.0	2.0
PHA0710021	2.0	2.0
PHA0810003	1.0	2.0
PHA0811010	1.0	1.0
PHA1109005	1.0	1.0
PHA1109007	1.0	1.0
PHA1109023	1.0	1.0
PHA1110003B	1.0	1.0
PHA1110004A	1.0	1.0
PHA1110021	1.0	1.0
VAR0209036	2.0	1.0
VAR0909008	1.0	1.0
VAR0910010	1.0	1.0
LANGUAGE: CZ, 1th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.06
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.95
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         9
         1.0       0.62      1.00      0.77        54
         2.0       0.00      0.00      0.00        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.62        87
   macro avg       0.16      0.25      0.19        87
weighted avg       0.39      0.62      0.48        87

[[ 0  9  0  0]
 [ 0 54  0  0]
 [ 0 23  0  0]
 [ 0  1  0  0]]
0.47542186353631705
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.88
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.96
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         9
         1.0       0.67      0.87      0.76        54
         2.0       0.47      0.35      0.40        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.63        87
   macro avg       0.29      0.30      0.29        87
weighted avg       0.54      0.63      0.58        87

[[ 0  8  1  0]
 [ 0 47  7  0]
 [ 0 15  8  0]
 [ 0  0  1  0]]
0.5762699295513534
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.77
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.90
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         9
         1.0       0.65      0.93      0.76        54
         2.0       0.40      0.17      0.24        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.62        87
   macro avg       0.26      0.27      0.25        87
weighted avg       0.51      0.62      0.54        87

[[ 0  8  1  0]
 [ 0 50  4  0]
 [ 0 19  4  0]
 [ 0  0  1  0]]
0.5378980646156218
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.64
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.92
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         9
         1.0       0.65      0.87      0.75        54
         2.0       0.40      0.26      0.32        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.61        87
   macro avg       0.26      0.28      0.27        87
weighted avg       0.51      0.61      0.55        87

[[ 0  8  1  0]
 [ 0 47  7  0]
 [ 0 17  6  0]
 [ 0  0  1  0]]
0.5465387606948405
87 87 87
Filename	True Label	Prediction
0602	1.0	1.0
0613	1.0	1.0
0620	1.0	1.0
0630	1.0	1.0
0638	1.0	1.0
0643	1.0	1.0
0715	1.0	1.0
0717	1.0	1.0
0719	1.0	1.0
0720	1.0	1.0
0723	1.0	1.0
0724	2.0	1.0
0803	1.0	1.0
0804	1.0	1.0
0809	1.0	1.0
0815	2.0	1.0
0822	1.0	1.0
0824	1.0	1.0
0910	1.0	1.0
0915	1.0	2.0
0916	1.0	1.0
0917	1.0	1.0
0919	1.0	1.0
0920	2.0	1.0
0925	1.0	1.0
0927	2.0	1.0
0928	2.0	1.0
1006	1.0	1.0
1014	2.0	1.0
1018	1.0	1.0
BER0611003	2.0	1.0
BER0611005	2.0	1.0
KYJ0611005A	0.0	1.0
LIB0611003A	0.0	1.0
LON0610002A	1.0	1.0
LON0611004A	1.0	1.0
MOS0611014	0.0	2.0
PHA0111002A	2.0	1.0
PHA0111004A	2.0	1.0
PHA0111012	1.0	2.0
PHA0112002A	1.0	1.0
PHA0112003A	1.0	1.0
PHA0112003B	0.0	1.0
PHA0112006B	2.0	1.0
PHA0209013	1.0	1.0
PHA0209024	0.0	1.0
PHA0209026	2.0	1.0
PHA0209039	1.0	2.0
PHA0411035	2.0	1.0
PHA0411036	1.0	1.0
PHA0411037	1.0	1.0
PHA0411038	2.0	2.0
PHA0411043	1.0	1.0
PHA0411047	1.0	2.0
PHA0411051	2.0	2.0
PHA0509007	1.0	1.0
PHA0509015	1.0	1.0
PHA0509019	1.0	1.0
PHA0509022	2.0	2.0
PHA0509024	1.0	1.0
PHA0509041	2.0	1.0
PHA0510004A	0.0	1.0
PHA0510013A	1.0	1.0
PHA0510036	2.0	2.0
PHA0510038	2.0	2.0
PHA0510039	1.0	2.0
PHA0510047	1.0	1.0
PHA0610005A	1.0	1.0
PHA0610016	2.0	1.0
PHA0610026	1.0	1.0
PHA0709008	1.0	1.0
PHA0809010	1.0	1.0
PHA0810004	1.0	1.0
PHA0810006	1.0	1.0
PHA0810011	1.0	2.0
PHA0811012	3.0	2.0
PHA0811016	1.0	1.0
PHA0811017	2.0	1.0
PHA1109027	1.0	1.0
PHA1110013	2.0	2.0
PHA1110014	1.0	2.0
PHA1110016	0.0	1.0
PHA1111003A	2.0	1.0
PHA1111004A	0.0	1.0
PHA1111008A	1.0	1.0
PHA1111008B	1.0	1.0
VAR0910005	0.0	1.0
LANGUAGE: CZ, 2th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.98
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.96
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         9
         1.0       0.72      0.61      0.66        54
         2.0       0.44      0.78      0.56        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.59        87
   macro avg       0.29      0.35      0.31        87
weighted avg       0.56      0.59      0.56        87

[[ 0  8  1  0]
 [ 0 33 21  0]
 [ 0  5 18  0]
 [ 0  0  1  0]]
0.5583620689655172
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.84
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         9
         1.0       0.68      0.85      0.75        54
         2.0       0.53      0.43      0.48        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.64        87
   macro avg       0.30      0.32      0.31        87
weighted avg       0.56      0.64      0.59        87

[[ 0  9  0  0]
 [ 0 46  8  0]
 [ 0 13 10  0]
 [ 0  0  1  0]]
0.5939504876757563
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.69
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.85
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         9
         1.0       0.72      0.67      0.69        54
         2.0       0.49      0.78      0.60        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.62        87
   macro avg       0.30      0.36      0.32        87
weighted avg       0.58      0.62      0.59        87

[[ 0  9  0  0]
 [ 0 36 18  0]
 [ 0  5 18  0]
 [ 0  0  1  0]]
0.5883289124668436
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.55
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.89
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         9
         1.0       0.72      0.67      0.69        54
         2.0       0.49      0.78      0.60        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.62        87
   macro avg       0.30      0.36      0.32        87
weighted avg       0.58      0.62      0.59        87

[[ 0  9  0  0]
 [ 0 36 18  0]
 [ 0  5 18  0]
 [ 0  0  1  0]]
0.5883289124668436
87 87 87
Filename	True Label	Prediction
0606	1.0	1.0
0616	1.0	1.0
0621	1.0	1.0
0623	1.0	1.0
0624	2.0	1.0
0631	1.0	2.0
0637	1.0	2.0
0641	0.0	1.0
0644	1.0	1.0
0801	1.0	1.0
0807	2.0	1.0
0819	2.0	2.0
0826	1.0	1.0
0828	1.0	1.0
0829	1.0	1.0
0904	1.0	1.0
0924	1.0	1.0
0926	2.0	1.0
0929	0.0	1.0
1007	1.0	1.0
1010	1.0	1.0
1015	1.0	1.0
1022	1.0	1.0
1111	1.0	1.0
1113	1.0	1.0
BER0609003	1.0	1.0
LIB0611011	0.0	1.0
LON0611002B	0.0	1.0
MOS0509001	1.0	2.0
PAR1011009A	1.0	1.0
PHA0111001A	1.0	1.0
PHA0111004B	1.0	1.0
PHA0111015	2.0	2.0
PHA0111016	2.0	2.0
PHA0112007A	1.0	1.0
PHA0112007B	0.0	1.0
PHA0112009B	1.0	1.0
PHA0112012A	1.0	1.0
PHA0209008	1.0	1.0
PHA0209028	2.0	2.0
PHA0209034	1.0	2.0
PHA0411009B	0.0	1.0
PHA0411010A	1.0	1.0
PHA0411011A	2.0	1.0
PHA0411027	1.0	2.0
PHA0411030	3.0	2.0
PHA0411031	2.0	2.0
PHA0411032	1.0	2.0
PHA0411033	1.0	1.0
PHA0411039	1.0	2.0
PHA0411056	2.0	2.0
PHA0411060	2.0	2.0
PHA0411062	1.0	2.0
PHA0509018	2.0	2.0
PHA0509021	2.0	1.0
PHA0509030	2.0	2.0
PHA0509037	2.0	2.0
PHA0509040	1.0	2.0
PHA0509044	1.0	2.0
PHA0509045	1.0	1.0
PHA0510002B	1.0	1.0
PHA0510004B	0.0	1.0
PHA0510034	2.0	2.0
PHA0510040	2.0	2.0
PHA0610005B	0.0	1.0
PHA0610017	2.0	2.0
PHA0710009	1.0	1.0
PHA0710012	2.0	2.0
PHA0710014	2.0	2.0
PHA0710017	1.0	2.0
PHA0810002	1.0	2.0
PHA0810008	1.0	2.0
PHA0811013	2.0	2.0
PHA0811020	1.0	2.0
PHA1109002	2.0	2.0
PHA1109004	1.0	2.0
PHA1110001A	1.0	1.0
PHA1110001B	1.0	1.0
PHA1110022	2.0	2.0
PHA1111003B	1.0	1.0
PHA1111006B	0.0	1.0
TI071122B	1.0	1.0
VAR0909003	1.0	1.0
VAR0909005	1.0	1.0
VAR0909009	1.0	2.0
VAR0910006	1.0	2.0
VAR0910007	1.0	2.0
LANGUAGE: CZ, 3th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.02
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.86
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.62      1.00      0.77        54
         2.0       0.00      0.00      0.00        22
         3.0       0.00      0.00      0.00         1

    accuracy                           0.62        87
   macro avg       0.16      0.25      0.19        87
weighted avg       0.39      0.62      0.48        87

[[ 0 10  0  0]
 [ 0 54  0  0]
 [ 0 22  0  0]
 [ 0  1  0  0]]
0.47542186353631705
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.85
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.74
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.71      0.89      0.79        54
         2.0       0.63      0.55      0.59        22
         3.0       0.00      0.00      0.00         1

    accuracy                           0.69        87
   macro avg       0.33      0.36      0.34        87
weighted avg       0.60      0.69      0.64        87

[[ 0 10  0  0]
 [ 0 48  6  0]
 [ 0 10 12  0]
 [ 0  0  1  0]]
0.6364350811399577
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.74
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.75
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.68      0.96      0.80        54
         2.0       0.73      0.36      0.48        22
         3.0       0.00      0.00      0.00         1

    accuracy                           0.69        87
   macro avg       0.35      0.33      0.32        87
weighted avg       0.61      0.69      0.62        87

[[ 0 10  0  0]
 [ 0 52  2  0]
 [ 0 14  8  0]
 [ 0  0  1  0]]
0.6191570881226053
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.63
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.77
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        10
         1.0       0.69      0.76      0.73        54
         2.0       0.50      0.64      0.56        22
         3.0       0.00      0.00      0.00         1

    accuracy                           0.63        87
   macro avg       0.30      0.35      0.32        87
weighted avg       0.56      0.63      0.59        87

[[ 0 10  0  0]
 [ 0 41 13  0]
 [ 0  8 14  0]
 [ 0  0  1  0]]
0.5920211575628115
87 87 87
Filename	True Label	Prediction
0603	1.0	2.0
0604	1.0	1.0
0605	1.0	2.0
0608	0.0	1.0
0610	1.0	2.0
0611	1.0	1.0
0617	1.0	1.0
0618	1.0	1.0
0619	1.0	1.0
0622	1.0	1.0
0629	1.0	1.0
0642	1.0	1.0
0718	1.0	1.0
0817	1.0	1.0
0820	0.0	1.0
0821	1.0	1.0
0823	2.0	2.0
0902	2.0	2.0
0911	1.0	2.0
0914	1.0	1.0
0918	1.0	2.0
0922	1.0	1.0
0930	1.0	1.0
1002	1.0	1.0
1003	0.0	1.0
1005	1.0	1.0
1009	1.0	2.0
1020	1.0	1.0
1021	1.0	1.0
1114	1.0	1.0
1115	1.0	1.0
KYJ0611003A	1.0	1.0
KYJ0611009B	0.0	1.0
LIB0611001B	1.0	1.0
LIB0611002A	1.0	2.0
LON0610002B	1.0	1.0
LON0611002A	1.0	1.0
MOS0509004	1.0	1.0
MOS0611012	2.0	2.0
MOS0611013	2.0	2.0
MOS0611015	2.0	2.0
PAR1011013	2.0	1.0
PAR1011015	2.0	2.0
PHA0111002B	1.0	1.0
PHA0112006A	2.0	1.0
PHA0112012B	1.0	1.0
PHA0411008A	1.0	1.0
PHA0411011B	1.0	1.0
PHA0411012A	1.0	1.0
PHA0411034	0.0	1.0
PHA0411054	2.0	1.0
PHA0411055	2.0	2.0
PHA0411061	2.0	2.0
PHA0509002	0.0	1.0
PHA0509017	1.0	2.0
PHA0509025	3.0	2.0
PHA0509028	2.0	2.0
PHA0509036	2.0	2.0
PHA0510029	2.0	2.0
PHA0510030	2.0	1.0
PHA0510037	1.0	2.0
PHA0510048	1.0	2.0
PHA0510050	2.0	2.0
PHA0610006A	0.0	1.0
PHA0610025	2.0	2.0
PHA0710011	2.0	1.0
PHA0710015	2.0	2.0
PHA0710016	2.0	1.0
PHA0710019	1.0	2.0
PHA0810009	1.0	1.0
PHA0810012	1.0	1.0
PHA0811014	1.0	1.0
PHA1109001	1.0	1.0
PHA1109003	1.0	1.0
PHA1110002B	1.0	1.0
PHA1110003A	1.0	1.0
PHA1110015	2.0	1.0
PHA1110019	2.0	1.0
PHA1111001A	1.0	1.0
PHA1111001B	0.0	1.0
PHA1111002B	0.0	1.0
PHA1111004B	0.0	1.0
PHA1111006A	1.0	1.0
VAR0909007	1.0	2.0
VAR0910004	1.0	2.0
VAR0910009	1.0	1.0
VAR0910011	1.0	1.0
LANGUAGE: CZ, 4th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.97
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.88
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         9
         1.0       0.62      1.00      0.76        53
         2.0       0.00      0.00      0.00        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.62        86
   macro avg       0.15      0.25      0.19        86
weighted avg       0.38      0.62      0.47        86

[[ 0  9  0  0]
 [ 0 53  0  0]
 [ 0 23  0  0]
 [ 0  1  0  0]]
0.46996821147732976
86 86 86



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.83
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.83
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         9
         1.0       0.72      0.64      0.68        53
         2.0       0.46      0.78      0.58        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.60        86
   macro avg       0.30      0.36      0.32        86
weighted avg       0.57      0.60      0.57        86

[[ 0  8  1  0]
 [ 0 34 19  0]
 [ 0  5 18  0]
 [ 0  0  1  0]]
0.5743585896474118
86 86 86



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.76
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.75
              precision    recall  f1-score   support

         0.0       0.50      0.22      0.31         9
         1.0       0.76      0.77      0.77        53
         2.0       0.57      0.70      0.63        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.69        86
   macro avg       0.46      0.42      0.43        86
weighted avg       0.67      0.69      0.67        86

[[ 2  6  1  0]
 [ 2 41 10  0]
 [ 0  7 16  0]
 [ 0  0  1  0]]
0.6722956482343827
86 86 86



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.60
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.71
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         9
         1.0       0.76      0.89      0.82        53
         2.0       0.67      0.70      0.68        23
         3.0       0.00      0.00      0.00         1

    accuracy                           0.73        86
   macro avg       0.36      0.40      0.37        86
weighted avg       0.65      0.73      0.69        86

[[ 0  8  1  0]
 [ 0 47  6  0]
 [ 0  7 16  0]
 [ 0  0  1  0]]
0.6858292278897662
86 86 86
Filename	True Label	Prediction
0607	2.0	1.0
0612	1.0	1.0
0614	1.0	1.0
0615	1.0	1.0
0627	1.0	1.0
0633	2.0	1.0
0634	2.0	1.0
0639	1.0	1.0
0640	1.0	1.0
0645	2.0	1.0
0716	1.0	1.0
0802	1.0	1.0
0806	1.0	1.0
0810	1.0	1.0
0811	2.0	1.0
0812	1.0	1.0
0813	1.0	1.0
0814	1.0	1.0
0816	2.0	1.0
0818	1.0	1.0
0901	2.0	2.0
0913	1.0	1.0
0921	1.0	1.0
1001	1.0	1.0
1116	1.0	1.0
1117	1.0	1.0
BER0611006	2.0	2.0
BER0611007	0.0	1.0
KYJ0611004A	1.0	1.0
KYJ0611006A	0.0	1.0
LIB0611001A	1.0	1.0
LIB0611004B	1.0	1.0
LON0611004B	0.0	1.0
PAR1011009B	1.0	1.0
PAR1011016	2.0	2.0
PAR1011018	1.0	2.0
PHA0111001B	1.0	1.0
PHA0111003A	0.0	1.0
PHA0111005B	1.0	1.0
PHA0111011	1.0	1.0
PHA0112009A	1.0	1.0
PHA0209031	2.0	2.0
PHA0209038	2.0	2.0
PHA0210004	1.0	1.0
PHA0210007	1.0	1.0
PHA0210008	0.0	1.0
PHA0411008B	1.0	1.0
PHA0411009A	1.0	1.0
PHA0411012B	1.0	1.0
PHA0411029	1.0	1.0
PHA0411042	2.0	2.0
PHA0411045	1.0	1.0
PHA0411059	2.0	2.0
PHA0509032	2.0	2.0
PHA0509034	1.0	1.0
PHA0509035	1.0	1.0
PHA0509038	1.0	1.0
PHA0509043	1.0	2.0
PHA0510003A	1.0	1.0
PHA0510003B	1.0	1.0
PHA0510010A	0.0	1.0
PHA0510023	2.0	2.0
PHA0510027	1.0	1.0
PHA0510046	1.0	2.0
PHA0510049	2.0	2.0
PHA0610015	2.0	2.0
PHA0710010	2.0	2.0
PHA0809009	1.0	1.0
PHA0810001	2.0	2.0
PHA0810010	1.0	2.0
PHA0810015	3.0	2.0
PHA0811019	2.0	2.0
PHA1109006	2.0	1.0
PHA1109008	0.0	1.0
PHA1109024	2.0	2.0
PHA1109025	0.0	1.0
PHA1109026	1.0	2.0
PHA1109028	1.0	2.0
PHA1110002A	1.0	1.0
PHA1110017	1.0	1.0
PHA1111002A	1.0	1.0
PHA1111009A	0.0	2.0
ST071122B	1.0	1.0
VAR0909004	1.0	1.0
VAR0909006	2.0	2.0
VAR0909010	1.0	1.0
Averaged weighted F1-scores 0.6233490949596623
144.90552995391704 65.35717405024758
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']
LANGUAGE: CZ, 0th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.10
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.85
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.76      0.73      0.75        26
         2.0       0.59      0.69      0.63        29
         3.0       0.75      0.75      0.75        28
         4.0       0.00      0.00      0.00         2

    accuracy                           0.69        87
   macro avg       0.42      0.43      0.43        87
weighted avg       0.66      0.69      0.68        87

[[ 0  2  0  0  0]
 [ 0 19  7  0  0]
 [ 0  4 20  5  0]
 [ 0  0  7 21  0]
 [ 0  0  0  2  0]]
0.675692499221911
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.66
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.67
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.78      0.81      0.79        26
         2.0       0.80      0.69      0.74        29
         3.0       0.80      1.00      0.89        28
         4.0       0.00      0.00      0.00         2

    accuracy                           0.79        87
   macro avg       0.48      0.50      0.48        87
weighted avg       0.76      0.79      0.77        87

[[ 0  2  0  0  0]
 [ 0 21  5  0  0]
 [ 0  4 20  5  0]
 [ 0  0  0 28  0]
 [ 0  0  0  2  0]]
0.7698177466123682
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.56
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.67
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.79      0.85      0.81        26
         2.0       0.69      0.76      0.72        29
         3.0       0.81      0.79      0.80        28
         4.0       0.00      0.00      0.00         2

    accuracy                           0.76        87
   macro avg       0.46      0.48      0.47        87
weighted avg       0.73      0.76      0.74        87

[[ 0  2  0  0  0]
 [ 0 22  4  0  0]
 [ 0  4 22  3  0]
 [ 0  0  6 22  0]
 [ 0  0  0  2  0]]
0.741416298529545
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.47
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.64
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.77      0.88      0.82        26
         2.0       0.79      0.66      0.72        29
         3.0       0.79      0.93      0.85        28
         4.0       0.00      0.00      0.00         2

    accuracy                           0.78        87
   macro avg       0.47      0.49      0.48        87
weighted avg       0.75      0.78      0.76        87

[[ 0  2  0  0  0]
 [ 0 23  3  0  0]
 [ 0  5 19  5  0]
 [ 0  0  2 26  0]
 [ 0  0  0  2  0]]
0.7588327373143441
87 87 87
Filename	True Label	Prediction
0601	2.0	2.0
0610	2.0	2.0
0616	1.0	2.0
0620	2.0	2.0
0629	1.0	2.0
0641	1.0	1.0
0644	2.0	2.0
0714	2.0	2.0
0715	2.0	2.0
0722	2.0	2.0
0809	2.0	1.0
0815	2.0	2.0
0819	2.0	2.0
0827	1.0	1.0
0905	2.0	2.0
0915	2.0	2.0
0920	2.0	2.0
0922	2.0	2.0
0927	1.0	2.0
0928	2.0	2.0
1002	2.0	2.0
1006	2.0	2.0
1021	2.0	2.0
1116	2.0	2.0
BER0611006	2.0	3.0
BER0611007	3.0	3.0
KYJ0611003A	1.0	1.0
KYJ0611005A	1.0	1.0
LIB0611002A	1.0	1.0
LIB0611004B	1.0	1.0
LON0611002A	1.0	1.0
LON0611002B	1.0	1.0
MOS0611013	3.0	3.0
PAR1011008A	2.0	1.0
PAR1011016	3.0	3.0
PAR1011017	3.0	2.0
PAR1011018	4.0	3.0
PHA0111001A	1.0	1.0
PHA0111001B	1.0	1.0
PHA0111012	2.0	3.0
PHA0111014	2.0	3.0
PHA0111015	3.0	3.0
PHA0111018	3.0	3.0
PHA0112006A	2.0	1.0
PHA0112006B	2.0	1.0
PHA0210008	1.0	1.0
PHA0411009A	1.0	1.0
PHA0411009B	1.0	1.0
PHA0411010A	0.0	1.0
PHA0411010B	0.0	1.0
PHA0411027	2.0	3.0
PHA0411028	2.0	3.0
PHA0411061	3.0	3.0
PHA0509007	1.0	1.0
PHA0509013	1.0	1.0
PHA0509015	3.0	3.0
PHA0509018	3.0	3.0
PHA0509026	3.0	3.0
PHA0509030	3.0	3.0
PHA0509034	3.0	2.0
PHA0509035	3.0	3.0
PHA0509039	3.0	3.0
PHA0510002B	1.0	1.0
PHA0510004B	1.0	1.0
PHA0510013B	1.0	1.0
PHA0510048	3.0	3.0
PHA0610005A	1.0	1.0
PHA0610025	3.0	3.0
PHA0710011	3.0	3.0
PHA0710016	3.0	3.0
PHA0810001	3.0	3.0
PHA0811013	4.0	3.0
PHA0811020	3.0	3.0
PHA1109002	3.0	3.0
PHA1109005	2.0	2.0
PHA1109023	2.0	1.0
PHA1110019	3.0	3.0
PHA1110022	3.0	3.0
PHA1111006A	1.0	1.0
PHA1111006B	1.0	1.0
PHA1111008A	1.0	1.0
PHA1111009A	1.0	1.0
VAR0909006	3.0	3.0
VAR0909007	3.0	3.0
VAR0910006	3.0	3.0
VAR0910007	3.0	3.0
VAR0910010	3.0	3.0
LANGUAGE: CZ, 1th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.06
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.70
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.88      0.78      0.82        27
         2.0       0.75      0.75      0.75        28
         3.0       0.80      0.97      0.88        29
         4.0       0.00      0.00      0.00         2

    accuracy                           0.80        87
   macro avg       0.48      0.50      0.49        87
weighted avg       0.78      0.80      0.79        87

[[ 0  1  0  0  0]
 [ 0 21  6  0  0]
 [ 0  2 21  5  0]
 [ 0  0  1 28  0]
 [ 0  0  0  2  0]]
0.7886240703177823
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.71
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.57
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.86      0.70      0.78        27
         2.0       0.67      0.93      0.78        28
         3.0       0.92      0.83      0.87        29
         4.0       0.00      0.00      0.00         2

    accuracy                           0.79        87
   macro avg       0.49      0.49      0.48        87
weighted avg       0.79      0.79      0.78        87

[[ 0  1  0  0  0]
 [ 0 19  8  0  0]
 [ 0  2 26  0  0]
 [ 0  0  5 24  0]
 [ 0  0  0  2  0]]
0.7813702264698515
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.57
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.51
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.84      0.78      0.81        27
         2.0       0.71      0.86      0.77        28
         3.0       0.89      0.86      0.88        29
         4.0       0.00      0.00      0.00         2

    accuracy                           0.80        87
   macro avg       0.49      0.50      0.49        87
weighted avg       0.79      0.80      0.79        87

[[ 0  1  0  0  0]
 [ 0 21  6  0  0]
 [ 0  3 24  1  0]
 [ 0  0  4 25  0]
 [ 0  0  0  2  0]]
0.792226530502978
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.50
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.49
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.83      0.74      0.78        27
         2.0       0.73      0.79      0.76        28
         3.0       0.85      0.97      0.90        29
         4.0       0.00      0.00      0.00         2

    accuracy                           0.80        87
   macro avg       0.48      0.50      0.49        87
weighted avg       0.78      0.80      0.79        87

[[ 0  1  0  0  0]
 [ 0 20  7  0  0]
 [ 0  3 22  3  0]
 [ 0  0  1 28  0]
 [ 0  0  0  2  0]]
0.788636761904332
87 87 87
Filename	True Label	Prediction
0605	2.0	2.0
0606	2.0	2.0
0608	1.0	2.0
0611	2.0	2.0
0614	2.0	2.0
0618	2.0	2.0
0622	1.0	1.0
0623	2.0	2.0
0624	2.0	2.0
0640	2.0	2.0
0717	2.0	2.0
0719	2.0	1.0
0806	1.0	2.0
0812	2.0	2.0
0814	1.0	2.0
0816	2.0	2.0
0824	2.0	2.0
0826	1.0	2.0
0914	2.0	2.0
0919	1.0	2.0
0925	1.0	2.0
0930	1.0	2.0
1007	2.0	2.0
1009	2.0	2.0
1015	2.0	2.0
1020	2.0	2.0
1022	2.0	2.0
1112	2.0	2.0
LIB0611001B	1.0	1.0
LIB0611002B	1.0	1.0
LIB0611011	2.0	2.0
LON0611003	3.0	3.0
PAR1011009B	1.0	1.0
PHA0111002A	1.0	1.0
PHA0111003B	1.0	1.0
PHA0111004B	1.0	1.0
PHA0111005A	1.0	1.0
PHA0111011	3.0	3.0
PHA0112009B	1.0	1.0
PHA0209013	1.0	1.0
PHA0209024	3.0	3.0
PHA0209028	3.0	3.0
PHA0209031	4.0	3.0
PHA0209039	3.0	3.0
PHA0210007	1.0	1.0
PHA0411008A	1.0	1.0
PHA0411012A	1.0	1.0
PHA0411043	3.0	3.0
PHA0509017	3.0	3.0
PHA0509019	3.0	2.0
PHA0509021	2.0	3.0
PHA0509022	4.0	3.0
PHA0509025	3.0	3.0
PHA0509028	3.0	3.0
PHA0509031	2.0	3.0
PHA0509038	2.0	2.0
PHA0509041	3.0	3.0
PHA0509044	3.0	3.0
PHA0510003B	1.0	1.0
PHA0510004A	1.0	1.0
PHA0510010B	0.0	1.0
PHA0510031	3.0	3.0
PHA0510035	3.0	3.0
PHA0510050	3.0	3.0
PHA0610006B	1.0	1.0
PHA0610018	3.0	3.0
PHA0610019B	1.0	1.0
PHA0710010	3.0	3.0
PHA0710014	3.0	3.0
PHA0710018	3.0	3.0
PHA0710021	3.0	3.0
PHA0810002	3.0	3.0
PHA0810004	3.0	3.0
PHA0811010	3.0	3.0
PHA1109001	1.0	1.0
PHA1109007	2.0	3.0
PHA1110001B	1.0	1.0
PHA1110002A	2.0	1.0
PHA1110015	3.0	3.0
PHA1110017	3.0	3.0
PHA1110021	3.0	3.0
PHA1111001A	2.0	1.0
PHA1111001B	1.0	1.0
VAR0209036	2.0	2.0
VAR0909008	3.0	3.0
VAR0910004	3.0	3.0
VAR0910011	3.0	3.0
LANGUAGE: CZ, 2th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.12
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.85
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.83      0.56      0.67        27
         2.0       0.56      0.68      0.61        28
         3.0       0.74      0.90      0.81        29
         4.0       0.00      0.00      0.00         2

    accuracy                           0.69        87
   macro avg       0.43      0.43      0.42        87
weighted avg       0.69      0.69      0.67        87

[[ 0  1  0  0  0]
 [ 0 15 12  0  0]
 [ 0  2 19  7  0]
 [ 0  0  3 26  0]
 [ 0  0  0  2  0]]
0.6749860956618465
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.68
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.74
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.84      0.59      0.70        27
         2.0       0.59      0.68      0.63        28
         3.0       0.75      0.93      0.83        29
         4.0       0.00      0.00      0.00         2

    accuracy                           0.71        87
   macro avg       0.44      0.44      0.43        87
weighted avg       0.70      0.71      0.70        87

[[ 0  1  0  0  0]
 [ 0 16 11  0  0]
 [ 0  2 19  7  0]
 [ 0  0  2 27  0]
 [ 0  0  0  2  0]]
0.6966465485206115
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.51
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.77      0.63      0.69        27
         2.0       0.56      0.68      0.61        28
         3.0       0.77      0.83      0.80        29
         4.0       0.00      0.00      0.00         2

    accuracy                           0.69        87
   macro avg       0.42      0.43      0.42        87
weighted avg       0.68      0.69      0.68        87

[[ 0  1  0  0  0]
 [ 0 17 10  0  0]
 [ 0  4 19  5  0]
 [ 0  0  5 24  0]
 [ 0  0  0  2  0]]
0.67926418620841
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.45
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.77      0.63      0.69        27
         2.0       0.59      0.68      0.63        28
         3.0       0.79      0.90      0.84        29
         4.0       0.00      0.00      0.00         2

    accuracy                           0.71        87
   macro avg       0.43      0.44      0.43        87
weighted avg       0.69      0.71      0.70        87

[[ 0  1  0  0  0]
 [ 0 17 10  0  0]
 [ 0  4 19  5  0]
 [ 0  0  3 26  0]
 [ 0  0  0  2  0]]
0.6987426190350076
87 87 87
Filename	True Label	Prediction
0607	2.0	2.0
0609	1.0	2.0
0613	1.0	2.0
0632	1.0	2.0
0634	2.0	2.0
0635	1.0	1.0
0637	2.0	2.0
0643	2.0	2.0
0716	2.0	2.0
0720	2.0	2.0
0802	1.0	1.0
0808	1.0	2.0
0810	2.0	2.0
0811	2.0	2.0
0813	2.0	1.0
0823	2.0	1.0
0901	2.0	2.0
0906	2.0	2.0
0907	2.0	2.0
0910	1.0	2.0
0911	2.0	2.0
0916	1.0	2.0
0923	1.0	2.0
0926	2.0	2.0
0929	1.0	2.0
1014	2.0	2.0
1017	1.0	2.0
1018	1.0	2.0
1019	2.0	2.0
1023	2.0	2.0
KYJ0611005B	1.0	1.0
KYJ0611009A	1.0	1.0
LIB0611001A	1.0	1.0
LIB0611003A	1.0	1.0
LON0610002A	1.0	1.0
LON0610002B	1.0	1.0
MOS0509001	2.0	2.0
MOS0509004	3.0	2.0
MOS0611012	3.0	3.0
MOS0611014	2.0	3.0
PAR1011014	2.0	3.0
PHA0111010	2.0	3.0
PHA0112003B	1.0	1.0
PHA0112012A	2.0	1.0
PHA0112012B	1.0	1.0
PHA0209038	4.0	3.0
PHA0411030	3.0	3.0
PHA0411033	3.0	2.0
PHA0411035	2.0	3.0
PHA0411036	3.0	3.0
PHA0411038	3.0	3.0
PHA0411047	3.0	3.0
PHA0411054	3.0	3.0
PHA0411055	3.0	3.0
PHA0411058	3.0	3.0
PHA0411059	3.0	3.0
PHA0509002	1.0	1.0
PHA0509027	3.0	3.0
PHA0509033	2.0	2.0
PHA0509042	3.0	3.0
PHA0509045	3.0	2.0
PHA0510002A	1.0	1.0
PHA0510034	3.0	3.0
PHA0510038	3.0	3.0
PHA0510047	2.0	2.0
PHA0510049	2.0	3.0
PHA0610005B	0.0	1.0
PHA0610006A	2.0	1.0
PHA0709008	3.0	3.0
PHA0809009	3.0	3.0
PHA0810003	3.0	3.0
PHA0810006	3.0	3.0
PHA0810011	3.0	3.0
PHA0811012	3.0	3.0
PHA0811017	3.0	3.0
PHA0811019	4.0	3.0
PHA1109024	3.0	3.0
PHA1110004A	1.0	1.0
PHA1110013	3.0	3.0
PHA1110014	3.0	3.0
PHA1111002A	1.0	1.0
PHA1111004A	1.0	1.0
PHA1111004B	1.0	1.0
PHA1111008B	1.0	1.0
VAR0909003	3.0	3.0
VAR0909004	3.0	3.0
VAR0909009	3.0	3.0
LANGUAGE: CZ, 3th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.04
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.66
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.70      0.88      0.78        26
         2.0       0.85      0.61      0.71        28
         3.0       0.85      1.00      0.92        29
         4.0       0.00      0.00      0.00         2

    accuracy                           0.79        87
   macro avg       0.48      0.50      0.48        87
weighted avg       0.77      0.79      0.77        87

[[ 0  2  0  0  0]
 [ 0 23  3  0  0]
 [ 0  8 17  3  0]
 [ 0  0  0 29  0]
 [ 0  0  0  2  0]]
0.7678497985336091
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.67
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.60
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.94      0.65      0.77        26
         2.0       0.68      0.93      0.79        28
         3.0       0.90      0.97      0.93        29
         4.0       0.00      0.00      0.00         2

    accuracy                           0.82        87
   macro avg       0.51      0.51      0.50        87
weighted avg       0.80      0.82      0.80        87

[[ 0  0  2  0  0]
 [ 0 17  9  0  0]
 [ 0  1 26  1  0]
 [ 0  0  1 28  0]
 [ 0  0  0  2  0]]
0.7956112852664576
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.55
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.60
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.71      0.77      0.74        26
         2.0       0.68      0.75      0.71        28
         3.0       0.89      0.86      0.88        29
         4.0       0.00      0.00      0.00         2

    accuracy                           0.76        87
   macro avg       0.46      0.48      0.47        87
weighted avg       0.73      0.76      0.74        87

[[ 0  2  0  0  0]
 [ 0 20  6  0  0]
 [ 0  6 21  1  0]
 [ 0  0  4 25  0]
 [ 0  0  0  2  0]]
0.7428742429921591
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.45
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.61
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.73      0.73      0.73        26
         2.0       0.64      0.75      0.69        28
         3.0       0.89      0.86      0.88        29
         4.0       0.00      0.00      0.00         2

    accuracy                           0.75        87
   macro avg       0.45      0.47      0.46        87
weighted avg       0.72      0.75      0.73        87

[[ 0  1  1  0  0]
 [ 0 19  7  0  0]
 [ 0  6 21  1  0]
 [ 0  0  4 25  0]
 [ 0  0  0  2  0]]
0.7323825863887153
87 87 87
Filename	True Label	Prediction
0602	2.0	2.0
0603	2.0	2.0
0604	2.0	2.0
0615	2.0	2.0
0625	2.0	1.0
0626	2.0	1.0
0631	2.0	1.0
0633	2.0	2.0
0642	2.0	2.0
0645	2.0	2.0
0721	2.0	2.0
0723	1.0	2.0
0724	2.0	2.0
0801	2.0	2.0
0804	1.0	2.0
0807	2.0	2.0
0817	2.0	2.0
0821	2.0	2.0
0822	1.0	1.0
0825	1.0	2.0
0829	2.0	2.0
0904	2.0	1.0
0917	1.0	1.0
0918	2.0	2.0
1005	2.0	2.0
1010	2.0	2.0
1016	1.0	2.0
1111	2.0	2.0
1114	2.0	2.0
1115	2.0	2.0
1117	1.0	2.0
9999	0.0	2.0
BER0611003	2.0	3.0
KYJ0611004A	1.0	1.0
KYJ0611006A	0.0	1.0
KYJ0611009B	1.0	2.0
LON0611004B	1.0	1.0
MOS0611015	3.0	3.0
PAR1011013	3.0	3.0
PHA0111005B	1.0	1.0
PHA0111016	3.0	3.0
PHA0112002A	1.0	1.0
PHA0112002B	1.0	1.0
PHA0112003A	1.0	1.0
PHA0112007A	1.0	1.0
PHA0112007B	1.0	1.0
PHA0209008	1.0	1.0
PHA0209034	3.0	3.0
PHA0210004	1.0	1.0
PHA0411008B	2.0	1.0
PHA0411011A	1.0	1.0
PHA0411032	3.0	3.0
PHA0411034	2.0	2.0
PHA0411037	3.0	3.0
PHA0411039	3.0	3.0
PHA0411041	3.0	3.0
PHA0411045	3.0	2.0
PHA0411051	4.0	3.0
PHA0411053	4.0	3.0
PHA0411062	3.0	2.0
PHA0509020	3.0	3.0
PHA0509024	3.0	3.0
PHA0509032	3.0	3.0
PHA0509040	3.0	3.0
PHA0509043	3.0	3.0
PHA0510003A	1.0	1.0
PHA0510027	3.0	3.0
PHA0510030	3.0	3.0
PHA0610007B	1.0	1.0
PHA0610015	3.0	3.0
PHA0610017	3.0	3.0
PHA0610019A	2.0	1.0
PHA0710015	3.0	3.0
PHA0710017	3.0	3.0
PHA0710019	3.0	3.0
PHA0810012	3.0	3.0
PHA0810015	3.0	3.0
PHA0811014	3.0	2.0
PHA0811016	3.0	3.0
PHA1109008	1.0	1.0
PHA1109026	3.0	3.0
PHA1109027	3.0	3.0
PHA1110001A	1.0	2.0
PHA1111002B	1.0	1.0
PHA1111003A	1.0	1.0
ST071122B	1.0	1.0
VAR0909005	3.0	2.0
LANGUAGE: CZ, 4th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.03
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.77
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.58      0.69      0.63        26
         2.0       0.58      0.54      0.56        28
         3.0       0.86      0.89      0.88        28
         4.0       0.00      0.00      0.00         2

    accuracy                           0.67        86
   macro avg       0.40      0.42      0.41        86
weighted avg       0.64      0.67      0.66        86

[[ 0  2  0  0  0]
 [ 0 18  8  0  0]
 [ 0 11 15  2  0]
 [ 0  0  3 25  0]
 [ 0  0  0  2  0]]
0.6574187406500748
86 86 86



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.69
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.53      0.69      0.60        26
         2.0       0.50      0.29      0.36        28
         3.0       0.78      1.00      0.88        28
         4.0       0.00      0.00      0.00         2

    accuracy                           0.63        86
   macro avg       0.36      0.40      0.37        86
weighted avg       0.58      0.63      0.58        86

[[ 0  2  0  0  0]
 [ 0 18  8  0  0]
 [ 0 14  8  6  0]
 [ 0  0  0 28  0]
 [ 0  0  0  2  0]]
0.5846723044397463
86 86 86



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.61
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.73
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.64      0.62      0.63        26
         2.0       0.60      0.64      0.62        28
         3.0       0.84      0.93      0.88        28
         4.0       0.00      0.00      0.00         2

    accuracy                           0.70        86
   macro avg       0.42      0.44      0.43        86
weighted avg       0.66      0.70      0.68        86

[[ 0  2  0  0  0]
 [ 0 16 10  0  0]
 [ 0  7 18  3  0]
 [ 0  0  2 26  0]
 [ 0  0  0  2  0]]
0.6787325806595185
86 86 86



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.52
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.74
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         2
         1.0       0.62      0.62      0.62        26
         2.0       0.61      0.61      0.61        28
         3.0       0.84      0.96      0.90        28
         4.0       0.00      0.00      0.00         2

    accuracy                           0.70        86
   macro avg       0.41      0.44      0.42        86
weighted avg       0.66      0.70      0.68        86

[[ 0  2  0  0  0]
 [ 0 16 10  0  0]
 [ 0  8 17  3  0]
 [ 0  0  1 27  0]
 [ 0  0  0  2  0]]
0.6767441860465115
86 86 86
Filename	True Label	Prediction
0612	1.0	2.0
0617	1.0	2.0
0619	2.0	1.0
0621	2.0	2.0
0627	2.0	2.0
0628	1.0	2.0
0630	1.0	2.0
0636	2.0	2.0
0638	1.0	2.0
0639	1.0	2.0
0718	1.0	2.0
0725	2.0	2.0
0803	1.0	2.0
0805	1.0	2.0
0818	2.0	2.0
0820	1.0	1.0
0828	2.0	1.0
0902	1.0	2.0
0903	2.0	2.0
0912	2.0	2.0
0913	2.0	2.0
0921	2.0	1.0
0924	2.0	1.0
1001	2.0	2.0
1003	2.0	2.0
1004	2.0	2.0
1008	2.0	2.0
1113	2.0	2.0
BER0609003	3.0	3.0
BER0611005	2.0	2.0
KYJ0611006B	0.0	1.0
LIB0611004A	1.0	1.0
LON0611004A	1.0	1.0
PAR1011009A	1.0	1.0
PAR1011015	3.0	3.0
PHA0111002B	2.0	1.0
PHA0111003A	1.0	1.0
PHA0111004A	1.0	1.0
PHA0112009A	2.0	1.0
PHA0209001	1.0	1.0
PHA0209026	3.0	3.0
PHA0210001	1.0	1.0
PHA0411011B	1.0	1.0
PHA0411012B	1.0	1.0
PHA0411029	3.0	3.0
PHA0411031	3.0	3.0
PHA0411042	3.0	3.0
PHA0411044	4.0	3.0
PHA0411056	3.0	3.0
PHA0411060	2.0	3.0
PHA0509036	3.0	3.0
PHA0509037	2.0	3.0
PHA0510010A	2.0	1.0
PHA0510013A	1.0	1.0
PHA0510023	3.0	3.0
PHA0510029	3.0	3.0
PHA0510032	3.0	3.0
PHA0510036	3.0	3.0
PHA0510037	2.0	3.0
PHA0510039	3.0	3.0
PHA0510040	3.0	3.0
PHA0510046	3.0	3.0
PHA0610007A	1.0	1.0
PHA0610016	3.0	3.0
PHA0610026	3.0	3.0
PHA0710009	3.0	3.0
PHA0710012	3.0	3.0
PHA0710013	4.0	3.0
PHA0809010	2.0	2.0
PHA0810008	3.0	3.0
PHA0810009	3.0	3.0
PHA0810010	3.0	3.0
PHA1109003	2.0	2.0
PHA1109004	3.0	3.0
PHA1109006	2.0	2.0
PHA1109025	2.0	1.0
PHA1109028	3.0	3.0
PHA1110002B	1.0	1.0
PHA1110003A	1.0	1.0
PHA1110003B	0.0	1.0
PHA1110016	3.0	2.0
PHA1111003B	1.0	1.0
TI071122B	1.0	1.0
VAR0909010	3.0	3.0
VAR0910005	3.0	3.0
VAR0910009	3.0	3.0
Averaged weighted F1-scores 0.7310677781377821
144.90552995391704 65.35717405024758
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']
LANGUAGE: CZ, 0th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.24
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.98
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.67      0.74      0.70        27
         2.0       0.50      0.42      0.45        36
         3.0       0.41      0.55      0.47        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.53        87
   macro avg       0.31      0.34      0.32        87
weighted avg       0.51      0.53      0.51        87

[[ 0  1  0  0  0]
 [ 0 20  7  0  0]
 [ 0  8 15 13  0]
 [ 0  1  8 11  0]
 [ 0  0  0  3  0]]
0.5134793897971099
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.96
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.95
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.66      0.78      0.71        27
         2.0       0.52      0.31      0.39        36
         3.0       0.44      0.75      0.56        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.54        87
   macro avg       0.32      0.37      0.33        87
weighted avg       0.52      0.54      0.51        87

[[ 0  1  0  0  0]
 [ 0 21  6  0  0]
 [ 0  9 11 16  0]
 [ 0  1  4 15  0]
 [ 0  0  0  3  0]]
0.5083469762789337
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.84
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.90
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.68      0.70      0.69        27
         2.0       0.54      0.53      0.54        36
         3.0       0.46      0.55      0.50        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.56        87
   macro avg       0.34      0.36      0.35        87
weighted avg       0.54      0.56      0.55        87

[[ 0  1  0  0  0]
 [ 0 19  8  0  0]
 [ 0  7 19 10  0]
 [ 0  1  8 11  0]
 [ 0  0  0  3  0]]
0.5508293228545779
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.75
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.90
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.67      0.74      0.70        27
         2.0       0.55      0.50      0.52        36
         3.0       0.46      0.55      0.50        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.56        87
   macro avg       0.33      0.36      0.34        87
weighted avg       0.54      0.56      0.55        87

[[ 0  1  0  0  0]
 [ 0 20  7  0  0]
 [ 0  8 18 10  0]
 [ 0  1  8 11  0]
 [ 0  0  0  3  0]]
0.5486204266287908
87 87 87
Filename	True Label	Prediction
0603	2.0	2.0
0610	1.0	2.0
0622	1.0	1.0
0636	2.0	2.0
0642	1.0	2.0
0723	2.0	2.0
0813	1.0	1.0
0824	1.0	2.0
0904	1.0	1.0
0905	2.0	2.0
0912	1.0	2.0
0913	2.0	1.0
0915	3.0	2.0
0923	2.0	2.0
0924	1.0	1.0
0927	2.0	1.0
1003	1.0	1.0
1006	2.0	2.0
1008	1.0	2.0
1014	2.0	2.0
1019	2.0	2.0
1111	1.0	2.0
1115	2.0	2.0
BER0611006	3.0	3.0
BER0611007	3.0	3.0
KYJ0611006B	0.0	1.0
LIB0611001A	1.0	1.0
LIB0611001B	1.0	1.0
LIB0611004A	1.0	1.0
LIB0611011	2.0	2.0
MOS0509004	2.0	2.0
PAR1011009A	2.0	1.0
PAR1011014	3.0	3.0
PHA0111003A	1.0	1.0
PHA0111005B	1.0	1.0
PHA0111011	3.0	2.0
PHA0111018	2.0	3.0
PHA0112012B	1.0	1.0
PHA0209028	2.0	3.0
PHA0411009A	1.0	2.0
PHA0411009B	1.0	1.0
PHA0411010A	1.0	1.0
PHA0411010B	1.0	1.0
PHA0411012A	1.0	1.0
PHA0411012B	1.0	1.0
PHA0411028	2.0	2.0
PHA0411035	3.0	2.0
PHA0411047	3.0	3.0
PHA0411060	3.0	2.0
PHA0509019	2.0	2.0
PHA0509020	3.0	3.0
PHA0509027	2.0	3.0
PHA0509028	3.0	3.0
PHA0509036	2.0	3.0
PHA0509041	3.0	3.0
PHA0510002A	2.0	1.0
PHA0510023	4.0	3.0
PHA0510030	2.0	2.0
PHA0510048	3.0	2.0
PHA0610007A	3.0	1.0
PHA0610019A	2.0	1.0
PHA0610019B	2.0	1.0
PHA0610025	3.0	3.0
PHA0710009	2.0	2.0
PHA0710012	3.0	2.0
PHA0710013	4.0	3.0
PHA0810003	2.0	3.0
PHA0810012	2.0	3.0
PHA0811010	2.0	2.0
PHA0811014	2.0	2.0
PHA0811017	3.0	3.0
PHA0811020	2.0	2.0
PHA1109007	3.0	2.0
PHA1109024	4.0	3.0
PHA1110003B	1.0	1.0
PHA1110017	2.0	3.0
PHA1110019	2.0	3.0
PHA1110021	3.0	2.0
PHA1111001A	2.0	1.0
PHA1111002A	1.0	1.0
PHA1111003A	1.0	1.0
PHA1111004B	1.0	1.0
PHA1111008A	2.0	1.0
VAR0909003	3.0	3.0
VAR0910004	3.0	3.0
VAR0910006	2.0	3.0
VAR0910007	2.0	3.0
LANGUAGE: CZ, 1th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.19
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.01
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.51      0.92      0.66        26
         2.0       0.57      0.11      0.18        37
         3.0       0.52      0.85      0.64        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.52        87
   macro avg       0.32      0.38      0.30        87
weighted avg       0.51      0.52      0.42        87

[[ 0  1  0  0  0]
 [ 0 24  2  0  0]
 [ 0 20  4 13  0]
 [ 0  2  1 17  0]
 [ 0  0  0  3  0]]
0.42130289445375757
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.96
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.05
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.48      1.00      0.65        26
         2.0       0.40      0.32      0.36        37
         3.0       0.33      0.05      0.09        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.45        87
   macro avg       0.24      0.27      0.22        87
weighted avg       0.39      0.45      0.37        87

[[ 0  1  0  0  0]
 [ 0 26  0  0  0]
 [ 0 24 12  1  0]
 [ 0  3 16  1  0]
 [ 0  0  2  1  0]]
0.36658461813869186
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.88
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.88
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.65      0.65      0.65        26
         2.0       0.56      0.68      0.61        37
         3.0       0.56      0.45      0.50        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.59        87
   macro avg       0.35      0.36      0.35        87
weighted avg       0.56      0.59      0.57        87

[[ 0  1  0  0  0]
 [ 0 17  9  0  0]
 [ 0  7 25  5  0]
 [ 0  1 10  9  0]
 [ 0  0  1  2  0]]
0.5696663863190357
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.77
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.89
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.65      0.85      0.73        26
         2.0       0.62      0.49      0.55        37
         3.0       0.50      0.60      0.55        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.60        87
   macro avg       0.35      0.39      0.36        87
weighted avg       0.57      0.60      0.58        87

[[ 0  1  0  0  0]
 [ 0 22  4  0  0]
 [ 0 10 18  9  0]
 [ 0  1  7 12  0]
 [ 0  0  0  3  0]]
0.57652385928248
87 87 87
Filename	True Label	Prediction
0604	2.0	2.0
0606	2.0	2.0
0609	2.0	2.0
0611	2.0	1.0
0612	1.0	1.0
0613	1.0	1.0
0618	1.0	2.0
0619	1.0	1.0
0624	2.0	2.0
0625	1.0	1.0
0629	2.0	2.0
0631	2.0	2.0
0634	3.0	2.0
0637	2.0	2.0
0714	2.0	2.0
0721	2.0	2.0
0804	1.0	1.0
0816	3.0	2.0
0817	1.0	1.0
0823	1.0	2.0
0825	2.0	1.0
0826	2.0	1.0
0829	1.0	2.0
0914	1.0	2.0
0920	2.0	2.0
0926	2.0	2.0
1004	1.0	1.0
1009	2.0	2.0
1023	2.0	2.0
KYJ0611004A	1.0	1.0
LON0611002A	1.0	1.0
LON0611003	3.0	3.0
LON0611004A	0.0	1.0
MOS0509001	2.0	2.0
PHA0111001B	1.0	1.0
PHA0111002A	2.0	1.0
PHA0111002B	3.0	1.0
PHA0111003B	1.0	1.0
PHA0111004A	1.0	1.0
PHA0112002B	1.0	1.0
PHA0112003B	1.0	1.0
PHA0112012A	2.0	2.0
PHA0209031	4.0	3.0
PHA0210004	2.0	1.0
PHA0210008	2.0	1.0
PHA0411008A	2.0	1.0
PHA0411031	3.0	3.0
PHA0411038	3.0	3.0
PHA0411043	3.0	2.0
PHA0411045	2.0	2.0
PHA0411058	3.0	3.0
PHA0411061	2.0	3.0
PHA0509013	1.0	1.0
PHA0509026	4.0	3.0
PHA0509030	3.0	3.0
PHA0509031	2.0	2.0
PHA0509035	3.0	2.0
PHA0509039	2.0	3.0
PHA0509044	2.0	3.0
PHA0510002B	1.0	1.0
PHA0510003A	2.0	1.0
PHA0510004A	1.0	1.0
PHA0510031	3.0	2.0
PHA0610006A	2.0	1.0
PHA0610015	2.0	3.0
PHA0610018	3.0	3.0
PHA0710011	3.0	3.0
PHA0710014	3.0	3.0
PHA0710016	3.0	3.0
PHA0710019	4.0	3.0
PHA0809009	2.0	3.0
PHA0810006	2.0	3.0
PHA0810009	3.0	3.0
PHA0810011	2.0	3.0
PHA0811016	3.0	2.0
PHA1109028	2.0	3.0
PHA1110001A	2.0	2.0
PHA1110001B	1.0	1.0
PHA1110002A	2.0	1.0
PHA1110003A	1.0	1.0
PHA1110016	2.0	3.0
PHA1111002B	1.0	1.0
PHA1111009A	1.0	1.0
ST071122B	1.0	1.0
VAR0209036	3.0	3.0
VAR0910009	3.0	3.0
VAR0910010	3.0	2.0
LANGUAGE: CZ, 2th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.33
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.07
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.61      0.42      0.50        26
         2.0       0.45      0.78      0.57        37
         3.0       0.50      0.10      0.17        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.48        87
   macro avg       0.31      0.26      0.25        87
weighted avg       0.49      0.48      0.43        87

[[ 0  0  1  0  0]
 [ 0 11 15  0  0]
 [ 0  6 29  2  0]
 [ 0  1 17  2  0]
 [ 0  0  3  0  0]]
0.4295695289610098
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.07
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.02
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.69      0.35      0.46        26
         2.0       0.49      0.54      0.51        37
         3.0       0.48      0.80      0.60        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.52        87
   macro avg       0.33      0.34      0.32        87
weighted avg       0.53      0.52      0.49        87

[[ 0  1  0  0  0]
 [ 0  9 17  0  0]
 [ 0  3 20 14  0]
 [ 0  0  4 16  0]
 [ 0  0  0  3  0]]
0.4948256399134734
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.97
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.93
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.57      0.92      0.71        26
         2.0       0.64      0.24      0.35        37
         3.0       0.52      0.80      0.63        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.56        87
   macro avg       0.35      0.39      0.34        87
weighted avg       0.56      0.56      0.51        87

[[ 0  1  0  0  0]
 [ 0 24  2  0  0]
 [ 0 16  9 12  0]
 [ 0  1  3 16  0]
 [ 0  0  0  3  0]]
0.505296371422132
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.93
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.90
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.67      0.69      0.68        26
         2.0       0.61      0.51      0.56        37
         3.0       0.52      0.75      0.61        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.60        87
   macro avg       0.36      0.39      0.37        87
weighted avg       0.58      0.60      0.58        87

[[ 0  1  0  0  0]
 [ 0 18  8  0  0]
 [ 0  7 19 11  0]
 [ 0  1  4 15  0]
 [ 0  0  0  3  0]]
0.5813993782288451
87 87 87
Filename	True Label	Prediction
0602	1.0	2.0
0608	1.0	2.0
0616	1.0	2.0
0617	1.0	2.0
0630	1.0	1.0
0632	1.0	2.0
0643	2.0	2.0
0644	1.0	2.0
0645	2.0	2.0
0715	2.0	2.0
0716	2.0	2.0
0722	2.0	2.0
0725	1.0	2.0
0806	2.0	2.0
0808	2.0	2.0
0809	2.0	1.0
0819	3.0	2.0
0828	2.0	2.0
0901	2.0	2.0
0902	2.0	1.0
0922	1.0	2.0
1010	2.0	2.0
1016	2.0	2.0
1017	2.0	2.0
1021	2.0	2.0
1022	2.0	2.0
1113	2.0	2.0
9999	1.0	1.0
BER0609003	3.0	3.0
BER0611005	3.0	2.0
KYJ0611003A	1.0	1.0
KYJ0611005A	0.0	1.0
KYJ0611005B	1.0	1.0
KYJ0611009A	1.0	1.0
MOS0611015	2.0	3.0
PAR1011008A	1.0	1.0
PAR1011009B	1.0	1.0
PAR1011015	2.0	3.0
PAR1011017	3.0	3.0
PHA0111001A	1.0	1.0
PHA0111016	4.0	3.0
PHA0112006A	3.0	1.0
PHA0112007B	1.0	1.0
PHA0112009B	1.0	1.0
PHA0209001	2.0	1.0
PHA0411008B	1.0	1.0
PHA0411011B	1.0	1.0
PHA0411027	2.0	2.0
PHA0411032	2.0	3.0
PHA0411042	2.0	3.0
PHA0411056	4.0	3.0
PHA0411059	2.0	3.0
PHA0509002	1.0	1.0
PHA0509018	3.0	3.0
PHA0509025	4.0	3.0
PHA0509037	3.0	2.0
PHA0509038	2.0	2.0
PHA0509040	2.0	3.0
PHA0509042	3.0	3.0
PHA0509045	2.0	2.0
PHA0510010A	2.0	1.0
PHA0510010B	1.0	1.0
PHA0510013B	1.0	1.0
PHA0510027	2.0	3.0
PHA0510036	3.0	3.0
PHA0510038	3.0	3.0
PHA0610006B	1.0	1.0
PHA0610017	3.0	3.0
PHA0710015	3.0	3.0
PHA0710018	3.0	3.0
PHA0810008	2.0	3.0
PHA1109002	3.0	3.0
PHA1109003	2.0	2.0
PHA1109004	3.0	3.0
PHA1109005	3.0	2.0
PHA1109006	2.0	3.0
PHA1109026	3.0	3.0
PHA1110004A	2.0	1.0
PHA1110014	3.0	3.0
PHA1110015	3.0	3.0
PHA1111004A	2.0	1.0
PHA1111006B	1.0	1.0
PHA1111008B	1.0	1.0
TI071122B	2.0	1.0
VAR0909006	2.0	3.0
VAR0909010	3.0	3.0
VAR0910011	2.0	3.0
LANGUAGE: CZ, 3th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.25
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.04
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.64      0.35      0.45        26
         2.0       0.45      0.78      0.57        36
         3.0       0.45      0.25      0.32        20
         4.0       0.00      0.00      0.00         4

    accuracy                           0.48        87
   macro avg       0.31      0.27      0.27        87
weighted avg       0.48      0.48      0.45        87

[[ 0  0  1  0  0]
 [ 0  9 17  0  0]
 [ 0  5 28  3  0]
 [ 0  0 15  5  0]
 [ 0  0  1  3  0]]
0.4450924307431538
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.03
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.04
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.56      0.38      0.45        26
         2.0       0.48      0.44      0.46        36
         3.0       0.50      0.90      0.64        20
         4.0       0.00      0.00      0.00         4

    accuracy                           0.51        87
   macro avg       0.31      0.35      0.31        87
weighted avg       0.48      0.51      0.48        87

[[ 0  1  0  0  0]
 [ 0 10 15  1  0]
 [ 0  7 16 13  0]
 [ 0  0  2 18  0]
 [ 0  0  0  4  0]]
0.47552846953146805
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.96
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.02
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.64      0.54      0.58        26
         2.0       0.47      0.39      0.42        36
         3.0       0.49      0.85      0.62        20
         4.0       0.00      0.00      0.00         4

    accuracy                           0.52        87
   macro avg       0.32      0.36      0.33        87
weighted avg       0.49      0.52      0.49        87

[[ 0  0  1  0  0]
 [ 0 14 12  0  0]
 [ 0  8 14 14  0]
 [ 0  0  3 17  0]
 [ 0  0  0  4  0]]
0.4919888540578196
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.88
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.02
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.64      0.54      0.58        26
         2.0       0.47      0.47      0.47        36
         3.0       0.48      0.70      0.57        20
         4.0       0.00      0.00      0.00         4

    accuracy                           0.52        87
   macro avg       0.32      0.34      0.33        87
weighted avg       0.50      0.52      0.50        87

[[ 0  0  1  0  0]
 [ 0 14 12  0  0]
 [ 0  8 17 11  0]
 [ 0  0  6 14  0]
 [ 0  0  0  4  0]]
0.5010946907498632
87 87 87
Filename	True Label	Prediction
0601	1.0	1.0
0605	2.0	2.0
0614	2.0	2.0
0620	1.0	2.0
0621	1.0	2.0
0623	1.0	2.0
0626	2.0	2.0
0633	2.0	2.0
0635	2.0	1.0
0639	1.0	2.0
0641	1.0	2.0
0718	2.0	2.0
0724	3.0	2.0
0801	2.0	2.0
0803	2.0	2.0
0805	2.0	2.0
0810	1.0	2.0
0812	1.0	1.0
0814	2.0	1.0
0815	2.0	2.0
0818	1.0	1.0
0822	1.0	2.0
0827	2.0	2.0
0921	1.0	1.0
0929	1.0	2.0
1005	1.0	2.0
1007	2.0	2.0
1015	1.0	2.0
1018	2.0	2.0
1020	2.0	2.0
1112	1.0	2.0
KYJ0611006A	1.0	1.0
KYJ0611009B	0.0	2.0
LIB0611002A	1.0	1.0
LIB0611002B	1.0	1.0
LIB0611003A	1.0	1.0
LON0610002B	2.0	1.0
LON0611004B	1.0	1.0
MOS0611012	2.0	2.0
PAR1011013	2.0	3.0
PAR1011016	2.0	3.0
PAR1011018	3.0	3.0
PHA0111005A	1.0	1.0
PHA0111010	4.0	3.0
PHA0111012	2.0	3.0
PHA0111014	1.0	2.0
PHA0112003A	2.0	1.0
PHA0112007A	2.0	1.0
PHA0112009A	2.0	1.0
PHA0209008	2.0	1.0
PHA0209013	1.0	1.0
PHA0209024	3.0	2.0
PHA0209034	2.0	3.0
PHA0209038	4.0	3.0
PHA0209039	3.0	3.0
PHA0210001	1.0	1.0
PHA0411029	3.0	2.0
PHA0411030	3.0	3.0
PHA0411033	2.0	3.0
PHA0411037	2.0	3.0
PHA0411039	3.0	3.0
PHA0411041	3.0	3.0
PHA0411044	3.0	3.0
PHA0411051	3.0	3.0
PHA0411053	3.0	3.0
PHA0509015	3.0	2.0
PHA0509017	3.0	3.0
PHA0509021	3.0	2.0
PHA0509024	3.0	3.0
PHA0509033	2.0	2.0
PHA0509043	3.0	3.0
PHA0510013A	1.0	1.0
PHA0510037	2.0	2.0
PHA0510039	2.0	3.0
PHA0510049	2.0	2.0
PHA0510050	2.0	3.0
PHA0610005A	1.0	1.0
PHA0610026	3.0	3.0
PHA0810004	3.0	2.0
PHA0810015	2.0	3.0
PHA0811012	4.0	3.0
PHA1109027	4.0	3.0
PHA1110002B	2.0	1.0
PHA1110013	2.0	3.0
VAR0909004	3.0	3.0
VAR0909005	2.0	3.0
VAR0909008	3.0	3.0
LANGUAGE: CZ, 4th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.17
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.01
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.58      0.81      0.68        26
         2.0       0.63      0.33      0.44        36
         3.0       0.48      0.75      0.59        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.56        86
   macro avg       0.34      0.38      0.34        86
weighted avg       0.55      0.56      0.52        86

[[ 0  1  0  0  0]
 [ 0 21  5  0  0]
 [ 0 11 12 13  0]
 [ 0  3  2 15  0]
 [ 0  0  0  3  0]]
0.5242639536889571
86 86 86



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.96
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 1.09
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.52      0.92      0.67        26
         2.0       0.45      0.50      0.47        36
         3.0       0.00      0.00      0.00        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.49        86
   macro avg       0.19      0.28      0.23        86
weighted avg       0.35      0.49      0.40        86

[[ 0  1  0  0  0]
 [ 0 24  2  0  0]
 [ 0 18 18  0  0]
 [ 0  3 17  0  0]
 [ 0  0  3  0  0]]
0.3998368013055896
86 86 86



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.83
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.97
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.61      0.73      0.67        26
         2.0       0.56      0.39      0.46        36
         3.0       0.47      0.70      0.56        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.55        86
   macro avg       0.33      0.36      0.34        86
weighted avg       0.53      0.55      0.52        86

[[ 0  1  0  0  0]
 [ 0 19  7  0  0]
 [ 0  9 14 13  0]
 [ 0  2  4 14  0]
 [ 0  0  0  3  0]]
0.5239293429914856
86 86 86



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.73
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.99
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.67      0.69      0.68        26
         2.0       0.56      0.53      0.54        36
         3.0       0.48      0.60      0.53        20
         4.0       0.00      0.00      0.00         3

    accuracy                           0.57        86
   macro avg       0.34      0.36      0.35        86
weighted avg       0.55      0.57      0.56        86

[[ 0  1  0  0  0]
 [ 0 18  8  0  0]
 [ 0  7 19 10  0]
 [ 0  1  7 12  0]
 [ 0  0  0  3  0]]
0.556626757767609
86 86 86
Filename	True Label	Prediction
0607	3.0	2.0
0615	1.0	1.0
0627	2.0	2.0
0628	2.0	2.0
0638	2.0	2.0
0640	2.0	2.0
0717	1.0	1.0
0719	2.0	1.0
0720	1.0	1.0
0802	2.0	1.0
0807	2.0	2.0
0811	2.0	2.0
0820	2.0	1.0
0821	1.0	2.0
0903	1.0	2.0
0906	2.0	2.0
0907	2.0	2.0
0910	1.0	1.0
0911	1.0	2.0
0916	2.0	2.0
0917	1.0	2.0
0918	1.0	2.0
0919	2.0	1.0
0925	2.0	2.0
0928	1.0	2.0
0930	2.0	1.0
1001	1.0	2.0
1002	2.0	2.0
1114	3.0	1.0
1116	2.0	2.0
1117	1.0	1.0
BER0611003	2.0	2.0
LIB0611004B	1.0	1.0
LON0610002A	1.0	1.0
LON0611002B	0.0	1.0
MOS0611013	2.0	3.0
MOS0611014	2.0	3.0
PHA0111004B	1.0	1.0
PHA0111015	4.0	3.0
PHA0112002A	2.0	2.0
PHA0112006B	3.0	2.0
PHA0209026	3.0	3.0
PHA0210007	2.0	2.0
PHA0411011A	1.0	1.0
PHA0411034	2.0	2.0
PHA0411036	3.0	2.0
PHA0411054	3.0	2.0
PHA0411055	3.0	3.0
PHA0411062	2.0	3.0
PHA0509007	1.0	2.0
PHA0509022	4.0	3.0
PHA0509032	2.0	3.0
PHA0509034	2.0	2.0
PHA0510003B	1.0	1.0
PHA0510004B	1.0	1.0
PHA0510029	3.0	3.0
PHA0510032	2.0	3.0
PHA0510034	3.0	3.0
PHA0510035	2.0	3.0
PHA0510040	2.0	3.0
PHA0510046	2.0	2.0
PHA0510047	2.0	2.0
PHA0610005B	1.0	1.0
PHA0610007B	1.0	1.0
PHA0610016	3.0	3.0
PHA0709008	3.0	3.0
PHA0710010	2.0	3.0
PHA0710017	3.0	3.0
PHA0710021	4.0	3.0
PHA0809010	3.0	2.0
PHA0810001	2.0	3.0
PHA0810002	2.0	3.0
PHA0810010	3.0	3.0
PHA0811013	3.0	3.0
PHA0811019	3.0	3.0
PHA1109001	2.0	1.0
PHA1109008	1.0	1.0
PHA1109023	2.0	1.0
PHA1109025	1.0	1.0
PHA1110022	3.0	3.0
PHA1111001B	1.0	1.0
PHA1111003B	1.0	1.0
PHA1111006A	1.0	1.0
VAR0909007	3.0	3.0
VAR0909009	3.0	2.0
VAR0910005	3.0	2.0
Averaged weighted F1-scores 0.5528530225315176
144.90552995391704 65.35717405024758
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']
LANGUAGE: CZ, 0th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.09
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.92
              precision    recall  f1-score   support

         1.0       0.88      0.35      0.50        20
         2.0       0.51      0.86      0.64        35
         3.0       0.75      0.48      0.59        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.60        87
   macro avg       0.53      0.42      0.43        87
weighted avg       0.67      0.60      0.58        87

[[ 7 13  0  0]
 [ 1 30  4  0]
 [ 0 16 15  0]
 [ 0  0  1  0]]
0.5813301109145051
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.66
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.68
              precision    recall  f1-score   support

         1.0       0.82      0.70      0.76        20
         2.0       0.68      0.80      0.74        35
         3.0       0.83      0.77      0.80        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.76        87
   macro avg       0.58      0.57      0.57        87
weighted avg       0.76      0.76      0.76        87

[[14  6  0  0]
 [ 3 28  4  0]
 [ 0  7 24  0]
 [ 0  0  1  0]]
0.7554552737855823
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.50
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.66
              precision    recall  f1-score   support

         1.0       0.81      0.65      0.72        20
         2.0       0.76      0.74      0.75        35
         3.0       0.78      0.94      0.85        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.78        87
   macro avg       0.59      0.58      0.58        87
weighted avg       0.77      0.78      0.77        87

[[13  6  1  0]
 [ 3 26  6  0]
 [ 0  2 29  0]
 [ 0  0  1  0]]
0.7731314081521331
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.43
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.72
              precision    recall  f1-score   support

         1.0       0.86      0.60      0.71        20
         2.0       0.66      0.83      0.73        35
         3.0       0.83      0.77      0.80        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.75        87
   macro avg       0.59      0.55      0.56        87
weighted avg       0.76      0.75      0.74        87

[[12  8  0  0]
 [ 2 29  4  0]
 [ 0  7 24  0]
 [ 0  0  1  0]]
0.742687926327231
87 87 87
Filename	True Label	Prediction
0612	2.0	2.0
0618	2.0	2.0
0624	2.0	2.0
0628	2.0	2.0
0631	2.0	2.0
0634	2.0	2.0
0636	2.0	2.0
0641	2.0	2.0
0644	2.0	2.0
0717	2.0	2.0
0802	1.0	2.0
0819	3.0	2.0
0822	2.0	2.0
0824	2.0	2.0
0826	2.0	2.0
0901	3.0	2.0
0904	1.0	2.0
0905	2.0	2.0
0910	1.0	2.0
0912	2.0	2.0
0920	2.0	2.0
0921	2.0	2.0
0923	2.0	2.0
0924	1.0	2.0
0929	1.0	2.0
1005	2.0	2.0
1006	2.0	2.0
1007	2.0	2.0
1009	2.0	2.0
1016	2.0	2.0
1019	2.0	2.0
1112	2.0	2.0
KYJ0611003A	1.0	1.0
KYJ0611004A	1.0	2.0
KYJ0611006B	1.0	1.0
LON0610002B	1.0	1.0
MOS0611015	3.0	3.0
PAR1011008A	2.0	2.0
PAR1011013	3.0	3.0
PAR1011017	3.0	3.0
PHA0111002A	1.0	2.0
PHA0111004B	1.0	1.0
PHA0111010	3.0	3.0
PHA0111015	4.0	3.0
PHA0111018	2.0	3.0
PHA0112012A	1.0	2.0
PHA0209001	2.0	1.0
PHA0209008	1.0	1.0
PHA0209024	3.0	3.0
PHA0209026	3.0	3.0
PHA0209038	3.0	3.0
PHA0210008	1.0	1.0
PHA0411028	2.0	3.0
PHA0411031	3.0	3.0
PHA0411034	3.0	3.0
PHA0411037	2.0	3.0
PHA0509019	3.0	2.0
PHA0509021	2.0	3.0
PHA0509022	3.0	3.0
PHA0509026	3.0	3.0
PHA0509037	3.0	2.0
PHA0509038	2.0	2.0
PHA0509041	3.0	3.0
PHA0509045	3.0	2.0
PHA0510003A	1.0	1.0
PHA0510010B	1.0	1.0
PHA0510023	3.0	3.0
PHA0610006B	1.0	1.0
PHA0610026	3.0	3.0
PHA0710015	3.0	3.0
PHA0810001	3.0	3.0
PHA0810008	3.0	3.0
PHA0811010	3.0	3.0
PHA0811012	3.0	3.0
PHA0811019	3.0	3.0
PHA1109002	3.0	3.0
PHA1109003	2.0	2.0
PHA1110001A	2.0	1.0
PHA1110002A	2.0	2.0
PHA1110003B	1.0	1.0
PHA1110013	3.0	3.0
PHA1110021	3.0	2.0
PHA1111003A	1.0	1.0
ST071122B	1.0	1.0
VAR0909008	3.0	2.0
VAR0909009	3.0	3.0
VAR0910011	3.0	3.0
LANGUAGE: CZ, 1th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.10
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.77
              precision    recall  f1-score   support

         1.0       1.00      0.24      0.38        21
         2.0       0.58      1.00      0.73        34
         3.0       0.96      0.71      0.81        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.70        87
   macro avg       0.63      0.49      0.48        87
weighted avg       0.81      0.70      0.67        87

[[ 5 16  0  0]
 [ 0 34  0  0]
 [ 0  9 22  0]
 [ 0  0  1  0]]
0.6689241079304112
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.61
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.55
              precision    recall  f1-score   support

         1.0       0.70      0.67      0.68        21
         2.0       0.76      0.76      0.76        34
         3.0       0.91      0.97      0.94        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.80        87
   macro avg       0.59      0.60      0.60        87
weighted avg       0.79      0.80      0.80        87

[[14  7  0  0]
 [ 6 26  2  0]
 [ 0  1 30  0]
 [ 0  0  1  0]]
0.797746705915335
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.45
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.61
              precision    recall  f1-score   support

         1.0       0.71      0.71      0.71        21
         2.0       0.72      0.82      0.77        34
         3.0       0.96      0.84      0.90        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.79        87
   macro avg       0.60      0.59      0.59        87
weighted avg       0.80      0.79      0.79        87

[[15  6  0  0]
 [ 6 28  0  0]
 [ 0  5 26  0]
 [ 0  0  1  0]]
0.7916700601045721
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.39
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.61
              precision    recall  f1-score   support

         1.0       0.70      0.67      0.68        21
         2.0       0.73      0.79      0.76        34
         3.0       0.93      0.90      0.92        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.79        87
   macro avg       0.59      0.59      0.59        87
weighted avg       0.79      0.79      0.79        87

[[14  7  0  0]
 [ 6 27  1  0]
 [ 0  3 28  0]
 [ 0  0  1  0]]
0.7891912038810831
87 87 87
Filename	True Label	Prediction
0601	2.0	2.0
0602	2.0	2.0
0606	2.0	2.0
0607	2.0	2.0
0608	1.0	2.0
0629	2.0	2.0
0635	2.0	2.0
0639	2.0	2.0
0640	2.0	2.0
0643	2.0	2.0
0716	2.0	2.0
0720	2.0	2.0
0723	1.0	2.0
0725	2.0	2.0
0801	1.0	2.0
0803	1.0	2.0
0807	2.0	2.0
0809	2.0	2.0
0810	2.0	2.0
0815	2.0	2.0
0818	1.0	2.0
0914	2.0	2.0
0915	2.0	2.0
0916	1.0	2.0
0918	2.0	2.0
0927	2.0	2.0
1002	2.0	2.0
1017	2.0	2.0
1023	2.0	2.0
1113	2.0	2.0
1117	2.0	2.0
9999	1.0	2.0
BER0611006	3.0	3.0
BER0611007	3.0	3.0
LIB0611001A	1.0	1.0
LON0610002A	2.0	1.0
LON0611002A	1.0	1.0
LON0611004B	1.0	1.0
MOS0509004	2.0	2.0
PAR1011009A	2.0	1.0
PHA0111011	3.0	3.0
PHA0111016	3.0	3.0
PHA0112003A	1.0	1.0
PHA0112007A	1.0	1.0
PHA0112009B	2.0	1.0
PHA0209028	3.0	3.0
PHA0411033	3.0	3.0
PHA0411036	3.0	3.0
PHA0411054	3.0	3.0
PHA0411060	3.0	2.0
PHA0509025	3.0	3.0
PHA0509028	3.0	3.0
PHA0509035	3.0	3.0
PHA0510002B	2.0	1.0
PHA0510003B	1.0	1.0
PHA0510029	3.0	3.0
PHA0510031	3.0	3.0
PHA0510037	2.0	2.0
PHA0510039	3.0	3.0
PHA0510049	3.0	3.0
PHA0610005B	1.0	1.0
PHA0610007A	1.0	1.0
PHA0610007B	1.0	1.0
PHA0610017	3.0	3.0
PHA0610019A	1.0	1.0
PHA0709008	3.0	3.0
PHA0710014	3.0	3.0
PHA0809009	3.0	2.0
PHA0810003	3.0	3.0
PHA0810010	3.0	3.0
PHA0810012	3.0	3.0
PHA0810015	3.0	3.0
PHA0811014	3.0	3.0
PHA0811017	4.0	3.0
PHA1109023	1.0	1.0
PHA1109026	3.0	3.0
PHA1110014	3.0	3.0
PHA1111003B	1.0	1.0
PHA1111004B	1.0	1.0
PHA1111006B	2.0	1.0
PHA1111008A	2.0	1.0
TI071122B	1.0	1.0
VAR0209036	2.0	3.0
VAR0909003	3.0	3.0
VAR0909005	3.0	2.0
VAR0909010	3.0	3.0
VAR0910009	3.0	3.0
LANGUAGE: CZ, 2th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.08
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.72
              precision    recall  f1-score   support

         1.0       0.57      1.00      0.73        20
         2.0       0.88      0.44      0.59        34
         3.0       0.86      0.94      0.90        32
         4.0       0.00      0.00      0.00         1

    accuracy                           0.75        87
   macro avg       0.58      0.59      0.55        87
weighted avg       0.79      0.75      0.73        87

[[20  0  0  0]
 [15 15  4  0]
 [ 0  2 30  0]
 [ 0  0  1  0]]
0.7264617352110918
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.64
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.58
              precision    recall  f1-score   support

         1.0       0.69      1.00      0.82        20
         2.0       0.91      0.59      0.71        34
         3.0       0.83      0.94      0.88        32
         4.0       0.00      0.00      0.00         1

    accuracy                           0.80        87
   macro avg       0.61      0.63      0.60        87
weighted avg       0.82      0.80      0.79        87

[[20  0  0  0]
 [ 9 20  5  0]
 [ 0  2 30  0]
 [ 0  0  1  0]]
0.7913510231678879
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 11

  Average training loss: 0.53
  Training epoch took: 12
Running Validation...
  Average evaluation loss: 0.54
              precision    recall  f1-score   support

         1.0       0.80      0.80      0.80        20
         2.0       0.81      0.65      0.72        34
         3.0       0.78      0.97      0.86        32
         4.0       0.00      0.00      0.00         1

    accuracy                           0.79        87
   macro avg       0.60      0.60      0.60        87
weighted avg       0.79      0.79      0.78        87

[[16  4  0  0]
 [ 4 22  8  0]
 [ 0  1 31  0]
 [ 0  0  1  0]]
0.7825304105688504
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 11

  Average training loss: 0.47
  Training epoch took: 12
Running Validation...
  Average evaluation loss: 0.49
              precision    recall  f1-score   support

         1.0       0.73      0.95      0.83        20
         2.0       0.88      0.68      0.77        34
         3.0       0.86      0.94      0.90        32
         4.0       0.00      0.00      0.00         1

    accuracy                           0.83        87
   macro avg       0.62      0.64      0.62        87
weighted avg       0.83      0.83      0.82        87

[[19  1  0  0]
 [ 7 23  4  0]
 [ 0  2 30  0]
 [ 0  0  1  0]]
0.8189094507472631
87 87 87
Filename	True Label	Prediction
0603	2.0	2.0
0604	2.0	2.0
0611	2.0	2.0
0619	2.0	2.0
0620	2.0	2.0
0621	2.0	2.0
0622	2.0	2.0
0714	2.0	2.0
0718	2.0	2.0
0724	3.0	2.0
0808	2.0	2.0
0816	2.0	2.0
0817	2.0	2.0
0903	2.0	2.0
0919	2.0	2.0
0926	2.0	2.0
1001	2.0	2.0
1004	2.0	2.0
1018	2.0	2.0
1022	2.0	2.0
1111	2.0	2.0
1115	2.0	2.0
1116	2.0	2.0
KYJ0611009A	2.0	1.0
LIB0611004B	2.0	1.0
LON0611002B	1.0	1.0
LON0611003	3.0	3.0
LON0611004A	1.0	1.0
MOS0509001	2.0	3.0
MOS0611013	3.0	3.0
PAR1011015	2.0	3.0
PAR1011018	3.0	3.0
PHA0111001B	1.0	1.0
PHA0111002B	2.0	1.0
PHA0111005B	2.0	1.0
PHA0111012	3.0	3.0
PHA0112002B	2.0	1.0
PHA0112003B	1.0	1.0
PHA0112006B	2.0	2.0
PHA0209034	3.0	3.0
PHA0210001	1.0	1.0
PHA0210004	1.0	1.0
PHA0210007	1.0	2.0
PHA0411008B	1.0	1.0
PHA0411012B	1.0	1.0
PHA0411027	3.0	3.0
PHA0411032	3.0	3.0
PHA0411039	3.0	3.0
PHA0411041	3.0	3.0
PHA0411045	3.0	3.0
PHA0411056	3.0	3.0
PHA0411058	3.0	3.0
PHA0411061	3.0	3.0
PHA0411062	3.0	3.0
PHA0509007	1.0	1.0
PHA0509013	1.0	1.0
PHA0509017	3.0	3.0
PHA0509018	3.0	3.0
PHA0509031	2.0	3.0
PHA0509044	3.0	3.0
PHA0510002A	1.0	1.0
PHA0510013A	2.0	1.0
PHA0510013B	1.0	1.0
PHA0510032	3.0	3.0
PHA0510036	3.0	3.0
PHA0510047	2.0	3.0
PHA0610019B	2.0	1.0
PHA0710013	3.0	3.0
PHA0710017	3.0	3.0
PHA0710018	3.0	3.0
PHA0710019	3.0	3.0
PHA0811013	3.0	3.0
PHA0811016	3.0	3.0
PHA1109005	3.0	2.0
PHA1109008	1.0	1.0
PHA1109025	1.0	1.0
PHA1109027	3.0	3.0
PHA1110003A	1.0	1.0
PHA1110004A	1.0	1.0
PHA1110017	3.0	3.0
PHA1110019	3.0	3.0
PHA1110022	4.0	3.0
PHA1111001B	1.0	1.0
PHA1111002B	1.0	1.0
PHA1111008B	1.0	1.0
VAR0909006	3.0	3.0
VAR0910004	3.0	3.0
LANGUAGE: CZ, 3th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 11

  Average training loss: 1.04
  Training epoch took: 12
Running Validation...
  Average evaluation loss: 1.07
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.00      0.00      0.00        20
         2.0       0.47      0.44      0.45        34
         3.0       0.56      1.00      0.72        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.53        87
   macro avg       0.21      0.29      0.24        87
weighted avg       0.38      0.53      0.43        87

[[ 0  0  1  0  0]
 [ 0  0 16  4  0]
 [ 0  0 15 19  0]
 [ 0  0  0 31  0]
 [ 0  0  0  1  0]]
0.43452163981434233
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 11

  Average training loss: 0.66
  Training epoch took: 12
Running Validation...
  Average evaluation loss: 0.55
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.68      0.85      0.76        20
         2.0       0.81      0.76      0.79        34
         3.0       0.93      0.90      0.92        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.82        87
   macro avg       0.49      0.50      0.49        87
weighted avg       0.81      0.82      0.81        87

[[ 0  1  0  0  0]
 [ 0 17  3  0  0]
 [ 0  7 26  1  0]
 [ 0  0  3 28  0]
 [ 0  0  0  1  0]]
0.8087127160049715
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 11

  Average training loss: 0.49
  Training epoch took: 12
Running Validation...
  Average evaluation loss: 0.58
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.68      0.75      0.71        20
         2.0       0.82      0.68      0.74        34
         3.0       0.84      1.00      0.91        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.79        87
   macro avg       0.47      0.49      0.47        87
weighted avg       0.78      0.79      0.78        87

[[ 0  1  0  0  0]
 [ 0 15  5  0  0]
 [ 0  6 23  5  0]
 [ 0  0  0 31  0]
 [ 0  0  0  1  0]]
0.7790370875825302
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.42
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.57
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         1
         1.0       0.70      0.80      0.74        20
         2.0       0.78      0.74      0.76        34
         3.0       0.88      0.90      0.89        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.79        87
   macro avg       0.47      0.49      0.48        87
weighted avg       0.78      0.79      0.78        87

[[ 0  1  0  0  0]
 [ 0 16  4  0  0]
 [ 0  6 25  3  0]
 [ 0  0  3 28  0]
 [ 0  0  0  1  0]]
0.7838718648662513
87 87 87
Filename	True Label	Prediction
0609	2.0	2.0
0610	2.0	2.0
0613	2.0	2.0
0614	2.0	2.0
0615	2.0	2.0
0616	2.0	2.0
0617	2.0	2.0
0625	1.0	2.0
0627	2.0	2.0
0630	1.0	1.0
0637	2.0	2.0
0638	2.0	2.0
0645	2.0	2.0
0715	2.0	2.0
0719	2.0	2.0
0721	2.0	2.0
0804	2.0	2.0
0805	2.0	2.0
0811	2.0	2.0
0828	2.0	2.0
0913	2.0	2.0
0917	2.0	2.0
0922	1.0	2.0
1003	2.0	2.0
1010	1.0	2.0
1014	2.0	2.0
BER0609003	3.0	2.0
BER0611003	3.0	3.0
BER0611005	3.0	2.0
KYJ0611005A	1.0	1.0
KYJ0611005B	1.0	1.0
KYJ0611009B	1.0	2.0
LIB0611001B	1.0	1.0
LIB0611002B	1.0	1.0
LIB0611003A	2.0	1.0
LIB0611004A	1.0	1.0
LIB0611011	2.0	2.0
MOS0611012	3.0	3.0
PHA0111001A	1.0	1.0
PHA0111004A	1.0	1.0
PHA0111005A	2.0	1.0
PHA0112007B	1.0	1.0
PHA0209031	3.0	3.0
PHA0411008A	2.0	1.0
PHA0411009A	1.0	1.0
PHA0411009B	2.0	1.0
PHA0411010A	0.0	1.0
PHA0411010B	1.0	1.0
PHA0411011B	1.0	1.0
PHA0411029	3.0	2.0
PHA0411042	3.0	3.0
PHA0411044	4.0	3.0
PHA0411051	3.0	3.0
PHA0411053	3.0	3.0
PHA0509020	3.0	3.0
PHA0509024	3.0	3.0
PHA0509032	3.0	3.0
PHA0509033	2.0	3.0
PHA0509040	3.0	3.0
PHA0509042	3.0	3.0
PHA0510004A	1.0	1.0
PHA0510004B	1.0	1.0
PHA0510027	3.0	3.0
PHA0510030	3.0	3.0
PHA0510034	3.0	3.0
PHA0510038	3.0	3.0
PHA0510040	3.0	3.0
PHA0510046	2.0	2.0
PHA0510048	3.0	3.0
PHA0610005A	1.0	1.0
PHA0610006A	1.0	1.0
PHA0610015	3.0	3.0
PHA0610016	3.0	3.0
PHA0610025	3.0	3.0
PHA0710011	3.0	3.0
PHA0710012	3.0	3.0
PHA0809010	2.0	2.0
PHA0810004	3.0	3.0
PHA0810006	3.0	3.0
PHA0811020	2.0	3.0
PHA1109001	2.0	1.0
PHA1109006	2.0	3.0
PHA1109024	3.0	3.0
PHA1109028	3.0	3.0
PHA1110015	3.0	3.0
PHA1111001A	2.0	1.0
VAR0909007	3.0	3.0
LANGUAGE: CZ, 4th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.06
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.72
              precision    recall  f1-score   support

         1.0       0.67      0.10      0.17        20
         2.0       0.60      0.82      0.69        34
         3.0       0.81      0.94      0.87        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.69        86
   macro avg       0.52      0.46      0.43        86
weighted avg       0.68      0.69      0.63        86

[[ 2 17  1  0]
 [ 1 28  5  0]
 [ 0  2 29  0]
 [ 0  0  1  0]]
0.6258169140069044
86 86 86



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.58
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.61
              precision    recall  f1-score   support

         1.0       0.74      0.70      0.72        20
         2.0       0.81      0.74      0.77        34
         3.0       0.81      0.94      0.87        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.79        86
   macro avg       0.59      0.59      0.59        86
weighted avg       0.78      0.79      0.78        86

[[14  5  1  0]
 [ 4 25  5  0]
 [ 1  1 29  0]
 [ 0  0  1  0]]
0.7831237373063128
86 86 86



======== Epoch 3 / 4 ========
Training...
Elapsed time 11

  Average training loss: 0.48
  Training epoch took: 12
Running Validation...
  Average evaluation loss: 0.62
              precision    recall  f1-score   support

         1.0       0.76      0.80      0.78        20
         2.0       0.86      0.74      0.79        34
         3.0       0.81      0.94      0.87        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.81        86
   macro avg       0.61      0.62      0.61        86
weighted avg       0.81      0.81      0.81        86

[[16  3  1  0]
 [ 4 25  5  0]
 [ 1  1 29  0]
 [ 0  0  1  0]]
0.8073221392698878
86 86 86



======== Epoch 4 / 4 ========
Training...
Elapsed time 11

  Average training loss: 0.42
  Training epoch took: 12
Running Validation...
  Average evaluation loss: 0.61
              precision    recall  f1-score   support

         1.0       0.72      0.65      0.68        20
         2.0       0.81      0.74      0.77        34
         3.0       0.81      0.97      0.88        31
         4.0       0.00      0.00      0.00         1

    accuracy                           0.79        86
   macro avg       0.58      0.59      0.58        86
weighted avg       0.78      0.79      0.78        86

[[13  6  1  0]
 [ 4 25  5  0]
 [ 1  0 30  0]
 [ 0  0  1  0]]
0.7812906727515411
86 86 86
Filename	True Label	Prediction
0605	2.0	2.0
0623	2.0	2.0
0626	2.0	2.0
0632	1.0	2.0
0633	2.0	2.0
0642	2.0	2.0
0722	2.0	2.0
0806	2.0	2.0
0812	1.0	2.0
0813	2.0	2.0
0814	1.0	2.0
0820	1.0	1.0
0821	2.0	2.0
0823	2.0	2.0
0825	2.0	2.0
0827	2.0	2.0
0829	2.0	2.0
0902	2.0	2.0
0906	2.0	2.0
0907	2.0	2.0
0911	2.0	2.0
0925	2.0	2.0
0928	2.0	2.0
0930	2.0	2.0
1008	2.0	2.0
1015	2.0	2.0
1020	2.0	2.0
1021	2.0	2.0
1114	2.0	2.0
KYJ0611006A	1.0	1.0
LIB0611002A	1.0	2.0
MOS0611014	1.0	3.0
PAR1011009B	1.0	1.0
PAR1011014	3.0	3.0
PAR1011016	3.0	3.0
PHA0111003A	1.0	1.0
PHA0111003B	1.0	1.0
PHA0111014	2.0	3.0
PHA0112002A	1.0	1.0
PHA0112006A	3.0	1.0
PHA0112009A	2.0	1.0
PHA0112012B	1.0	2.0
PHA0209013	1.0	1.0
PHA0209039	3.0	3.0
PHA0411011A	1.0	1.0
PHA0411012A	2.0	1.0
PHA0411030	3.0	3.0
PHA0411035	3.0	3.0
PHA0411038	3.0	3.0
PHA0411043	3.0	3.0
PHA0411047	3.0	3.0
PHA0411055	3.0	3.0
PHA0411059	3.0	3.0
PHA0509002	1.0	1.0
PHA0509015	3.0	3.0
PHA0509027	2.0	3.0
PHA0509030	3.0	3.0
PHA0509034	2.0	3.0
PHA0509036	3.0	3.0
PHA0509039	3.0	3.0
PHA0509043	3.0	3.0
PHA0510010A	1.0	2.0
PHA0510035	3.0	3.0
PHA0510050	3.0	3.0
PHA0610018	3.0	3.0
PHA0710009	3.0	3.0
PHA0710010	3.0	3.0
PHA0710016	3.0	3.0
PHA0710021	4.0	3.0
PHA0810002	3.0	3.0
PHA0810009	3.0	3.0
PHA0810011	3.0	3.0
PHA1109004	3.0	3.0
PHA1109007	2.0	3.0
PHA1110001B	2.0	1.0
PHA1110002B	2.0	1.0
PHA1110016	2.0	3.0
PHA1111002A	1.0	1.0
PHA1111004A	1.0	1.0
PHA1111006A	1.0	1.0
PHA1111009A	1.0	1.0
VAR0909004	3.0	3.0
VAR0910005	3.0	3.0
VAR0910006	3.0	3.0
VAR0910007	3.0	3.0
VAR0910010	3.0	3.0
Averaged weighted F1-scores 0.783190223714674
144.90552995391704 65.35717405024758
LABEL SET ['A1', 'A2', 'B1', 'B2']
LANGUAGE: CZ, 0th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.77
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.52
              precision    recall  f1-score   support

         0.0       0.96      0.92      0.94        52
         1.0       0.57      0.95      0.71        22
         2.0       0.00      0.00      0.00        12
         3.0       0.00      0.00      0.00         1

    accuracy                           0.79        87
   macro avg       0.38      0.47      0.41        87
weighted avg       0.72      0.79      0.74        87

[[48  4  0  0]
 [ 1 21  0  0]
 [ 1 11  0  0]
 [ 0  1  0  0]]
0.7425539473533422
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.53
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.49
              precision    recall  f1-score   support

         0.0       0.98      0.92      0.95        52
         1.0       0.55      0.95      0.70        22
         2.0       0.00      0.00      0.00        12
         3.0       0.00      0.00      0.00         1

    accuracy                           0.79        87
   macro avg       0.38      0.47      0.41        87
weighted avg       0.73      0.79      0.75        87

[[48  4  0  0]
 [ 1 21  0  0]
 [ 0 12  0  0]
 [ 0  1  0  0]]
0.745123477865028
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.42
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.45
              precision    recall  f1-score   support

         0.0       0.98      0.92      0.95        52
         1.0       0.54      0.91      0.68        22
         2.0       0.00      0.00      0.00        12
         3.0       0.00      0.00      0.00         1

    accuracy                           0.78        87
   macro avg       0.38      0.46      0.41        87
weighted avg       0.72      0.78      0.74        87

[[48  4  0  0]
 [ 1 20  1  0]
 [ 0 12  0  0]
 [ 0  1  0  0]]
0.7395516874890294
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.36
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.46
              precision    recall  f1-score   support

         0.0       0.98      0.92      0.95        52
         1.0       0.67      0.55      0.60        22
         2.0       0.40      0.67      0.50        12
         3.0       0.00      0.00      0.00         1

    accuracy                           0.78        87
   macro avg       0.51      0.53      0.51        87
weighted avg       0.81      0.78      0.79        87

[[48  2  2  0]
 [ 1 12  9  0]
 [ 0  4  8  0]
 [ 0  0  1  0]]
0.7888016387845681
87 87 87
Filename	True Label	Prediction
0603	2.0	1.0
0618	2.0	1.0
0621	2.0	2.0
0627	2.0	2.0
0630	0.0	2.0
0633	2.0	2.0
0636	2.0	2.0
0722	2.0	2.0
0802	1.0	2.0
0806	2.0	1.0
0808	1.0	2.0
0812	1.0	2.0
0814	1.0	2.0
0816	3.0	2.0
0828	1.0	2.0
0903	1.0	2.0
0907	2.0	2.0
0914	1.0	2.0
0915	2.0	2.0
0922	1.0	2.0
0927	2.0	1.0
1004	1.0	1.0
1005	1.0	1.0
1017	1.0	1.0
1020	1.0	2.0
1022	1.0	1.0
1113	1.0	0.0
9999	0.0	2.0
KYJ0611003A	1.0	1.0
KYJ0611005A	1.0	1.0
KYJ0611006A	1.0	1.0
KYJ0611006B	0.0	0.0
KYJ0611009A	1.0	1.0
KYJ0611009B	0.0	0.0
LIB0611003A	1.0	1.0
LIB0611004B	0.0	0.0
LON0610002A	1.0	1.0
LON0610002B	0.0	0.0
LON0611002A	1.0	1.0
PAR1011009A	2.0	2.0
PHA0112006B	0.0	0.0
PHA0112007B	0.0	0.0
PHA0112009B	0.0	0.0
PHA0112012B	0.0	0.0
PHA0209028	0.0	0.0
PHA0209034	0.0	0.0
PHA0210008	0.0	0.0
PHA0411008A	0.0	1.0
PHA0411011B	0.0	0.0
PHA0411051	0.0	0.0
PHA0411053	0.0	0.0
PHA0411062	0.0	0.0
PHA0509021	0.0	0.0
PHA0509024	0.0	0.0
PHA0509026	0.0	0.0
PHA0509034	0.0	0.0
PHA0509036	0.0	0.0
PHA0509038	0.0	0.0
PHA0509039	0.0	0.0
PHA0509044	0.0	0.0
PHA0510003B	0.0	0.0
PHA0510004A	0.0	0.0
PHA0510030	0.0	0.0
PHA0510031	0.0	0.0
PHA0510034	0.0	0.0
PHA0510047	0.0	0.0
PHA0610005B	0.0	0.0
PHA0610018	0.0	0.0
PHA0610019A	0.0	1.0
PHA0710011	0.0	0.0
PHA0710015	0.0	0.0
PHA0710021	0.0	0.0
PHA0810003	0.0	0.0
PHA0810009	0.0	0.0
PHA0810012	0.0	0.0
PHA1109003	0.0	0.0
PHA1109007	0.0	0.0
PHA1109026	0.0	0.0
PHA1110003A	1.0	1.0
PHA1110013	0.0	0.0
PHA1110022	0.0	0.0
PHA1111003B	0.0	0.0
PHA1111008B	0.0	0.0
VAR0909003	0.0	0.0
VAR0910006	0.0	0.0
VAR0910007	0.0	0.0
VAR0910009	0.0	0.0
LANGUAGE: CZ, 1th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.82
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.52
              precision    recall  f1-score   support

         0.0       0.96      0.94      0.95        52
         1.0       0.58      1.00      0.74        21
         2.0       0.00      0.00      0.00        13
         3.0       0.00      0.00      0.00         1

    accuracy                           0.80        87
   macro avg       0.39      0.49      0.42        87
weighted avg       0.72      0.80      0.75        87

[[49  3  0  0]
 [ 0 21  0  0]
 [ 2 11  0  0]
 [ 0  1  0  0]]
0.7465449697225991
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.48
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.43
              precision    recall  f1-score   support

         0.0       1.00      0.94      0.97        52
         1.0       0.55      1.00      0.71        21
         2.0       0.00      0.00      0.00        13
         3.0       0.00      0.00      0.00         1

    accuracy                           0.80        87
   macro avg       0.39      0.49      0.42        87
weighted avg       0.73      0.80      0.75        87

[[49  3  0  0]
 [ 0 21  0  0]
 [ 0 13  0  0]
 [ 0  1  0  0]]
0.7517769895049119
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.43
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.41
              precision    recall  f1-score   support

         0.0       1.00      0.94      0.97        52
         1.0       0.75      0.57      0.65        21
         2.0       0.50      0.85      0.63        13
         3.0       0.00      0.00      0.00         1

    accuracy                           0.83        87
   macro avg       0.56      0.59      0.56        87
weighted avg       0.85      0.83      0.83        87

[[49  2  1  0]
 [ 0 12  9  0]
 [ 0  2 11  0]
 [ 0  0  1  0]]
0.8304424797425819
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.38
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.41
              precision    recall  f1-score   support

         0.0       1.00      0.94      0.97        52
         1.0       0.52      0.76      0.62        21
         2.0       0.29      0.15      0.20        13
         3.0       0.00      0.00      0.00         1

    accuracy                           0.77        87
   macro avg       0.45      0.46      0.45        87
weighted avg       0.76      0.77      0.76        87

[[49  3  0  0]
 [ 0 16  5  0]
 [ 0 11  2  0]
 [ 0  1  0  0]]
0.7583738214670271
87 87 87
Filename	True Label	Prediction
0601	1.0	1.0
0608	0.0	1.0
0622	1.0	1.0
0624	2.0	1.0
0632	1.0	2.0
0637	2.0	1.0
0642	2.0	1.0
0644	1.0	2.0
0717	1.0	1.0
0719	1.0	1.0
0723	2.0	1.0
0725	1.0	2.0
0805	2.0	1.0
0807	2.0	2.0
0811	2.0	1.0
0818	1.0	1.0
0819	3.0	1.0
0821	2.0	1.0
0824	1.0	2.0
0913	2.0	1.0
0918	1.0	2.0
0919	2.0	1.0
0923	2.0	2.0
0928	2.0	1.0
1015	1.0	1.0
1021	1.0	1.0
1111	1.0	1.0
1114	1.0	1.0
BER0609003	0.0	0.0
BER0611005	0.0	0.0
KYJ0611005B	0.0	0.0
LIB0611011	0.0	0.0
LON0611002B	0.0	0.0
LON0611003	0.0	0.0
MOS0611012	0.0	0.0
PAR1011008A	2.0	1.0
PAR1011009B	0.0	0.0
PAR1011013	0.0	0.0
PAR1011016	0.0	0.0
PHA0111002B	0.0	0.0
PHA0111003A	1.0	1.0
PHA0111004A	1.0	1.0
PHA0111004B	0.0	0.0
PHA0209001	0.0	0.0
PHA0209008	0.0	0.0
PHA0209024	0.0	0.0
PHA0210007	0.0	0.0
PHA0411012B	0.0	0.0
PHA0411029	0.0	0.0
PHA0411033	0.0	0.0
PHA0411042	0.0	0.0
PHA0411047	0.0	0.0
PHA0411058	0.0	0.0
PHA0411059	0.0	0.0
PHA0509022	0.0	0.0
PHA0509025	0.0	0.0
PHA0510002B	0.0	0.0
PHA0510003A	0.0	1.0
PHA0510004B	0.0	0.0
PHA0510013A	1.0	1.0
PHA0510038	0.0	0.0
PHA0510046	0.0	0.0
PHA0510049	0.0	0.0
PHA0510050	0.0	0.0
PHA0610005A	1.0	1.0
PHA0610007A	0.0	1.0
PHA0610015	0.0	0.0
PHA0709008	0.0	0.0
PHA0710012	0.0	0.0
PHA0710014	0.0	0.0
PHA0710018	0.0	0.0
PHA0809009	0.0	0.0
PHA0809010	0.0	0.0
PHA0810004	0.0	0.0
PHA0811014	0.0	0.0
PHA0811020	0.0	0.0
PHA1109001	0.0	0.0
PHA1109004	0.0	0.0
PHA1109028	0.0	0.0
PHA1110001A	1.0	1.0
PHA1110002B	0.0	0.0
PHA1110004A	1.0	1.0
PHA1110015	0.0	0.0
PHA1110021	0.0	0.0
PHA1111003A	1.0	1.0
VAR0909005	0.0	0.0
VAR0909006	0.0	0.0
LANGUAGE: CZ, 2th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.77
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.46
              precision    recall  f1-score   support

         0.0       0.98      0.98      0.98        52
         1.0       0.57      0.95      0.71        21
         2.0       0.00      0.00      0.00        13
         3.0       0.00      0.00      0.00         1

    accuracy                           0.82        87
   macro avg       0.39      0.48      0.42        87
weighted avg       0.72      0.82      0.76        87

[[51  1  0  0]
 [ 1 20  0  0]
 [ 0 13  0  0]
 [ 0  1  0  0]]
0.7586206896551724
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.48
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.57
              precision    recall  f1-score   support

         0.0       0.93      0.98      0.95        52
         1.0       0.53      0.81      0.64        21
         2.0       0.00      0.00      0.00        13
         3.0       0.00      0.00      0.00         1

    accuracy                           0.78        87
   macro avg       0.36      0.45      0.40        87
weighted avg       0.68      0.78      0.72        87

[[51  1  0  0]
 [ 4 17  0  0]
 [ 0 13  0  0]
 [ 0  1  0  0]]
0.7246182939212812
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.46
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.40
              precision    recall  f1-score   support

         0.0       0.98      0.98      0.98        52
         1.0       0.59      0.95      0.73        21
         2.0       1.00      0.08      0.14        13
         3.0       0.00      0.00      0.00         1

    accuracy                           0.83        87
   macro avg       0.64      0.50      0.46        87
weighted avg       0.88      0.83      0.78        87

[[51  1  0  0]
 [ 1 20  0  0]
 [ 0 12  1  0]
 [ 0  1  0  0]]
0.7831019555157488
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.37
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.39
              precision    recall  f1-score   support

         0.0       1.00      0.98      0.99        52
         1.0       0.61      0.90      0.73        21
         2.0       0.60      0.23      0.33        13
         3.0       0.00      0.00      0.00         1

    accuracy                           0.84        87
   macro avg       0.55      0.53      0.51        87
weighted avg       0.84      0.84      0.82        87

[[51  1  0  0]
 [ 0 19  2  0]
 [ 0 10  3  0]
 [ 0  1  0  0]]
0.8180992277075302
87 87 87
Filename	True Label	Prediction
0602	1.0	2.0
0606	2.0	1.0
0609	1.0	1.0
0612	2.0	1.0
0615	1.0	1.0
0620	1.0	1.0
0625	1.0	1.0
0626	2.0	1.0
0629	2.0	1.0
0631	2.0	2.0
0641	1.0	1.0
0714	2.0	1.0
0716	2.0	1.0
0718	1.0	1.0
0720	2.0	1.0
0813	2.0	2.0
0820	1.0	1.0
0827	2.0	1.0
0904	1.0	1.0
0906	2.0	2.0
0921	2.0	1.0
1014	1.0	1.0
1018	1.0	1.0
1019	1.0	1.0
1023	1.0	2.0
1115	1.0	1.0
1116	1.0	1.0
BER0611006	0.0	0.0
LON0611004B	0.0	0.0
MOS0509004	0.0	0.0
MOS0611013	0.0	0.0
PAR1011017	0.0	0.0
PHA0111002A	0.0	1.0
PHA0111010	0.0	0.0
PHA0111011	0.0	0.0
PHA0111012	0.0	0.0
PHA0111016	0.0	0.0
PHA0111018	0.0	0.0
PHA0112002A	1.0	1.0
PHA0112006A	3.0	1.0
PHA0112009A	2.0	1.0
PHA0411008B	0.0	0.0
PHA0411010A	1.0	1.0
PHA0411038	0.0	0.0
PHA0411039	0.0	0.0
PHA0411043	0.0	0.0
PHA0411054	0.0	0.0
PHA0411061	0.0	0.0
PHA0509002	0.0	0.0
PHA0509017	0.0	0.0
PHA0509028	0.0	0.0
PHA0509032	0.0	0.0
PHA0509033	0.0	0.0
PHA0509043	0.0	0.0
PHA0509045	0.0	0.0
PHA0510010A	1.0	1.0
PHA0510013B	0.0	0.0
PHA0510023	0.0	0.0
PHA0510029	0.0	0.0
PHA0510035	0.0	0.0
PHA0510036	0.0	0.0
PHA0510039	0.0	0.0
PHA0510040	0.0	0.0
PHA0610006B	0.0	0.0
PHA0610007B	0.0	0.0
PHA0610019B	0.0	0.0
PHA0610025	0.0	0.0
PHA0710019	0.0	0.0
PHA0810006	0.0	0.0
PHA0811012	0.0	0.0
PHA0811013	0.0	0.0
PHA1109002	0.0	0.0
PHA1109005	0.0	0.0
PHA1109006	0.0	0.0
PHA1109027	0.0	0.0
PHA1110002A	1.0	1.0
PHA1110016	0.0	0.0
PHA1110017	0.0	0.0
PHA1110019	0.0	0.0
PHA1111001B	0.0	0.0
PHA1111002A	1.0	1.0
PHA1111006B	0.0	0.0
PHA1111008A	1.0	1.0
VAR0209036	0.0	0.0
VAR0909008	0.0	0.0
VAR0910005	0.0	0.0
VAR0910010	0.0	0.0
LANGUAGE: CZ, 3th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.68
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.73
              precision    recall  f1-score   support

         0.0       0.96      0.87      0.91        52
         1.0       0.47      0.90      0.62        21
         2.0       0.00      0.00      0.00        13
         3.0       0.00      0.00      0.00         1

    accuracy                           0.74        87
   macro avg       0.36      0.44      0.38        87
weighted avg       0.69      0.74      0.69        87

[[45  7  0  0]
 [ 2 19  0  0]
 [ 0 13  0  0]
 [ 0  1  0  0]]
0.6937321205269198
87 87 87



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.44
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.66
              precision    recall  f1-score   support

         0.0       0.96      0.87      0.91        52
         1.0       0.47      0.90      0.62        21
         2.0       0.00      0.00      0.00        13
         3.0       0.00      0.00      0.00         1

    accuracy                           0.74        87
   macro avg       0.36      0.44      0.38        87
weighted avg       0.69      0.74      0.69        87

[[45  7  0  0]
 [ 2 19  0  0]
 [ 0 13  0  0]
 [ 0  1  0  0]]
0.6937321205269198
87 87 87



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.39
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.63
              precision    recall  f1-score   support

         0.0       0.98      0.87      0.92        52
         1.0       0.72      0.62      0.67        21
         2.0       0.52      0.92      0.67        13
         3.0       0.00      0.00      0.00         1

    accuracy                           0.80        87
   macro avg       0.56      0.60      0.56        87
weighted avg       0.84      0.80      0.81        87

[[45  4  3  0]
 [ 1 13  7  0]
 [ 0  1 12  0]
 [ 0  0  1  0]]
0.8094456173273907
87 87 87



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.33
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.66
              precision    recall  f1-score   support

         0.0       0.96      0.87      0.91        52
         1.0       0.47      0.81      0.60        21
         2.0       0.50      0.15      0.24        13
         3.0       0.00      0.00      0.00         1

    accuracy                           0.74        87
   macro avg       0.48      0.46      0.44        87
weighted avg       0.76      0.74      0.72        87

[[45  7  0  0]
 [ 2 17  2  0]
 [ 0 11  2  0]
 [ 0  1  0  0]]
0.7225042136966979
87 87 87
Filename	True Label	Prediction
0607	2.0	1.0
0610	1.0	1.0
0613	0.0	1.0
0614	2.0	1.0
0617	0.0	1.0
0619	2.0	1.0
0623	0.0	1.0
0628	2.0	1.0
0634	3.0	1.0
0638	1.0	1.0
0640	2.0	1.0
0721	2.0	1.0
0724	2.0	1.0
0804	2.0	1.0
0810	1.0	1.0
0823	1.0	2.0
0826	2.0	1.0
0829	1.0	2.0
0912	2.0	2.0
0916	1.0	1.0
0920	2.0	1.0
0924	1.0	1.0
0925	2.0	1.0
0926	2.0	2.0
0929	0.0	1.0
0930	1.0	1.0
1001	1.0	1.0
1008	1.0	1.0
1016	1.0	1.0
1112	1.0	1.0
1117	1.0	1.0
BER0611007	0.0	0.0
KYJ0611004A	1.0	1.0
LIB0611002A	0.0	1.0
LIB0611004A	1.0	1.0
LON0611004A	1.0	1.0
PHA0111003B	0.0	0.0
PHA0111005B	0.0	0.0
PHA0112002B	0.0	0.0
PHA0112003B	0.0	0.0
PHA0112007A	1.0	0.0
PHA0112012A	1.0	0.0
PHA0209031	0.0	0.0
PHA0209039	0.0	0.0
PHA0210004	0.0	0.0
PHA0411009A	1.0	1.0
PHA0411009B	0.0	0.0
PHA0411027	0.0	0.0
PHA0411028	0.0	0.0
PHA0411032	0.0	0.0
PHA0411044	0.0	0.0
PHA0411055	0.0	0.0
PHA0411060	0.0	0.0
PHA0509007	0.0	0.0
PHA0509013	0.0	0.0
PHA0509015	0.0	0.0
PHA0509019	0.0	0.0
PHA0509027	0.0	0.0
PHA0509030	0.0	0.0
PHA0509035	0.0	0.0
PHA0509040	0.0	0.0
PHA0509042	0.0	0.0
PHA0510010B	0.0	0.0
PHA0510027	0.0	0.0
PHA0510032	0.0	0.0
PHA0610006A	0.0	1.0
PHA0610026	0.0	0.0
PHA0710009	0.0	0.0
PHA0710013	0.0	0.0
PHA0710016	0.0	0.0
PHA0710017	0.0	0.0
PHA0810011	0.0	0.0
PHA0810015	0.0	0.0
PHA0811017	0.0	0.0
PHA0811019	0.0	0.0
PHA1109008	0.0	0.0
PHA1109023	0.0	0.0
PHA1109024	0.0	0.0
PHA1110003B	0.0	0.0
PHA1111001A	1.0	1.0
PHA1111002B	0.0	0.0
PHA1111004A	1.0	1.0
PHA1111004B	0.0	0.0
PHA1111006A	0.0	1.0
TI071122B	0.0	0.0
VAR0909007	0.0	0.0
VAR0909010	0.0	0.0
LANGUAGE: CZ, 4th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.73
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.47
              precision    recall  f1-score   support

         0.0       0.93      0.96      0.94        53
         1.0       0.65      0.95      0.77        21
         2.0       0.00      0.00      0.00        12

    accuracy                           0.83        86
   macro avg       0.52      0.64      0.57        86
weighted avg       0.73      0.83      0.77        86

[[51  2  0]
 [ 1 20  0]
 [ 3  9  0]]
0.7698767640628105
86 86 86



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.50
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.40
              precision    recall  f1-score   support

         0.0       1.00      0.96      0.98        53
         1.0       0.60      1.00      0.75        21
         2.0       0.00      0.00      0.00        12

    accuracy                           0.84        86
   macro avg       0.53      0.65      0.58        86
weighted avg       0.76      0.84      0.79        86

[[51  2  0]
 [ 0 21  0]
 [ 0 12  0]]
0.7875670840787119
86 86 86



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.41
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.40
              precision    recall  f1-score   support

         0.0       1.00      0.96      0.98        53
         1.0       0.60      1.00      0.75        21
         2.0       0.00      0.00      0.00        12

    accuracy                           0.84        86
   macro avg       0.53      0.65      0.58        86
weighted avg       0.76      0.84      0.79        86

[[51  2  0]
 [ 0 21  0]
 [ 0 12  0]]
0.7875670840787119
86 86 86



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.36
  Training epoch took: 13
Running Validation...
  Average evaluation loss: 0.39
              precision    recall  f1-score   support

         0.0       1.00      0.96      0.98        53
         1.0       0.60      1.00      0.75        21
         2.0       0.00      0.00      0.00        12

    accuracy                           0.84        86
   macro avg       0.53      0.65      0.58        86
weighted avg       0.76      0.84      0.79        86

[[51  2  0]
 [ 0 21  0]
 [ 0 12  0]]
0.7875670840787119
86 86 86
Filename	True Label	Prediction
0604	1.0	1.0
0605	2.0	1.0
0611	1.0	1.0
0616	2.0	1.0
0635	2.0	1.0
0639	1.0	1.0
0643	1.0	1.0
0645	2.0	1.0
0715	2.0	1.0
0801	1.0	1.0
0803	1.0	1.0
0809	1.0	1.0
0815	2.0	1.0
0817	1.0	1.0
0822	1.0	1.0
0825	1.0	1.0
0901	2.0	1.0
0902	2.0	1.0
0905	1.0	1.0
0910	2.0	1.0
0911	2.0	1.0
0917	2.0	1.0
1002	1.0	1.0
1003	1.0	1.0
1006	1.0	1.0
1007	1.0	1.0
1009	2.0	1.0
1010	1.0	1.0
BER0611003	0.0	0.0
LIB0611001A	0.0	1.0
LIB0611001B	0.0	0.0
LIB0611002B	0.0	0.0
MOS0509001	0.0	0.0
MOS0611014	0.0	0.0
MOS0611015	0.0	0.0
PAR1011014	0.0	0.0
PAR1011015	0.0	0.0
PAR1011018	0.0	0.0
PHA0111001A	1.0	1.0
PHA0111001B	0.0	0.0
PHA0111005A	1.0	1.0
PHA0111014	0.0	0.0
PHA0111015	0.0	0.0
PHA0112003A	1.0	1.0
PHA0209013	0.0	0.0
PHA0209026	0.0	0.0
PHA0209038	0.0	0.0
PHA0210001	0.0	0.0
PHA0411010B	0.0	0.0
PHA0411011A	1.0	1.0
PHA0411012A	1.0	1.0
PHA0411030	0.0	0.0
PHA0411031	0.0	0.0
PHA0411034	0.0	0.0
PHA0411035	0.0	0.0
PHA0411036	0.0	0.0
PHA0411037	0.0	0.0
PHA0411041	0.0	0.0
PHA0411045	0.0	0.0
PHA0411056	0.0	0.0
PHA0509018	0.0	0.0
PHA0509020	0.0	0.0
PHA0509031	0.0	0.0
PHA0509037	0.0	0.0
PHA0509041	0.0	0.0
PHA0510002A	0.0	0.0
PHA0510037	0.0	0.0
PHA0510048	0.0	0.0
PHA0610016	0.0	0.0
PHA0610017	0.0	0.0
PHA0710010	0.0	0.0
PHA0810001	0.0	0.0
PHA0810002	0.0	0.0
PHA0810008	0.0	0.0
PHA0810010	0.0	0.0
PHA0811010	0.0	0.0
PHA0811016	0.0	0.0
PHA1109025	0.0	0.0
PHA1110001B	0.0	0.0
PHA1110014	0.0	0.0
PHA1111009A	0.0	1.0
ST071122B	0.0	0.0
VAR0909004	0.0	0.0
VAR0909009	0.0	0.0
VAR0910004	0.0	0.0
VAR0910011	0.0	0.0
Averaged weighted F1-scores 0.775069197146907
MONOLINGUAL Experiments with:  IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1']
LANGUAGE: IT, 0th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.65
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.59
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.86      0.67      0.76        76
         2.0       0.74      0.95      0.83        79

    accuracy                           0.79       160
   macro avg       0.54      0.54      0.53       160
weighted avg       0.78      0.79      0.77       160

[[ 0  4  1]
 [ 0 51 25]
 [ 0  4 75]]
0.7703472222222223
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.53
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.62
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.85      0.72      0.78        76
         2.0       0.78      0.94      0.85        79

    accuracy                           0.81       160
   macro avg       0.54      0.55      0.54       160
weighted avg       0.79      0.81      0.79       160

[[ 0  5  0]
 [ 0 55 21]
 [ 0  5 74]]
0.7905386402543411
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.47
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.54
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.83      0.76      0.79        76
         2.0       0.80      0.91      0.85        79

    accuracy                           0.81       160
   macro avg       0.54      0.56      0.55       160
weighted avg       0.79      0.81      0.80       160

[[ 0  5  0]
 [ 0 58 18]
 [ 0  7 72]]
0.7981073194455702
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.43
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.54
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.83      0.83      0.83        76
         2.0       0.85      0.90      0.87        79

    accuracy                           0.84       160
   macro avg       0.56      0.58      0.57       160
weighted avg       0.81      0.84      0.82       160

[[ 0  5  0]
 [ 0 63 13]
 [ 0  8 71]]
0.8238880368098158
160 160 160
Filename	True Label	Prediction
1325_1001008	2.0	2.0
1325_1001009	2.0	2.0
1325_1001011	2.0	2.0
1325_1001016	2.0	2.0
1325_1001020	2.0	2.0
1325_1001023	2.0	2.0
1325_1001036	2.0	2.0
1325_1001041	2.0	2.0
1325_1001044	2.0	2.0
1325_1001046	2.0	2.0
1325_1001054	2.0	2.0
1325_1001059	2.0	2.0
1325_1001075	1.0	2.0
1325_1001077	2.0	2.0
1325_1001087	2.0	2.0
1325_1001092	2.0	2.0
1325_1001093	2.0	2.0
1325_1001107	2.0	2.0
1325_1001111	2.0	2.0
1325_1001121	2.0	2.0
1325_1001124	2.0	2.0
1325_1001136	2.0	2.0
1325_1001138	2.0	2.0
1325_9000059	2.0	2.0
1325_9000087	2.0	2.0
1325_9000090	2.0	2.0
1325_9000102	2.0	2.0
1325_9000106	2.0	2.0
1325_9000137	2.0	2.0
1325_9000140	2.0	2.0
1325_9000187	2.0	2.0
1325_9000213	2.0	2.0
1325_9000239	2.0	2.0
1325_9000279	2.0	2.0
1325_9000304	2.0	2.0
1325_9000322	2.0	2.0
1325_9000503	2.0	2.0
1325_9000505	2.0	2.0
1325_9000675	2.0	2.0
1365_0100004	1.0	2.0
1365_0100006	2.0	2.0
1365_0100012	2.0	1.0
1365_0100017	2.0	2.0
1365_0100031	2.0	1.0
1365_0100057	2.0	2.0
1365_0100058	2.0	2.0
1365_0100067	1.0	2.0
1365_0100073	2.0	2.0
1365_0100074	1.0	2.0
1365_0100119	2.0	2.0
1365_0100125	2.0	2.0
1365_0100135	2.0	1.0
1365_0100137	2.0	2.0
1365_0100148	2.0	2.0
1365_0100151	1.0	1.0
1365_0100164	2.0	2.0
1365_0100167	1.0	2.0
1365_0100169	2.0	2.0
1365_0100171	1.0	2.0
1365_0100176	2.0	2.0
1365_0100178	2.0	2.0
1365_0100187	2.0	2.0
1365_0100192	2.0	2.0
1365_0100198	1.0	2.0
1365_0100199	2.0	2.0
1365_0100200	2.0	2.0
1365_0100211	2.0	2.0
1365_0100217	2.0	2.0
1365_0100223	2.0	2.0
1365_0100228	1.0	2.0
1365_0100230	2.0	2.0
1365_0100231	2.0	2.0
1365_0100257	2.0	2.0
1365_0100268	1.0	2.0
1365_0100270	2.0	2.0
1365_0100279	2.0	2.0
1365_0100282	2.0	2.0
1365_0100285	2.0	2.0
1365_0100286	1.0	1.0
1365_0100289	2.0	2.0
1365_0100448	1.0	2.0
1365_0100458	2.0	2.0
1365_0100474	2.0	2.0
1365_0100475	2.0	2.0
1365_0100476	2.0	2.0
1365_0100477	1.0	2.0
1385_0000036	1.0	1.0
1385_0000038	1.0	1.0
1385_0000051	1.0	1.0
1385_0000097	1.0	1.0
1385_0000123	1.0	1.0
1385_0000130	1.0	1.0
1385_0001103	1.0	1.0
1385_0001107	1.0	1.0
1385_0001124	1.0	1.0
1385_0001125	1.0	1.0
1385_0001129	1.0	1.0
1385_0001132	1.0	1.0
1385_0001133	1.0	1.0
1385_0001151	1.0	1.0
1385_0001152	1.0	1.0
1385_0001161	1.0	1.0
1385_0001165	1.0	1.0
1385_0001167	1.0	1.0
1385_0001170	1.0	1.0
1385_0001178	0.0	1.0
1385_0001197	1.0	1.0
1385_0001503	1.0	1.0
1385_0001526	0.0	1.0
1385_0001528	1.0	1.0
1385_0001725	1.0	1.0
1385_0001737	1.0	1.0
1385_0001738	0.0	1.0
1385_0001748	1.0	1.0
1385_0001751	1.0	1.0
1385_0001759	1.0	1.0
1385_0001767	1.0	1.0
1385_0001789	1.0	1.0
1385_0001790	1.0	1.0
1385_0001795	1.0	1.0
1385_0001796	1.0	1.0
1385_0001798	1.0	1.0
1395_0000355	1.0	2.0
1395_0000360	2.0	1.0
1395_0000364	1.0	1.0
1395_0000376	2.0	1.0
1395_0000415	1.0	1.0
1395_0000432	1.0	1.0
1395_0000451	1.0	1.0
1395_0000462	2.0	1.0
1395_0000471	1.0	1.0
1395_0000515	2.0	1.0
1395_0000531	1.0	1.0
1395_0000537	1.0	1.0
1395_0000555	1.0	1.0
1395_0000585	1.0	1.0
1395_0000587	0.0	1.0
1395_0000593	1.0	1.0
1395_0000595	0.0	1.0
1395_0000597	1.0	1.0
1395_0000599	1.0	1.0
1395_0000608	1.0	1.0
1395_0000610	1.0	1.0
1395_0000627	1.0	1.0
1395_0000630	1.0	1.0
1395_0000646	1.0	1.0
1395_0001016	2.0	1.0
1395_0001021	1.0	1.0
1395_0001045	1.0	1.0
1395_0001068	1.0	1.0
1395_0001071	1.0	1.0
1395_0001078	1.0	1.0
1395_0001101	1.0	1.0
1395_0001109	1.0	1.0
1395_0001133	1.0	1.0
1395_0001145	2.0	2.0
1395_0001150	1.0	1.0
1395_0001161	1.0	2.0
1395_0001170	1.0	1.0
1395_0001171	1.0	1.0
LANGUAGE: IT, 1th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.61
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.52
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.81      0.74      0.77        76
         2.0       0.78      0.90      0.84        79

    accuracy                           0.79       160
   macro avg       0.53      0.55      0.54       160
weighted avg       0.77      0.79      0.78       160

[[ 0  5  0]
 [ 0 56 20]
 [ 0  8 71]]
0.7793230223123732
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.43
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.48
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.76      0.96      0.85        76
         2.0       0.95      0.77      0.85        79

    accuracy                           0.84       160
   macro avg       0.57      0.58      0.57       160
weighted avg       0.83      0.84      0.82       160

[[ 0  5  0]
 [ 0 73  3]
 [ 0 18 61]]
0.8244389331598633
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.35
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.51
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.78      0.96      0.86        76
         2.0       0.96      0.81      0.88        79

    accuracy                           0.86       160
   macro avg       0.58      0.59      0.58       160
weighted avg       0.84      0.86      0.84       160

[[ 0  5  0]
 [ 0 73  3]
 [ 0 15 64]]
0.8432317419145658
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.27
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.45
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.83      0.88      0.85        76
         2.0       0.89      0.89      0.89        79

    accuracy                           0.86       160
   macro avg       0.57      0.59      0.58       160
weighted avg       0.83      0.86      0.84       160

[[ 0  5  0]
 [ 0 67  9]
 [ 0  9 70]]
0.8429140127388536
160 160 160
Filename	True Label	Prediction
1325_1001024	2.0	2.0
1325_1001037	2.0	2.0
1325_1001042	2.0	2.0
1325_1001047	2.0	2.0
1325_1001051	2.0	2.0
1325_1001055	2.0	2.0
1325_1001063	2.0	2.0
1325_1001080	2.0	2.0
1325_1001085	2.0	2.0
1325_1001086	2.0	2.0
1325_1001091	2.0	2.0
1325_1001097	1.0	2.0
1325_1001098	2.0	2.0
1325_1001109	2.0	2.0
1325_1001130	2.0	2.0
1325_1001132	2.0	2.0
1325_1001142	2.0	2.0
1325_1001143	2.0	2.0
1325_1001144	2.0	2.0
1325_1001155	2.0	2.0
1325_1001156	2.0	2.0
1325_1001161	2.0	2.0
1325_1001163	2.0	2.0
1325_1001164	2.0	2.0
1325_1001168	2.0	2.0
1325_1001170	2.0	2.0
1325_9000143	2.0	2.0
1325_9000185	2.0	2.0
1325_9000186	2.0	2.0
1325_9000209	2.0	2.0
1325_9000215	2.0	2.0
1325_9000302	2.0	2.0
1325_9000315	1.0	2.0
1325_9000317	2.0	2.0
1325_9000323	2.0	2.0
1325_9000554	2.0	2.0
1325_9000612	1.0	2.0
1325_9000678	2.0	2.0
1325_9000684	2.0	2.0
1325_9000685	2.0	2.0
1365_0100002	2.0	2.0
1365_0100003	1.0	1.0
1365_0100005	1.0	1.0
1365_0100014	2.0	2.0
1365_0100019	1.0	1.0
1365_0100028	2.0	1.0
1365_0100029	1.0	1.0
1365_0100065	1.0	1.0
1365_0100071	2.0	2.0
1365_0100092	2.0	2.0
1365_0100105	2.0	2.0
1365_0100138	2.0	2.0
1365_0100165	2.0	2.0
1365_0100168	2.0	2.0
1365_0100173	2.0	2.0
1365_0100184	2.0	2.0
1365_0100190	2.0	2.0
1365_0100195	1.0	2.0
1365_0100204	2.0	2.0
1365_0100215	2.0	2.0
1365_0100219	2.0	2.0
1365_0100221	2.0	2.0
1365_0100227	2.0	2.0
1365_0100229	2.0	2.0
1365_0100253	1.0	2.0
1365_0100256	2.0	2.0
1365_0100258	2.0	2.0
1365_0100260	2.0	2.0
1365_0100265	2.0	2.0
1365_0100267	2.0	2.0
1365_0100275	2.0	2.0
1365_0100277	2.0	2.0
1365_0100278	2.0	2.0
1365_0100280	1.0	2.0
1365_0100447	2.0	2.0
1365_0100457	2.0	2.0
1365_0100469	2.0	2.0
1365_0100478	2.0	2.0
1365_0100479	2.0	2.0
1365_0100482	2.0	2.0
1385_0000012	1.0	1.0
1385_0000013	0.0	1.0
1385_0000016	1.0	1.0
1385_0000021	1.0	1.0
1385_0000023	1.0	1.0
1385_0000042	1.0	1.0
1385_0000047	1.0	1.0
1385_0000058	1.0	1.0
1385_0000059	1.0	1.0
1385_0000099	1.0	1.0
1385_0000100	1.0	1.0
1385_0000124	1.0	1.0
1385_0000127	1.0	1.0
1385_0001104	1.0	1.0
1385_0001110	1.0	1.0
1385_0001111	1.0	1.0
1385_0001119	1.0	1.0
1385_0001120	1.0	1.0
1385_0001121	1.0	1.0
1385_0001128	0.0	1.0
1385_0001130	1.0	1.0
1385_0001137	1.0	1.0
1385_0001150	1.0	1.0
1385_0001158	1.0	1.0
1385_0001159	1.0	1.0
1385_0001163	1.0	1.0
1385_0001173	0.0	1.0
1385_0001190	1.0	1.0
1385_0001193	2.0	1.0
1385_0001196	1.0	1.0
1385_0001198	2.0	1.0
1385_0001199	1.0	1.0
1385_0001523	1.0	1.0
1385_0001714	1.0	1.0
1385_0001730	2.0	2.0
1385_0001739	1.0	1.0
1385_0001746	1.0	1.0
1385_0001747	1.0	1.0
1385_0001749	1.0	1.0
1385_0001754	1.0	1.0
1385_0001758	1.0	1.0
1385_0001764	1.0	1.0
1385_0001766	1.0	1.0
1385_0001773	1.0	1.0
1385_0001775	1.0	1.0
1385_0001800	1.0	1.0
1395_0000333	1.0	1.0
1395_0000341	1.0	1.0
1395_0000365	2.0	1.0
1395_0000389	0.0	1.0
1395_0000392	1.0	1.0
1395_0000398	2.0	2.0
1395_0000402	1.0	1.0
1395_0000409	2.0	1.0
1395_0000448	1.0	1.0
1395_0000452	1.0	1.0
1395_0000470	1.0	1.0
1395_0000500	1.0	1.0
1395_0000526	1.0	1.0
1395_0000529	1.0	1.0
1395_0000534	1.0	2.0
1395_0000548	1.0	1.0
1395_0000551	2.0	1.0
1395_0000554	1.0	1.0
1395_0000559	1.0	2.0
1395_0000563	1.0	1.0
1395_0000564	1.0	1.0
1395_0000591	0.0	1.0
1395_0000644	1.0	1.0
1395_0001024	1.0	1.0
1395_0001060	2.0	1.0
1395_0001064	1.0	2.0
1395_0001074	1.0	1.0
1395_0001090	2.0	1.0
1395_0001103	1.0	1.0
1395_0001108	1.0	1.0
1395_0001115	2.0	1.0
1395_0001117	1.0	1.0
1395_0001122	1.0	1.0
1395_0001167	1.0	1.0
LANGUAGE: IT, 2th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.64
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.40
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         6
         1.0       0.81      0.89      0.85        76
         2.0       0.89      0.87      0.88        78

    accuracy                           0.85       160
   macro avg       0.57      0.59      0.58       160
weighted avg       0.82      0.85      0.83       160

[[ 0  6  0]
 [ 0 68  8]
 [ 0 10 68]]
0.8342694805194807
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.47
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.42
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         6
         1.0       0.81      0.86      0.83        76
         2.0       0.86      0.88      0.87        78

    accuracy                           0.84       160
   macro avg       0.56      0.58      0.57       160
weighted avg       0.81      0.84      0.82       160

[[ 0  6  0]
 [ 0 65 11]
 [ 0  9 69]]
0.8216244725738397
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.39
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.46
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         6
         1.0       0.81      0.86      0.83        76
         2.0       0.86      0.88      0.87        78

    accuracy                           0.84       160
   macro avg       0.56      0.58      0.57       160
weighted avg       0.81      0.84      0.82       160

[[ 0  6  0]
 [ 0 65 11]
 [ 0  9 69]]
0.8216244725738397
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.30
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.46
              precision    recall  f1-score   support

         0.0       1.00      0.17      0.29         6
         1.0       0.82      0.88      0.85        76
         2.0       0.88      0.87      0.88        78

    accuracy                           0.85       160
   macro avg       0.90      0.64      0.67       160
weighted avg       0.86      0.85      0.84       160

[[ 1  5  0]
 [ 0 67  9]
 [ 0 10 68]]
0.8413043224639795
160 160 160
Filename	True Label	Prediction
1325_1001013	2.0	2.0
1325_1001018	2.0	2.0
1325_1001021	2.0	2.0
1325_1001027	2.0	2.0
1325_1001033	2.0	2.0
1325_1001039	2.0	2.0
1325_1001043	2.0	2.0
1325_1001045	2.0	2.0
1325_1001050	2.0	2.0
1325_1001058	2.0	2.0
1325_1001062	2.0	2.0
1325_1001076	2.0	2.0
1325_1001083	2.0	2.0
1325_1001090	2.0	2.0
1325_1001099	2.0	2.0
1325_1001100	2.0	2.0
1325_1001108	2.0	2.0
1325_1001119	2.0	2.0
1325_1001122	2.0	2.0
1325_1001123	2.0	2.0
1325_1001127	2.0	2.0
1325_1001128	2.0	2.0
1325_1001131	2.0	2.0
1325_1001153	2.0	2.0
1325_1001157	2.0	2.0
1325_1001162	2.0	2.0
1325_1001165	2.0	2.0
1325_9000088	2.0	2.0
1325_9000095	2.0	2.0
1325_9000107	2.0	2.0
1325_9000136	2.0	2.0
1325_9000139	2.0	2.0
1325_9000144	2.0	2.0
1325_9000303	2.0	2.0
1325_9000314	2.0	2.0
1325_9000318	2.0	2.0
1325_9000321	2.0	2.0
1325_9000533	2.0	2.0
1325_9000601	2.0	2.0
1325_9000611	2.0	2.0
1325_9000676	2.0	2.0
1325_9000677	2.0	2.0
1325_9000686	2.0	2.0
1325_9000700	2.0	2.0
1325_9000750	2.0	2.0
1365_0100008	1.0	2.0
1365_0100009	1.0	1.0
1365_0100015	1.0	1.0
1365_0100018	1.0	2.0
1365_0100020	2.0	2.0
1365_0100021	2.0	2.0
1365_0100023	1.0	2.0
1365_0100094	2.0	1.0
1365_0100095	2.0	2.0
1365_0100096	2.0	2.0
1365_0100099	1.0	2.0
1365_0100103	2.0	2.0
1365_0100107	2.0	2.0
1365_0100116	2.0	2.0
1365_0100117	2.0	2.0
1365_0100133	2.0	2.0
1365_0100175	2.0	2.0
1365_0100180	1.0	2.0
1365_0100182	2.0	2.0
1365_0100201	2.0	2.0
1365_0100202	1.0	2.0
1365_0100205	2.0	1.0
1365_0100212	2.0	2.0
1365_0100213	2.0	1.0
1365_0100226	2.0	2.0
1365_0100233	2.0	2.0
1365_0100261	2.0	2.0
1365_0100269	2.0	2.0
1365_0100276	2.0	2.0
1365_0100287	2.0	2.0
1365_0100456	2.0	2.0
1365_0100481	2.0	2.0
1385_0000033	1.0	1.0
1385_0000037	1.0	1.0
1385_0000044	1.0	1.0
1385_0000045	1.0	1.0
1385_0000049	1.0	1.0
1385_0000050	1.0	1.0
1385_0000057	1.0	1.0
1385_0000095	1.0	1.0
1385_0000101	1.0	1.0
1385_0000102	1.0	1.0
1385_0000103	1.0	1.0
1385_0000120	0.0	1.0
1385_0000125	1.0	1.0
1385_0000128	1.0	1.0
1385_0000129	1.0	1.0
1385_0001105	1.0	1.0
1385_0001108	1.0	1.0
1385_0001123	1.0	1.0
1385_0001127	1.0	1.0
1385_0001136	1.0	1.0
1385_0001148	1.0	1.0
1385_0001153	2.0	1.0
1385_0001154	1.0	1.0
1385_0001155	1.0	1.0
1385_0001160	1.0	1.0
1385_0001162	1.0	1.0
1385_0001171	1.0	1.0
1385_0001175	1.0	1.0
1385_0001189	1.0	1.0
1385_0001194	1.0	1.0
1385_0001522	1.0	1.0
1385_0001525	1.0	2.0
1385_0001527	1.0	1.0
1385_0001715	1.0	1.0
1385_0001718	1.0	1.0
1385_0001723	0.0	1.0
1385_0001732	1.0	1.0
1385_0001741	0.0	1.0
1385_0001742	0.0	0.0
1385_0001756	1.0	1.0
1385_0001760	1.0	1.0
1385_0001768	1.0	1.0
1385_0001791	1.0	1.0
1385_0001792	1.0	1.0
1385_0001799	1.0	2.0
1395_0000337	0.0	1.0
1395_0000354	1.0	1.0
1395_0000359	1.0	1.0
1395_0000378	1.0	1.0
1395_0000379	1.0	1.0
1395_0000383	1.0	1.0
1395_0000438	2.0	2.0
1395_0000443	2.0	1.0
1395_0000447	1.0	1.0
1395_0000454	1.0	1.0
1395_0000499	1.0	1.0
1395_0000512	1.0	2.0
1395_0000516	1.0	1.0
1395_0000535	1.0	1.0
1395_0000552	1.0	1.0
1395_0000565	1.0	1.0
1395_0000575	1.0	1.0
1395_0000579	1.0	1.0
1395_0000581	1.0	1.0
1395_0000602	1.0	1.0
1395_0000628	1.0	1.0
1395_0000635	1.0	1.0
1395_0000636	1.0	1.0
1395_0000649	2.0	1.0
1395_0001010	2.0	1.0
1395_0001019	1.0	1.0
1395_0001020	1.0	1.0
1395_0001061	2.0	2.0
1395_0001065	1.0	1.0
1395_0001066	1.0	1.0
1395_0001067	1.0	1.0
1395_0001073	2.0	1.0
1395_0001118	1.0	1.0
1395_0001121	0.0	1.0
1395_0001131	1.0	1.0
1395_0001132	2.0	1.0
1395_0001160	2.0	1.0
1395_0001169	1.0	1.0
LANGUAGE: IT, 3th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.62
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.61
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         6
         1.0       0.75      0.84      0.80        76
         2.0       0.84      0.81      0.82        78

    accuracy                           0.79       160
   macro avg       0.53      0.55      0.54       160
weighted avg       0.77      0.79      0.78       160

[[ 0  6  0]
 [ 0 64 12]
 [ 0 15 63]]
0.7791103397880891
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.45
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.61
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         6
         1.0       0.80      0.72      0.76        76
         2.0       0.77      0.90      0.83        78

    accuracy                           0.78       160
   macro avg       0.52      0.54      0.53       160
weighted avg       0.75      0.78      0.76       160

[[ 0  6  0]
 [ 0 55 21]
 [ 0  8 70]]
0.7641909814323607
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.36
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.56
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         6
         1.0       0.76      0.83      0.79        76
         2.0       0.83      0.82      0.83        78

    accuracy                           0.79       160
   macro avg       0.53      0.55      0.54       160
weighted avg       0.77      0.79      0.78       160

[[ 0  6  0]
 [ 0 63 13]
 [ 0 14 64]]
0.7789957395009128
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.26
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.60
              precision    recall  f1-score   support

         0.0       1.00      0.17      0.29         6
         1.0       0.82      0.72      0.77        76
         2.0       0.77      0.91      0.84        78

    accuracy                           0.79       160
   macro avg       0.86      0.60      0.63       160
weighted avg       0.80      0.79      0.78       160

[[ 1  5  0]
 [ 0 55 21]
 [ 0  7 71]]
0.7833047834518423
160 160 160
Filename	True Label	Prediction
1325_1001010	2.0	2.0
1325_1001012	2.0	2.0
1325_1001015	2.0	2.0
1325_1001019	2.0	2.0
1325_1001029	2.0	2.0
1325_1001032	2.0	2.0
1325_1001035	2.0	2.0
1325_1001048	2.0	2.0
1325_1001053	1.0	2.0
1325_1001056	2.0	2.0
1325_1001082	2.0	2.0
1325_1001094	2.0	2.0
1325_1001096	2.0	2.0
1325_1001120	2.0	2.0
1325_1001135	2.0	2.0
1325_1001152	2.0	2.0
1325_1001159	2.0	2.0
1325_1001160	2.0	2.0
1325_1001167	2.0	2.0
1325_1001169	2.0	2.0
1325_9000099	2.0	2.0
1325_9000104	2.0	2.0
1325_9000105	2.0	2.0
1325_9000188	2.0	2.0
1325_9000211	2.0	2.0
1325_9000214	2.0	2.0
1325_9000237	2.0	2.0
1325_9000241	2.0	2.0
1325_9000278	2.0	2.0
1325_9000316	2.0	2.0
1325_9000319	2.0	2.0
1325_9000504	2.0	2.0
1325_9000534	2.0	2.0
1325_9000536	2.0	2.0
1325_9000602	2.0	2.0
1365_0100010	1.0	1.0
1365_0100011	2.0	2.0
1365_0100013	2.0	2.0
1365_0100016	2.0	2.0
1365_0100056	2.0	2.0
1365_0100061	2.0	2.0
1365_0100063	2.0	2.0
1365_0100066	1.0	2.0
1365_0100072	2.0	2.0
1365_0100097	2.0	2.0
1365_0100100	2.0	2.0
1365_0100101	2.0	2.0
1365_0100102	2.0	2.0
1365_0100104	1.0	2.0
1365_0100106	1.0	2.0
1365_0100118	2.0	2.0
1365_0100134	2.0	2.0
1365_0100136	2.0	2.0
1365_0100145	2.0	2.0
1365_0100166	1.0	2.0
1365_0100170	1.0	2.0
1365_0100174	1.0	2.0
1365_0100177	2.0	2.0
1365_0100181	1.0	2.0
1365_0100185	1.0	2.0
1365_0100186	2.0	2.0
1365_0100191	1.0	2.0
1365_0100194	2.0	2.0
1365_0100220	2.0	2.0
1365_0100225	2.0	2.0
1365_0100251	2.0	2.0
1365_0100252	2.0	2.0
1365_0100259	2.0	2.0
1365_0100262	2.0	2.0
1365_0100263	2.0	2.0
1365_0100266	2.0	2.0
1365_0100281	2.0	2.0
1365_0100288	2.0	2.0
1365_0100290	2.0	2.0
1365_0100471	1.0	2.0
1365_0100480	2.0	2.0
1385_0000011	0.0	1.0
1385_0000034	1.0	1.0
1385_0000035	1.0	2.0
1385_0000041	1.0	1.0
1385_0000053	1.0	1.0
1385_0000054	1.0	1.0
1385_0000098	1.0	1.0
1385_0000114	1.0	1.0
1385_0000122	1.0	1.0
1385_0001109	1.0	1.0
1385_0001113	1.0	1.0
1385_0001131	1.0	1.0
1385_0001134	1.0	1.0
1385_0001138	1.0	1.0
1385_0001147	1.0	1.0
1385_0001166	1.0	1.0
1385_0001172	1.0	1.0
1385_0001192	1.0	1.0
1385_0001501	1.0	1.0
1385_0001524	1.0	1.0
1385_0001717	2.0	1.0
1385_0001719	1.0	1.0
1385_0001728	1.0	2.0
1385_0001734	1.0	1.0
1385_0001740	1.0	1.0
1385_0001744	0.0	1.0
1385_0001750	0.0	1.0
1385_0001752	1.0	1.0
1385_0001753	1.0	1.0
1385_0001761	1.0	1.0
1385_0001762	1.0	1.0
1385_0001771	1.0	1.0
1385_0001788	1.0	1.0
1385_0001794	1.0	1.0
1395_0000340	1.0	1.0
1395_0000357	2.0	2.0
1395_0000361	1.0	1.0
1395_0000366	2.0	1.0
1395_0000368	0.0	0.0
1395_0000369	2.0	1.0
1395_0000380	1.0	1.0
1395_0000399	1.0	1.0
1395_0000403	1.0	1.0
1395_0000404	1.0	2.0
1395_0000446	2.0	2.0
1395_0000449	2.0	1.0
1395_0000465	1.0	1.0
1395_0000504	1.0	1.0
1395_0000513	2.0	2.0
1395_0000514	2.0	2.0
1395_0000518	2.0	2.0
1395_0000549	1.0	2.0
1395_0000550	1.0	2.0
1395_0000556	1.0	1.0
1395_0000582	0.0	1.0
1395_0000583	1.0	1.0
1395_0000584	0.0	1.0
1395_0000596	2.0	1.0
1395_0000604	1.0	1.0
1395_0000609	1.0	1.0
1395_0000626	2.0	2.0
1395_0000631	1.0	1.0
1395_0000639	1.0	2.0
1395_0001015	1.0	2.0
1395_0001017	1.0	2.0
1395_0001022	1.0	1.0
1395_0001023	1.0	1.0
1395_0001033	1.0	1.0
1395_0001034	1.0	1.0
1395_0001040	1.0	1.0
1395_0001058	1.0	1.0
1395_0001070	2.0	2.0
1395_0001075	1.0	1.0
1395_0001076	1.0	2.0
1395_0001080	1.0	1.0
1395_0001084	1.0	1.0
1395_0001104	1.0	2.0
1395_0001114	1.0	1.0
1395_0001116	1.0	1.0
1395_0001123	1.0	1.0
1395_0001126	1.0	1.0
1395_0001141	1.0	1.0
1395_0001158	2.0	1.0
1395_0001164	2.0	1.0
LANGUAGE: IT, 4th Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.60
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.44
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         6
         1.0       0.83      0.89      0.86        76
         2.0       0.90      0.90      0.90        78

    accuracy                           0.86       160
   macro avg       0.58      0.60      0.59       160
weighted avg       0.83      0.86      0.85       160

[[ 0  6  0]
 [ 0 68  8]
 [ 0  8 70]]
0.8463607594936707
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.46
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.57
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         6
         1.0       0.74      0.93      0.83        76
         2.0       0.92      0.76      0.83        78

    accuracy                           0.81       160
   macro avg       0.55      0.56      0.55       160
weighted avg       0.80      0.81      0.80       160

[[ 0  6  0]
 [ 0 71  5]
 [ 0 19 59]]
0.7972567965935144
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.38
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.46
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         6
         1.0       0.83      0.88      0.85        76
         2.0       0.89      0.90      0.89        78

    accuracy                           0.86       160
   macro avg       0.57      0.59      0.58       160
weighted avg       0.82      0.86      0.84       160

[[ 0  6  0]
 [ 0 67  9]
 [ 0  8 70]]
0.8401273885350318
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.24
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.52
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         6
         1.0       0.83      0.78      0.80        76
         2.0       0.81      0.92      0.86        78

    accuracy                           0.82       160
   macro avg       0.55      0.57      0.55       160
weighted avg       0.79      0.82      0.80       160

[[ 0  6  0]
 [ 0 59 17]
 [ 0  6 72]]
0.8016517984439284
160 160 160
Filename	True Label	Prediction
1325_1001014	2.0	2.0
1325_1001017	2.0	2.0
1325_1001022	2.0	2.0
1325_1001025	2.0	2.0
1325_1001028	2.0	2.0
1325_1001040	2.0	2.0
1325_1001052	2.0	2.0
1325_1001057	2.0	2.0
1325_1001078	2.0	2.0
1325_1001079	2.0	2.0
1325_1001081	2.0	2.0
1325_1001084	2.0	2.0
1325_1001088	2.0	2.0
1325_1001089	2.0	2.0
1325_1001095	2.0	2.0
1325_1001101	2.0	2.0
1325_1001110	2.0	2.0
1325_1001113	2.0	2.0
1325_1001125	2.0	2.0
1325_1001126	2.0	2.0
1325_1001129	1.0	2.0
1325_1001133	2.0	2.0
1325_1001134	2.0	2.0
1325_1001139	2.0	2.0
1325_1001141	1.0	2.0
1325_1001154	2.0	2.0
1325_1001158	2.0	2.0
1325_1001166	2.0	2.0
1325_9000089	2.0	2.0
1325_9000138	2.0	2.0
1325_9000152	2.0	2.0
1325_9000210	1.0	2.0
1325_9000240	2.0	2.0
1325_9000296	2.0	2.0
1325_9000320	2.0	2.0
1325_9000674	2.0	2.0
1365_0100007	1.0	1.0
1365_0100022	2.0	2.0
1365_0100024	1.0	1.0
1365_0100026	1.0	1.0
1365_0100027	2.0	2.0
1365_0100030	1.0	2.0
1365_0100051	1.0	2.0
1365_0100064	2.0	2.0
1365_0100069	2.0	2.0
1365_0100070	2.0	2.0
1365_0100079	2.0	2.0
1365_0100080	2.0	2.0
1365_0100093	2.0	2.0
1365_0100098	1.0	2.0
1365_0100120	2.0	2.0
1365_0100121	2.0	2.0
1365_0100123	2.0	2.0
1365_0100139	2.0	2.0
1365_0100146	2.0	1.0
1365_0100147	2.0	2.0
1365_0100162	2.0	2.0
1365_0100163	2.0	2.0
1365_0100172	2.0	2.0
1365_0100179	2.0	2.0
1365_0100183	2.0	2.0
1365_0100188	2.0	2.0
1365_0100196	1.0	2.0
1365_0100203	2.0	2.0
1365_0100218	2.0	2.0
1365_0100222	2.0	2.0
1365_0100224	2.0	2.0
1365_0100232	2.0	2.0
1365_0100255	1.0	2.0
1365_0100274	2.0	2.0
1365_0100299	2.0	2.0
1365_0100451	2.0	2.0
1365_0100455	2.0	2.0
1365_0100459	2.0	2.0
1365_0100461	2.0	2.0
1365_0100470	2.0	2.0
1365_0100472	2.0	2.0
1365_0100473	2.0	2.0
1385_0000017	1.0	1.0
1385_0000020	1.0	1.0
1385_0000022	1.0	1.0
1385_0000039	1.0	1.0
1385_0000040	1.0	1.0
1385_0000043	1.0	1.0
1385_0000048	1.0	1.0
1385_0000052	1.0	1.0
1385_0000104	1.0	1.0
1385_0000119	1.0	1.0
1385_0000126	1.0	1.0
1385_0001112	1.0	1.0
1385_0001118	1.0	1.0
1385_0001122	1.0	1.0
1385_0001126	0.0	1.0
1385_0001135	1.0	1.0
1385_0001149	1.0	1.0
1385_0001156	1.0	1.0
1385_0001157	1.0	1.0
1385_0001164	1.0	1.0
1385_0001169	1.0	1.0
1385_0001174	1.0	1.0
1385_0001188	1.0	1.0
1385_0001191	1.0	1.0
1385_0001195	1.0	2.0
1385_0001712	1.0	1.0
1385_0001716	1.0	1.0
1385_0001720	0.0	1.0
1385_0001724	2.0	2.0
1385_0001726	1.0	2.0
1385_0001727	0.0	1.0
1385_0001729	1.0	2.0
1385_0001733	1.0	1.0
1385_0001736	2.0	1.0
1385_0001757	2.0	1.0
1385_0001765	0.0	1.0
1385_0001772	1.0	1.0
1385_0001774	0.0	1.0
1385_0001785	1.0	1.0
1385_0001786	1.0	1.0
1385_0001787	1.0	1.0
1385_0001793	1.0	1.0
1395_0000338	1.0	1.0
1395_0000353	1.0	1.0
1395_0000356	1.0	1.0
1395_0000387	2.0	2.0
1395_0000388	2.0	2.0
1395_0000390	1.0	1.0
1395_0000391	2.0	2.0
1395_0000396	1.0	1.0
1395_0000413	1.0	2.0
1395_0000414	1.0	1.0
1395_0000450	1.0	1.0
1395_0000455	1.0	1.0
1395_0000458	1.0	1.0
1395_0000460	1.0	1.0
1395_0000469	1.0	1.0
1395_0000525	2.0	1.0
1395_0000527	1.0	1.0
1395_0000528	2.0	1.0
1395_0000533	2.0	2.0
1395_0000547	1.0	1.0
1395_0000553	1.0	2.0
1395_0000557	2.0	1.0
1395_0000560	1.0	2.0
1395_0000572	1.0	1.0
1395_0000598	1.0	1.0
1395_0000606	1.0	1.0
1395_0000607	1.0	1.0
1395_0000611	1.0	1.0
1395_0000612	1.0	1.0
1395_0000642	1.0	1.0
1395_0001013	1.0	1.0
1395_0001028	1.0	2.0
1395_0001069	1.0	2.0
1395_0001093	1.0	1.0
1395_0001119	2.0	2.0
1395_0001120	1.0	1.0
1395_0001124	1.0	1.0
1395_0001146	0.0	1.0
1395_0001147	1.0	2.0
1395_0001149	1.0	1.0
Averaged weighted F1-scores 0.8186125907816839
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2']
LANGUAGE: IT, 0th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.13
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.03
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.47      0.38      0.42        48
         2.0       0.52      0.84      0.65        76
         3.0       0.00      0.00      0.00        22

    accuracy                           0.51       160
   macro avg       0.25      0.30      0.27       160
weighted avg       0.39      0.51      0.43       160

[[ 0  8  6  0]
 [ 0 18 30  0]
 [ 0 12 64  0]
 [ 0  0 22  0]]
0.43265210241954427
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.98
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.03
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.49      0.42      0.45        48
         2.0       0.55      0.86      0.67        76
         3.0       0.00      0.00      0.00        22

    accuracy                           0.53       160
   macro avg       0.26      0.32      0.28       160
weighted avg       0.41      0.53      0.45       160

[[ 0 10  4  0]
 [ 0 20 28  0]
 [ 0 11 65  0]
 [ 0  0 22  0]]
0.4514981273408239
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.89
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.97
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.50      0.71      0.59        48
         2.0       0.60      0.72      0.65        76
         3.0       0.00      0.00      0.00        22

    accuracy                           0.56       160
   macro avg       0.27      0.36      0.31       160
weighted avg       0.43      0.56      0.49       160

[[ 0 13  1  0]
 [ 0 34 14  0]
 [ 0 21 55  0]
 [ 0  0 22  0]]
0.486873973727422
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.81
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.03
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.44      0.42      0.43        48
         2.0       0.56      0.84      0.67        76
         3.0       0.00      0.00      0.00        22

    accuracy                           0.53       160
   macro avg       0.25      0.31      0.28       160
weighted avg       0.40      0.53      0.45       160

[[ 0 13  1  0]
 [ 0 20 28  0]
 [ 0 12 64  0]
 [ 0  0 22  0]]
0.4473568653943591
160 160 160
Filename	True Label	Prediction
1325_1001017	2.0	2.0
1325_1001019	2.0	2.0
1325_1001023	2.0	2.0
1325_1001036	2.0	2.0
1325_1001037	2.0	2.0
1325_1001044	3.0	2.0
1325_1001045	2.0	2.0
1325_1001048	1.0	2.0
1325_1001053	1.0	2.0
1325_1001077	2.0	2.0
1325_1001079	3.0	2.0
1325_1001080	2.0	2.0
1325_1001081	3.0	2.0
1325_1001092	2.0	2.0
1325_1001095	2.0	2.0
1325_1001109	2.0	2.0
1325_1001123	3.0	2.0
1325_1001124	2.0	2.0
1325_1001125	2.0	2.0
1325_1001128	2.0	2.0
1325_1001131	3.0	2.0
1325_1001132	2.0	2.0
1325_1001139	2.0	2.0
1325_1001141	2.0	2.0
1325_1001144	3.0	2.0
1325_9000087	2.0	2.0
1325_9000089	2.0	2.0
1325_9000099	2.0	2.0
1325_9000104	2.0	2.0
1325_9000138	3.0	2.0
1325_9000139	2.0	2.0
1325_9000185	3.0	2.0
1325_9000186	3.0	2.0
1325_9000187	3.0	2.0
1325_9000188	2.0	2.0
1325_9000278	3.0	2.0
1325_9000303	3.0	2.0
1325_9000317	3.0	2.0
1325_9000554	2.0	2.0
1325_9000678	3.0	2.0
1325_9000685	3.0	2.0
1325_9000700	2.0	2.0
1365_0100003	1.0	2.0
1365_0100004	2.0	2.0
1365_0100008	1.0	2.0
1365_0100009	2.0	1.0
1365_0100018	1.0	2.0
1365_0100019	1.0	2.0
1365_0100021	2.0	2.0
1365_0100028	2.0	2.0
1365_0100030	1.0	2.0
1365_0100061	3.0	2.0
1365_0100063	3.0	2.0
1365_0100070	2.0	2.0
1365_0100074	2.0	2.0
1365_0100097	2.0	2.0
1365_0100098	1.0	2.0
1365_0100100	2.0	2.0
1365_0100106	2.0	2.0
1365_0100138	2.0	2.0
1365_0100164	2.0	2.0
1365_0100167	1.0	2.0
1365_0100169	2.0	2.0
1365_0100170	1.0	2.0
1365_0100174	2.0	2.0
1365_0100178	2.0	2.0
1365_0100181	1.0	2.0
1365_0100182	2.0	2.0
1365_0100183	1.0	2.0
1365_0100184	2.0	2.0
1365_0100194	3.0	2.0
1365_0100196	1.0	2.0
1365_0100201	2.0	2.0
1365_0100253	2.0	2.0
1365_0100258	2.0	2.0
1365_0100261	2.0	2.0
1365_0100262	2.0	2.0
1365_0100263	3.0	2.0
1365_0100275	2.0	2.0
1365_0100282	2.0	2.0
1365_0100288	2.0	2.0
1365_0100456	2.0	2.0
1365_0100458	2.0	2.0
1365_0100478	2.0	2.0
1365_0100480	2.0	2.0
1385_0000020	2.0	1.0
1385_0000023	2.0	1.0
1385_0000034	2.0	1.0
1385_0000040	1.0	1.0
1385_0000044	2.0	2.0
1385_0000050	2.0	2.0
1385_0000057	1.0	2.0
1385_0000097	2.0	1.0
1385_0000098	1.0	1.0
1385_0000101	1.0	2.0
1385_0000123	1.0	2.0
1385_0000126	2.0	1.0
1385_0000127	2.0	1.0
1385_0001107	2.0	1.0
1385_0001122	1.0	2.0
1385_0001125	1.0	1.0
1385_0001130	1.0	1.0
1385_0001132	1.0	1.0
1385_0001134	1.0	1.0
1385_0001147	1.0	1.0
1385_0001150	2.0	2.0
1385_0001161	2.0	2.0
1385_0001173	0.0	1.0
1385_0001188	1.0	1.0
1385_0001191	1.0	1.0
1385_0001193	2.0	2.0
1385_0001198	2.0	1.0
1385_0001716	1.0	1.0
1385_0001726	1.0	1.0
1385_0001736	2.0	2.0
1385_0001741	0.0	1.0
1385_0001748	1.0	2.0
1385_0001751	1.0	1.0
1385_0001760	1.0	1.0
1385_0001761	0.0	1.0
1385_0001785	0.0	1.0
1385_0001792	0.0	2.0
1385_0001794	0.0	1.0
1395_0000338	1.0	1.0
1395_0000355	2.0	2.0
1395_0000357	3.0	2.0
1395_0000368	0.0	1.0
1395_0000369	2.0	2.0
1395_0000376	2.0	2.0
1395_0000378	1.0	1.0
1395_0000380	2.0	2.0
1395_0000447	1.0	2.0
1395_0000449	2.0	1.0
1395_0000462	2.0	1.0
1395_0000470	1.0	1.0
1395_0000514	3.0	2.0
1395_0000533	3.0	2.0
1395_0000537	1.0	2.0
1395_0000550	1.0	2.0
1395_0000551	2.0	2.0
1395_0000563	1.0	1.0
1395_0000584	0.0	1.0
1395_0000587	0.0	1.0
1395_0000591	0.0	1.0
1395_0000598	1.0	2.0
1395_0000608	1.0	2.0
1395_0000639	1.0	2.0
1395_0001016	2.0	1.0
1395_0001023	1.0	1.0
1395_0001024	1.0	2.0
1395_0001061	1.0	2.0
1395_0001067	1.0	1.0
1395_0001076	0.0	1.0
1395_0001103	1.0	1.0
1395_0001108	0.0	1.0
1395_0001109	0.0	1.0
1395_0001115	1.0	2.0
1395_0001123	1.0	2.0
1395_0001133	0.0	1.0
1395_0001158	2.0	2.0
LANGUAGE: IT, 1th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.19
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.04
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.53      0.19      0.28        48
         2.0       0.51      0.97      0.67        75
         3.0       1.00      0.04      0.08        23

    accuracy                           0.52       160
   macro avg       0.51      0.30      0.26       160
weighted avg       0.54      0.52      0.41       160

[[ 0  6  8  0]
 [ 0  9 39  0]
 [ 0  2 73  0]
 [ 0  0 22  1]]
0.4104362740753871
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.02
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.96
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.48      0.90      0.62        48
         2.0       0.78      0.41      0.54        75
         3.0       0.53      0.70      0.60        23

    accuracy                           0.56       160
   macro avg       0.45      0.50      0.44       160
weighted avg       0.58      0.56      0.53       160

[[ 0 14  0  0]
 [ 0 43  4  1]
 [ 0 31 31 13]
 [ 0  2  5 16]]
0.526466365873667
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.92
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.85
              precision    recall  f1-score   support

         0.0       1.00      0.21      0.35        14
         1.0       0.58      0.67      0.62        48
         2.0       0.62      0.84      0.71        75
         3.0       0.00      0.00      0.00        23

    accuracy                           0.61       160
   macro avg       0.55      0.43      0.42       160
weighted avg       0.55      0.61      0.55       160

[[ 3 11  0  0]
 [ 0 32 16  0]
 [ 0 12 63  0]
 [ 0  0 23  0]]
0.5509765606094338
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.81
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.82
              precision    recall  f1-score   support

         0.0       0.67      0.29      0.40        14
         1.0       0.55      0.85      0.67        48
         2.0       0.72      0.65      0.69        75
         3.0       0.58      0.30      0.40        23

    accuracy                           0.63       160
   macro avg       0.63      0.52      0.54       160
weighted avg       0.65      0.63      0.62       160

[[ 4 10  0  0]
 [ 2 41  4  1]
 [ 0 22 49  4]
 [ 0  1 15  7]]
0.6153806030035539
160 160 160
Filename	True Label	Prediction
1325_1001009	2.0	2.0
1325_1001011	2.0	3.0
1325_1001013	2.0	2.0
1325_1001020	2.0	2.0
1325_1001022	2.0	2.0
1325_1001024	2.0	2.0
1325_1001027	3.0	2.0
1325_1001032	2.0	2.0
1325_1001035	3.0	3.0
1325_1001042	2.0	2.0
1325_1001043	2.0	2.0
1325_1001046	2.0	2.0
1325_1001057	2.0	2.0
1325_1001062	2.0	3.0
1325_1001075	1.0	3.0
1325_1001076	3.0	2.0
1325_1001082	2.0	2.0
1325_1001084	2.0	2.0
1325_1001099	3.0	2.0
1325_1001126	2.0	2.0
1325_1001135	2.0	2.0
1325_1001142	2.0	2.0
1325_1001154	3.0	3.0
1325_1001162	2.0	2.0
1325_1001164	2.0	2.0
1325_1001165	2.0	2.0
1325_9000059	2.0	3.0
1325_9000088	3.0	2.0
1325_9000137	3.0	2.0
1325_9000140	3.0	3.0
1325_9000144	3.0	3.0
1325_9000152	3.0	2.0
1325_9000214	2.0	2.0
1325_9000237	3.0	3.0
1325_9000241	3.0	2.0
1325_9000322	3.0	2.0
1325_9000534	2.0	2.0
1325_9000677	3.0	2.0
1325_9000686	3.0	3.0
1325_9000750	3.0	1.0
1365_0100002	2.0	2.0
1365_0100005	2.0	1.0
1365_0100007	1.0	1.0
1365_0100017	2.0	2.0
1365_0100020	2.0	2.0
1365_0100023	2.0	2.0
1365_0100024	1.0	2.0
1365_0100026	1.0	1.0
1365_0100031	2.0	1.0
1365_0100057	2.0	2.0
1365_0100072	2.0	2.0
1365_0100092	2.0	2.0
1365_0100099	2.0	2.0
1365_0100121	3.0	2.0
1365_0100134	2.0	2.0
1365_0100145	2.0	2.0
1365_0100146	2.0	2.0
1365_0100147	2.0	2.0
1365_0100162	2.0	2.0
1365_0100163	3.0	3.0
1365_0100172	2.0	2.0
1365_0100177	2.0	2.0
1365_0100198	2.0	2.0
1365_0100200	3.0	2.0
1365_0100211	2.0	3.0
1365_0100228	1.0	2.0
1365_0100229	2.0	2.0
1365_0100265	3.0	2.0
1365_0100266	3.0	2.0
1365_0100287	2.0	2.0
1365_0100290	2.0	2.0
1365_0100455	3.0	2.0
1365_0100472	2.0	2.0
1365_0100482	2.0	2.0
1385_0000013	1.0	1.0
1385_0000022	1.0	1.0
1385_0000036	1.0	2.0
1385_0000039	1.0	1.0
1385_0000042	2.0	2.0
1385_0000047	1.0	1.0
1385_0000049	2.0	1.0
1385_0000053	2.0	2.0
1385_0000058	2.0	2.0
1385_0000099	0.0	1.0
1385_0000100	1.0	0.0
1385_0000102	1.0	1.0
1385_0000122	2.0	1.0
1385_0000124	2.0	2.0
1385_0001105	2.0	1.0
1385_0001108	2.0	1.0
1385_0001110	2.0	1.0
1385_0001126	0.0	0.0
1385_0001127	2.0	1.0
1385_0001136	1.0	1.0
1385_0001138	1.0	1.0
1385_0001148	2.0	1.0
1385_0001155	2.0	1.0
1385_0001156	2.0	2.0
1385_0001162	1.0	1.0
1385_0001166	0.0	1.0
1385_0001169	1.0	0.0
1385_0001197	1.0	1.0
1385_0001526	0.0	0.0
1385_0001528	1.0	1.0
1385_0001714	1.0	1.0
1385_0001723	0.0	0.0
1385_0001727	0.0	1.0
1385_0001734	1.0	1.0
1385_0001738	0.0	1.0
1385_0001742	0.0	1.0
1385_0001752	0.0	1.0
1385_0001756	1.0	1.0
1385_0001762	1.0	1.0
1385_0001766	2.0	2.0
1385_0001767	0.0	1.0
1385_0001768	1.0	1.0
1385_0001773	0.0	1.0
1395_0000337	0.0	0.0
1395_0000340	2.0	2.0
1395_0000353	1.0	1.0
1395_0000364	1.0	1.0
1395_0000365	2.0	1.0
1395_0000383	2.0	1.0
1395_0000388	2.0	1.0
1395_0000398	2.0	2.0
1395_0000402	1.0	1.0
1395_0000404	2.0	1.0
1395_0000409	2.0	1.0
1395_0000414	2.0	1.0
1395_0000451	2.0	1.0
1395_0000452	1.0	1.0
1395_0000460	1.0	1.0
1395_0000469	1.0	1.0
1395_0000499	1.0	1.0
1395_0000500	1.0	1.0
1395_0000518	2.0	1.0
1395_0000525	2.0	1.0
1395_0000526	1.0	1.0
1395_0000527	1.0	1.0
1395_0000547	2.0	1.0
1395_0000557	3.0	2.0
1395_0000559	2.0	1.0
1395_0000607	1.0	1.0
1395_0000626	1.0	1.0
1395_0000630	1.0	1.0
1395_0000636	0.0	1.0
1395_0000646	1.0	1.0
1395_0001013	1.0	1.0
1395_0001020	1.0	1.0
1395_0001033	1.0	1.0
1395_0001060	1.0	1.0
1395_0001070	1.0	2.0
1395_0001071	1.0	1.0
1395_0001084	1.0	1.0
1395_0001090	2.0	1.0
1395_0001093	1.0	1.0
1395_0001101	1.0	1.0
1395_0001114	1.0	1.0
1395_0001149	0.0	1.0
1395_0001170	1.0	1.0
LANGUAGE: IT, 2th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.12
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.97
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.66      0.55      0.60        49
         2.0       0.59      0.93      0.72        75
         3.0       0.00      0.00      0.00        22

    accuracy                           0.61       160
   macro avg       0.31      0.37      0.33       160
weighted avg       0.48      0.61      0.52       160

[[ 0  9  5  0]
 [ 0 27 22  0]
 [ 0  5 70  0]
 [ 0  0 22  0]]
0.5220231958762886
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.94
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.95
              precision    recall  f1-score   support

         0.0       0.45      0.36      0.40        14
         1.0       0.62      0.49      0.55        49
         2.0       0.62      0.91      0.74        75
         3.0       0.00      0.00      0.00        22

    accuracy                           0.61       160
   macro avg       0.42      0.44      0.42       160
weighted avg       0.52      0.61      0.55       160

[[ 5  8  1  0]
 [ 6 24 19  0]
 [ 0  7 68  0]
 [ 0  0 22  0]]
0.5466400491400492
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.80
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.94
              precision    recall  f1-score   support

         0.0       0.40      0.29      0.33        14
         1.0       0.58      0.61      0.59        49
         2.0       0.64      0.72      0.67        75
         3.0       0.31      0.18      0.23        22

    accuracy                           0.57       160
   macro avg       0.48      0.45      0.46       160
weighted avg       0.55      0.57      0.56       160

[[ 4 10  0  0]
 [ 6 30 13  0]
 [ 0 12 54  9]
 [ 0  0 18  4]]
0.5589321811645449
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.69
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.98
              precision    recall  f1-score   support

         0.0       0.31      0.29      0.30        14
         1.0       0.52      0.49      0.51        49
         2.0       0.62      0.77      0.69        75
         3.0       0.29      0.09      0.14        22

    accuracy                           0.55       160
   macro avg       0.43      0.41      0.41       160
weighted avg       0.52      0.55      0.52       160

[[ 4 10  0  0]
 [ 9 24 16  0]
 [ 0 12 58  5]
 [ 0  0 20  2]]
0.521373847402746
160 160 160
Filename	True Label	Prediction
1325_1001021	2.0	3.0
1325_1001025	2.0	3.0
1325_1001033	3.0	2.0
1325_1001040	3.0	2.0
1325_1001041	3.0	2.0
1325_1001047	1.0	2.0
1325_1001050	2.0	2.0
1325_1001051	2.0	2.0
1325_1001058	2.0	2.0
1325_1001063	2.0	2.0
1325_1001083	2.0	2.0
1325_1001086	2.0	2.0
1325_1001087	2.0	2.0
1325_1001090	2.0	2.0
1325_1001091	3.0	2.0
1325_1001093	2.0	2.0
1325_1001096	2.0	2.0
1325_1001098	2.0	2.0
1325_1001100	2.0	2.0
1325_1001110	3.0	3.0
1325_1001111	3.0	2.0
1325_1001113	3.0	2.0
1325_1001120	3.0	2.0
1325_1001121	2.0	2.0
1325_1001122	2.0	2.0
1325_1001129	1.0	2.0
1325_1001133	2.0	2.0
1325_1001134	2.0	2.0
1325_1001136	2.0	2.0
1325_1001152	2.0	2.0
1325_1001156	2.0	2.0
1325_1001158	2.0	3.0
1325_1001161	2.0	2.0
1325_1001163	2.0	2.0
1325_1001166	2.0	2.0
1325_1001168	2.0	2.0
1325_9000102	2.0	2.0
1325_9000211	2.0	2.0
1325_9000239	3.0	2.0
1325_9000304	2.0	3.0
1325_9000316	1.0	2.0
1325_9000318	3.0	3.0
1325_9000601	3.0	2.0
1325_9000674	2.0	3.0
1325_9000684	3.0	2.0
1365_0100006	2.0	2.0
1365_0100012	2.0	1.0
1365_0100014	2.0	2.0
1365_0100015	1.0	1.0
1365_0100051	1.0	1.0
1365_0100058	2.0	2.0
1365_0100065	1.0	2.0
1365_0100066	1.0	2.0
1365_0100069	1.0	2.0
1365_0100080	2.0	2.0
1365_0100093	2.0	2.0
1365_0100102	3.0	2.0
1365_0100103	3.0	2.0
1365_0100107	3.0	2.0
1365_0100118	3.0	2.0
1365_0100137	2.0	2.0
1365_0100168	3.0	2.0
1365_0100175	2.0	2.0
1365_0100180	1.0	2.0
1365_0100192	3.0	2.0
1365_0100202	1.0	2.0
1365_0100213	2.0	2.0
1365_0100215	2.0	2.0
1365_0100218	3.0	2.0
1365_0100219	2.0	2.0
1365_0100220	2.0	2.0
1365_0100224	2.0	2.0
1365_0100226	2.0	2.0
1365_0100230	2.0	2.0
1365_0100231	2.0	2.0
1365_0100232	3.0	2.0
1365_0100233	3.0	2.0
1365_0100251	2.0	2.0
1365_0100259	2.0	2.0
1365_0100285	2.0	2.0
1365_0100451	2.0	2.0
1365_0100469	2.0	2.0
1365_0100470	2.0	2.0
1365_0100473	2.0	2.0
1365_0100475	2.0	2.0
1365_0100477	1.0	2.0
1385_0000011	0.0	1.0
1385_0000035	2.0	1.0
1385_0000038	1.0	2.0
1385_0000043	2.0	2.0
1385_0000045	2.0	2.0
1385_0000048	2.0	2.0
1385_0000095	1.0	0.0
1385_0000103	1.0	1.0
1385_0000119	2.0	2.0
1385_0000120	0.0	1.0
1385_0000130	2.0	1.0
1385_0001111	2.0	1.0
1385_0001113	1.0	1.0
1385_0001121	1.0	1.0
1385_0001131	1.0	0.0
1385_0001137	1.0	1.0
1385_0001149	2.0	2.0
1385_0001152	2.0	2.0
1385_0001159	1.0	2.0
1385_0001165	1.0	2.0
1385_0001170	1.0	0.0
1385_0001171	1.0	0.0
1385_0001190	0.0	0.0
1385_0001192	1.0	1.0
1385_0001194	1.0	1.0
1385_0001524	0.0	1.0
1385_0001527	1.0	1.0
1385_0001715	1.0	1.0
1385_0001717	2.0	2.0
1385_0001719	0.0	1.0
1385_0001725	1.0	1.0
1385_0001746	0.0	1.0
1385_0001749	1.0	1.0
1385_0001750	0.0	0.0
1385_0001775	0.0	1.0
1385_0001795	1.0	0.0
1385_0001799	1.0	2.0
1385_0001800	1.0	1.0
1395_0000333	2.0	1.0
1395_0000356	1.0	0.0
1395_0000389	0.0	0.0
1395_0000396	2.0	2.0
1395_0000403	2.0	2.0
1395_0000415	1.0	1.0
1395_0000438	3.0	2.0
1395_0000450	1.0	1.0
1395_0000516	1.0	0.0
1395_0000529	2.0	1.0
1395_0000535	1.0	1.0
1395_0000565	1.0	1.0
1395_0000579	1.0	0.0
1395_0000585	1.0	1.0
1395_0000595	0.0	1.0
1395_0000599	1.0	1.0
1395_0000606	1.0	0.0
1395_0000610	2.0	1.0
1395_0000611	1.0	1.0
1395_0000628	0.0	1.0
1395_0000649	2.0	1.0
1395_0001017	1.0	1.0
1395_0001021	1.0	1.0
1395_0001022	1.0	2.0
1395_0001040	0.0	0.0
1395_0001045	2.0	1.0
1395_0001073	2.0	1.0
1395_0001078	0.0	1.0
1395_0001116	2.0	1.0
1395_0001117	1.0	1.0
1395_0001119	1.0	2.0
1395_0001120	1.0	1.0
1395_0001124	0.0	1.0
1395_0001145	2.0	1.0
1395_0001161	1.0	2.0
1395_0001164	2.0	2.0
LANGUAGE: IT, 3th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.10
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.08
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.57      0.08      0.14        49
         2.0       0.48      0.99      0.65        75
         3.0       0.00      0.00      0.00        22

    accuracy                           0.49       160
   macro avg       0.26      0.27      0.20       160
weighted avg       0.40      0.49      0.35       160

[[ 0  1 13  0]
 [ 0  4 45  0]
 [ 0  1 74  0]
 [ 0  1 21  0]]
0.3480263157894737
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.00
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.98
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.49      0.73      0.59        49
         2.0       0.62      0.72      0.67        75
         3.0       0.00      0.00      0.00        22

    accuracy                           0.56       160
   macro avg       0.28      0.36      0.31       160
weighted avg       0.44      0.56      0.49       160

[[ 0 12  2  0]
 [ 0 36 13  0]
 [ 0 21 54  0]
 [ 0  4 18  0]]
0.4932377049180328
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.88
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.04
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.52      0.55      0.53        49
         2.0       0.59      0.85      0.70        75
         3.0       0.00      0.00      0.00        22

    accuracy                           0.57       160
   macro avg       0.28      0.35      0.31       160
weighted avg       0.44      0.57      0.49       160

[[ 0 12  2  0]
 [ 0 27 22  0]
 [ 0 11 64  0]
 [ 0  2 20  0]]
0.4916064762213927
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.79
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       1.00      0.07      0.13        14
         1.0       0.54      0.59      0.56        49
         2.0       0.60      0.84      0.70        75
         3.0       0.00      0.00      0.00        22

    accuracy                           0.58       160
   macro avg       0.53      0.38      0.35       160
weighted avg       0.53      0.58      0.51       160

[[ 1 11  2  0]
 [ 0 29 20  0]
 [ 0 12 63  0]
 [ 0  2 20  0]]
0.5122431229773463
160 160 160
Filename	True Label	Prediction
1325_1001010	2.0	2.0
1325_1001014	3.0	2.0
1325_1001018	2.0	2.0
1325_1001028	2.0	2.0
1325_1001039	3.0	2.0
1325_1001052	2.0	2.0
1325_1001054	2.0	2.0
1325_1001088	2.0	2.0
1325_1001094	2.0	2.0
1325_1001097	1.0	2.0
1325_1001101	3.0	2.0
1325_1001108	3.0	2.0
1325_1001130	2.0	2.0
1325_1001138	2.0	2.0
1325_1001157	2.0	2.0
1325_1001159	3.0	2.0
1325_1001160	2.0	2.0
1325_1001167	3.0	2.0
1325_1001169	2.0	2.0
1325_1001170	3.0	2.0
1325_9000107	2.0	2.0
1325_9000215	3.0	2.0
1325_9000240	2.0	2.0
1325_9000314	2.0	2.0
1325_9000319	2.0	2.0
1325_9000323	2.0	2.0
1325_9000504	3.0	2.0
1325_9000533	3.0	2.0
1325_9000536	2.0	2.0
1325_9000602	3.0	2.0
1325_9000676	3.0	2.0
1365_0100011	1.0	2.0
1365_0100027	2.0	2.0
1365_0100029	1.0	1.0
1365_0100056	2.0	2.0
1365_0100064	2.0	2.0
1365_0100079	2.0	2.0
1365_0100094	2.0	2.0
1365_0100101	3.0	2.0
1365_0100104	2.0	2.0
1365_0100105	3.0	2.0
1365_0100123	2.0	2.0
1365_0100125	3.0	2.0
1365_0100133	2.0	2.0
1365_0100135	2.0	2.0
1365_0100148	2.0	2.0
1365_0100166	2.0	2.0
1365_0100171	2.0	2.0
1365_0100173	2.0	2.0
1365_0100179	2.0	2.0
1365_0100185	1.0	2.0
1365_0100186	2.0	2.0
1365_0100187	2.0	2.0
1365_0100188	2.0	2.0
1365_0100191	2.0	2.0
1365_0100195	1.0	2.0
1365_0100199	2.0	2.0
1365_0100203	2.0	2.0
1365_0100205	3.0	2.0
1365_0100212	3.0	2.0
1365_0100221	2.0	2.0
1365_0100225	2.0	2.0
1365_0100227	3.0	2.0
1365_0100252	2.0	2.0
1365_0100255	2.0	2.0
1365_0100267	2.0	2.0
1365_0100270	2.0	2.0
1365_0100276	3.0	2.0
1365_0100280	1.0	2.0
1365_0100281	1.0	2.0
1365_0100286	1.0	2.0
1365_0100457	2.0	2.0
1365_0100474	2.0	2.0
1365_0100476	2.0	2.0
1385_0000017	1.0	1.0
1385_0000021	2.0	2.0
1385_0000033	1.0	1.0
1385_0000037	1.0	1.0
1385_0000054	2.0	2.0
1385_0000114	2.0	2.0
1385_0000125	1.0	1.0
1385_0000128	1.0	2.0
1385_0001103	2.0	1.0
1385_0001109	2.0	1.0
1385_0001112	2.0	1.0
1385_0001123	2.0	1.0
1385_0001133	2.0	1.0
1385_0001135	1.0	1.0
1385_0001153	3.0	2.0
1385_0001154	2.0	2.0
1385_0001157	1.0	1.0
1385_0001160	3.0	1.0
1385_0001163	2.0	1.0
1385_0001164	1.0	1.0
1385_0001172	0.0	1.0
1385_0001175	0.0	1.0
1385_0001178	0.0	0.0
1385_0001189	0.0	1.0
1385_0001196	1.0	1.0
1385_0001199	1.0	1.0
1385_0001503	1.0	2.0
1385_0001523	1.0	1.0
1385_0001718	0.0	1.0
1385_0001728	1.0	2.0
1385_0001729	1.0	1.0
1385_0001730	1.0	1.0
1385_0001733	2.0	2.0
1385_0001737	2.0	2.0
1385_0001744	0.0	1.0
1385_0001753	1.0	1.0
1385_0001754	1.0	1.0
1385_0001758	1.0	1.0
1385_0001764	0.0	1.0
1385_0001786	1.0	2.0
1385_0001787	0.0	2.0
1385_0001788	1.0	2.0
1385_0001791	0.0	2.0
1395_0000360	3.0	1.0
1395_0000392	2.0	1.0
1395_0000399	1.0	1.0
1395_0000413	2.0	2.0
1395_0000446	2.0	2.0
1395_0000448	1.0	1.0
1395_0000454	2.0	1.0
1395_0000455	1.0	1.0
1395_0000458	2.0	1.0
1395_0000528	2.0	2.0
1395_0000531	2.0	1.0
1395_0000534	2.0	2.0
1395_0000548	2.0	2.0
1395_0000549	2.0	2.0
1395_0000552	2.0	2.0
1395_0000555	1.0	1.0
1395_0000556	1.0	2.0
1395_0000560	2.0	2.0
1395_0000564	1.0	1.0
1395_0000581	1.0	2.0
1395_0000582	0.0	1.0
1395_0000583	1.0	2.0
1395_0000596	2.0	1.0
1395_0000604	1.0	1.0
1395_0000609	0.0	1.0
1395_0000612	1.0	2.0
1395_0000627	1.0	1.0
1395_0000642	0.0	1.0
1395_0001010	1.0	1.0
1395_0001028	1.0	2.0
1395_0001034	1.0	1.0
1395_0001064	2.0	2.0
1395_0001065	1.0	2.0
1395_0001069	2.0	2.0
1395_0001074	1.0	2.0
1395_0001080	1.0	1.0
1395_0001104	0.0	1.0
1395_0001118	0.0	1.0
1395_0001126	1.0	1.0
1395_0001141	1.0	1.0
1395_0001147	1.0	2.0
1395_0001150	1.0	1.0
1395_0001160	2.0	1.0
LANGUAGE: IT, 4th Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.14
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.05
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        15
         1.0       0.43      0.79      0.56        48
         2.0       0.61      0.59      0.60        75
         3.0       0.00      0.00      0.00        22

    accuracy                           0.51       160
   macro avg       0.26      0.34      0.29       160
weighted avg       0.42      0.51      0.45       160

[[ 0 15  0  0]
 [ 0 38 10  0]
 [ 0 31 44  0]
 [ 0  4 18  0]]
0.4482593037214887
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.96
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.03
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        15
         1.0       0.48      0.92      0.63        48
         2.0       0.66      0.60      0.63        75
         3.0       0.00      0.00      0.00        22

    accuracy                           0.56       160
   macro avg       0.29      0.38      0.31       160
weighted avg       0.45      0.56      0.48       160

[[ 0 15  0  0]
 [ 0 44  4  0]
 [ 0 30 45  0]
 [ 0  3 19  0]]
0.4835889110889111
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.91
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.94
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        15
         1.0       0.58      0.58      0.58        48
         2.0       0.61      0.87      0.72        75
         3.0       0.50      0.14      0.21        22

    accuracy                           0.60       160
   macro avg       0.42      0.40      0.38       160
weighted avg       0.53      0.60      0.54       160

[[ 0 13  2  0]
 [ 0 28 20  0]
 [ 0  7 65  3]
 [ 0  0 19  3]]
0.5411355564325177
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.79
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.91
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        15
         1.0       0.56      0.58      0.57        48
         2.0       0.61      0.87      0.71        75
         3.0       0.67      0.09      0.16        22

    accuracy                           0.59       160
   macro avg       0.46      0.39      0.36       160
weighted avg       0.54      0.59      0.53       160

[[ 0 13  2  0]
 [ 0 28 20  0]
 [ 0  9 65  1]
 [ 0  0 20  2]]
0.5282499999999999
160 160 160
Filename	True Label	Prediction
1325_1001008	2.0	2.0
1325_1001012	2.0	2.0
1325_1001015	2.0	2.0
1325_1001016	2.0	2.0
1325_1001029	2.0	2.0
1325_1001055	2.0	2.0
1325_1001056	2.0	2.0
1325_1001059	2.0	2.0
1325_1001078	2.0	2.0
1325_1001085	2.0	2.0
1325_1001089	2.0	2.0
1325_1001107	3.0	2.0
1325_1001119	3.0	2.0
1325_1001127	3.0	2.0
1325_1001143	2.0	3.0
1325_1001153	2.0	2.0
1325_1001155	2.0	2.0
1325_9000090	2.0	2.0
1325_9000095	2.0	2.0
1325_9000105	1.0	2.0
1325_9000106	2.0	2.0
1325_9000136	2.0	2.0
1325_9000143	3.0	3.0
1325_9000209	2.0	2.0
1325_9000210	2.0	2.0
1325_9000213	3.0	2.0
1325_9000279	3.0	2.0
1325_9000296	2.0	2.0
1325_9000302	2.0	2.0
1325_9000315	2.0	2.0
1325_9000320	3.0	2.0
1325_9000321	3.0	2.0
1325_9000503	3.0	3.0
1325_9000505	2.0	2.0
1325_9000611	2.0	2.0
1325_9000612	2.0	2.0
1325_9000675	3.0	2.0
1365_0100010	1.0	2.0
1365_0100013	3.0	2.0
1365_0100016	2.0	2.0
1365_0100022	2.0	2.0
1365_0100067	2.0	2.0
1365_0100071	2.0	2.0
1365_0100073	2.0	2.0
1365_0100095	2.0	2.0
1365_0100096	2.0	2.0
1365_0100116	2.0	2.0
1365_0100117	3.0	2.0
1365_0100119	3.0	2.0
1365_0100120	3.0	2.0
1365_0100136	2.0	2.0
1365_0100139	2.0	2.0
1365_0100151	2.0	1.0
1365_0100165	3.0	2.0
1365_0100176	2.0	2.0
1365_0100190	2.0	2.0
1365_0100204	2.0	2.0
1365_0100217	3.0	2.0
1365_0100222	1.0	2.0
1365_0100223	2.0	2.0
1365_0100256	2.0	2.0
1365_0100257	2.0	2.0
1365_0100260	2.0	2.0
1365_0100268	2.0	2.0
1365_0100269	2.0	2.0
1365_0100274	2.0	2.0
1365_0100277	3.0	2.0
1365_0100278	2.0	2.0
1365_0100279	1.0	2.0
1365_0100289	2.0	2.0
1365_0100299	3.0	2.0
1365_0100447	3.0	2.0
1365_0100448	1.0	2.0
1365_0100459	2.0	2.0
1365_0100461	2.0	2.0
1365_0100471	2.0	2.0
1365_0100479	3.0	2.0
1365_0100481	2.0	2.0
1385_0000012	2.0	2.0
1385_0000016	2.0	2.0
1385_0000041	2.0	2.0
1385_0000051	2.0	2.0
1385_0000052	1.0	1.0
1385_0000059	2.0	2.0
1385_0000104	2.0	1.0
1385_0000129	2.0	1.0
1385_0001104	1.0	1.0
1385_0001118	1.0	1.0
1385_0001119	1.0	1.0
1385_0001120	1.0	1.0
1385_0001124	1.0	1.0
1385_0001128	0.0	2.0
1385_0001129	1.0	1.0
1385_0001151	2.0	2.0
1385_0001158	1.0	1.0
1385_0001167	1.0	1.0
1385_0001174	0.0	1.0
1385_0001195	1.0	2.0
1385_0001501	1.0	2.0
1385_0001522	0.0	1.0
1385_0001525	1.0	2.0
1385_0001712	1.0	2.0
1385_0001720	0.0	1.0
1385_0001724	1.0	2.0
1385_0001732	1.0	2.0
1385_0001739	0.0	2.0
1385_0001740	1.0	1.0
1385_0001747	1.0	1.0
1385_0001757	2.0	1.0
1385_0001759	0.0	1.0
1385_0001765	0.0	1.0
1385_0001771	0.0	1.0
1385_0001772	0.0	1.0
1385_0001774	0.0	1.0
1385_0001789	1.0	2.0
1385_0001790	2.0	1.0
1385_0001793	1.0	1.0
1385_0001796	2.0	2.0
1385_0001798	1.0	2.0
1395_0000341	1.0	1.0
1395_0000354	1.0	1.0
1395_0000359	2.0	2.0
1395_0000361	2.0	1.0
1395_0000366	2.0	2.0
1395_0000379	1.0	1.0
1395_0000387	3.0	2.0
1395_0000390	1.0	1.0
1395_0000391	3.0	2.0
1395_0000432	2.0	1.0
1395_0000443	2.0	2.0
1395_0000465	1.0	1.0
1395_0000471	1.0	1.0
1395_0000504	1.0	1.0
1395_0000512	2.0	2.0
1395_0000513	2.0	2.0
1395_0000515	2.0	2.0
1395_0000553	2.0	1.0
1395_0000554	2.0	2.0
1395_0000572	1.0	1.0
1395_0000575	1.0	1.0
1395_0000593	1.0	1.0
1395_0000597	1.0	1.0
1395_0000602	1.0	1.0
1395_0000631	1.0	2.0
1395_0000635	0.0	1.0
1395_0000644	1.0	2.0
1395_0001015	1.0	2.0
1395_0001019	0.0	1.0
1395_0001058	1.0	1.0
1395_0001066	1.0	2.0
1395_0001068	1.0	1.0
1395_0001075	1.0	2.0
1395_0001121	0.0	1.0
1395_0001122	1.0	1.0
1395_0001131	0.0	1.0
1395_0001132	1.0	2.0
1395_0001146	0.0	1.0
1395_0001167	1.0	2.0
1395_0001169	2.0	1.0
1395_0001171	1.0	1.0
Averaged weighted F1-scores 0.5249208877556011
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
LANGUAGE: IT, 0th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.42
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.25
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.00      0.00      0.00        29
         2.0       0.45      0.84      0.59        44
         3.0       0.00      0.00      0.00        15
         4.0       0.60      0.90      0.72        52
         5.0       0.00      0.00      0.00        16

    accuracy                           0.53       160
   macro avg       0.18      0.29      0.22       160
weighted avg       0.32      0.53      0.40       160

[[ 0  0  4  0  0  0]
 [ 0  0 28  0  1  0]
 [ 0  0 37  0  7  0]
 [ 0  0  4  0 11  0]
 [ 0  0  5  0 47  0]
 [ 0  0  4  0 12  0]]
0.39650793650793653
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.15
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.29
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.00      0.00      0.00        29
         2.0       0.46      0.86      0.60        44
         3.0       0.00      0.00      0.00        15
         4.0       0.60      0.88      0.71        52
         5.0       0.00      0.00      0.00        16

    accuracy                           0.53       160
   macro avg       0.18      0.29      0.22       160
weighted avg       0.32      0.53      0.40       160

[[ 0  0  4  0  0  0]
 [ 0  0 28  0  1  0]
 [ 0  0 38  0  6  0]
 [ 0  0  4  0 11  0]
 [ 0  0  6  0 46  0]
 [ 0  0  3  0 13  0]]
0.3963498748702924
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.10
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.27
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       1.00      0.03      0.07        29
         2.0       0.48      0.80      0.60        44
         3.0       0.00      0.00      0.00        15
         4.0       0.57      0.94      0.71        52
         5.0       0.00      0.00      0.00        16

    accuracy                           0.53       160
   macro avg       0.34      0.30      0.23       160
weighted avg       0.50      0.53      0.41       160

[[ 0  0  4  0  0  0]
 [ 0  1 26  0  2  0]
 [ 0  0 35  0  9  0]
 [ 0  0  3  0 12  0]
 [ 0  0  3  0 49  0]
 [ 0  0  2  0 14  0]]
0.40741034931252323
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.01
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.23
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.36      0.17      0.23        29
         2.0       0.46      0.66      0.54        44
         3.0       0.00      0.00      0.00        15
         4.0       0.59      0.92      0.72        52
         5.0       0.00      0.00      0.00        16

    accuracy                           0.51       160
   macro avg       0.23      0.29      0.25       160
weighted avg       0.38      0.51      0.42       160

[[ 0  1  3  0  0  0]
 [ 0  5 22  1  1  0]
 [ 0  8 29  0  7  0]
 [ 0  0  3  0 12  0]
 [ 0  0  4  0 48  0]
 [ 0  0  2  0 14  0]]
0.4240524042469677
160 160 160
Filename	True Label	Prediction
1325_1001012	4.0	4.0
1325_1001019	4.0	4.0
1325_1001021	5.0	4.0
1325_1001027	4.0	4.0
1325_1001028	4.0	4.0
1325_1001032	4.0	4.0
1325_1001047	4.0	4.0
1325_1001052	5.0	4.0
1325_1001076	5.0	4.0
1325_1001077	4.0	4.0
1325_1001087	4.0	4.0
1325_1001089	3.0	4.0
1325_1001108	4.0	4.0
1325_1001111	4.0	4.0
1325_1001125	4.0	4.0
1325_1001134	4.0	4.0
1325_1001136	3.0	4.0
1325_1001143	4.0	4.0
1325_1001160	4.0	4.0
1325_1001163	4.0	4.0
1325_1001169	4.0	4.0
1325_9000105	4.0	4.0
1325_9000186	4.0	4.0
1325_9000187	4.0	4.0
1325_9000209	4.0	4.0
1325_9000278	3.0	4.0
1325_9000317	4.0	4.0
1325_9000319	4.0	4.0
1325_9000322	4.0	4.0
1325_9000602	4.0	4.0
1325_9000686	3.0	4.0
1365_0100005	3.0	4.0
1365_0100007	2.0	4.0
1365_0100015	4.0	4.0
1365_0100016	4.0	4.0
1365_0100022	4.0	4.0
1365_0100026	2.0	2.0
1365_0100029	1.0	4.0
1365_0100031	5.0	2.0
1365_0100056	4.0	4.0
1365_0100069	4.0	4.0
1365_0100072	4.0	4.0
1365_0100073	3.0	4.0
1365_0100092	4.0	4.0
1365_0100104	3.0	4.0
1365_0100105	5.0	4.0
1365_0100107	4.0	4.0
1365_0100119	5.0	4.0
1365_0100120	5.0	4.0
1365_0100135	4.0	4.0
1365_0100136	4.0	4.0
1365_0100137	5.0	4.0
1365_0100147	4.0	4.0
1365_0100163	5.0	4.0
1365_0100165	4.0	4.0
1365_0100167	2.0	4.0
1365_0100168	4.0	4.0
1365_0100171	4.0	4.0
1365_0100180	5.0	4.0
1365_0100182	4.0	4.0
1365_0100185	3.0	4.0
1365_0100186	5.0	4.0
1365_0100188	5.0	4.0
1365_0100190	4.0	2.0
1365_0100202	3.0	4.0
1365_0100205	5.0	4.0
1365_0100211	4.0	4.0
1365_0100224	3.0	4.0
1365_0100230	4.0	4.0
1365_0100231	4.0	4.0
1365_0100256	4.0	4.0
1365_0100257	3.0	4.0
1365_0100258	5.0	4.0
1365_0100259	4.0	4.0
1365_0100262	4.0	4.0
1365_0100270	3.0	4.0
1365_0100280	2.0	4.0
1365_0100281	5.0	4.0
1365_0100290	4.0	4.0
1365_0100451	2.0	4.0
1365_0100456	2.0	4.0
1365_0100457	2.0	4.0
1365_0100476	4.0	4.0
1385_0000011	1.0	2.0
1385_0000012	2.0	2.0
1385_0000016	1.0	2.0
1385_0000033	1.0	2.0
1385_0000037	2.0	2.0
1385_0000043	2.0	2.0
1385_0000049	2.0	2.0
1385_0000050	2.0	2.0
1385_0000051	2.0	2.0
1385_0000057	1.0	2.0
1385_0000058	2.0	2.0
1385_0000104	2.0	1.0
1385_0000123	2.0	2.0
1385_0000124	2.0	4.0
1385_0001103	2.0	1.0
1385_0001105	2.0	1.0
1385_0001129	1.0	1.0
1385_0001131	2.0	2.0
1385_0001136	2.0	1.0
1385_0001138	2.0	1.0
1385_0001159	1.0	2.0
1385_0001167	2.0	2.0
1385_0001169	2.0	1.0
1385_0001192	1.0	2.0
1385_0001193	2.0	2.0
1385_0001197	2.0	2.0
1385_0001524	1.0	2.0
1385_0001726	1.0	2.0
1385_0001728	2.0	1.0
1385_0001733	1.0	2.0
1385_0001737	2.0	2.0
1385_0001739	1.0	2.0
1385_0001750	0.0	1.0
1385_0001766	2.0	2.0
1385_0001775	2.0	2.0
1385_0001785	1.0	1.0
1385_0001786	2.0	2.0
1385_0001788	1.0	2.0
1385_0001790	2.0	2.0
1385_0001791	1.0	2.0
1385_0001796	1.0	2.0
1395_0000360	3.0	2.0
1395_0000378	2.0	2.0
1395_0000390	1.0	1.0
1395_0000403	5.0	2.0
1395_0000414	2.0	2.0
1395_0000452	2.0	2.0
1395_0000470	2.0	1.0
1395_0000514	4.0	4.0
1395_0000515	4.0	2.0
1395_0000518	4.0	2.0
1395_0000535	2.0	2.0
1395_0000552	3.0	2.0
1395_0000556	2.0	2.0
1395_0000557	4.0	2.0
1395_0000575	2.0	2.0
1395_0000581	1.0	3.0
1395_0000582	0.0	2.0
1395_0000584	0.0	2.0
1395_0000596	3.0	2.0
1395_0000599	1.0	2.0
1395_0000627	1.0	2.0
1395_0000628	1.0	2.0
1395_0000630	1.0	2.0
1395_0001013	2.0	2.0
1395_0001022	2.0	2.0
1395_0001040	1.0	1.0
1395_0001064	2.0	2.0
1395_0001066	1.0	2.0
1395_0001074	1.0	2.0
1395_0001076	1.0	2.0
1395_0001084	2.0	2.0
1395_0001109	0.0	2.0
1395_0001119	2.0	2.0
1395_0001120	1.0	1.0
1395_0001126	1.0	2.0
1395_0001150	1.0	2.0
LANGUAGE: IT, 1th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.43
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.20
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.36      0.31      0.33        29
         2.0       0.50      0.70      0.58        44
         3.0       0.00      0.00      0.00        15
         4.0       0.64      0.89      0.75        53
         5.0       0.00      0.00      0.00        15

    accuracy                           0.54       160
   macro avg       0.25      0.32      0.28       160
weighted avg       0.42      0.54      0.47       160

[[ 0  3  1  0  0  0]
 [ 0  9 18  0  2  0]
 [ 0 10 31  0  3  0]
 [ 0  0  4  0 11  0]
 [ 0  2  4  0 47  0]
 [ 0  1  4  0 10  0]]
0.46838873914345613
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.16
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.17
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.31      0.52      0.38        29
         2.0       0.52      0.39      0.44        44
         3.0       0.00      0.00      0.00        15
         4.0       0.60      0.89      0.72        53
         5.0       0.00      0.00      0.00        15

    accuracy                           0.49       160
   macro avg       0.24      0.30      0.26       160
weighted avg       0.40      0.49      0.43       160

[[ 0  4  0  0  0  0]
 [ 0 15 12  0  2  0]
 [ 0 24 17  0  3  0]
 [ 0  0  1  0 14  0]
 [ 0  3  3  0 47  0]
 [ 0  3  0  0 12  0]]
0.4288309495847663
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.08
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.16
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.00      0.00      0.00        29
         2.0       0.52      0.91      0.66        44
         3.0       0.00      0.00      0.00        15
         4.0       0.59      0.92      0.72        53
         5.0       0.00      0.00      0.00        15

    accuracy                           0.56       160
   macro avg       0.18      0.31      0.23       160
weighted avg       0.34      0.56      0.42       160

[[ 0  0  4  0  0  0]
 [ 0  0 26  0  3  0]
 [ 0  0 40  0  4  0]
 [ 0  0  0  0 15  0]
 [ 0  0  4  0 49  0]
 [ 0  0  3  0 12  0]]
0.4205130347593583
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.99
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.17
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.67      0.07      0.12        29
         2.0       0.54      0.91      0.68        44
         3.0       0.00      0.00      0.00        15
         4.0       0.59      0.92      0.72        53
         5.0       0.00      0.00      0.00        15

    accuracy                           0.57       160
   macro avg       0.30      0.32      0.25       160
weighted avg       0.47      0.57      0.45       160

[[ 0  1  3  0  0  0]
 [ 0  2 24  0  3  0]
 [ 0  0 40  0  4  0]
 [ 0  0  0  0 15  0]
 [ 0  0  4  0 49  0]
 [ 0  0  3  0 12  0]]
0.44779178090727817
160 160 160
Filename	True Label	Prediction
1325_1001010	5.0	4.0
1325_1001011	4.0	4.0
1325_1001014	4.0	4.0
1325_1001015	4.0	4.0
1325_1001029	4.0	4.0
1325_1001042	5.0	4.0
1325_1001050	4.0	4.0
1325_1001053	5.0	4.0
1325_1001056	4.0	4.0
1325_1001059	4.0	4.0
1325_1001062	4.0	4.0
1325_1001075	3.0	4.0
1325_1001086	4.0	4.0
1325_1001088	4.0	4.0
1325_1001090	4.0	4.0
1325_1001091	4.0	4.0
1325_1001095	5.0	4.0
1325_1001097	1.0	4.0
1325_1001098	4.0	4.0
1325_1001099	4.0	4.0
1325_1001100	4.0	4.0
1325_1001101	4.0	4.0
1325_1001120	5.0	4.0
1325_1001121	4.0	4.0
1325_1001128	4.0	4.0
1325_1001131	3.0	4.0
1325_1001133	5.0	4.0
1325_1001138	4.0	4.0
1325_1001152	4.0	4.0
1325_1001153	4.0	4.0
1325_1001154	4.0	4.0
1325_1001162	4.0	4.0
1325_9000089	4.0	4.0
1325_9000099	5.0	4.0
1325_9000152	4.0	4.0
1325_9000211	4.0	4.0
1325_9000240	2.0	4.0
1325_9000241	4.0	4.0
1325_9000302	3.0	4.0
1325_9000303	4.0	4.0
1325_9000315	3.0	4.0
1325_9000316	3.0	4.0
1325_9000323	4.0	4.0
1325_9000536	4.0	4.0
1325_9000611	3.0	4.0
1325_9000674	4.0	4.0
1365_0100010	1.0	4.0
1365_0100058	4.0	4.0
1365_0100066	3.0	4.0
1365_0100099	3.0	4.0
1365_0100100	4.0	4.0
1365_0100106	3.0	4.0
1365_0100121	4.0	4.0
1365_0100125	4.0	4.0
1365_0100133	5.0	4.0
1365_0100145	5.0	4.0
1365_0100146	4.0	4.0
1365_0100148	4.0	4.0
1365_0100162	4.0	4.0
1365_0100166	3.0	4.0
1365_0100179	4.0	4.0
1365_0100181	3.0	4.0
1365_0100184	4.0	4.0
1365_0100194	4.0	4.0
1365_0100203	4.0	4.0
1365_0100221	3.0	4.0
1365_0100222	3.0	4.0
1365_0100223	4.0	4.0
1365_0100227	4.0	4.0
1365_0100228	3.0	4.0
1365_0100261	5.0	4.0
1365_0100282	5.0	4.0
1365_0100455	2.0	4.0
1365_0100473	4.0	4.0
1365_0100474	4.0	4.0
1365_0100478	4.0	4.0
1365_0100481	4.0	4.0
1385_0000021	1.0	2.0
1385_0000022	0.0	2.0
1385_0000038	2.0	2.0
1385_0000045	2.0	2.0
1385_0000053	1.0	2.0
1385_0000054	2.0	2.0
1385_0000098	2.0	2.0
1385_0000129	2.0	2.0
1385_0001109	2.0	2.0
1385_0001112	2.0	2.0
1385_0001118	2.0	2.0
1385_0001135	2.0	2.0
1385_0001137	2.0	2.0
1385_0001149	2.0	2.0
1385_0001151	2.0	2.0
1385_0001156	2.0	2.0
1385_0001170	1.0	2.0
1385_0001172	1.0	2.0
1385_0001174	1.0	2.0
1385_0001188	2.0	2.0
1385_0001190	1.0	2.0
1385_0001196	2.0	2.0
1385_0001522	1.0	2.0
1385_0001738	0.0	2.0
1385_0001748	1.0	2.0
1385_0001752	1.0	2.0
1385_0001756	2.0	2.0
1385_0001761	1.0	2.0
1385_0001767	1.0	2.0
1385_0001771	1.0	2.0
1385_0001772	1.0	2.0
1385_0001792	1.0	2.0
1385_0001795	1.0	2.0
1385_0001798	1.0	2.0
1395_0000337	1.0	1.0
1395_0000340	2.0	2.0
1395_0000355	2.0	2.0
1395_0000356	2.0	2.0
1395_0000368	1.0	1.0
1395_0000369	4.0	2.0
1395_0000383	5.0	2.0
1395_0000387	5.0	4.0
1395_0000392	5.0	2.0
1395_0000399	2.0	2.0
1395_0000404	4.0	4.0
1395_0000432	5.0	2.0
1395_0000446	3.0	4.0
1395_0000449	4.0	2.0
1395_0000460	2.0	2.0
1395_0000462	4.0	2.0
1395_0000513	4.0	2.0
1395_0000526	2.0	2.0
1395_0000529	2.0	2.0
1395_0000549	2.0	2.0
1395_0000550	2.0	4.0
1395_0000559	2.0	2.0
1395_0000564	2.0	2.0
1395_0000587	0.0	1.0
1395_0000602	1.0	2.0
1395_0000606	0.0	2.0
1395_0000609	1.0	2.0
1395_0000610	2.0	2.0
1395_0000626	2.0	2.0
1395_0000639	1.0	2.0
1395_0001010	2.0	2.0
1395_0001017	2.0	2.0
1395_0001023	2.0	2.0
1395_0001033	2.0	2.0
1395_0001045	2.0	2.0
1395_0001058	2.0	2.0
1395_0001061	2.0	4.0
1395_0001068	1.0	4.0
1395_0001078	2.0	2.0
1395_0001080	2.0	2.0
1395_0001101	2.0	2.0
1395_0001115	2.0	2.0
1395_0001132	1.0	2.0
1395_0001149	1.0	2.0
1395_0001158	2.0	2.0
1395_0001161	1.0	2.0
1395_0001169	2.0	2.0
1395_0001170	1.0	2.0
1395_0001171	1.0	2.0
LANGUAGE: IT, 2th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.38
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.17
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.60      0.10      0.18        29
         2.0       0.47      0.82      0.60        44
         3.0       0.00      0.00      0.00        15
         4.0       0.65      0.96      0.77        53
         5.0       0.00      0.00      0.00        15

    accuracy                           0.56       160
   macro avg       0.29      0.31      0.26       160
weighted avg       0.45      0.56      0.45       160

[[ 0  0  4  0  0  0]
 [ 0  3 25  0  1  0]
 [ 0  2 36  0  6  0]
 [ 0  0  4  0 11  0]
 [ 0  0  2  0 51  0]
 [ 0  0  5  0 10  0]]
0.45295120320855614
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.15
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.06
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       1.00      0.07      0.13        29
         2.0       0.53      0.89      0.67        44
         3.0       0.00      0.00      0.00        15
         4.0       0.61      0.98      0.75        53
         5.0       0.00      0.00      0.00        15

    accuracy                           0.58       160
   macro avg       0.36      0.32      0.26       160
weighted avg       0.53      0.58      0.46       160

[[ 0  0  4  0  0  0]
 [ 0  2 26  0  1  0]
 [ 0  0 39  0  5  0]
 [ 0  0  1  0 14  0]
 [ 0  0  1  0 52  0]
 [ 0  0  2  0 13  0]]
0.45635811126694714
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.05
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.04
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.40      0.14      0.21        29
         2.0       0.52      0.82      0.64        44
         3.0       0.00      0.00      0.00        15
         4.0       0.64      0.98      0.78        53
         5.0       0.00      0.00      0.00        15

    accuracy                           0.57       160
   macro avg       0.26      0.32      0.27       160
weighted avg       0.43      0.57      0.47       160

[[ 0  2  2  0  0  0]
 [ 0  4 25  0  0  0]
 [ 0  4 36  0  4  0]
 [ 0  0  3  0 12  0]
 [ 0  0  1  0 52  0]
 [ 0  0  2  0 13  0]]
0.4694902783563462
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.96
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.05
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         4
         1.0       0.50      0.34      0.41        29
         2.0       0.52      0.73      0.60        44
         3.0       0.00      0.00      0.00        15
         4.0       0.64      0.94      0.76        53
         5.0       0.00      0.00      0.00        15

    accuracy                           0.57       160
   macro avg       0.28      0.34      0.30       160
weighted avg       0.44      0.57      0.49       160

[[ 0  2  2  0  0  0]
 [ 0 10 19  0  0  0]
 [ 0  8 32  0  4  0]
 [ 0  0  4  0 11  0]
 [ 0  0  3  0 50  0]
 [ 0  0  2  0 13  0]]
0.4928799231056386
160 160 160
Filename	True Label	Prediction
1325_1001018	4.0	4.0
1325_1001022	4.0	4.0
1325_1001024	4.0	4.0
1325_1001025	4.0	4.0
1325_1001033	5.0	4.0
1325_1001037	5.0	4.0
1325_1001039	5.0	4.0
1325_1001040	4.0	4.0
1325_1001041	4.0	4.0
1325_1001043	4.0	4.0
1325_1001045	5.0	4.0
1325_1001063	4.0	4.0
1325_1001094	4.0	4.0
1325_1001096	4.0	4.0
1325_1001107	4.0	4.0
1325_1001129	3.0	4.0
1325_1001139	4.0	4.0
1325_1001144	4.0	4.0
1325_1001155	4.0	4.0
1325_1001165	4.0	4.0
1325_1001167	4.0	4.0
1325_1001170	4.0	4.0
1325_9000087	5.0	4.0
1325_9000088	4.0	4.0
1325_9000095	4.0	4.0
1325_9000102	3.0	4.0
1325_9000107	4.0	4.0
1325_9000136	4.0	4.0
1325_9000144	4.0	4.0
1325_9000188	4.0	4.0
1325_9000213	4.0	4.0
1325_9000215	4.0	4.0
1325_9000279	3.0	4.0
1325_9000318	4.0	4.0
1325_9000504	4.0	4.0
1325_9000534	4.0	4.0
1325_9000676	3.0	4.0
1325_9000700	4.0	4.0
1325_9000750	4.0	2.0
1365_0100013	4.0	4.0
1365_0100020	3.0	4.0
1365_0100021	3.0	4.0
1365_0100023	2.0	4.0
1365_0100063	4.0	4.0
1365_0100065	2.0	4.0
1365_0100071	4.0	4.0
1365_0100074	3.0	4.0
1365_0100094	5.0	4.0
1365_0100101	4.0	4.0
1365_0100103	5.0	4.0
1365_0100118	4.0	4.0
1365_0100123	4.0	4.0
1365_0100134	5.0	4.0
1365_0100151	4.0	4.0
1365_0100164	3.0	4.0
1365_0100173	5.0	4.0
1365_0100177	5.0	4.0
1365_0100187	5.0	4.0
1365_0100196	4.0	4.0
1365_0100198	4.0	4.0
1365_0100199	4.0	4.0
1365_0100201	5.0	4.0
1365_0100204	5.0	4.0
1365_0100213	4.0	2.0
1365_0100218	4.0	4.0
1365_0100220	4.0	4.0
1365_0100225	4.0	4.0
1365_0100232	4.0	4.0
1365_0100251	3.0	4.0
1365_0100252	4.0	4.0
1365_0100255	3.0	4.0
1365_0100260	4.0	4.0
1365_0100266	3.0	4.0
1365_0100269	4.0	4.0
1365_0100279	4.0	4.0
1365_0100286	3.0	2.0
1365_0100459	4.0	4.0
1365_0100461	2.0	4.0
1365_0100475	4.0	4.0
1365_0100477	4.0	4.0
1385_0000013	1.0	1.0
1385_0000023	2.0	1.0
1385_0000034	2.0	2.0
1385_0000035	2.0	1.0
1385_0000052	1.0	1.0
1385_0000095	1.0	1.0
1385_0000099	1.0	2.0
1385_0000114	2.0	2.0
1385_0000122	2.0	2.0
1385_0000127	2.0	2.0
1385_0001123	2.0	1.0
1385_0001127	2.0	2.0
1385_0001130	1.0	1.0
1385_0001132	2.0	1.0
1385_0001133	2.0	2.0
1385_0001148	2.0	2.0
1385_0001152	2.0	2.0
1385_0001153	2.0	1.0
1385_0001157	2.0	1.0
1385_0001163	1.0	2.0
1385_0001166	2.0	2.0
1385_0001175	1.0	2.0
1385_0001189	1.0	2.0
1385_0001503	2.0	2.0
1385_0001523	2.0	2.0
1385_0001527	2.0	2.0
1385_0001712	1.0	2.0
1385_0001714	1.0	2.0
1385_0001719	1.0	2.0
1385_0001724	2.0	2.0
1385_0001727	1.0	2.0
1385_0001729	2.0	2.0
1385_0001732	1.0	2.0
1385_0001736	2.0	1.0
1385_0001740	2.0	2.0
1385_0001741	1.0	1.0
1385_0001742	0.0	1.0
1385_0001744	0.0	2.0
1385_0001754	1.0	2.0
1385_0001757	2.0	2.0
1385_0001758	1.0	1.0
1385_0001759	1.0	2.0
1385_0001764	1.0	2.0
1385_0001800	1.0	1.0
1395_0000333	2.0	2.0
1395_0000353	2.0	1.0
1395_0000359	3.0	2.0
1395_0000361	4.0	2.0
1395_0000380	5.0	2.0
1395_0000389	1.0	1.0
1395_0000391	3.0	2.0
1395_0000443	5.0	2.0
1395_0000450	2.0	2.0
1395_0000458	2.0	2.0
1395_0000465	2.0	2.0
1395_0000499	2.0	2.0
1395_0000527	1.0	1.0
1395_0000531	2.0	2.0
1395_0000548	2.0	2.0
1395_0000560	3.0	2.0
1395_0000563	2.0	2.0
1395_0000565	2.0	2.0
1395_0000597	1.0	2.0
1395_0000598	1.0	2.0
1395_0000646	2.0	2.0
1395_0000649	2.0	2.0
1395_0001016	2.0	2.0
1395_0001019	2.0	2.0
1395_0001021	2.0	2.0
1395_0001065	1.0	2.0
1395_0001067	1.0	2.0
1395_0001093	2.0	2.0
1395_0001116	2.0	2.0
1395_0001122	1.0	1.0
1395_0001124	0.0	2.0
1395_0001131	1.0	2.0
1395_0001133	1.0	2.0
1395_0001146	0.0	1.0
1395_0001164	2.0	4.0
1395_0001167	1.0	2.0
LANGUAGE: IT, 3th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.39
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.20
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.00      0.00      0.00        28
         2.0       0.48      0.86      0.62        44
         3.0       0.00      0.00      0.00        16
         4.0       0.63      0.98      0.77        52
         5.0       0.00      0.00      0.00        15

    accuracy                           0.56       160
   macro avg       0.19      0.31      0.23       160
weighted avg       0.34      0.56      0.42       160

[[ 0  0  4  0  1  0]
 [ 0  0 27  0  1  0]
 [ 0  0 38  0  6  0]
 [ 0  0  6  0 10  0]
 [ 0  0  1  0 51  0]
 [ 0  0  3  0 12  0]]
0.41916681948774376
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.16
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.21
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.00      0.00      0.00        28
         2.0       0.50      0.77      0.61        44
         3.0       0.00      0.00      0.00        16
         4.0       0.55      0.98      0.71        52
         5.0       0.00      0.00      0.00        15

    accuracy                           0.53       160
   macro avg       0.18      0.29      0.22       160
weighted avg       0.32      0.53      0.40       160

[[ 0  0  3  0  2  0]
 [ 0  0 26  0  2  0]
 [ 0  0 34  0 10  0]
 [ 0  0  3  0 13  0]
 [ 0  0  1  0 51  0]
 [ 0  0  1  0 14  0]]
0.39717261904761897
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.04
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.11
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.57      0.46      0.51        28
         2.0       0.54      0.73      0.62        44
         3.0       0.00      0.00      0.00        16
         4.0       0.65      0.98      0.78        52
         5.0       0.00      0.00      0.00        15

    accuracy                           0.60       160
   macro avg       0.29      0.36      0.32       160
weighted avg       0.46      0.60      0.52       160

[[ 0  2  3  0  0  0]
 [ 0 13 15  0  0  0]
 [ 0  7 32  0  5  0]
 [ 0  0  6  0 10  0]
 [ 0  0  1  0 51  0]
 [ 0  1  2  0 12  0]]
0.5150894726822768
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.96
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 1.14
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.55      0.39      0.46        28
         2.0       0.58      0.70      0.64        44
         3.0       0.00      0.00      0.00        16
         4.0       0.59      0.98      0.73        52
         5.0       0.00      0.00      0.00        15

    accuracy                           0.58       160
   macro avg       0.29      0.35      0.31       160
weighted avg       0.45      0.58      0.49       160

[[ 0  2  2  0  1  0]
 [ 0 11 14  0  3  0]
 [ 0  6 31  0  7  0]
 [ 0  0  5  0 11  0]
 [ 0  0  1  0 51  0]
 [ 0  1  0  0 14  0]]
0.49447073784271556
160 160 160
Filename	True Label	Prediction
1325_1001017	4.0	4.0
1325_1001023	4.0	4.0
1325_1001035	5.0	4.0
1325_1001036	5.0	4.0
1325_1001044	3.0	4.0
1325_1001046	4.0	4.0
1325_1001054	4.0	4.0
1325_1001058	4.0	4.0
1325_1001078	4.0	4.0
1325_1001080	4.0	4.0
1325_1001092	4.0	4.0
1325_1001093	4.0	4.0
1325_1001109	4.0	4.0
1325_1001122	4.0	4.0
1325_1001123	4.0	4.0
1325_1001124	3.0	4.0
1325_1001135	3.0	4.0
1325_1001141	2.0	4.0
1325_1001142	4.0	4.0
1325_1001156	4.0	4.0
1325_1001157	4.0	4.0
1325_1001158	4.0	4.0
1325_1001166	4.0	4.0
1325_9000090	4.0	4.0
1325_9000106	4.0	4.0
1325_9000138	4.0	4.0
1325_9000139	4.0	4.0
1325_9000185	4.0	4.0
1325_9000210	3.0	4.0
1325_9000214	4.0	4.0
1325_9000237	3.0	4.0
1325_9000304	3.0	4.0
1325_9000503	4.0	4.0
1325_9000612	2.0	4.0
1325_9000677	4.0	4.0
1325_9000685	4.0	4.0
1365_0100003	2.0	4.0
1365_0100006	4.0	4.0
1365_0100009	3.0	2.0
1365_0100011	4.0	4.0
1365_0100014	4.0	4.0
1365_0100024	4.0	4.0
1365_0100030	4.0	4.0
1365_0100051	2.0	2.0
1365_0100057	4.0	4.0
1365_0100067	3.0	4.0
1365_0100096	4.0	4.0
1365_0100097	4.0	4.0
1365_0100098	2.0	4.0
1365_0100116	4.0	4.0
1365_0100117	5.0	4.0
1365_0100138	5.0	4.0
1365_0100139	4.0	4.0
1365_0100169	5.0	4.0
1365_0100172	5.0	4.0
1365_0100174	3.0	4.0
1365_0100175	4.0	4.0
1365_0100192	4.0	4.0
1365_0100195	4.0	4.0
1365_0100212	5.0	4.0
1365_0100215	4.0	4.0
1365_0100217	5.0	4.0
1365_0100219	4.0	4.0
1365_0100233	4.0	4.0
1365_0100253	3.0	4.0
1365_0100263	4.0	4.0
1365_0100275	4.0	4.0
1365_0100276	4.0	4.0
1365_0100277	4.0	4.0
1365_0100287	4.0	4.0
1365_0100288	4.0	4.0
1365_0100289	5.0	4.0
1365_0100299	5.0	4.0
1365_0100447	5.0	4.0
1365_0100469	5.0	4.0
1365_0100470	4.0	4.0
1365_0100471	3.0	4.0
1365_0100479	4.0	4.0
1365_0100482	4.0	4.0
1385_0000020	2.0	2.0
1385_0000100	2.0	1.0
1385_0000101	1.0	1.0
1385_0000102	2.0	2.0
1385_0000103	2.0	1.0
1385_0000126	2.0	2.0
1385_0000130	2.0	2.0
1385_0001104	1.0	2.0
1385_0001107	2.0	1.0
1385_0001108	2.0	2.0
1385_0001110	2.0	1.0
1385_0001113	1.0	1.0
1385_0001120	2.0	1.0
1385_0001122	2.0	1.0
1385_0001124	1.0	2.0
1385_0001125	2.0	2.0
1385_0001126	1.0	1.0
1385_0001150	2.0	2.0
1385_0001158	2.0	2.0
1385_0001160	1.0	4.0
1385_0001161	2.0	2.0
1385_0001162	1.0	2.0
1385_0001165	2.0	2.0
1385_0001171	1.0	1.0
1385_0001173	0.0	1.0
1385_0001194	2.0	2.0
1385_0001195	2.0	2.0
1385_0001198	2.0	2.0
1385_0001528	2.0	2.0
1385_0001715	1.0	2.0
1385_0001720	1.0	2.0
1385_0001725	1.0	1.0
1385_0001730	2.0	2.0
1385_0001753	1.0	2.0
1385_0001760	0.0	2.0
1385_0001774	1.0	1.0
1385_0001799	2.0	2.0
1395_0000338	2.0	2.0
1395_0000364	2.0	2.0
1395_0000366	3.0	4.0
1395_0000379	1.0	2.0
1395_0000396	2.0	4.0
1395_0000398	5.0	4.0
1395_0000409	3.0	2.0
1395_0000413	2.0	4.0
1395_0000415	2.0	2.0
1395_0000438	5.0	4.0
1395_0000447	2.0	2.0
1395_0000454	2.0	2.0
1395_0000455	2.0	2.0
1395_0000469	2.0	2.0
1395_0000471	2.0	2.0
1395_0000512	3.0	2.0
1395_0000516	1.0	1.0
1395_0000525	5.0	1.0
1395_0000528	4.0	2.0
1395_0000534	2.0	2.0
1395_0000537	2.0	2.0
1395_0000547	3.0	2.0
1395_0000551	3.0	2.0
1395_0000553	2.0	2.0
1395_0000572	2.0	2.0
1395_0000583	1.0	4.0
1395_0000591	0.0	1.0
1395_0000593	0.0	2.0
1395_0000604	1.0	1.0
1395_0000607	1.0	1.0
1395_0000608	1.0	2.0
1395_0000611	1.0	2.0
1395_0000612	0.0	4.0
1395_0000635	1.0	2.0
1395_0000642	1.0	2.0
1395_0001020	1.0	2.0
1395_0001028	2.0	2.0
1395_0001070	2.0	4.0
1395_0001075	1.0	2.0
1395_0001103	1.0	2.0
1395_0001117	1.0	1.0
1395_0001118	1.0	1.0
1395_0001147	1.0	4.0
1395_0001160	2.0	2.0
LANGUAGE: IT, 4th Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.43
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.10
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.00      0.00      0.00        28
         2.0       0.53      0.95      0.68        44
         3.0       0.00      0.00      0.00        16
         4.0       0.62      0.96      0.75        52
         5.0       0.00      0.00      0.00        15

    accuracy                           0.57       160
   macro avg       0.19      0.32      0.24       160
weighted avg       0.35      0.57      0.43       160

[[ 0  0  5  0  0  0]
 [ 0  0 28  0  0  0]
 [ 0  0 42  0  2  0]
 [ 0  0  1  0 15  0]
 [ 0  0  2  0 50  0]
 [ 0  0  1  0 14  0]]
0.43216578030441954
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.17
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.04
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.00      0.00      0.00        28
         2.0       0.54      0.98      0.70        44
         3.0       0.00      0.00      0.00        16
         4.0       0.63      0.98      0.77        52
         5.0       0.00      0.00      0.00        15

    accuracy                           0.59       160
   macro avg       0.20      0.33      0.24       160
weighted avg       0.35      0.59      0.44       160

[[ 0  0  5  0  0  0]
 [ 0  0 28  0  0  0]
 [ 0  0 43  0  1  0]
 [ 0  0  1  0 15  0]
 [ 0  0  1  0 51  0]
 [ 0  0  1  0 14  0]]
0.44152454306497957
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.09
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.05
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.00      0.00      0.00        28
         2.0       0.54      0.98      0.69        44
         3.0       0.00      0.00      0.00        16
         4.0       0.62      0.96      0.76        52
         5.0       0.00      0.00      0.00        15

    accuracy                           0.58       160
   macro avg       0.19      0.32      0.24       160
weighted avg       0.35      0.58      0.44       160

[[ 0  0  5  0  0  0]
 [ 0  0 28  0  0  0]
 [ 0  0 43  0  1  0]
 [ 0  0  1  0 15  0]
 [ 0  0  2  0 50  0]
 [ 0  0  1  0 14  0]]
0.43693792766373407
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.03
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.06
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         5
         1.0       0.00      0.00      0.00        28
         2.0       0.54      0.98      0.69        44
         3.0       0.00      0.00      0.00        16
         4.0       0.61      0.87      0.71        52
         5.0       0.00      0.00      0.00        15

    accuracy                           0.55       160
   macro avg       0.19      0.31      0.23       160
weighted avg       0.35      0.55      0.42       160

[[ 0  0  5  0  0  0]
 [ 0  0 28  0  0  0]
 [ 0  0 43  0  1  0]
 [ 0  0  1  0 14  1]
 [ 0  0  2  0 45  5]
 [ 0  0  1  0 14  0]]
0.4228686635944701
160 160 160
Filename	True Label	Prediction
1325_1001008	5.0	4.0
1325_1001009	5.0	4.0
1325_1001013	4.0	4.0
1325_1001016	5.0	4.0
1325_1001020	4.0	4.0
1325_1001048	4.0	4.0
1325_1001051	4.0	4.0
1325_1001055	5.0	4.0
1325_1001057	4.0	4.0
1325_1001079	4.0	4.0
1325_1001081	4.0	4.0
1325_1001082	5.0	4.0
1325_1001083	5.0	4.0
1325_1001084	4.0	4.0
1325_1001085	4.0	4.0
1325_1001110	4.0	4.0
1325_1001113	4.0	4.0
1325_1001119	4.0	4.0
1325_1001126	3.0	4.0
1325_1001127	4.0	4.0
1325_1001130	4.0	4.0
1325_1001132	4.0	4.0
1325_1001159	5.0	4.0
1325_1001161	4.0	4.0
1325_1001164	3.0	4.0
1325_1001168	4.0	4.0
1325_9000059	5.0	4.0
1325_9000104	3.0	4.0
1325_9000137	4.0	4.0
1325_9000140	4.0	4.0
1325_9000143	4.0	4.0
1325_9000239	3.0	4.0
1325_9000296	3.0	4.0
1325_9000314	4.0	4.0
1325_9000320	3.0	4.0
1325_9000321	4.0	4.0
1325_9000505	4.0	4.0
1325_9000533	4.0	4.0
1325_9000554	3.0	4.0
1325_9000601	4.0	4.0
1325_9000675	4.0	4.0
1325_9000678	4.0	4.0
1325_9000684	4.0	4.0
1365_0100002	4.0	4.0
1365_0100004	3.0	4.0
1365_0100008	4.0	5.0
1365_0100012	4.0	5.0
1365_0100017	4.0	4.0
1365_0100018	3.0	4.0
1365_0100019	4.0	5.0
1365_0100027	4.0	4.0
1365_0100028	4.0	4.0
1365_0100061	5.0	4.0
1365_0100064	4.0	4.0
1365_0100070	4.0	4.0
1365_0100079	5.0	4.0
1365_0100080	4.0	4.0
1365_0100093	4.0	5.0
1365_0100095	4.0	4.0
1365_0100102	4.0	4.0
1365_0100170	4.0	4.0
1365_0100176	4.0	4.0
1365_0100178	4.0	5.0
1365_0100183	4.0	4.0
1365_0100191	3.0	4.0
1365_0100200	5.0	4.0
1365_0100226	5.0	4.0
1365_0100229	5.0	4.0
1365_0100265	4.0	4.0
1365_0100267	4.0	4.0
1365_0100268	3.0	4.0
1365_0100274	4.0	4.0
1365_0100278	4.0	4.0
1365_0100285	3.0	4.0
1365_0100448	2.0	4.0
1365_0100458	4.0	4.0
1365_0100472	3.0	4.0
1365_0100480	5.0	4.0
1385_0000017	1.0	2.0
1385_0000036	2.0	2.0
1385_0000039	2.0	2.0
1385_0000040	1.0	2.0
1385_0000041	2.0	2.0
1385_0000042	2.0	2.0
1385_0000044	2.0	2.0
1385_0000047	2.0	2.0
1385_0000048	2.0	2.0
1385_0000059	2.0	2.0
1385_0000097	2.0	2.0
1385_0000119	2.0	2.0
1385_0000120	1.0	2.0
1385_0000125	2.0	2.0
1385_0000128	1.0	2.0
1385_0001111	2.0	2.0
1385_0001119	2.0	2.0
1385_0001121	2.0	2.0
1385_0001128	1.0	2.0
1385_0001134	2.0	2.0
1385_0001147	2.0	2.0
1385_0001154	2.0	2.0
1385_0001155	2.0	2.0
1385_0001164	1.0	2.0
1385_0001178	1.0	2.0
1385_0001191	2.0	2.0
1385_0001199	2.0	2.0
1385_0001501	1.0	2.0
1385_0001525	2.0	2.0
1385_0001526	0.0	2.0
1385_0001716	2.0	2.0
1385_0001717	2.0	2.0
1385_0001718	1.0	2.0
1385_0001723	0.0	2.0
1385_0001734	2.0	2.0
1385_0001746	1.0	2.0
1385_0001747	0.0	2.0
1385_0001749	1.0	2.0
1385_0001751	1.0	2.0
1385_0001762	2.0	2.0
1385_0001765	0.0	2.0
1385_0001768	2.0	2.0
1385_0001773	1.0	2.0
1385_0001787	1.0	2.0
1385_0001789	2.0	2.0
1385_0001793	1.0	2.0
1385_0001794	1.0	2.0
1395_0000341	2.0	2.0
1395_0000354	1.0	2.0
1395_0000357	3.0	4.0
1395_0000365	3.0	2.0
1395_0000376	5.0	2.0
1395_0000388	4.0	2.0
1395_0000402	2.0	2.0
1395_0000448	2.0	2.0
1395_0000451	2.0	2.0
1395_0000500	2.0	2.0
1395_0000504	2.0	2.0
1395_0000533	4.0	2.0
1395_0000554	2.0	2.0
1395_0000555	2.0	2.0
1395_0000579	1.0	2.0
1395_0000585	1.0	2.0
1395_0000595	0.0	2.0
1395_0000631	1.0	2.0
1395_0000636	1.0	2.0
1395_0000644	2.0	2.0
1395_0001015	2.0	2.0
1395_0001024	1.0	2.0
1395_0001034	2.0	2.0
1395_0001060	2.0	2.0
1395_0001069	2.0	2.0
1395_0001071	2.0	2.0
1395_0001073	1.0	2.0
1395_0001090	2.0	2.0
1395_0001104	1.0	2.0
1395_0001108	1.0	2.0
1395_0001114	1.0	2.0
1395_0001121	1.0	2.0
1395_0001123	1.0	2.0
1395_0001141	2.0	2.0
1395_0001145	3.0	5.0
Averaged weighted F1-scores 0.4564127019394141
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']
LANGUAGE: IT, 0th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 11

  Average training loss: 1.15
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.89
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         7
         1.0       0.70      0.77      0.73        48
         2.0       0.69      0.47      0.56        66
         3.0       0.61      1.00      0.76        38
         4.0       0.00      0.00      0.00         1

    accuracy                           0.66       160
   macro avg       0.40      0.45      0.41       160
weighted avg       0.64      0.66      0.63       160

[[ 0  4  3  0  0]
 [ 0 37 11  0  0]
 [ 0 12 31 23  0]
 [ 0  0  0 38  0]
 [ 0  0  0  1  0]]
0.6307073856034252
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.84
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.76
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         7
         1.0       0.73      0.67      0.70        48
         2.0       0.69      0.73      0.71        66
         3.0       0.70      0.84      0.76        38
         4.0       0.00      0.00      0.00         1

    accuracy                           0.70       160
   macro avg       0.42      0.45      0.43       160
weighted avg       0.67      0.70      0.68       160

[[ 0  7  0  0  0]
 [ 0 32 16  0  0]
 [ 0  5 48 13  0]
 [ 0  0  6 32  0]
 [ 0  0  0  1  0]]
0.6808245037145293
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.68
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.76
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         7
         1.0       0.65      0.85      0.74        48
         2.0       0.64      0.68      0.66        66
         3.0       0.74      0.53      0.62        38
         4.0       0.00      0.00      0.00         1

    accuracy                           0.66       160
   macro avg       0.41      0.41      0.40       160
weighted avg       0.64      0.66      0.64       160

[[ 0  7  0  0  0]
 [ 0 41  7  0  0]
 [ 0 15 45  6  0]
 [ 0  0 18 20  0]
 [ 0  0  0  1  0]]
0.6407534089519384
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.57
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.72
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         7
         1.0       0.67      0.85      0.75        48
         2.0       0.76      0.62      0.68        66
         3.0       0.71      0.84      0.77        38
         4.0       0.00      0.00      0.00         1

    accuracy                           0.71       160
   macro avg       0.43      0.46      0.44       160
weighted avg       0.68      0.71      0.69       160

[[ 0  7  0  0  0]
 [ 0 41  7  0  0]
 [ 0 13 41 12  0]
 [ 0  0  6 32  0]
 [ 0  0  0  1  0]]
0.6906956035149773
160 160 160
Filename	True Label	Prediction
1325_1001015	3.0	3.0
1325_1001017	3.0	3.0
1325_1001025	2.0	3.0
1325_1001028	3.0	3.0
1325_1001033	3.0	3.0
1325_1001037	2.0	2.0
1325_1001041	3.0	3.0
1325_1001045	3.0	3.0
1325_1001046	2.0	3.0
1325_1001048	2.0	3.0
1325_1001081	3.0	3.0
1325_1001085	3.0	3.0
1325_1001086	3.0	3.0
1325_1001087	2.0	2.0
1325_1001108	3.0	3.0
1325_1001120	3.0	3.0
1325_1001127	3.0	3.0
1325_1001132	3.0	3.0
1325_1001139	3.0	3.0
1325_1001141	2.0	3.0
1325_1001159	3.0	3.0
1325_1001161	3.0	3.0
1325_1001166	3.0	3.0
1325_9000088	2.0	3.0
1325_9000090	2.0	3.0
1325_9000095	3.0	3.0
1325_9000107	3.0	3.0
1325_9000138	4.0	3.0
1325_9000185	3.0	3.0
1325_9000186	3.0	3.0
1325_9000209	3.0	3.0
1325_9000303	3.0	3.0
1325_9000304	3.0	3.0
1325_9000314	3.0	3.0
1325_9000317	3.0	3.0
1325_9000504	3.0	3.0
1325_9000601	3.0	3.0
1325_9000685	3.0	3.0
1365_0100009	2.0	2.0
1365_0100017	2.0	2.0
1365_0100030	2.0	2.0
1365_0100057	2.0	2.0
1365_0100061	3.0	2.0
1365_0100064	2.0	2.0
1365_0100072	2.0	2.0
1365_0100074	2.0	2.0
1365_0100096	2.0	2.0
1365_0100097	2.0	2.0
1365_0100099	2.0	2.0
1365_0100100	2.0	2.0
1365_0100102	3.0	2.0
1365_0100105	3.0	3.0
1365_0100116	3.0	2.0
1365_0100121	2.0	3.0
1365_0100139	2.0	2.0
1365_0100164	2.0	3.0
1365_0100165	3.0	2.0
1365_0100166	2.0	2.0
1365_0100185	2.0	2.0
1365_0100188	2.0	2.0
1365_0100190	2.0	3.0
1365_0100192	3.0	3.0
1365_0100195	2.0	2.0
1365_0100219	2.0	3.0
1365_0100227	3.0	2.0
1365_0100228	2.0	2.0
1365_0100251	2.0	3.0
1365_0100252	2.0	2.0
1365_0100259	2.0	2.0
1365_0100266	2.0	2.0
1365_0100267	2.0	2.0
1365_0100269	2.0	2.0
1365_0100276	3.0	3.0
1365_0100278	3.0	2.0
1365_0100280	1.0	2.0
1365_0100287	2.0	2.0
1365_0100290	2.0	2.0
1365_0100299	2.0	2.0
1365_0100461	2.0	3.0
1365_0100478	2.0	2.0
1365_0100482	2.0	2.0
1385_0000013	1.0	1.0
1385_0000021	1.0	1.0
1385_0000022	1.0	1.0
1385_0000033	1.0	1.0
1385_0000035	1.0	1.0
1385_0000036	1.0	1.0
1385_0000038	1.0	1.0
1385_0000047	1.0	1.0
1385_0000051	2.0	1.0
1385_0000053	1.0	1.0
1385_0000097	2.0	1.0
1385_0000099	1.0	1.0
1385_0000100	1.0	1.0
1385_0000101	1.0	1.0
1385_0000119	1.0	1.0
1385_0001103	1.0	1.0
1385_0001109	1.0	1.0
1385_0001110	2.0	1.0
1385_0001120	2.0	1.0
1385_0001130	1.0	1.0
1385_0001148	2.0	1.0
1385_0001160	1.0	2.0
1385_0001165	1.0	1.0
1385_0001166	1.0	1.0
1385_0001167	1.0	1.0
1385_0001178	1.0	1.0
1385_0001199	1.0	1.0
1385_0001501	1.0	1.0
1385_0001522	0.0	1.0
1385_0001524	1.0	1.0
1385_0001526	0.0	1.0
1385_0001527	2.0	1.0
1385_0001715	1.0	1.0
1385_0001733	1.0	1.0
1385_0001740	1.0	1.0
1385_0001742	0.0	1.0
1385_0001751	1.0	1.0
1385_0001753	1.0	1.0
1385_0001754	1.0	1.0
1385_0001757	1.0	1.0
1385_0001766	2.0	1.0
1385_0001775	1.0	1.0
1385_0001790	1.0	1.0
1385_0001798	1.0	1.0
1395_0000361	2.0	2.0
1395_0000366	2.0	2.0
1395_0000376	2.0	2.0
1395_0000389	1.0	1.0
1395_0000399	2.0	2.0
1395_0000446	2.0	2.0
1395_0000448	2.0	2.0
1395_0000449	2.0	1.0
1395_0000455	2.0	2.0
1395_0000525	2.0	1.0
1395_0000526	1.0	1.0
1395_0000537	2.0	2.0
1395_0000549	2.0	2.0
1395_0000553	2.0	1.0
1395_0000557	2.0	2.0
1395_0000560	2.0	1.0
1395_0000582	0.0	1.0
1395_0000587	0.0	1.0
1395_0000595	1.0	1.0
1395_0000596	2.0	1.0
1395_0000611	1.0	1.0
1395_0000636	1.0	1.0
1395_0001017	1.0	1.0
1395_0001028	1.0	2.0
1395_0001066	1.0	2.0
1395_0001068	1.0	2.0
1395_0001069	2.0	2.0
1395_0001076	1.0	2.0
1395_0001119	2.0	2.0
1395_0001122	0.0	1.0
1395_0001123	1.0	2.0
1395_0001126	1.0	1.0
1395_0001131	0.0	1.0
1395_0001141	2.0	1.0
1395_0001167	1.0	1.0
LANGUAGE: IT, 1th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.02
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.80
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.60      0.94      0.73        49
         2.0       0.76      0.52      0.62        65
         3.0       0.76      0.76      0.76        38

    accuracy                           0.68       160
   macro avg       0.53      0.56      0.53       160
weighted avg       0.67      0.68      0.66       160

[[ 0  8  0  0]
 [ 0 46  3  0]
 [ 0 22 34  9]
 [ 0  1  8 29]]
0.6559974747474747
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.76
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.77
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.58      1.00      0.74        49
         2.0       0.84      0.49      0.62        65
         3.0       0.79      0.79      0.79        38

    accuracy                           0.69       160
   macro avg       0.55      0.57      0.54       160
weighted avg       0.71      0.69      0.67       160

[[ 0  8  0  0]
 [ 0 49  0  0]
 [ 0 25 32  8]
 [ 0  2  6 30]]
0.6655850792028615
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.66
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.74
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.67      0.73      0.70        49
         2.0       0.69      0.63      0.66        65
         3.0       0.70      0.87      0.78        38

    accuracy                           0.69       160
   macro avg       0.52      0.56      0.53       160
weighted avg       0.65      0.69      0.67       160

[[ 0  8  0  0]
 [ 0 36 13  0]
 [ 0 10 41 14]
 [ 0  0  5 33]]
0.6671386281571822
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.55
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.70
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.63      0.78      0.70        49
         2.0       0.71      0.63      0.67        65
         3.0       0.76      0.84      0.80        38

    accuracy                           0.69       160
   macro avg       0.53      0.56      0.54       160
weighted avg       0.66      0.69      0.67       160

[[ 0  8  0  0]
 [ 0 38 11  0]
 [ 0 14 41 10]
 [ 0  0  6 32]]
0.6743654434250764
160 160 160
Filename	True Label	Prediction
1325_1001009	3.0	3.0
1325_1001010	3.0	3.0
1325_1001022	3.0	3.0
1325_1001027	3.0	3.0
1325_1001036	3.0	3.0
1325_1001042	2.0	3.0
1325_1001044	3.0	3.0
1325_1001052	2.0	3.0
1325_1001057	2.0	3.0
1325_1001059	3.0	3.0
1325_1001062	3.0	3.0
1325_1001076	3.0	3.0
1325_1001079	3.0	3.0
1325_1001095	2.0	3.0
1325_1001100	2.0	3.0
1325_1001110	3.0	3.0
1325_1001113	3.0	3.0
1325_1001122	3.0	3.0
1325_1001123	3.0	3.0
1325_1001135	3.0	3.0
1325_1001138	2.0	3.0
1325_1001143	3.0	3.0
1325_1001144	3.0	3.0
1325_1001152	3.0	3.0
1325_1001160	3.0	3.0
1325_1001164	3.0	3.0
1325_9000059	3.0	3.0
1325_9000087	2.0	3.0
1325_9000105	2.0	3.0
1325_9000139	3.0	3.0
1325_9000140	3.0	3.0
1325_9000144	3.0	3.0
1325_9000152	3.0	3.0
1325_9000215	3.0	3.0
1325_9000278	3.0	3.0
1325_9000534	3.0	3.0
1325_9000536	3.0	3.0
1325_9000554	3.0	3.0
1325_9000611	3.0	3.0
1365_0100006	2.0	2.0
1365_0100007	1.0	2.0
1365_0100008	2.0	2.0
1365_0100013	3.0	2.0
1365_0100014	2.0	2.0
1365_0100018	2.0	2.0
1365_0100019	2.0	2.0
1365_0100024	2.0	2.0
1365_0100051	2.0	2.0
1365_0100058	2.0	2.0
1365_0100079	2.0	2.0
1365_0100092	2.0	2.0
1365_0100098	2.0	2.0
1365_0100103	2.0	3.0
1365_0100120	3.0	3.0
1365_0100133	2.0	2.0
1365_0100151	2.0	2.0
1365_0100167	2.0	2.0
1365_0100168	2.0	2.0
1365_0100169	2.0	2.0
1365_0100172	2.0	2.0
1365_0100183	2.0	2.0
1365_0100194	2.0	3.0
1365_0100203	2.0	2.0
1365_0100211	3.0	2.0
1365_0100220	3.0	2.0
1365_0100230	2.0	2.0
1365_0100231	2.0	2.0
1365_0100255	2.0	2.0
1365_0100256	2.0	2.0
1365_0100257	2.0	2.0
1365_0100261	2.0	2.0
1365_0100268	2.0	2.0
1365_0100279	2.0	2.0
1365_0100448	2.0	2.0
1365_0100457	2.0	2.0
1365_0100469	2.0	2.0
1365_0100470	2.0	2.0
1365_0100475	2.0	2.0
1365_0100477	2.0	2.0
1385_0000034	1.0	1.0
1385_0000042	1.0	1.0
1385_0000048	1.0	1.0
1385_0000054	2.0	1.0
1385_0000058	1.0	1.0
1385_0000103	1.0	1.0
1385_0000120	0.0	1.0
1385_0000126	1.0	1.0
1385_0001111	2.0	1.0
1385_0001126	0.0	1.0
1385_0001128	1.0	1.0
1385_0001133	2.0	1.0
1385_0001135	1.0	1.0
1385_0001138	1.0	1.0
1385_0001147	1.0	1.0
1385_0001149	2.0	1.0
1385_0001157	1.0	1.0
1385_0001164	1.0	1.0
1385_0001172	1.0	1.0
1385_0001189	1.0	1.0
1385_0001192	1.0	1.0
1385_0001503	1.0	1.0
1385_0001712	1.0	2.0
1385_0001714	0.0	1.0
1385_0001739	1.0	1.0
1385_0001746	1.0	1.0
1385_0001747	1.0	1.0
1385_0001759	1.0	1.0
1385_0001761	1.0	1.0
1385_0001767	1.0	1.0
1385_0001771	1.0	1.0
1385_0001786	1.0	1.0
1385_0001787	0.0	1.0
1385_0001795	0.0	1.0
1395_0000338	2.0	2.0
1395_0000341	2.0	1.0
1395_0000353	1.0	1.0
1395_0000355	2.0	2.0
1395_0000368	0.0	1.0
1395_0000369	2.0	2.0
1395_0000378	2.0	1.0
1395_0000388	2.0	2.0
1395_0000391	3.0	2.0
1395_0000438	3.0	2.0
1395_0000454	2.0	2.0
1395_0000460	1.0	1.0
1395_0000465	1.0	1.0
1395_0000500	1.0	1.0
1395_0000504	2.0	1.0
1395_0000514	3.0	2.0
1395_0000515	2.0	2.0
1395_0000529	2.0	1.0
1395_0000551	2.0	2.0
1395_0000554	2.0	1.0
1395_0000555	1.0	1.0
1395_0000556	1.0	2.0
1395_0000559	2.0	1.0
1395_0000563	2.0	1.0
1395_0000564	2.0	1.0
1395_0000581	1.0	2.0
1395_0000584	1.0	1.0
1395_0000597	1.0	2.0
1395_0000598	1.0	1.0
1395_0000609	1.0	1.0
1395_0000628	1.0	2.0
1395_0000631	1.0	2.0
1395_0000646	1.0	1.0
1395_0001010	1.0	2.0
1395_0001013	1.0	2.0
1395_0001016	1.0	1.0
1395_0001022	1.0	2.0
1395_0001023	1.0	1.0
1395_0001045	2.0	1.0
1395_0001058	1.0	1.0
1395_0001060	1.0	2.0
1395_0001080	2.0	2.0
1395_0001116	2.0	1.0
1395_0001121	0.0	1.0
1395_0001124	1.0	1.0
1395_0001150	0.0	1.0
1395_0001161	1.0	1.0
LANGUAGE: IT, 2th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.08
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.89
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.52      0.94      0.67        49
         2.0       0.66      0.32      0.43        65
         3.0       0.77      0.79      0.78        38

    accuracy                           0.61       160
   macro avg       0.49      0.51      0.47       160
weighted avg       0.61      0.61      0.57       160

[[ 0  8  0  0]
 [ 0 46  3  0]
 [ 0 35 21  9]
 [ 0  0  8 30]]
0.5651336635872719
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.78
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.66      0.51      0.57        49
         2.0       0.61      0.72      0.66        65
         3.0       0.73      0.87      0.80        38

    accuracy                           0.66       160
   macro avg       0.50      0.53      0.51       160
weighted avg       0.62      0.66      0.63       160

[[ 0  7  1  0]
 [ 0 25 24  0]
 [ 0  6 47 12]
 [ 0  0  5 33]]
0.633787225151212
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.65
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.78
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.64      0.57      0.60        49
         2.0       0.66      0.71      0.68        65
         3.0       0.76      0.92      0.83        38

    accuracy                           0.68       160
   macro avg       0.51      0.55      0.53       160
weighted avg       0.64      0.68      0.66       160

[[ 0  8  0  0]
 [ 0 28 21  0]
 [ 0  8 46 11]
 [ 0  0  3 35]]
0.6591771206690561
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.57
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.72
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.66      0.78      0.71        49
         2.0       0.70      0.68      0.69        65
         3.0       0.77      0.79      0.78        38

    accuracy                           0.70       160
   macro avg       0.53      0.56      0.54       160
weighted avg       0.67      0.70      0.68       160

[[ 0  8  0  0]
 [ 0 38 11  0]
 [ 0 12 44  9]
 [ 0  0  8 30]]
0.6818851745509165
160 160 160
Filename	True Label	Prediction
1325_1001008	3.0	3.0
1325_1001021	3.0	3.0
1325_1001040	3.0	3.0
1325_1001047	3.0	2.0
1325_1001054	3.0	2.0
1325_1001055	3.0	3.0
1325_1001058	2.0	3.0
1325_1001077	3.0	3.0
1325_1001080	2.0	3.0
1325_1001084	3.0	3.0
1325_1001089	3.0	2.0
1325_1001096	3.0	3.0
1325_1001101	3.0	3.0
1325_1001119	3.0	3.0
1325_1001121	2.0	3.0
1325_1001128	3.0	3.0
1325_1001130	3.0	2.0
1325_1001133	3.0	3.0
1325_1001136	3.0	2.0
1325_1001153	2.0	3.0
1325_1001154	3.0	3.0
1325_1001162	3.0	3.0
1325_1001163	2.0	3.0
1325_1001167	3.0	3.0
1325_9000089	2.0	3.0
1325_9000187	3.0	3.0
1325_9000188	3.0	3.0
1325_9000210	2.0	3.0
1325_9000211	3.0	3.0
1325_9000240	3.0	3.0
1325_9000296	3.0	3.0
1325_9000302	3.0	3.0
1325_9000315	2.0	3.0
1325_9000322	3.0	3.0
1325_9000503	3.0	3.0
1325_9000533	3.0	3.0
1325_9000612	2.0	3.0
1325_9000674	3.0	3.0
1325_9000675	3.0	3.0
1325_9000676	3.0	3.0
1325_9000678	3.0	3.0
1325_9000686	3.0	3.0
1325_9000700	3.0	3.0
1365_0100003	2.0	1.0
1365_0100010	2.0	2.0
1365_0100012	2.0	1.0
1365_0100016	2.0	2.0
1365_0100026	2.0	1.0
1365_0100027	2.0	2.0
1365_0100029	1.0	2.0
1365_0100063	3.0	2.0
1365_0100066	2.0	2.0
1365_0100069	2.0	2.0
1365_0100080	2.0	2.0
1365_0100093	2.0	2.0
1365_0100095	2.0	2.0
1365_0100117	2.0	2.0
1365_0100118	2.0	2.0
1365_0100134	2.0	2.0
1365_0100136	2.0	2.0
1365_0100138	2.0	2.0
1365_0100147	2.0	2.0
1365_0100148	2.0	2.0
1365_0100177	2.0	2.0
1365_0100178	2.0	2.0
1365_0100179	2.0	2.0
1365_0100186	2.0	2.0
1365_0100196	2.0	2.0
1365_0100198	2.0	2.0
1365_0100200	3.0	2.0
1365_0100204	2.0	2.0
1365_0100205	2.0	2.0
1365_0100217	3.0	2.0
1365_0100222	3.0	3.0
1365_0100225	2.0	2.0
1365_0100233	2.0	2.0
1365_0100260	2.0	2.0
1365_0100281	2.0	2.0
1365_0100471	2.0	2.0
1365_0100479	2.0	2.0
1385_0000011	0.0	1.0
1385_0000012	1.0	1.0
1385_0000017	1.0	1.0
1385_0000037	1.0	1.0
1385_0000040	1.0	1.0
1385_0000041	1.0	1.0
1385_0000127	2.0	1.0
1385_0000129	1.0	1.0
1385_0000130	1.0	1.0
1385_0001107	1.0	1.0
1385_0001112	2.0	1.0
1385_0001118	2.0	1.0
1385_0001125	1.0	1.0
1385_0001127	2.0	1.0
1385_0001129	1.0	1.0
1385_0001134	1.0	1.0
1385_0001159	1.0	1.0
1385_0001161	1.0	1.0
1385_0001162	1.0	1.0
1385_0001170	1.0	1.0
1385_0001171	0.0	1.0
1385_0001174	1.0	1.0
1385_0001188	1.0	1.0
1385_0001193	1.0	2.0
1385_0001195	2.0	2.0
1385_0001197	1.0	1.0
1385_0001718	0.0	1.0
1385_0001719	1.0	1.0
1385_0001720	0.0	1.0
1385_0001724	1.0	1.0
1385_0001732	1.0	1.0
1385_0001734	1.0	1.0
1385_0001748	1.0	2.0
1385_0001764	1.0	1.0
1385_0001768	2.0	1.0
1385_0001785	1.0	1.0
1385_0001794	1.0	1.0
1395_0000333	1.0	1.0
1395_0000337	1.0	1.0
1395_0000354	1.0	1.0
1395_0000359	2.0	2.0
1395_0000364	2.0	1.0
1395_0000365	2.0	2.0
1395_0000392	2.0	2.0
1395_0000402	2.0	2.0
1395_0000404	2.0	2.0
1395_0000413	2.0	2.0
1395_0000443	2.0	1.0
1395_0000450	1.0	1.0
1395_0000462	2.0	1.0
1395_0000469	2.0	1.0
1395_0000499	2.0	2.0
1395_0000512	2.0	2.0
1395_0000513	2.0	2.0
1395_0000534	2.0	2.0
1395_0000550	2.0	2.0
1395_0000565	1.0	1.0
1395_0000583	1.0	2.0
1395_0000585	1.0	1.0
1395_0000593	1.0	2.0
1395_0000602	1.0	1.0
1395_0000606	0.0	1.0
1395_0000607	0.0	1.0
1395_0000608	1.0	2.0
1395_0000635	1.0	1.0
1395_0000644	1.0	1.0
1395_0001034	1.0	1.0
1395_0001040	0.0	1.0
1395_0001064	2.0	2.0
1395_0001065	1.0	2.0
1395_0001070	2.0	2.0
1395_0001103	1.0	2.0
1395_0001108	1.0	1.0
1395_0001109	1.0	2.0
1395_0001120	1.0	1.0
1395_0001133	1.0	1.0
1395_0001145	2.0	2.0
1395_0001146	0.0	1.0
1395_0001160	1.0	2.0
1395_0001170	1.0	2.0
LANGUAGE: IT, 3th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 11

  Average training loss: 0.98
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.75
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.69      0.78      0.73        49
         2.0       0.70      0.59      0.64        66
         3.0       0.65      0.86      0.74        37

    accuracy                           0.68       160
   macro avg       0.51      0.56      0.53       160
weighted avg       0.65      0.68      0.66       160

[[ 0  6  2  0]
 [ 0 38 10  1]
 [ 0 11 39 16]
 [ 0  0  5 32]]
0.6596206083756122
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.78
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.74
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.67      0.90      0.77        49
         2.0       0.71      0.67      0.69        66
         3.0       0.75      0.65      0.70        37

    accuracy                           0.70       160
   macro avg       0.53      0.55      0.54       160
weighted avg       0.67      0.70      0.68       160

[[ 0  7  1  0]
 [ 0 44  4  1]
 [ 0 15 44  7]
 [ 0  0 13 24]]
0.6788111413043478
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.65
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.77
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.77      0.73      0.75        49
         2.0       0.69      0.76      0.72        66
         3.0       0.73      0.81      0.77        37

    accuracy                           0.73       160
   macro avg       0.55      0.58      0.56       160
weighted avg       0.69      0.72      0.71       160

[[ 0  5  3  0]
 [ 0 36 12  1]
 [ 0  6 50 10]
 [ 0  0  7 30]]
0.7064851588628762
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.54
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.74
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.64      0.84      0.73        49
         2.0       0.70      0.68      0.69        66
         3.0       0.78      0.68      0.72        37

    accuracy                           0.69       160
   macro avg       0.53      0.55      0.54       160
weighted avg       0.67      0.69      0.68       160

[[ 0  8  0  0]
 [ 0 41  7  1]
 [ 0 15 45  6]
 [ 0  0 12 25]]
0.6753839001193753
160 160 160
Filename	True Label	Prediction
1325_1001012	3.0	3.0
1325_1001013	3.0	3.0
1325_1001014	3.0	2.0
1325_1001016	2.0	3.0
1325_1001018	3.0	3.0
1325_1001019	3.0	3.0
1325_1001024	3.0	3.0
1325_1001029	3.0	3.0
1325_1001039	3.0	3.0
1325_1001043	3.0	3.0
1325_1001050	3.0	3.0
1325_1001051	3.0	3.0
1325_1001075	2.0	3.0
1325_1001078	3.0	3.0
1325_1001082	3.0	3.0
1325_1001083	3.0	3.0
1325_1001088	2.0	3.0
1325_1001090	2.0	3.0
1325_1001091	3.0	3.0
1325_1001097	1.0	3.0
1325_1001099	3.0	3.0
1325_1001124	3.0	2.0
1325_1001125	3.0	3.0
1325_1001131	3.0	3.0
1325_1001155	3.0	3.0
1325_1001157	3.0	3.0
1325_1001158	3.0	3.0
1325_1001165	2.0	3.0
1325_9000099	2.0	3.0
1325_9000137	3.0	3.0
1325_9000143	3.0	3.0
1325_9000213	3.0	2.0
1325_9000318	3.0	3.0
1325_9000320	3.0	3.0
1365_0100002	2.0	2.0
1365_0100011	2.0	2.0
1365_0100020	2.0	2.0
1365_0100021	2.0	2.0
1365_0100028	2.0	2.0
1365_0100031	2.0	2.0
1365_0100056	2.0	2.0
1365_0100065	1.0	2.0
1365_0100071	3.0	2.0
1365_0100101	3.0	2.0
1365_0100104	2.0	2.0
1365_0100125	3.0	2.0
1365_0100135	2.0	2.0
1365_0100137	2.0	2.0
1365_0100146	2.0	2.0
1365_0100163	3.0	2.0
1365_0100170	2.0	2.0
1365_0100171	2.0	2.0
1365_0100176	2.0	2.0
1365_0100187	2.0	2.0
1365_0100212	3.0	2.0
1365_0100213	2.0	2.0
1365_0100215	2.0	2.0
1365_0100224	3.0	3.0
1365_0100226	3.0	2.0
1365_0100232	2.0	2.0
1365_0100253	2.0	2.0
1365_0100258	2.0	2.0
1365_0100262	3.0	2.0
1365_0100270	2.0	2.0
1365_0100282	2.0	2.0
1365_0100285	2.0	2.0
1365_0100286	2.0	2.0
1365_0100289	2.0	2.0
1365_0100451	2.0	2.0
1365_0100456	2.0	2.0
1365_0100458	2.0	2.0
1365_0100472	2.0	2.0
1365_0100473	2.0	2.0
1365_0100474	2.0	2.0
1365_0100476	2.0	2.0
1365_0100480	2.0	2.0
1365_0100481	2.0	2.0
1385_0000016	1.0	1.0
1385_0000020	1.0	1.0
1385_0000039	1.0	1.0
1385_0000043	1.0	1.0
1385_0000045	2.0	1.0
1385_0000050	1.0	1.0
1385_0000102	2.0	1.0
1385_0000114	2.0	1.0
1385_0000123	1.0	1.0
1385_0001105	1.0	1.0
1385_0001119	2.0	1.0
1385_0001122	2.0	1.0
1385_0001124	1.0	1.0
1385_0001137	1.0	1.0
1385_0001151	2.0	1.0
1385_0001153	2.0	1.0
1385_0001154	1.0	1.0
1385_0001169	1.0	1.0
1385_0001190	1.0	1.0
1385_0001191	1.0	1.0
1385_0001194	1.0	1.0
1385_0001196	1.0	1.0
1385_0001523	1.0	1.0
1385_0001525	1.0	1.0
1385_0001716	1.0	1.0
1385_0001727	0.0	1.0
1385_0001728	1.0	1.0
1385_0001729	1.0	2.0
1385_0001736	1.0	1.0
1385_0001741	0.0	1.0
1385_0001744	0.0	1.0
1385_0001750	0.0	1.0
1385_0001752	1.0	1.0
1385_0001758	1.0	1.0
1385_0001760	1.0	1.0
1385_0001772	1.0	1.0
1385_0001788	1.0	1.0
1385_0001791	1.0	1.0
1385_0001792	1.0	1.0
1385_0001793	1.0	1.0
1385_0001799	2.0	1.0
1385_0001800	1.0	1.0
1395_0000340	2.0	1.0
1395_0000356	1.0	1.0
1395_0000357	3.0	2.0
1395_0000379	2.0	2.0
1395_0000380	2.0	2.0
1395_0000387	3.0	2.0
1395_0000390	1.0	1.0
1395_0000396	2.0	2.0
1395_0000398	2.0	2.0
1395_0000409	2.0	1.0
1395_0000415	2.0	2.0
1395_0000432	2.0	2.0
1395_0000458	1.0	1.0
1395_0000470	2.0	1.0
1395_0000471	2.0	1.0
1395_0000516	1.0	1.0
1395_0000527	1.0	1.0
1395_0000533	2.0	2.0
1395_0000548	2.0	2.0
1395_0000552	2.0	2.0
1395_0000591	0.0	1.0
1395_0000604	0.0	1.0
1395_0000610	2.0	1.0
1395_0000627	1.0	1.0
1395_0000639	1.0	2.0
1395_0001019	1.0	1.0
1395_0001020	1.0	2.0
1395_0001024	2.0	2.0
1395_0001033	1.0	2.0
1395_0001067	1.0	1.0
1395_0001071	2.0	1.0
1395_0001074	1.0	2.0
1395_0001078	0.0	1.0
1395_0001090	1.0	1.0
1395_0001104	1.0	1.0
1395_0001114	0.0	1.0
1395_0001147	2.0	1.0
1395_0001149	1.0	1.0
1395_0001158	1.0	2.0
1395_0001164	2.0	2.0
1395_0001171	1.0	1.0
LANGUAGE: IT, 4th Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.04
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.86
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.55      0.98      0.70        48
         2.0       0.69      0.44      0.54        66
         3.0       0.75      0.65      0.70        37
         4.0       0.00      0.00      0.00         1

    accuracy                           0.62       160
   macro avg       0.40      0.41      0.39       160
weighted avg       0.62      0.62      0.59       160

[[ 0  8  0  0  0]
 [ 0 47  1  0  0]
 [ 0 30 29  7  0]
 [ 0  1 12 24  0]
 [ 0  0  0  1  0]]
0.5928451041891989
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.76
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.74
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.71      0.77      0.74        48
         2.0       0.66      0.77      0.71        66
         3.0       0.74      0.62      0.68        37
         4.0       0.00      0.00      0.00         1

    accuracy                           0.69       160
   macro avg       0.42      0.43      0.43       160
weighted avg       0.66      0.69      0.67       160

[[ 0  7  1  0  0]
 [ 0 37 11  0  0]
 [ 0  8 51  7  0]
 [ 0  0 14 23  0]
 [ 0  0  0  1  0]]
0.672664592760181
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.63
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.74
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.71      0.75      0.73        48
         2.0       0.66      0.71      0.69        66
         3.0       0.68      0.70      0.69        37
         4.0       0.00      0.00      0.00         1

    accuracy                           0.68       160
   macro avg       0.41      0.43      0.42       160
weighted avg       0.64      0.68      0.66       160

[[ 0  7  1  0  0]
 [ 0 36 12  0  0]
 [ 0  8 47 11  0]
 [ 0  0 11 26  0]
 [ 0  0  0  1  0]]
0.6615443485954435
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.53
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.75
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.71      0.77      0.74        48
         2.0       0.70      0.71      0.71        66
         3.0       0.68      0.76      0.72        37
         4.0       0.00      0.00      0.00         1

    accuracy                           0.70       160
   macro avg       0.42      0.45      0.43       160
weighted avg       0.66      0.70      0.68       160

[[ 0  8  0  0  0]
 [ 0 37 11  0  0]
 [ 0  7 47 12  0]
 [ 0  0  9 28  0]
 [ 0  0  0  1  0]]
0.6795669944090997
160 160 160
Filename	True Label	Prediction
1325_1001011	3.0	3.0
1325_1001020	3.0	2.0
1325_1001023	3.0	2.0
1325_1001032	3.0	3.0
1325_1001035	3.0	3.0
1325_1001053	2.0	2.0
1325_1001056	3.0	3.0
1325_1001063	2.0	2.0
1325_1001092	2.0	3.0
1325_1001093	2.0	3.0
1325_1001094	2.0	2.0
1325_1001098	3.0	3.0
1325_1001107	3.0	3.0
1325_1001109	2.0	3.0
1325_1001111	3.0	3.0
1325_1001126	2.0	3.0
1325_1001129	2.0	3.0
1325_1001134	2.0	3.0
1325_1001142	3.0	3.0
1325_1001156	3.0	3.0
1325_1001168	3.0	2.0
1325_1001169	3.0	2.0
1325_1001170	3.0	3.0
1325_9000102	3.0	3.0
1325_9000104	3.0	3.0
1325_9000106	3.0	3.0
1325_9000136	3.0	3.0
1325_9000214	3.0	3.0
1325_9000237	3.0	3.0
1325_9000239	3.0	3.0
1325_9000241	3.0	3.0
1325_9000279	3.0	3.0
1325_9000316	3.0	3.0
1325_9000319	3.0	3.0
1325_9000321	3.0	2.0
1325_9000323	3.0	3.0
1325_9000505	3.0	3.0
1325_9000602	4.0	3.0
1325_9000677	3.0	3.0
1325_9000684	3.0	3.0
1325_9000750	3.0	2.0
1365_0100004	2.0	2.0
1365_0100005	2.0	2.0
1365_0100015	2.0	2.0
1365_0100022	2.0	2.0
1365_0100023	2.0	2.0
1365_0100067	2.0	2.0
1365_0100070	2.0	2.0
1365_0100073	2.0	2.0
1365_0100094	2.0	2.0
1365_0100106	2.0	3.0
1365_0100107	2.0	3.0
1365_0100119	3.0	3.0
1365_0100123	2.0	3.0
1365_0100145	2.0	2.0
1365_0100162	2.0	2.0
1365_0100173	2.0	2.0
1365_0100174	2.0	2.0
1365_0100175	2.0	2.0
1365_0100180	2.0	2.0
1365_0100181	2.0	2.0
1365_0100182	2.0	2.0
1365_0100184	2.0	2.0
1365_0100191	2.0	2.0
1365_0100199	2.0	2.0
1365_0100201	2.0	2.0
1365_0100202	2.0	2.0
1365_0100218	2.0	2.0
1365_0100221	2.0	2.0
1365_0100223	2.0	3.0
1365_0100229	2.0	3.0
1365_0100263	3.0	3.0
1365_0100265	2.0	2.0
1365_0100274	2.0	3.0
1365_0100275	3.0	2.0
1365_0100277	3.0	3.0
1365_0100288	2.0	2.0
1365_0100447	2.0	2.0
1365_0100455	2.0	2.0
1365_0100459	3.0	2.0
1385_0000023	1.0	1.0
1385_0000044	2.0	1.0
1385_0000049	1.0	1.0
1385_0000052	1.0	1.0
1385_0000057	1.0	1.0
1385_0000059	1.0	2.0
1385_0000095	1.0	1.0
1385_0000098	2.0	1.0
1385_0000104	2.0	1.0
1385_0000122	1.0	1.0
1385_0000124	2.0	2.0
1385_0000125	2.0	1.0
1385_0000128	1.0	1.0
1385_0001104	1.0	1.0
1385_0001108	1.0	1.0
1385_0001113	1.0	1.0
1385_0001121	2.0	1.0
1385_0001123	2.0	1.0
1385_0001131	1.0	1.0
1385_0001132	1.0	1.0
1385_0001136	1.0	1.0
1385_0001150	1.0	1.0
1385_0001152	2.0	2.0
1385_0001155	1.0	1.0
1385_0001156	1.0	1.0
1385_0001158	1.0	1.0
1385_0001163	1.0	1.0
1385_0001173	0.0	1.0
1385_0001175	0.0	1.0
1385_0001198	1.0	1.0
1385_0001528	1.0	2.0
1385_0001717	1.0	2.0
1385_0001723	0.0	1.0
1385_0001725	1.0	1.0
1385_0001726	1.0	2.0
1385_0001730	1.0	2.0
1385_0001737	1.0	1.0
1385_0001738	0.0	1.0
1385_0001749	1.0	1.0
1385_0001756	1.0	1.0
1385_0001762	1.0	1.0
1385_0001765	0.0	1.0
1385_0001773	0.0	1.0
1385_0001774	0.0	1.0
1385_0001789	1.0	1.0
1385_0001796	1.0	1.0
1395_0000360	3.0	2.0
1395_0000383	2.0	2.0
1395_0000403	2.0	2.0
1395_0000414	2.0	2.0
1395_0000447	2.0	2.0
1395_0000451	2.0	2.0
1395_0000452	1.0	1.0
1395_0000518	2.0	2.0
1395_0000528	2.0	2.0
1395_0000531	2.0	2.0
1395_0000535	1.0	1.0
1395_0000547	2.0	2.0
1395_0000572	1.0	1.0
1395_0000575	1.0	1.0
1395_0000579	1.0	1.0
1395_0000599	1.0	1.0
1395_0000612	0.0	1.0
1395_0000626	2.0	2.0
1395_0000630	1.0	1.0
1395_0000642	1.0	1.0
1395_0000649	1.0	2.0
1395_0001015	1.0	2.0
1395_0001021	1.0	1.0
1395_0001061	2.0	2.0
1395_0001073	1.0	2.0
1395_0001075	1.0	2.0
1395_0001084	1.0	2.0
1395_0001093	1.0	2.0
1395_0001101	2.0	1.0
1395_0001115	2.0	2.0
1395_0001117	1.0	1.0
1395_0001118	1.0	1.0
1395_0001132	2.0	2.0
1395_0001169	2.0	2.0
Averaged weighted F1-scores 0.680379423203889
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']
LANGUAGE: IT, 0th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.21
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.96
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.56      0.44      0.49        41
         2.0       0.53      0.58      0.55        65
         3.0       0.61      0.85      0.71        40

    accuracy                           0.56       160
   macro avg       0.42      0.47      0.44       160
weighted avg       0.51      0.56      0.53       160

[[ 0  9  5  0]
 [ 0 18 23  0]
 [ 0  5 38 22]
 [ 0  0  6 34]]
0.5288181598506816
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.92
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.83
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.58      0.85      0.69        41
         2.0       0.63      0.60      0.61        65
         3.0       0.68      0.65      0.67        40

    accuracy                           0.62       160
   macro avg       0.47      0.53      0.49       160
weighted avg       0.58      0.62      0.59       160

[[ 0 11  3  0]
 [ 0 35  6  0]
 [ 0 14 39 12]
 [ 0  0 14 26]]
0.5937735505834049
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.82
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.82
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.62      0.78      0.69        41
         2.0       0.61      0.63      0.62        65
         3.0       0.68      0.70      0.69        40

    accuracy                           0.63       160
   macro avg       0.48      0.53      0.50       160
weighted avg       0.58      0.63      0.60       160

[[ 0  9  5  0]
 [ 0 32  9  0]
 [ 0 11 41 13]
 [ 0  0 12 28]]
0.6015510164367692
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.71
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.84
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.60      0.73      0.66        41
         2.0       0.60      0.49      0.54        65
         3.0       0.61      0.88      0.72        40

    accuracy                           0.61       160
   macro avg       0.45      0.52      0.48       160
weighted avg       0.55      0.61      0.57       160

[[ 0  9  5  0]
 [ 0 30 11  0]
 [ 0 11 32 22]
 [ 0  0  5 35]]
0.5697073981409121
160 160 160
Filename	True Label	Prediction
1325_1001016	2.0	3.0
1325_1001018	2.0	3.0
1325_1001022	3.0	3.0
1325_1001024	2.0	3.0
1325_1001025	2.0	3.0
1325_1001036	3.0	3.0
1325_1001040	3.0	3.0
1325_1001043	3.0	3.0
1325_1001054	3.0	2.0
1325_1001057	2.0	3.0
1325_1001062	3.0	3.0
1325_1001095	2.0	3.0
1325_1001100	3.0	3.0
1325_1001101	3.0	3.0
1325_1001111	3.0	3.0
1325_1001113	3.0	3.0
1325_1001126	2.0	3.0
1325_1001128	3.0	3.0
1325_1001135	3.0	3.0
1325_1001136	2.0	3.0
1325_1001153	3.0	3.0
1325_1001160	3.0	3.0
1325_1001169	3.0	3.0
1325_9000087	2.0	3.0
1325_9000137	3.0	3.0
1325_9000139	3.0	3.0
1325_9000143	3.0	3.0
1325_9000185	3.0	3.0
1325_9000214	3.0	3.0
1325_9000241	3.0	3.0
1325_9000296	2.0	3.0
1325_9000322	3.0	3.0
1325_9000503	3.0	3.0
1325_9000533	3.0	3.0
1365_0100003	2.0	2.0
1365_0100005	1.0	2.0
1365_0100008	2.0	2.0
1365_0100010	2.0	2.0
1365_0100015	1.0	2.0
1365_0100016	2.0	2.0
1365_0100018	2.0	2.0
1365_0100026	2.0	2.0
1365_0100027	3.0	2.0
1365_0100028	2.0	2.0
1365_0100031	2.0	2.0
1365_0100051	2.0	2.0
1365_0100058	3.0	3.0
1365_0100061	3.0	3.0
1365_0100074	2.0	3.0
1365_0100107	3.0	3.0
1365_0100118	2.0	3.0
1365_0100123	2.0	3.0
1365_0100125	3.0	3.0
1365_0100165	3.0	3.0
1365_0100178	2.0	2.0
1365_0100181	2.0	2.0
1365_0100182	2.0	2.0
1365_0100185	2.0	2.0
1365_0100194	3.0	3.0
1365_0100205	2.0	2.0
1365_0100217	3.0	3.0
1365_0100218	3.0	2.0
1365_0100220	3.0	3.0
1365_0100221	2.0	3.0
1365_0100231	2.0	3.0
1365_0100232	2.0	3.0
1365_0100251	3.0	3.0
1365_0100252	3.0	3.0
1365_0100259	2.0	2.0
1365_0100267	2.0	3.0
1365_0100268	2.0	3.0
1365_0100270	2.0	3.0
1365_0100289	2.0	2.0
1365_0100447	3.0	2.0
1365_0100457	3.0	3.0
1365_0100458	2.0	3.0
1365_0100461	3.0	3.0
1365_0100469	2.0	3.0
1365_0100477	2.0	3.0
1385_0000016	1.0	1.0
1385_0000017	1.0	1.0
1385_0000022	1.0	2.0
1385_0000036	1.0	1.0
1385_0000038	1.0	1.0
1385_0000039	1.0	1.0
1385_0000043	1.0	1.0
1385_0000047	1.0	1.0
1385_0000048	1.0	1.0
1385_0000053	2.0	1.0
1385_0000057	1.0	1.0
1385_0000098	2.0	1.0
1385_0000103	2.0	1.0
1385_0000114	2.0	1.0
1385_0000119	1.0	1.0
1385_0001104	1.0	1.0
1385_0001109	2.0	1.0
1385_0001119	2.0	1.0
1385_0001125	2.0	1.0
1385_0001126	0.0	1.0
1385_0001151	2.0	1.0
1385_0001152	2.0	2.0
1385_0001158	1.0	1.0
1385_0001159	1.0	1.0
1385_0001166	1.0	1.0
1385_0001167	1.0	1.0
1385_0001174	0.0	1.0
1385_0001191	1.0	1.0
1385_0001522	1.0	1.0
1385_0001524	1.0	1.0
1385_0001716	1.0	1.0
1385_0001719	1.0	1.0
1385_0001720	0.0	1.0
1385_0001724	1.0	2.0
1385_0001736	1.0	2.0
1385_0001742	0.0	1.0
1385_0001744	0.0	1.0
1385_0001750	0.0	1.0
1385_0001752	1.0	1.0
1385_0001766	2.0	2.0
1385_0001771	1.0	1.0
1385_0001772	1.0	1.0
1385_0001795	0.0	1.0
1385_0001799	1.0	1.0
1395_0000337	1.0	1.0
1395_0000340	2.0	2.0
1395_0000355	2.0	2.0
1395_0000379	2.0	2.0
1395_0000389	1.0	1.0
1395_0000413	2.0	2.0
1395_0000432	2.0	2.0
1395_0000438	2.0	2.0
1395_0000447	2.0	2.0
1395_0000460	1.0	1.0
1395_0000512	2.0	2.0
1395_0000514	3.0	2.0
1395_0000529	2.0	2.0
1395_0000537	2.0	2.0
1395_0000550	2.0	2.0
1395_0000552	2.0	2.0
1395_0000554	2.0	1.0
1395_0000560	2.0	1.0
1395_0000579	1.0	1.0
1395_0000595	0.0	1.0
1395_0000604	0.0	1.0
1395_0000626	1.0	2.0
1395_0000630	0.0	2.0
1395_0000636	0.0	2.0
1395_0001010	1.0	2.0
1395_0001022	1.0	1.0
1395_0001033	1.0	2.0
1395_0001060	2.0	2.0
1395_0001067	0.0	2.0
1395_0001074	1.0	2.0
1395_0001076	0.0	2.0
1395_0001080	2.0	1.0
1395_0001109	0.0	2.0
1395_0001119	2.0	2.0
1395_0001133	1.0	2.0
1395_0001147	1.0	2.0
1395_0001167	1.0	1.0
LANGUAGE: IT, 1th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.16
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.01
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.50      0.61      0.55        41
         2.0       0.49      0.82      0.61        66
         3.0       0.00      0.00      0.00        39

    accuracy                           0.49       160
   macro avg       0.25      0.36      0.29       160
weighted avg       0.33      0.49      0.39       160

[[ 0 13  1  0]
 [ 0 25 16  0]
 [ 0 12 54  0]
 [ 0  0 39  0]]
0.3939217032967033
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.96
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.98
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.45      0.51      0.48        41
         2.0       0.50      0.35      0.41        66
         3.0       0.55      0.95      0.70        39

    accuracy                           0.51       160
   macro avg       0.37      0.45      0.40       160
weighted avg       0.46      0.51      0.46       160

[[ 0 13  1  0]
 [ 0 21 20  0]
 [ 0 13 23 30]
 [ 0  0  2 37]]
0.4618858735604019
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.83
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.91
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.45      0.54      0.49        41
         2.0       0.52      0.55      0.53        66
         3.0       0.62      0.67      0.64        39

    accuracy                           0.53       160
   macro avg       0.40      0.44      0.42       160
weighted avg       0.48      0.53      0.50       160

[[ 0 13  1  0]
 [ 0 22 19  0]
 [ 0 14 36 16]
 [ 0  0 13 26]]
0.5017592592592592
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.74
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.88
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.45      0.54      0.49        41
         2.0       0.52      0.55      0.53        66
         3.0       0.62      0.67      0.64        39

    accuracy                           0.53       160
   macro avg       0.40      0.44      0.42       160
weighted avg       0.48      0.53      0.50       160

[[ 0 13  1  0]
 [ 0 22 19  0]
 [ 0 14 36 16]
 [ 0  0 13 26]]
0.5017592592592592
160 160 160
Filename	True Label	Prediction
1325_1001009	3.0	3.0
1325_1001013	3.0	3.0
1325_1001020	2.0	3.0
1325_1001027	3.0	3.0
1325_1001028	3.0	3.0
1325_1001029	3.0	3.0
1325_1001032	2.0	3.0
1325_1001041	3.0	2.0
1325_1001047	2.0	2.0
1325_1001077	3.0	3.0
1325_1001080	3.0	3.0
1325_1001082	2.0	2.0
1325_1001083	2.0	2.0
1325_1001084	3.0	2.0
1325_1001087	3.0	2.0
1325_1001094	2.0	3.0
1325_1001122	2.0	3.0
1325_1001130	3.0	2.0
1325_1001132	3.0	3.0
1325_1001139	3.0	3.0
1325_1001141	2.0	3.0
1325_1001143	3.0	3.0
1325_1001166	3.0	3.0
1325_1001168	2.0	3.0
1325_9000059	2.0	3.0
1325_9000089	3.0	3.0
1325_9000090	3.0	3.0
1325_9000099	3.0	3.0
1325_9000140	3.0	3.0
1325_9000152	3.0	3.0
1325_9000210	2.0	3.0
1325_9000213	3.0	2.0
1325_9000302	2.0	3.0
1325_9000303	2.0	3.0
1325_9000304	3.0	3.0
1325_9000316	3.0	3.0
1325_9000320	3.0	2.0
1325_9000321	3.0	3.0
1325_9000554	2.0	3.0
1325_9000674	3.0	3.0
1325_9000677	3.0	3.0
1325_9000684	3.0	3.0
1365_0100006	2.0	2.0
1365_0100009	2.0	2.0
1365_0100019	2.0	2.0
1365_0100021	2.0	2.0
1365_0100030	2.0	2.0
1365_0100056	2.0	3.0
1365_0100080	2.0	3.0
1365_0100097	2.0	2.0
1365_0100104	2.0	2.0
1365_0100119	3.0	2.0
1365_0100120	3.0	3.0
1365_0100121	3.0	2.0
1365_0100133	2.0	2.0
1365_0100134	2.0	2.0
1365_0100135	2.0	2.0
1365_0100138	2.0	2.0
1365_0100145	3.0	3.0
1365_0100175	2.0	2.0
1365_0100184	2.0	2.0
1365_0100192	3.0	3.0
1365_0100198	2.0	2.0
1365_0100223	2.0	3.0
1365_0100225	3.0	2.0
1365_0100228	2.0	2.0
1365_0100255	2.0	2.0
1365_0100256	2.0	2.0
1365_0100257	2.0	2.0
1365_0100261	2.0	2.0
1365_0100262	3.0	2.0
1365_0100263	3.0	2.0
1365_0100269	2.0	2.0
1365_0100276	3.0	3.0
1365_0100282	2.0	2.0
1365_0100471	2.0	3.0
1365_0100475	2.0	3.0
1365_0100478	2.0	2.0
1365_0100479	2.0	2.0
1385_0000021	1.0	1.0
1385_0000034	1.0	1.0
1385_0000035	1.0	2.0
1385_0000044	2.0	1.0
1385_0000051	2.0	2.0
1385_0000101	1.0	2.0
1385_0000104	2.0	1.0
1385_0000122	1.0	1.0
1385_0000124	1.0	2.0
1385_0001110	2.0	1.0
1385_0001124	2.0	1.0
1385_0001127	2.0	1.0
1385_0001129	1.0	1.0
1385_0001131	1.0	1.0
1385_0001134	1.0	1.0
1385_0001156	1.0	1.0
1385_0001157	1.0	2.0
1385_0001162	1.0	1.0
1385_0001169	1.0	1.0
1385_0001171	0.0	1.0
1385_0001178	0.0	1.0
1385_0001189	0.0	1.0
1385_0001194	1.0	1.0
1385_0001196	1.0	1.0
1385_0001527	2.0	1.0
1385_0001712	1.0	2.0
1385_0001727	0.0	1.0
1385_0001734	1.0	1.0
1385_0001737	2.0	1.0
1385_0001747	1.0	1.0
1385_0001756	2.0	1.0
1385_0001757	1.0	1.0
1385_0001761	0.0	1.0
1385_0001765	0.0	1.0
1385_0001767	0.0	1.0
1385_0001775	0.0	1.0
1385_0001789	1.0	1.0
1385_0001793	0.0	1.0
1385_0001794	1.0	1.0
1395_0000333	1.0	2.0
1395_0000341	2.0	1.0
1395_0000354	1.0	1.0
1395_0000359	2.0	2.0
1395_0000369	2.0	2.0
1395_0000391	3.0	2.0
1395_0000446	2.0	2.0
1395_0000450	1.0	1.0
1395_0000462	2.0	1.0
1395_0000469	1.0	1.0
1395_0000499	1.0	2.0
1395_0000500	2.0	1.0
1395_0000515	2.0	2.0
1395_0000533	3.0	2.0
1395_0000548	2.0	2.0
1395_0000551	2.0	2.0
1395_0000553	2.0	1.0
1395_0000555	1.0	1.0
1395_0000557	2.0	2.0
1395_0000559	2.0	1.0
1395_0000564	1.0	2.0
1395_0000583	1.0	2.0
1395_0000585	0.0	2.0
1395_0000591	0.0	1.0
1395_0000596	2.0	1.0
1395_0000597	1.0	2.0
1395_0000606	0.0	1.0
1395_0000607	0.0	1.0
1395_0001019	1.0	1.0
1395_0001020	1.0	2.0
1395_0001021	1.0	2.0
1395_0001024	1.0	2.0
1395_0001028	1.0	2.0
1395_0001058	1.0	1.0
1395_0001065	1.0	2.0
1395_0001069	2.0	2.0
1395_0001073	2.0	2.0
1395_0001078	1.0	2.0
1395_0001093	1.0	2.0
1395_0001104	0.0	1.0
1395_0001145	1.0	2.0
1395_0001161	1.0	2.0
LANGUAGE: IT, 2th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.20
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.05
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.50      0.55      0.52        40
         2.0       0.48      0.48      0.48        66
         3.0       0.51      0.64      0.57        39
         4.0       0.00      0.00      0.00         1

    accuracy                           0.49       160
   macro avg       0.30      0.34      0.31       160
weighted avg       0.45      0.49      0.47       160

[[ 0 11  3  0  0]
 [ 0 22 18  0  0]
 [ 0 11 32 23  0]
 [ 0  0 14 25  0]
 [ 0  0  0  1  0]]
0.4679429397357029
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.91
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.51      0.93      0.66        40
         2.0       0.59      0.44      0.50        66
         3.0       0.56      0.56      0.56        39
         4.0       0.00      0.00      0.00         1

    accuracy                           0.55       160
   macro avg       0.33      0.39      0.35       160
weighted avg       0.51      0.55      0.51       160

[[ 0 12  2  0  0]
 [ 0 37  3  0  0]
 [ 0 21 29 16  0]
 [ 0  2 15 22  0]
 [ 0  0  0  1  0]]
0.510722049689441
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.82
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       1.00      0.07      0.13        14
         1.0       0.53      0.88      0.66        40
         2.0       0.58      0.44      0.50        66
         3.0       0.56      0.62      0.59        39
         4.0       0.00      0.00      0.00         1

    accuracy                           0.56       160
   macro avg       0.53      0.40      0.38       160
weighted avg       0.60      0.56      0.53       160

[[ 1 11  2  0  0]
 [ 0 35  5  0  0]
 [ 0 19 29 18  0]
 [ 0  1 14 24  0]
 [ 0  0  0  1  0]]
0.5256939331185764
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.71
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.99
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.52      0.78      0.62        40
         2.0       0.54      0.48      0.51        66
         3.0       0.56      0.59      0.57        39
         4.0       0.00      0.00      0.00         1

    accuracy                           0.54       160
   macro avg       0.32      0.37      0.34       160
weighted avg       0.49      0.54      0.51       160

[[ 0 12  2  0  0]
 [ 0 31  9  0  0]
 [ 0 17 32 17  0]
 [ 0  0 16 23  0]
 [ 0  0  0  1  0]]
0.5063562500000001
160 160 160
Filename	True Label	Prediction
1325_1001008	3.0	3.0
1325_1001012	3.0	3.0
1325_1001021	3.0	3.0
1325_1001033	3.0	3.0
1325_1001037	3.0	2.0
1325_1001042	3.0	3.0
1325_1001044	3.0	2.0
1325_1001045	3.0	3.0
1325_1001048	2.0	3.0
1325_1001056	2.0	3.0
1325_1001058	3.0	3.0
1325_1001078	3.0	3.0
1325_1001079	3.0	3.0
1325_1001089	2.0	2.0
1325_1001090	3.0	2.0
1325_1001096	2.0	3.0
1325_1001097	1.0	2.0
1325_1001098	2.0	3.0
1325_1001110	3.0	3.0
1325_1001119	3.0	3.0
1325_1001124	3.0	2.0
1325_1001125	3.0	3.0
1325_1001129	2.0	2.0
1325_1001142	2.0	3.0
1325_1001144	3.0	3.0
1325_1001152	3.0	3.0
1325_1001156	2.0	3.0
1325_1001157	2.0	3.0
1325_1001162	2.0	3.0
1325_9000088	2.0	3.0
1325_9000102	3.0	2.0
1325_9000105	2.0	2.0
1325_9000138	4.0	3.0
1325_9000144	3.0	3.0
1325_9000240	2.0	3.0
1325_9000504	3.0	3.0
1325_9000505	3.0	3.0
1325_9000534	3.0	3.0
1325_9000611	2.0	3.0
1325_9000612	2.0	3.0
1325_9000750	3.0	2.0
1365_0100002	3.0	2.0
1365_0100011	2.0	2.0
1365_0100013	2.0	3.0
1365_0100020	2.0	2.0
1365_0100022	2.0	2.0
1365_0100024	2.0	2.0
1365_0100065	1.0	2.0
1365_0100070	2.0	2.0
1365_0100072	2.0	2.0
1365_0100092	2.0	2.0
1365_0100094	2.0	2.0
1365_0100096	2.0	2.0
1365_0100100	3.0	3.0
1365_0100101	2.0	2.0
1365_0100102	2.0	2.0
1365_0100105	3.0	3.0
1365_0100117	3.0	2.0
1365_0100146	3.0	2.0
1365_0100147	3.0	2.0
1365_0100148	3.0	2.0
1365_0100163	3.0	3.0
1365_0100168	3.0	2.0
1365_0100169	2.0	2.0
1365_0100170	2.0	2.0
1365_0100171	2.0	2.0
1365_0100177	3.0	2.0
1365_0100179	2.0	2.0
1365_0100186	2.0	2.0
1365_0100188	2.0	2.0
1365_0100199	2.0	3.0
1365_0100200	3.0	2.0
1365_0100201	2.0	2.0
1365_0100211	3.0	3.0
1365_0100219	2.0	3.0
1365_0100230	2.0	3.0
1365_0100253	2.0	2.0
1365_0100260	2.0	2.0
1365_0100275	3.0	2.0
1365_0100277	3.0	3.0
1365_0100285	2.0	2.0
1365_0100480	2.0	3.0
1385_0000011	1.0	1.0
1385_0000037	1.0	1.0
1385_0000050	1.0	1.0
1385_0000054	2.0	1.0
1385_0000128	1.0	1.0
1385_0001105	1.0	1.0
1385_0001108	2.0	1.0
1385_0001111	2.0	1.0
1385_0001112	2.0	1.0
1385_0001113	1.0	1.0
1385_0001123	2.0	1.0
1385_0001130	1.0	1.0
1385_0001132	1.0	1.0
1385_0001135	1.0	1.0
1385_0001136	1.0	1.0
1385_0001149	2.0	1.0
1385_0001153	2.0	1.0
1385_0001161	1.0	1.0
1385_0001163	1.0	1.0
1385_0001165	1.0	1.0
1385_0001173	0.0	1.0
1385_0001193	1.0	1.0
1385_0001525	1.0	1.0
1385_0001714	1.0	1.0
1385_0001715	0.0	1.0
1385_0001718	0.0	1.0
1385_0001728	1.0	2.0
1385_0001732	0.0	1.0
1385_0001740	1.0	1.0
1385_0001741	0.0	1.0
1385_0001748	1.0	2.0
1385_0001759	0.0	1.0
1385_0001762	2.0	1.0
1385_0001768	2.0	1.0
1385_0001773	0.0	1.0
1385_0001774	0.0	1.0
1385_0001791	0.0	1.0
1385_0001796	1.0	1.0
1395_0000338	2.0	2.0
1395_0000353	1.0	1.0
1395_0000356	1.0	1.0
1395_0000376	2.0	2.0
1395_0000380	2.0	2.0
1395_0000387	3.0	2.0
1395_0000388	2.0	2.0
1395_0000396	1.0	2.0
1395_0000398	2.0	2.0
1395_0000409	2.0	2.0
1395_0000448	2.0	1.0
1395_0000449	2.0	1.0
1395_0000451	1.0	2.0
1395_0000452	1.0	1.0
1395_0000458	2.0	1.0
1395_0000470	2.0	1.0
1395_0000471	2.0	1.0
1395_0000528	2.0	1.0
1395_0000547	2.0	1.0
1395_0000572	1.0	1.0
1395_0000593	0.0	2.0
1395_0000598	0.0	1.0
1395_0000599	1.0	1.0
1395_0000602	1.0	1.0
1395_0000609	1.0	1.0
1395_0000612	0.0	2.0
1395_0000635	0.0	1.0
1395_0000642	1.0	1.0
1395_0000646	1.0	1.0
1395_0000649	1.0	2.0
1395_0001016	1.0	1.0
1395_0001034	1.0	1.0
1395_0001045	2.0	1.0
1395_0001061	2.0	2.0
1395_0001066	1.0	2.0
1395_0001103	1.0	1.0
1395_0001118	0.0	1.0
1395_0001132	2.0	2.0
1395_0001149	1.0	1.0
1395_0001160	1.0	2.0
LANGUAGE: IT, 3th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.17
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.94
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.57      0.93      0.70        41
         2.0       0.66      0.29      0.40        65
         3.0       0.56      0.92      0.70        39
         4.0       0.00      0.00      0.00         1

    accuracy                           0.58       160
   macro avg       0.36      0.43      0.36       160
weighted avg       0.55      0.58      0.51       160

[[ 0 10  4  0  0]
 [ 0 38  3  0  0]
 [ 0 19 19 27  0]
 [ 0  0  3 36  0]
 [ 0  0  0  1  0]]
0.5149411469928925
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.91
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.87
              precision    recall  f1-score   support

         0.0       0.50      0.14      0.22        14
         1.0       0.51      0.90      0.65        41
         2.0       0.70      0.43      0.53        65
         3.0       0.64      0.72      0.67        39
         4.0       0.00      0.00      0.00         1

    accuracy                           0.59       160
   macro avg       0.47      0.44      0.42       160
weighted avg       0.61      0.59      0.57       160

[[ 2 12  0  0  0]
 [ 2 37  2  0  0]
 [ 0 22 28 15  0]
 [ 0  1 10 28  0]
 [ 0  0  0  1  0]]
0.5683786769496867
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.81
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.86
              precision    recall  f1-score   support

         0.0       0.43      0.21      0.29        14
         1.0       0.56      0.80      0.66        41
         2.0       0.69      0.45      0.54        65
         3.0       0.60      0.79      0.68        39
         4.0       0.00      0.00      0.00         1

    accuracy                           0.60       160
   macro avg       0.45      0.45      0.43       160
weighted avg       0.61      0.60      0.58       160

[[ 3  9  2  0  0]
 [ 4 33  4  0  0]
 [ 0 16 29 20  0]
 [ 0  1  7 31  0]
 [ 0  0  0  1  0]]
0.5804067089452604
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.71
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.86
              precision    recall  f1-score   support

         0.0       0.50      0.21      0.30        14
         1.0       0.61      0.85      0.71        41
         2.0       0.68      0.49      0.57        65
         3.0       0.60      0.77      0.67        39
         4.0       0.00      0.00      0.00         1

    accuracy                           0.62       160
   macro avg       0.48      0.47      0.45       160
weighted avg       0.62      0.62      0.61       160

[[ 3  8  3  0  0]
 [ 3 35  3  0  0]
 [ 0 14 32 19  0]
 [ 0  0  9 30  0]
 [ 0  0  0  1  0]]
0.6057544141252006
160 160 160
Filename	True Label	Prediction
1325_1001010	3.0	3.0
1325_1001011	3.0	3.0
1325_1001015	2.0	3.0
1325_1001023	3.0	2.0
1325_1001035	3.0	3.0
1325_1001039	3.0	3.0
1325_1001050	3.0	3.0
1325_1001053	2.0	2.0
1325_1001059	2.0	3.0
1325_1001063	2.0	3.0
1325_1001085	3.0	2.0
1325_1001086	3.0	3.0
1325_1001088	2.0	3.0
1325_1001107	3.0	3.0
1325_1001108	3.0	3.0
1325_1001109	2.0	3.0
1325_1001120	3.0	3.0
1325_1001127	3.0	3.0
1325_1001133	3.0	3.0
1325_1001134	3.0	3.0
1325_1001138	2.0	3.0
1325_1001154	3.0	3.0
1325_1001155	3.0	3.0
1325_1001161	2.0	3.0
1325_1001170	3.0	3.0
1325_9000095	2.0	3.0
1325_9000104	3.0	3.0
1325_9000106	3.0	2.0
1325_9000187	3.0	3.0
1325_9000278	3.0	3.0
1325_9000279	3.0	3.0
1325_9000314	2.0	3.0
1325_9000315	2.0	3.0
1325_9000317	3.0	3.0
1325_9000318	3.0	3.0
1325_9000536	3.0	3.0
1325_9000685	4.0	3.0
1325_9000686	2.0	3.0
1325_9000700	3.0	3.0
1365_0100004	2.0	2.0
1365_0100007	2.0	2.0
1365_0100023	2.0	2.0
1365_0100057	2.0	3.0
1365_0100069	3.0	2.0
1365_0100071	3.0	3.0
1365_0100093	2.0	2.0
1365_0100095	2.0	2.0
1365_0100099	2.0	2.0
1365_0100136	2.0	2.0
1365_0100137	2.0	2.0
1365_0100167	2.0	2.0
1365_0100180	2.0	2.0
1365_0100187	3.0	2.0
1365_0100202	2.0	2.0
1365_0100203	2.0	2.0
1365_0100212	3.0	3.0
1365_0100226	3.0	2.0
1365_0100227	3.0	3.0
1365_0100229	2.0	3.0
1365_0100258	2.0	2.0
1365_0100274	3.0	3.0
1365_0100278	3.0	3.0
1365_0100279	2.0	2.0
1365_0100286	2.0	2.0
1365_0100287	2.0	2.0
1365_0100299	3.0	2.0
1365_0100448	2.0	3.0
1365_0100451	3.0	3.0
1365_0100459	3.0	3.0
1365_0100472	2.0	3.0
1365_0100474	2.0	3.0
1365_0100476	2.0	3.0
1365_0100481	2.0	3.0
1365_0100482	2.0	3.0
1385_0000013	1.0	1.0
1385_0000023	1.0	1.0
1385_0000033	2.0	1.0
1385_0000041	1.0	1.0
1385_0000045	2.0	2.0
1385_0000049	1.0	1.0
1385_0000052	1.0	1.0
1385_0000059	2.0	1.0
1385_0000095	1.0	0.0
1385_0000097	2.0	1.0
1385_0000102	2.0	1.0
1385_0000126	1.0	1.0
1385_0000127	2.0	1.0
1385_0000130	2.0	1.0
1385_0001118	2.0	1.0
1385_0001121	2.0	1.0
1385_0001133	2.0	1.0
1385_0001138	1.0	1.0
1385_0001148	2.0	1.0
1385_0001154	1.0	1.0
1385_0001155	1.0	1.0
1385_0001160	1.0	1.0
1385_0001164	1.0	1.0
1385_0001172	0.0	1.0
1385_0001192	1.0	1.0
1385_0001195	2.0	2.0
1385_0001197	1.0	1.0
1385_0001501	1.0	1.0
1385_0001717	1.0	1.0
1385_0001723	0.0	0.0
1385_0001725	1.0	1.0
1385_0001729	1.0	1.0
1385_0001730	2.0	2.0
1385_0001733	1.0	1.0
1385_0001746	1.0	1.0
1385_0001749	1.0	1.0
1385_0001751	0.0	1.0
1385_0001753	1.0	1.0
1385_0001758	0.0	0.0
1385_0001760	1.0	1.0
1385_0001785	1.0	1.0
1385_0001787	1.0	1.0
1385_0001788	1.0	1.0
1395_0000360	3.0	2.0
1395_0000361	2.0	2.0
1395_0000364	2.0	1.0
1395_0000365	3.0	2.0
1395_0000368	1.0	0.0
1395_0000378	2.0	1.0
1395_0000390	1.0	1.0
1395_0000392	2.0	2.0
1395_0000403	2.0	2.0
1395_0000404	2.0	2.0
1395_0000415	1.0	1.0
1395_0000454	2.0	2.0
1395_0000455	2.0	2.0
1395_0000516	1.0	0.0
1395_0000525	2.0	1.0
1395_0000534	2.0	2.0
1395_0000549	2.0	2.0
1395_0000556	1.0	1.0
1395_0000563	2.0	1.0
1395_0000575	1.0	1.0
1395_0000581	2.0	2.0
1395_0000582	0.0	1.0
1395_0000608	0.0	1.0
1395_0000611	0.0	1.0
1395_0000627	1.0	1.0
1395_0000628	0.0	2.0
1395_0000639	0.0	2.0
1395_0001015	1.0	2.0
1395_0001023	1.0	1.0
1395_0001040	0.0	1.0
1395_0001064	2.0	2.0
1395_0001068	0.0	2.0
1395_0001101	1.0	1.0
1395_0001114	1.0	1.0
1395_0001115	1.0	2.0
1395_0001117	1.0	1.0
1395_0001124	0.0	1.0
1395_0001126	1.0	1.0
1395_0001131	0.0	1.0
1395_0001141	2.0	2.0
1395_0001146	0.0	0.0
1395_0001164	2.0	2.0
1395_0001170	1.0	2.0
LANGUAGE: IT, 4th Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.22
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.28
              precision    recall  f1-score   support

         0.0       0.40      0.14      0.21        14
         1.0       0.58      0.93      0.72        41
         2.0       0.50      0.69      0.58        65
         3.0       0.00      0.00      0.00        39
         4.0       0.00      0.00      0.00         1

    accuracy                           0.53       160
   macro avg       0.30      0.35      0.30       160
weighted avg       0.39      0.53      0.44       160

[[ 2 10  2  0  0]
 [ 0 38  3  0  0]
 [ 3 17 45  0  0]
 [ 0  0 39  0  0]
 [ 0  0  1  0  0]]
0.43803456450011213
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.97
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.89
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.56      0.46      0.51        41
         2.0       0.53      0.60      0.57        65
         3.0       0.62      0.85      0.72        39
         4.0       0.00      0.00      0.00         1

    accuracy                           0.57       160
   macro avg       0.34      0.38      0.36       160
weighted avg       0.51      0.57      0.53       160

[[ 0  8  6  0  0]
 [ 0 19 22  0  0]
 [ 0  7 39 19  0]
 [ 0  0  6 33  0]
 [ 0  0  0  1  0]]
0.5343170289855073
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.83
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.82
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        14
         1.0       0.54      0.78      0.64        41
         2.0       0.62      0.51      0.56        65
         3.0       0.62      0.77      0.69        39
         4.0       0.00      0.00      0.00         1

    accuracy                           0.59       160
   macro avg       0.36      0.41      0.38       160
weighted avg       0.54      0.59      0.56       160

[[ 0 12  2  0  0]
 [ 0 32  9  0  0]
 [ 0 15 33 17  0]
 [ 0  0  9 30  0]
 [ 0  0  0  1  0]]
0.5593280245470484
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.71
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.86
              precision    recall  f1-score   support

         0.0       1.00      0.14      0.25        14
         1.0       0.53      0.56      0.55        41
         2.0       0.57      0.54      0.56        65
         3.0       0.61      0.85      0.71        39
         4.0       0.00      0.00      0.00         1

    accuracy                           0.58       160
   macro avg       0.54      0.42      0.41       160
weighted avg       0.61      0.58      0.56       160

[[ 2 10  2  0  0]
 [ 0 23 18  0  0]
 [ 0 10 35 20  0]
 [ 0  0  6 33  0]
 [ 0  0  0  1  0]]
0.5608806963645673
160 160 160
Filename	True Label	Prediction
1325_1001014	3.0	3.0
1325_1001017	3.0	3.0
1325_1001019	3.0	3.0
1325_1001046	2.0	3.0
1325_1001051	3.0	3.0
1325_1001052	2.0	3.0
1325_1001055	3.0	3.0
1325_1001075	2.0	3.0
1325_1001076	2.0	3.0
1325_1001081	2.0	3.0
1325_1001091	2.0	3.0
1325_1001092	2.0	3.0
1325_1001093	2.0	3.0
1325_1001099	3.0	3.0
1325_1001121	3.0	3.0
1325_1001123	3.0	3.0
1325_1001131	3.0	3.0
1325_1001158	3.0	3.0
1325_1001159	3.0	3.0
1325_1001163	2.0	3.0
1325_1001164	3.0	3.0
1325_1001165	2.0	3.0
1325_1001167	3.0	3.0
1325_9000107	3.0	3.0
1325_9000136	3.0	3.0
1325_9000186	3.0	3.0
1325_9000188	3.0	3.0
1325_9000209	3.0	3.0
1325_9000211	2.0	3.0
1325_9000215	3.0	3.0
1325_9000237	3.0	3.0
1325_9000239	3.0	3.0
1325_9000319	2.0	3.0
1325_9000323	2.0	3.0
1325_9000601	3.0	3.0
1325_9000602	4.0	3.0
1325_9000675	3.0	3.0
1325_9000676	3.0	3.0
1325_9000678	3.0	3.0
1365_0100012	2.0	2.0
1365_0100014	2.0	2.0
1365_0100017	2.0	3.0
1365_0100029	1.0	2.0
1365_0100063	3.0	3.0
1365_0100064	3.0	2.0
1365_0100066	2.0	2.0
1365_0100067	2.0	2.0
1365_0100073	3.0	2.0
1365_0100079	2.0	2.0
1365_0100098	2.0	2.0
1365_0100103	3.0	3.0
1365_0100106	2.0	3.0
1365_0100116	3.0	2.0
1365_0100139	2.0	2.0
1365_0100151	2.0	2.0
1365_0100162	3.0	3.0
1365_0100164	3.0	3.0
1365_0100166	2.0	2.0
1365_0100172	2.0	2.0
1365_0100173	2.0	2.0
1365_0100174	2.0	2.0
1365_0100176	2.0	2.0
1365_0100183	2.0	2.0
1365_0100190	3.0	2.0
1365_0100191	2.0	3.0
1365_0100195	2.0	2.0
1365_0100196	2.0	3.0
1365_0100204	2.0	2.0
1365_0100213	2.0	2.0
1365_0100215	2.0	2.0
1365_0100222	2.0	3.0
1365_0100224	3.0	3.0
1365_0100233	3.0	3.0
1365_0100265	3.0	2.0
1365_0100266	2.0	3.0
1365_0100280	1.0	2.0
1365_0100281	2.0	2.0
1365_0100288	2.0	2.0
1365_0100290	2.0	2.0
1365_0100455	3.0	3.0
1365_0100456	3.0	3.0
1365_0100470	2.0	3.0
1365_0100473	2.0	2.0
1385_0000012	1.0	1.0
1385_0000020	1.0	1.0
1385_0000040	1.0	1.0
1385_0000042	1.0	1.0
1385_0000058	1.0	1.0
1385_0000099	1.0	1.0
1385_0000100	2.0	1.0
1385_0000120	0.0	1.0
1385_0000123	1.0	1.0
1385_0000125	2.0	1.0
1385_0000129	2.0	2.0
1385_0001103	2.0	1.0
1385_0001107	2.0	1.0
1385_0001120	2.0	1.0
1385_0001122	2.0	1.0
1385_0001128	1.0	2.0
1385_0001137	2.0	1.0
1385_0001147	1.0	1.0
1385_0001150	1.0	1.0
1385_0001170	0.0	1.0
1385_0001175	0.0	1.0
1385_0001188	0.0	1.0
1385_0001190	0.0	1.0
1385_0001198	1.0	2.0
1385_0001199	1.0	2.0
1385_0001503	1.0	1.0
1385_0001523	1.0	2.0
1385_0001526	0.0	1.0
1385_0001528	1.0	2.0
1385_0001726	1.0	2.0
1385_0001738	0.0	0.0
1385_0001739	1.0	2.0
1385_0001754	1.0	1.0
1385_0001764	1.0	2.0
1385_0001786	1.0	1.0
1385_0001790	1.0	1.0
1385_0001792	1.0	1.0
1385_0001798	1.0	1.0
1385_0001800	1.0	1.0
1395_0000357	3.0	2.0
1395_0000366	2.0	2.0
1395_0000383	2.0	2.0
1395_0000399	2.0	2.0
1395_0000402	1.0	2.0
1395_0000414	2.0	2.0
1395_0000443	2.0	2.0
1395_0000465	1.0	1.0
1395_0000504	2.0	1.0
1395_0000513	2.0	2.0
1395_0000518	2.0	2.0
1395_0000526	1.0	1.0
1395_0000527	1.0	1.0
1395_0000531	2.0	2.0
1395_0000535	1.0	1.0
1395_0000565	2.0	1.0
1395_0000584	0.0	1.0
1395_0000587	0.0	0.0
1395_0000610	2.0	2.0
1395_0000631	0.0	2.0
1395_0000644	1.0	2.0
1395_0001013	1.0	2.0
1395_0001017	1.0	2.0
1395_0001070	2.0	2.0
1395_0001071	2.0	2.0
1395_0001075	0.0	2.0
1395_0001084	1.0	2.0
1395_0001090	1.0	2.0
1395_0001108	0.0	1.0
1395_0001116	2.0	1.0
1395_0001120	1.0	1.0
1395_0001121	0.0	1.0
1395_0001122	0.0	1.0
1395_0001123	1.0	2.0
1395_0001150	1.0	1.0
1395_0001158	1.0	2.0
1395_0001169	2.0	2.0
1395_0001171	1.0	1.0
Averaged weighted F1-scores 0.5488916035779878
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2']
LANGUAGE: IT, 0th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.07
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.96
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        21
         1.0       0.66      0.58      0.62        67
         2.0       0.57      0.97      0.72        60
         3.0       0.00      0.00      0.00        12

    accuracy                           0.61       160
   macro avg       0.31      0.39      0.33       160
weighted avg       0.49      0.61      0.53       160

[[ 0 18  3  0]
 [ 0 39 28  0]
 [ 0  2 58  0]
 [ 0  0 12  0]]
0.5294125258799172
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.84
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.74
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        21
         1.0       0.65      0.90      0.75        67
         2.0       0.72      0.82      0.77        60
         3.0       0.00      0.00      0.00        12

    accuracy                           0.68       160
   macro avg       0.34      0.43      0.38       160
weighted avg       0.54      0.68      0.60       160

[[ 0 21  0  0]
 [ 0 60  7  0]
 [ 0 11 49  0]
 [ 0  0 12  0]]
0.6031471108490566
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.76
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.69
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        21
         1.0       0.67      0.84      0.74        67
         2.0       0.70      0.88      0.78        60
         3.0       0.00      0.00      0.00        12

    accuracy                           0.68       160
   macro avg       0.34      0.43      0.38       160
weighted avg       0.54      0.68      0.60       160

[[ 0 21  0  0]
 [ 0 56 11  0]
 [ 0  7 53  0]
 [ 0  0 12  0]]
0.6028754382547721
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.68
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.71
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        21
         1.0       0.67      0.82      0.74        67
         2.0       0.69      0.90      0.78        60
         3.0       0.00      0.00      0.00        12

    accuracy                           0.68       160
   macro avg       0.34      0.43      0.38       160
weighted avg       0.54      0.68      0.60       160

[[ 0 21  0  0]
 [ 0 55 12  0]
 [ 0  6 54  0]
 [ 0  0 12  0]]
0.6026225561715787
160 160 160
Filename	True Label	Prediction
1325_1001009	3.0	2.0
1325_1001025	2.0	2.0
1325_1001027	3.0	2.0
1325_1001035	3.0	2.0
1325_1001042	2.0	2.0
1325_1001043	2.0	2.0
1325_1001056	2.0	2.0
1325_1001075	1.0	2.0
1325_1001076	2.0	2.0
1325_1001077	2.0	2.0
1325_1001083	2.0	2.0
1325_1001089	2.0	2.0
1325_1001107	2.0	2.0
1325_1001108	3.0	2.0
1325_1001111	3.0	2.0
1325_1001119	2.0	2.0
1325_1001125	3.0	2.0
1325_1001130	2.0	2.0
1325_1001139	2.0	2.0
1325_1001143	2.0	2.0
1325_1001156	2.0	2.0
1325_1001157	2.0	2.0
1325_1001159	2.0	2.0
1325_1001160	2.0	2.0
1325_1001170	2.0	2.0
1325_9000090	2.0	2.0
1325_9000106	2.0	2.0
1325_9000136	2.0	2.0
1325_9000137	2.0	2.0
1325_9000140	3.0	2.0
1325_9000188	2.0	2.0
1325_9000209	2.0	2.0
1325_9000240	3.0	2.0
1325_9000278	3.0	2.0
1325_9000316	2.0	2.0
1325_9000533	2.0	2.0
1325_9000536	3.0	2.0
1325_9000602	3.0	2.0
1325_9000612	2.0	2.0
1325_9000675	2.0	2.0
1325_9000685	3.0	2.0
1365_0100002	2.0	2.0
1365_0100009	1.0	1.0
1365_0100015	2.0	1.0
1365_0100019	1.0	1.0
1365_0100027	2.0	2.0
1365_0100057	2.0	2.0
1365_0100071	2.0	2.0
1365_0100073	2.0	2.0
1365_0100092	2.0	2.0
1365_0100093	1.0	1.0
1365_0100094	2.0	2.0
1365_0100095	2.0	2.0
1365_0100099	1.0	2.0
1365_0100104	2.0	2.0
1365_0100133	2.0	2.0
1365_0100134	2.0	1.0
1365_0100148	2.0	2.0
1365_0100172	1.0	2.0
1365_0100178	1.0	2.0
1365_0100180	1.0	1.0
1365_0100183	2.0	2.0
1365_0100200	2.0	2.0
1365_0100223	2.0	2.0
1365_0100227	2.0	2.0
1365_0100229	2.0	2.0
1365_0100230	2.0	2.0
1365_0100231	2.0	2.0
1365_0100261	2.0	2.0
1365_0100265	2.0	2.0
1365_0100270	2.0	2.0
1365_0100275	2.0	2.0
1365_0100455	2.0	2.0
1365_0100456	2.0	2.0
1365_0100457	2.0	2.0
1385_0000020	1.0	1.0
1385_0000040	1.0	1.0
1385_0000048	1.0	1.0
1385_0000054	1.0	1.0
1385_0000059	1.0	1.0
1385_0000122	1.0	1.0
1385_0000124	1.0	1.0
1385_0001113	1.0	1.0
1385_0001125	1.0	1.0
1385_0001129	0.0	1.0
1385_0001135	1.0	1.0
1385_0001137	1.0	1.0
1385_0001153	2.0	1.0
1385_0001154	1.0	1.0
1385_0001155	1.0	1.0
1385_0001159	1.0	1.0
1385_0001164	1.0	1.0
1385_0001169	1.0	1.0
1385_0001189	0.0	1.0
1385_0001191	1.0	1.0
1385_0001522	0.0	1.0
1385_0001712	1.0	1.0
1385_0001723	0.0	1.0
1385_0001725	0.0	1.0
1385_0001737	1.0	1.0
1385_0001744	0.0	1.0
1385_0001748	2.0	1.0
1385_0001750	0.0	1.0
1385_0001751	0.0	1.0
1385_0001756	1.0	1.0
1385_0001757	1.0	1.0
1385_0001758	0.0	1.0
1385_0001762	1.0	1.0
1385_0001789	1.0	1.0
1385_0001795	0.0	1.0
1385_0001798	1.0	1.0
1395_0000340	1.0	1.0
1395_0000355	1.0	1.0
1395_0000359	1.0	1.0
1395_0000361	1.0	1.0
1395_0000383	1.0	1.0
1395_0000409	2.0	1.0
1395_0000413	1.0	1.0
1395_0000414	1.0	1.0
1395_0000415	1.0	1.0
1395_0000447	1.0	2.0
1395_0000458	1.0	1.0
1395_0000515	2.0	1.0
1395_0000518	1.0	1.0
1395_0000525	1.0	1.0
1395_0000527	0.0	1.0
1395_0000529	1.0	1.0
1395_0000531	1.0	1.0
1395_0000535	1.0	1.0
1395_0000556	1.0	1.0
1395_0000564	1.0	1.0
1395_0000581	1.0	2.0
1395_0000584	0.0	1.0
1395_0000607	0.0	1.0
1395_0000611	0.0	1.0
1395_0000612	1.0	1.0
1395_0000626	1.0	2.0
1395_0000644	1.0	2.0
1395_0001010	1.0	1.0
1395_0001016	1.0	1.0
1395_0001017	0.0	1.0
1395_0001020	1.0	1.0
1395_0001024	1.0	1.0
1395_0001028	1.0	2.0
1395_0001045	1.0	1.0
1395_0001064	1.0	1.0
1395_0001070	1.0	2.0
1395_0001090	1.0	1.0
1395_0001104	0.0	1.0
1395_0001115	1.0	1.0
1395_0001120	0.0	1.0
1395_0001122	0.0	1.0
1395_0001132	2.0	2.0
1395_0001145	1.0	2.0
1395_0001146	0.0	1.0
1395_0001149	0.0	1.0
1395_0001150	0.0	1.0
1395_0001161	1.0	1.0
1395_0001164	1.0	2.0
1395_0001169	1.0	1.0
LANGUAGE: IT, 1th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.99
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.77
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.72      0.86      0.79        66
         2.0       0.72      0.97      0.82        60
         3.0       0.00      0.00      0.00        12

    accuracy                           0.72       160
   macro avg       0.36      0.46      0.40       160
weighted avg       0.57      0.72      0.63       160

[[ 0 20  2  0]
 [ 0 57  9  0]
 [ 0  2 58  0]
 [ 0  0 12  0]]
0.6328209831254584
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.84
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.73
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.69      0.82      0.75        66
         2.0       0.71      0.97      0.82        60
         3.0       0.00      0.00      0.00        12

    accuracy                           0.70       160
   macro avg       0.35      0.45      0.39       160
weighted avg       0.55      0.70      0.62       160

[[ 0 22  0  0]
 [ 0 54 12  0]
 [ 0  2 58  0]
 [ 0  0 12  0]]
0.6157130281690142
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.72
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.69
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.70      0.88      0.78        66
         2.0       0.73      0.88      0.80        60
         3.0       0.25      0.08      0.12        12

    accuracy                           0.70       160
   macro avg       0.42      0.46      0.43       160
weighted avg       0.58      0.70      0.63       160

[[ 0 21  1  0]
 [ 0 58  8  0]
 [ 0  4 53  3]
 [ 0  0 11  1]]
0.6293881200484434
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.62
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.68
              precision    recall  f1-score   support

         0.0       0.80      0.18      0.30        22
         1.0       0.73      0.91      0.81        66
         2.0       0.76      0.92      0.83        60
         3.0       0.00      0.00      0.00        12

    accuracy                           0.74       160
   macro avg       0.57      0.50      0.49       160
weighted avg       0.70      0.74      0.69       160

[[ 4 17  1  0]
 [ 1 60  5  0]
 [ 0  4 55  1]
 [ 0  1 11  0]]
0.6877002002002002
160 160 160
Filename	True Label	Prediction
1325_1001011	2.0	2.0
1325_1001013	2.0	2.0
1325_1001019	2.0	3.0
1325_1001021	2.0	2.0
1325_1001022	2.0	2.0
1325_1001023	2.0	2.0
1325_1001033	2.0	2.0
1325_1001036	2.0	2.0
1325_1001040	2.0	2.0
1325_1001046	2.0	2.0
1325_1001051	2.0	2.0
1325_1001053	1.0	2.0
1325_1001078	2.0	2.0
1325_1001080	2.0	2.0
1325_1001088	2.0	2.0
1325_1001092	2.0	2.0
1325_1001094	2.0	2.0
1325_1001096	2.0	2.0
1325_1001113	3.0	2.0
1325_1001124	2.0	2.0
1325_1001126	2.0	2.0
1325_1001127	3.0	2.0
1325_1001131	2.0	2.0
1325_1001136	2.0	2.0
1325_1001144	2.0	2.0
1325_1001152	2.0	2.0
1325_1001154	3.0	2.0
1325_1001161	2.0	2.0
1325_1001166	2.0	2.0
1325_1001167	2.0	2.0
1325_9000059	2.0	2.0
1325_9000087	2.0	2.0
1325_9000088	2.0	2.0
1325_9000099	3.0	2.0
1325_9000317	3.0	2.0
1325_9000505	3.0	2.0
1325_9000676	3.0	2.0
1365_0100013	2.0	2.0
1365_0100021	2.0	2.0
1365_0100026	1.0	1.0
1365_0100028	1.0	2.0
1365_0100030	1.0	2.0
1365_0100056	2.0	2.0
1365_0100063	2.0	2.0
1365_0100066	1.0	1.0
1365_0100080	2.0	2.0
1365_0100096	2.0	2.0
1365_0100105	3.0	2.0
1365_0100120	3.0	2.0
1365_0100121	2.0	2.0
1365_0100125	2.0	2.0
1365_0100137	1.0	1.0
1365_0100166	1.0	2.0
1365_0100176	2.0	2.0
1365_0100184	1.0	1.0
1365_0100185	1.0	1.0
1365_0100188	2.0	2.0
1365_0100191	2.0	2.0
1365_0100194	2.0	2.0
1365_0100203	2.0	2.0
1365_0100212	3.0	2.0
1365_0100218	2.0	2.0
1365_0100222	2.0	2.0
1365_0100225	2.0	1.0
1365_0100226	2.0	2.0
1365_0100252	2.0	2.0
1365_0100253	1.0	1.0
1365_0100262	2.0	2.0
1365_0100269	2.0	2.0
1365_0100276	3.0	2.0
1365_0100282	2.0	2.0
1365_0100289	2.0	1.0
1365_0100451	2.0	2.0
1365_0100458	2.0	2.0
1365_0100459	2.0	2.0
1365_0100472	2.0	2.0
1365_0100474	2.0	2.0
1365_0100475	2.0	2.0
1365_0100476	2.0	2.0
1385_0000011	0.0	1.0
1385_0000021	1.0	1.0
1385_0000034	1.0	1.0
1385_0000039	1.0	1.0
1385_0000045	1.0	1.0
1385_0000051	1.0	1.0
1385_0000058	1.0	1.0
1385_0000095	1.0	0.0
1385_0000097	1.0	1.0
1385_0000103	1.0	1.0
1385_0000123	1.0	1.0
1385_0000125	1.0	1.0
1385_0000129	1.0	1.0
1385_0000130	1.0	1.0
1385_0001104	0.0	1.0
1385_0001105	1.0	1.0
1385_0001107	1.0	1.0
1385_0001110	1.0	1.0
1385_0001112	1.0	1.0
1385_0001118	1.0	1.0
1385_0001128	0.0	1.0
1385_0001130	1.0	1.0
1385_0001138	1.0	1.0
1385_0001147	1.0	1.0
1385_0001157	1.0	1.0
1385_0001175	0.0	1.0
1385_0001188	1.0	1.0
1385_0001192	1.0	1.0
1385_0001194	1.0	1.0
1385_0001195	1.0	1.0
1385_0001196	0.0	1.0
1385_0001198	2.0	1.0
1385_0001501	0.0	1.0
1385_0001526	0.0	0.0
1385_0001528	1.0	1.0
1385_0001715	1.0	1.0
1385_0001719	1.0	1.0
1385_0001726	1.0	1.0
1385_0001727	0.0	1.0
1385_0001728	0.0	2.0
1385_0001742	0.0	1.0
1385_0001760	1.0	1.0
1385_0001761	0.0	1.0
1385_0001766	1.0	1.0
1385_0001774	0.0	0.0
1385_0001787	0.0	1.0
1385_0001792	1.0	1.0
1385_0001793	0.0	1.0
1385_0001794	1.0	1.0
1395_0000337	0.0	0.0
1395_0000338	1.0	1.0
1395_0000376	2.0	1.0
1395_0000387	3.0	1.0
1395_0000390	1.0	1.0
1395_0000392	1.0	1.0
1395_0000402	1.0	1.0
1395_0000432	1.0	1.0
1395_0000448	1.0	1.0
1395_0000455	1.0	1.0
1395_0000465	1.0	1.0
1395_0000499	1.0	1.0
1395_0000500	1.0	1.0
1395_0000526	1.0	1.0
1395_0000548	1.0	1.0
1395_0000550	1.0	1.0
1395_0000552	1.0	1.0
1395_0000553	1.0	1.0
1395_0000575	1.0	1.0
1395_0000598	1.0	1.0
1395_0000599	1.0	1.0
1395_0000606	0.0	0.0
1395_0000628	0.0	1.0
1395_0000646	0.0	1.0
1395_0001015	0.0	1.0
1395_0001023	1.0	1.0
1395_0001060	1.0	1.0
1395_0001061	1.0	2.0
1395_0001065	0.0	1.0
1395_0001078	0.0	1.0
1395_0001114	0.0	1.0
1395_0001170	1.0	1.0
LANGUAGE: IT, 2th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.01
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.86
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.62      0.87      0.72        67
         2.0       0.70      0.77      0.73        60
         3.0       0.00      0.00      0.00        11

    accuracy                           0.65       160
   macro avg       0.33      0.41      0.36       160
weighted avg       0.52      0.65      0.58       160

[[ 0 22  0  0]
 [ 0 58  9  0]
 [ 0 14 46  0]
 [ 0  0 11  0]]
0.5755175983436853
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.84
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.88
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.62      0.94      0.75        67
         2.0       0.74      0.72      0.73        60
         3.0       0.00      0.00      0.00        11

    accuracy                           0.66       160
   macro avg       0.34      0.41      0.37       160
weighted avg       0.54      0.66      0.59       160

[[ 0 22  0  0]
 [ 0 63  4  0]
 [ 0 17 43  0]
 [ 0  0 11  0]]
0.5855092267575971
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.74
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.84
              precision    recall  f1-score   support

         0.0       1.00      0.14      0.24        22
         1.0       0.65      0.87      0.74        67
         2.0       0.71      0.80      0.75        60
         3.0       0.00      0.00      0.00        11

    accuracy                           0.68       160
   macro avg       0.59      0.45      0.43       160
weighted avg       0.68      0.68      0.63       160

[[ 3 19  0  0]
 [ 0 58  9  0]
 [ 0 12 48  0]
 [ 0  0 11  0]]
0.6256282051282052
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.66
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.85
              precision    recall  f1-score   support

         0.0       0.60      0.14      0.22        22
         1.0       0.64      0.81      0.72        67
         2.0       0.69      0.82      0.75        60
         3.0       0.00      0.00      0.00        11

    accuracy                           0.66       160
   macro avg       0.48      0.44      0.42       160
weighted avg       0.61      0.66      0.61       160

[[ 3 19  0  0]
 [ 2 54 11  0]
 [ 0 11 49  0]
 [ 0  0 11  0]]
0.6105932179588718
160 160 160
Filename	True Label	Prediction
1325_1001008	2.0	2.0
1325_1001012	2.0	2.0
1325_1001017	2.0	2.0
1325_1001029	2.0	2.0
1325_1001032	2.0	2.0
1325_1001037	2.0	2.0
1325_1001044	2.0	2.0
1325_1001052	2.0	2.0
1325_1001058	2.0	2.0
1325_1001079	2.0	2.0
1325_1001081	2.0	2.0
1325_1001085	2.0	2.0
1325_1001095	2.0	2.0
1325_1001098	2.0	2.0
1325_1001099	3.0	2.0
1325_1001101	3.0	2.0
1325_1001121	2.0	2.0
1325_1001132	2.0	2.0
1325_1001134	2.0	2.0
1325_1001155	2.0	2.0
1325_1001162	2.0	2.0
1325_1001164	2.0	2.0
1325_9000089	2.0	2.0
1325_9000107	2.0	2.0
1325_9000139	2.0	2.0
1325_9000143	3.0	2.0
1325_9000185	3.0	2.0
1325_9000187	2.0	2.0
1325_9000210	1.0	2.0
1325_9000211	2.0	2.0
1325_9000215	3.0	2.0
1325_9000241	3.0	2.0
1325_9000296	1.0	2.0
1325_9000303	2.0	2.0
1325_9000315	2.0	2.0
1325_9000321	3.0	2.0
1325_9000322	3.0	2.0
1325_9000503	3.0	2.0
1325_9000534	2.0	2.0
1325_9000601	2.0	2.0
1365_0100007	1.0	1.0
1365_0100008	2.0	2.0
1365_0100012	1.0	1.0
1365_0100016	2.0	2.0
1365_0100017	2.0	2.0
1365_0100029	1.0	1.0
1365_0100065	1.0	1.0
1365_0100097	2.0	1.0
1365_0100101	2.0	2.0
1365_0100102	2.0	2.0
1365_0100116	2.0	2.0
1365_0100118	2.0	2.0
1365_0100119	3.0	2.0
1365_0100123	2.0	2.0
1365_0100138	2.0	1.0
1365_0100147	2.0	2.0
1365_0100151	1.0	1.0
1365_0100162	2.0	2.0
1365_0100181	1.0	1.0
1365_0100186	1.0	2.0
1365_0100190	2.0	2.0
1365_0100195	1.0	2.0
1365_0100198	1.0	2.0
1365_0100199	2.0	2.0
1365_0100211	3.0	2.0
1365_0100219	2.0	2.0
1365_0100220	2.0	2.0
1365_0100232	1.0	2.0
1365_0100251	2.0	2.0
1365_0100255	1.0	2.0
1365_0100257	2.0	2.0
1365_0100260	2.0	2.0
1365_0100267	2.0	2.0
1365_0100268	1.0	2.0
1365_0100281	2.0	2.0
1365_0100286	1.0	1.0
1365_0100290	1.0	2.0
1365_0100469	2.0	2.0
1365_0100471	1.0	2.0
1385_0000035	1.0	1.0
1385_0000038	1.0	1.0
1385_0000041	1.0	1.0
1385_0000044	1.0	1.0
1385_0000049	1.0	1.0
1385_0000057	1.0	1.0
1385_0000101	1.0	1.0
1385_0000102	1.0	1.0
1385_0000119	1.0	1.0
1385_0000126	1.0	1.0
1385_0001109	1.0	1.0
1385_0001119	1.0	1.0
1385_0001120	1.0	1.0
1385_0001124	1.0	0.0
1385_0001148	1.0	1.0
1385_0001149	1.0	1.0
1385_0001151	1.0	1.0
1385_0001156	1.0	1.0
1385_0001158	1.0	1.0
1385_0001161	1.0	1.0
1385_0001165	1.0	1.0
1385_0001166	1.0	1.0
1385_0001167	0.0	1.0
1385_0001174	0.0	1.0
1385_0001190	0.0	1.0
1385_0001503	1.0	1.0
1385_0001523	1.0	1.0
1385_0001525	1.0	1.0
1385_0001716	1.0	1.0
1385_0001720	0.0	1.0
1385_0001732	0.0	1.0
1385_0001734	0.0	1.0
1385_0001741	0.0	1.0
1385_0001747	0.0	1.0
1385_0001749	0.0	1.0
1385_0001772	1.0	1.0
1385_0001775	1.0	1.0
1385_0001785	0.0	1.0
1385_0001786	1.0	1.0
1385_0001788	0.0	1.0
1395_0000353	1.0	1.0
1395_0000360	2.0	1.0
1395_0000364	1.0	1.0
1395_0000365	2.0	1.0
1395_0000378	1.0	1.0
1395_0000404	1.0	1.0
1395_0000443	2.0	1.0
1395_0000469	1.0	1.0
1395_0000470	1.0	0.0
1395_0000471	1.0	1.0
1395_0000504	1.0	1.0
1395_0000513	2.0	1.0
1395_0000514	2.0	1.0
1395_0000533	2.0	1.0
1395_0000549	1.0	1.0
1395_0000555	1.0	1.0
1395_0000559	1.0	1.0
1395_0000560	2.0	1.0
1395_0000563	1.0	1.0
1395_0000582	0.0	1.0
1395_0000591	0.0	0.0
1395_0000596	2.0	1.0
1395_0000597	1.0	1.0
1395_0000604	0.0	0.0
1395_0000631	0.0	1.0
1395_0000635	0.0	1.0
1395_0000649	2.0	1.0
1395_0001022	1.0	1.0
1395_0001040	0.0	0.0
1395_0001067	0.0	1.0
1395_0001068	0.0	1.0
1395_0001071	1.0	1.0
1395_0001074	1.0	1.0
1395_0001080	1.0	1.0
1395_0001084	0.0	1.0
1395_0001119	1.0	2.0
1395_0001121	0.0	1.0
1395_0001124	1.0	1.0
1395_0001133	1.0	1.0
1395_0001147	0.0	1.0
1395_0001160	1.0	1.0
LANGUAGE: IT, 3th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.04
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.88
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.63      0.82      0.71        67
         2.0       0.68      0.83      0.75        60
         3.0       0.00      0.00      0.00        11

    accuracy                           0.66       160
   macro avg       0.33      0.41      0.37       160
weighted avg       0.52      0.66      0.58       160

[[ 0 21  1  0]
 [ 0 55 12  0]
 [ 0 10 50  0]
 [ 0  1 10  0]]
0.5810620300751881
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.80
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.85
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.63      0.93      0.75        67
         2.0       0.77      0.78      0.78        60
         3.0       0.00      0.00      0.00        11

    accuracy                           0.68       160
   macro avg       0.35      0.43      0.38       160
weighted avg       0.55      0.68      0.60       160

[[ 0 22  0  0]
 [ 0 62  5  0]
 [ 0 13 47  0]
 [ 0  2  9  0]]
0.6041235188688638
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.70
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.81
              precision    recall  f1-score   support

         0.0       0.47      0.36      0.41        22
         1.0       0.67      0.75      0.70        67
         2.0       0.74      0.83      0.78        60
         3.0       0.00      0.00      0.00        11

    accuracy                           0.68       160
   macro avg       0.47      0.49      0.47       160
weighted avg       0.62      0.68      0.64       160

[[ 8 14  0  0]
 [ 9 50  8  0]
 [ 0 10 50  0]
 [ 0  1 10  0]]
0.6442733726074396
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.60
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.80
              precision    recall  f1-score   support

         0.0       0.64      0.32      0.42        22
         1.0       0.68      0.81      0.74        67
         2.0       0.71      0.83      0.77        60
         3.0       0.00      0.00      0.00        11

    accuracy                           0.69       160
   macro avg       0.51      0.49      0.48       160
weighted avg       0.64      0.69      0.66       160

[[ 7 15  0  0]
 [ 4 54  9  0]
 [ 0 10 50  0]
 [ 0  0 11  0]]
0.6565551457674744
160 160 160
Filename	True Label	Prediction
1325_1001010	2.0	2.0
1325_1001014	3.0	2.0
1325_1001018	2.0	2.0
1325_1001024	2.0	2.0
1325_1001039	3.0	2.0
1325_1001041	3.0	2.0
1325_1001045	2.0	2.0
1325_1001048	2.0	2.0
1325_1001050	2.0	2.0
1325_1001054	2.0	2.0
1325_1001055	2.0	2.0
1325_1001057	1.0	2.0
1325_1001059	2.0	2.0
1325_1001082	2.0	2.0
1325_1001084	2.0	2.0
1325_1001090	2.0	2.0
1325_1001091	2.0	2.0
1325_1001093	2.0	2.0
1325_1001128	2.0	2.0
1325_1001138	2.0	2.0
1325_1001153	2.0	2.0
1325_1001158	2.0	2.0
1325_1001163	2.0	2.0
1325_1001165	2.0	2.0
1325_1001168	2.0	2.0
1325_1001169	2.0	2.0
1325_9000102	2.0	2.0
1325_9000104	2.0	2.0
1325_9000138	3.0	2.0
1325_9000152	2.0	2.0
1325_9000237	2.0	2.0
1325_9000239	2.0	2.0
1325_9000279	3.0	2.0
1325_9000302	2.0	2.0
1325_9000314	2.0	2.0
1325_9000318	3.0	2.0
1325_9000320	3.0	2.0
1325_9000323	2.0	2.0
1325_9000504	2.0	2.0
1325_9000677	2.0	2.0
1325_9000678	3.0	2.0
1325_9000686	2.0	2.0
1325_9000700	2.0	2.0
1325_9000750	3.0	2.0
1365_0100003	1.0	2.0
1365_0100006	2.0	2.0
1365_0100011	2.0	1.0
1365_0100031	2.0	2.0
1365_0100051	0.0	1.0
1365_0100061	3.0	2.0
1365_0100067	1.0	2.0
1365_0100072	2.0	2.0
1365_0100074	2.0	2.0
1365_0100079	2.0	2.0
1365_0100098	1.0	2.0
1365_0100135	2.0	1.0
1365_0100136	1.0	1.0
1365_0100163	2.0	2.0
1365_0100164	2.0	2.0
1365_0100167	1.0	2.0
1365_0100168	2.0	1.0
1365_0100169	1.0	1.0
1365_0100170	2.0	2.0
1365_0100174	1.0	1.0
1365_0100177	2.0	1.0
1365_0100179	1.0	2.0
1365_0100182	2.0	2.0
1365_0100187	2.0	2.0
1365_0100204	1.0	1.0
1365_0100215	2.0	2.0
1365_0100228	1.0	2.0
1365_0100259	2.0	2.0
1365_0100266	2.0	2.0
1365_0100279	2.0	2.0
1365_0100280	1.0	1.0
1365_0100288	1.0	1.0
1365_0100447	2.0	2.0
1365_0100473	2.0	2.0
1365_0100478	1.0	2.0
1365_0100481	1.0	2.0
1385_0000013	1.0	1.0
1385_0000016	1.0	1.0
1385_0000022	1.0	1.0
1385_0000033	1.0	1.0
1385_0000037	1.0	1.0
1385_0000042	1.0	1.0
1385_0000043	1.0	1.0
1385_0000052	1.0	1.0
1385_0000053	1.0	1.0
1385_0000098	1.0	1.0
1385_0000100	1.0	0.0
1385_0000120	0.0	1.0
1385_0000128	1.0	1.0
1385_0001103	1.0	0.0
1385_0001108	1.0	1.0
1385_0001127	1.0	1.0
1385_0001132	1.0	1.0
1385_0001133	1.0	1.0
1385_0001134	1.0	1.0
1385_0001136	1.0	1.0
1385_0001150	1.0	1.0
1385_0001152	1.0	1.0
1385_0001160	1.0	1.0
1385_0001170	0.0	0.0
1385_0001171	0.0	0.0
1385_0001178	0.0	0.0
1385_0001197	0.0	1.0
1385_0001524	0.0	1.0
1385_0001714	1.0	1.0
1385_0001717	1.0	1.0
1385_0001730	1.0	1.0
1385_0001740	1.0	1.0
1385_0001746	1.0	1.0
1385_0001764	0.0	1.0
1385_0001768	1.0	1.0
1385_0001771	0.0	1.0
1385_0001773	0.0	1.0
1385_0001791	1.0	1.0
1385_0001796	1.0	1.0
1385_0001800	1.0	1.0
1395_0000333	1.0	1.0
1395_0000356	1.0	1.0
1395_0000366	2.0	1.0
1395_0000368	0.0	0.0
1395_0000369	2.0	1.0
1395_0000379	1.0	1.0
1395_0000380	1.0	1.0
1395_0000391	3.0	2.0
1395_0000449	2.0	1.0
1395_0000450	1.0	1.0
1395_0000454	1.0	1.0
1395_0000460	1.0	0.0
1395_0000516	1.0	0.0
1395_0000537	1.0	1.0
1395_0000551	2.0	1.0
1395_0000554	1.0	1.0
1395_0000565	1.0	1.0
1395_0000572	1.0	1.0
1395_0000579	1.0	1.0
1395_0000583	1.0	1.0
1395_0000587	0.0	0.0
1395_0000593	0.0	1.0
1395_0000595	0.0	0.0
1395_0000627	1.0	1.0
1395_0000630	0.0	1.0
1395_0000639	0.0	1.0
1395_0001013	1.0	1.0
1395_0001033	1.0	1.0
1395_0001034	0.0	1.0
1395_0001069	2.0	1.0
1395_0001073	1.0	1.0
1395_0001076	1.0	1.0
1395_0001103	0.0	1.0
1395_0001108	0.0	1.0
1395_0001118	0.0	1.0
1395_0001123	0.0	1.0
1395_0001131	0.0	0.0
1395_0001141	1.0	1.0
1395_0001167	2.0	1.0
1395_0001171	1.0	1.0
LANGUAGE: IT, 4th Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.07
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 1.01
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.58      0.90      0.70        67
         2.0       0.68      0.63      0.66        60
         3.0       0.00      0.00      0.00        11

    accuracy                           0.61       160
   macro avg       0.31      0.38      0.34       160
weighted avg       0.50      0.61      0.54       160

[[ 0 21  1  0]
 [ 0 60  7  0]
 [ 0 22 38  0]
 [ 0  1 10  0]]
0.5395493042952209
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.82
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.87
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.60      0.87      0.71        67
         2.0       0.67      0.72      0.69        60
         3.0       0.00      0.00      0.00        11

    accuracy                           0.63       160
   macro avg       0.32      0.40      0.35       160
weighted avg       0.50      0.63      0.56       160

[[ 0 21  1  0]
 [ 0 58  9  0]
 [ 0 17 43  0]
 [ 0  0 11  0]]
0.5580867801306154
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.76
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.88
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.63      0.85      0.73        67
         2.0       0.69      0.80      0.74        60
         3.0       0.00      0.00      0.00        11

    accuracy                           0.66       160
   macro avg       0.33      0.41      0.37       160
weighted avg       0.52      0.66      0.58       160

[[ 0 21  1  0]
 [ 0 57 10  0]
 [ 0 12 48  0]
 [ 0  0 11  0]]
0.580983586477217
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.68
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.86
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        22
         1.0       0.65      0.81      0.72        67
         2.0       0.68      0.87      0.76        60
         3.0       0.00      0.00      0.00        11

    accuracy                           0.66       160
   macro avg       0.33      0.42      0.37       160
weighted avg       0.53      0.66      0.59       160

[[ 0 21  1  0]
 [ 0 54 13  0]
 [ 0  8 52  0]
 [ 0  0 11  0]]
0.5861715328467154
160 160 160
Filename	True Label	Prediction
1325_1001015	2.0	2.0
1325_1001016	2.0	2.0
1325_1001020	2.0	2.0
1325_1001028	2.0	2.0
1325_1001047	1.0	2.0
1325_1001062	2.0	2.0
1325_1001063	2.0	2.0
1325_1001086	2.0	2.0
1325_1001087	2.0	2.0
1325_1001097	0.0	2.0
1325_1001100	2.0	2.0
1325_1001109	2.0	2.0
1325_1001110	2.0	2.0
1325_1001120	2.0	2.0
1325_1001122	2.0	2.0
1325_1001123	2.0	2.0
1325_1001129	1.0	2.0
1325_1001133	2.0	2.0
1325_1001135	2.0	2.0
1325_1001141	2.0	2.0
1325_1001142	2.0	2.0
1325_9000095	2.0	2.0
1325_9000105	2.0	2.0
1325_9000144	3.0	2.0
1325_9000186	3.0	2.0
1325_9000213	3.0	2.0
1325_9000214	3.0	2.0
1325_9000304	2.0	2.0
1325_9000319	2.0	2.0
1325_9000554	2.0	2.0
1325_9000611	2.0	2.0
1325_9000674	3.0	2.0
1325_9000684	3.0	2.0
1365_0100004	2.0	2.0
1365_0100005	1.0	1.0
1365_0100010	1.0	1.0
1365_0100014	2.0	1.0
1365_0100018	2.0	2.0
1365_0100020	2.0	2.0
1365_0100022	2.0	2.0
1365_0100023	1.0	2.0
1365_0100024	1.0	2.0
1365_0100058	2.0	2.0
1365_0100064	2.0	2.0
1365_0100069	1.0	2.0
1365_0100070	2.0	2.0
1365_0100100	2.0	2.0
1365_0100103	2.0	2.0
1365_0100106	2.0	2.0
1365_0100107	2.0	2.0
1365_0100117	2.0	2.0
1365_0100139	1.0	2.0
1365_0100145	2.0	2.0
1365_0100146	2.0	1.0
1365_0100165	2.0	2.0
1365_0100171	1.0	1.0
1365_0100173	1.0	1.0
1365_0100175	1.0	1.0
1365_0100192	3.0	2.0
1365_0100196	1.0	2.0
1365_0100201	1.0	2.0
1365_0100202	1.0	1.0
1365_0100205	2.0	1.0
1365_0100213	1.0	1.0
1365_0100217	3.0	2.0
1365_0100221	3.0	2.0
1365_0100224	2.0	2.0
1365_0100233	2.0	2.0
1365_0100256	2.0	2.0
1365_0100258	2.0	2.0
1365_0100263	3.0	2.0
1365_0100274	2.0	2.0
1365_0100277	3.0	2.0
1365_0100278	2.0	2.0
1365_0100285	2.0	2.0
1365_0100287	1.0	1.0
1365_0100299	2.0	2.0
1365_0100448	1.0	2.0
1365_0100461	2.0	2.0
1365_0100470	1.0	2.0
1365_0100477	1.0	2.0
1365_0100479	2.0	2.0
1365_0100480	1.0	2.0
1365_0100482	2.0	2.0
1385_0000012	1.0	1.0
1385_0000017	1.0	1.0
1385_0000023	1.0	1.0
1385_0000036	1.0	1.0
1385_0000047	1.0	1.0
1385_0000050	1.0	1.0
1385_0000099	1.0	1.0
1385_0000104	1.0	1.0
1385_0000114	1.0	1.0
1385_0000127	1.0	1.0
1385_0001111	1.0	1.0
1385_0001121	1.0	1.0
1385_0001122	1.0	1.0
1385_0001123	1.0	1.0
1385_0001126	0.0	1.0
1385_0001131	1.0	1.0
1385_0001162	1.0	1.0
1385_0001163	1.0	1.0
1385_0001172	0.0	1.0
1385_0001173	0.0	1.0
1385_0001193	1.0	1.0
1385_0001199	1.0	1.0
1385_0001527	1.0	1.0
1385_0001718	0.0	1.0
1385_0001724	2.0	1.0
1385_0001729	1.0	1.0
1385_0001733	0.0	1.0
1385_0001736	1.0	1.0
1385_0001738	0.0	1.0
1385_0001739	1.0	1.0
1385_0001752	1.0	1.0
1385_0001753	1.0	1.0
1385_0001754	0.0	1.0
1385_0001759	1.0	1.0
1385_0001765	0.0	1.0
1385_0001767	0.0	1.0
1385_0001790	0.0	1.0
1385_0001799	1.0	1.0
1395_0000341	1.0	1.0
1395_0000354	1.0	1.0
1395_0000357	2.0	2.0
1395_0000388	2.0	1.0
1395_0000389	0.0	1.0
1395_0000396	1.0	1.0
1395_0000398	2.0	1.0
1395_0000399	1.0	1.0
1395_0000403	1.0	1.0
1395_0000438	2.0	2.0
1395_0000446	2.0	2.0
1395_0000451	1.0	1.0
1395_0000452	1.0	1.0
1395_0000462	1.0	1.0
1395_0000512	1.0	1.0
1395_0000528	1.0	1.0
1395_0000534	1.0	2.0
1395_0000547	1.0	1.0
1395_0000557	2.0	1.0
1395_0000585	0.0	1.0
1395_0000602	1.0	1.0
1395_0000608	0.0	1.0
1395_0000609	1.0	1.0
1395_0000610	1.0	1.0
1395_0000636	0.0	1.0
1395_0000642	0.0	1.0
1395_0001019	0.0	1.0
1395_0001021	1.0	1.0
1395_0001058	1.0	1.0
1395_0001066	0.0	1.0
1395_0001075	0.0	1.0
1395_0001093	0.0	1.0
1395_0001101	1.0	1.0
1395_0001109	0.0	1.0
1395_0001116	1.0	1.0
1395_0001117	0.0	1.0
1395_0001126	1.0	1.0
1395_0001158	2.0	1.0
Averaged weighted F1-scores 0.628728530588968
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2']
LANGUAGE: IT, 0th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.93
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.77
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.67      0.81      0.73        67
         2.0       0.75      0.79      0.77        75
         3.0       0.00      0.00      0.00         7

    accuracy                           0.71       160
   macro avg       0.35      0.40      0.37       160
weighted avg       0.63      0.71      0.66       160

[[ 0 11  0  0]
 [ 0 54 13  0]
 [ 0 16 59  0]
 [ 0  0  7  0]]
0.6647464022464022
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.78
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.76
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.72      0.76      0.74        67
         2.0       0.74      0.88      0.80        75
         3.0       0.00      0.00      0.00         7

    accuracy                           0.73       160
   macro avg       0.36      0.41      0.39       160
weighted avg       0.65      0.73      0.69       160

[[ 0 11  0  0]
 [ 0 51 16  0]
 [ 0  9 66  0]
 [ 0  0  7  0]]
0.686797454931071
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.69
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.69      0.70      0.70        67
         2.0       0.71      0.87      0.78        75
         3.0       0.00      0.00      0.00         7

    accuracy                           0.70       160
   macro avg       0.35      0.39      0.37       160
weighted avg       0.62      0.70      0.66       160

[[ 0 11  0  0]
 [ 0 47 20  0]
 [ 0 10 65  0]
 [ 0  0  7  0]]
0.6564692836549124
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.61
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.68      0.70      0.69        67
         2.0       0.70      0.85      0.77        75
         3.0       0.00      0.00      0.00         7

    accuracy                           0.69       160
   macro avg       0.35      0.39      0.37       160
weighted avg       0.61      0.69      0.65       160

[[ 0 11  0  0]
 [ 0 47 20  0]
 [ 0 11 64  0]
 [ 0  0  7  0]]
0.6508759301913537
160 160 160
Filename	True Label	Prediction
1325_1001015	2.0	2.0
1325_1001016	1.0	2.0
1325_1001021	2.0	2.0
1325_1001022	2.0	2.0
1325_1001033	2.0	2.0
1325_1001045	2.0	2.0
1325_1001048	2.0	2.0
1325_1001052	2.0	2.0
1325_1001058	2.0	2.0
1325_1001079	2.0	2.0
1325_1001082	2.0	2.0
1325_1001083	2.0	2.0
1325_1001085	2.0	2.0
1325_1001090	2.0	2.0
1325_1001091	2.0	2.0
1325_1001094	2.0	2.0
1325_1001099	3.0	2.0
1325_1001101	3.0	2.0
1325_1001110	2.0	2.0
1325_1001122	2.0	2.0
1325_1001124	1.0	2.0
1325_1001138	2.0	2.0
1325_1001139	2.0	2.0
1325_1001142	1.0	2.0
1325_1001158	2.0	2.0
1325_1001159	2.0	2.0
1325_9000102	2.0	2.0
1325_9000136	2.0	2.0
1325_9000138	2.0	2.0
1325_9000187	2.0	2.0
1325_9000239	3.0	2.0
1325_9000240	2.0	2.0
1325_9000296	2.0	2.0
1325_9000316	2.0	2.0
1325_9000533	2.0	2.0
1325_9000611	2.0	2.0
1325_9000612	1.0	2.0
1325_9000678	3.0	2.0
1365_0100005	2.0	1.0
1365_0100008	1.0	1.0
1365_0100009	2.0	1.0
1365_0100012	2.0	2.0
1365_0100019	1.0	2.0
1365_0100024	1.0	2.0
1365_0100026	2.0	1.0
1365_0100030	1.0	2.0
1365_0100070	2.0	2.0
1365_0100072	2.0	2.0
1365_0100079	1.0	2.0
1365_0100093	2.0	2.0
1365_0100097	2.0	1.0
1365_0100099	2.0	2.0
1365_0100100	2.0	2.0
1365_0100105	3.0	2.0
1365_0100120	2.0	2.0
1365_0100166	2.0	1.0
1365_0100174	1.0	2.0
1365_0100175	2.0	2.0
1365_0100178	2.0	2.0
1365_0100179	2.0	2.0
1365_0100187	2.0	2.0
1365_0100194	2.0	2.0
1365_0100198	1.0	2.0
1365_0100201	2.0	2.0
1365_0100211	2.0	2.0
1365_0100212	3.0	2.0
1365_0100215	2.0	2.0
1365_0100227	2.0	2.0
1365_0100228	2.0	2.0
1365_0100233	2.0	2.0
1365_0100256	2.0	2.0
1365_0100261	2.0	2.0
1365_0100263	3.0	2.0
1365_0100269	2.0	2.0
1365_0100276	2.0	2.0
1365_0100285	2.0	2.0
1365_0100448	2.0	2.0
1365_0100451	2.0	2.0
1365_0100459	2.0	2.0
1365_0100478	2.0	2.0
1385_0000012	1.0	1.0
1385_0000021	2.0	1.0
1385_0000034	1.0	1.0
1385_0000035	1.0	1.0
1385_0000037	0.0	1.0
1385_0000041	2.0	2.0
1385_0000042	2.0	2.0
1385_0000045	2.0	2.0
1385_0000057	2.0	2.0
1385_0000100	1.0	1.0
1385_0000120	0.0	1.0
1385_0000123	1.0	2.0
1385_0000124	2.0	2.0
1385_0000125	1.0	1.0
1385_0000127	1.0	1.0
1385_0000129	2.0	1.0
1385_0000130	2.0	1.0
1385_0001104	1.0	1.0
1385_0001121	1.0	1.0
1385_0001124	1.0	1.0
1385_0001129	1.0	1.0
1385_0001130	1.0	1.0
1385_0001133	1.0	1.0
1385_0001160	1.0	2.0
1385_0001161	2.0	2.0
1385_0001170	0.0	1.0
1385_0001171	1.0	1.0
1385_0001172	1.0	1.0
1385_0001174	1.0	1.0
1385_0001199	0.0	1.0
1385_0001503	1.0	2.0
1385_0001524	1.0	1.0
1385_0001714	0.0	1.0
1385_0001728	1.0	1.0
1385_0001734	1.0	2.0
1385_0001742	0.0	1.0
1385_0001744	0.0	1.0
1385_0001751	1.0	1.0
1385_0001756	1.0	1.0
1385_0001761	1.0	1.0
1385_0001773	1.0	1.0
1385_0001794	1.0	1.0
1385_0001799	1.0	2.0
1395_0000357	2.0	1.0
1395_0000359	1.0	2.0
1395_0000378	1.0	1.0
1395_0000403	1.0	1.0
1395_0000443	1.0	1.0
1395_0000449	1.0	1.0
1395_0000452	1.0	1.0
1395_0000458	1.0	1.0
1395_0000512	2.0	1.0
1395_0000527	1.0	1.0
1395_0000535	1.0	1.0
1395_0000550	1.0	1.0
1395_0000552	2.0	2.0
1395_0000553	1.0	2.0
1395_0000559	1.0	1.0
1395_0000560	1.0	1.0
1395_0000565	1.0	1.0
1395_0000583	1.0	1.0
1395_0000587	0.0	1.0
1395_0000609	0.0	1.0
1395_0000628	1.0	1.0
1395_0000631	1.0	2.0
1395_0000636	1.0	1.0
1395_0000639	0.0	1.0
1395_0000646	1.0	1.0
1395_0000649	1.0	1.0
1395_0001016	1.0	1.0
1395_0001019	1.0	1.0
1395_0001040	1.0	1.0
1395_0001045	2.0	1.0
1395_0001075	1.0	1.0
1395_0001101	1.0	1.0
1395_0001104	1.0	1.0
1395_0001115	1.0	2.0
1395_0001119	1.0	2.0
1395_0001131	1.0	1.0
1395_0001147	0.0	1.0
LANGUAGE: IT, 1th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.94
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.83
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.71      0.73      0.72        67
         2.0       0.73      0.88      0.80        75
         3.0       0.00      0.00      0.00         7

    accuracy                           0.72       160
   macro avg       0.36      0.40      0.38       160
weighted avg       0.64      0.72      0.67       160

[[ 0 11  0  0]
 [ 0 49 18  0]
 [ 0  9 66  0]
 [ 0  0  7  0]]
0.6744872873848334
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.78
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.72
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.71      0.82      0.76        67
         2.0       0.77      0.84      0.80        75
         3.0       0.00      0.00      0.00         7

    accuracy                           0.74       160
   macro avg       0.37      0.42      0.39       160
weighted avg       0.66      0.74      0.69       160

[[ 0 11  0  0]
 [ 0 55 12  0]
 [ 0 12 63  0]
 [ 0  0  7  0]]
0.6938666813090271
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.70
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.72
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.66      0.85      0.75        67
         2.0       0.77      0.76      0.77        75
         3.0       0.00      0.00      0.00         7

    accuracy                           0.71       160
   macro avg       0.36      0.40      0.38       160
weighted avg       0.64      0.71      0.67       160

[[ 0 11  0  0]
 [ 0 57 10  0]
 [ 0 18 57  0]
 [ 0  0  7  0]]
0.6706507435188841
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.60
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.73
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.69      0.82      0.75        67
         2.0       0.76      0.81      0.79        75
         3.0       0.00      0.00      0.00         7

    accuracy                           0.73       160
   macro avg       0.36      0.41      0.38       160
weighted avg       0.65      0.72      0.68       160

[[ 0 11  0  0]
 [ 0 55 12  0]
 [ 0 14 61  0]
 [ 0  0  7  0]]
0.6823019530392802
160 160 160
Filename	True Label	Prediction
1325_1001012	2.0	2.0
1325_1001017	1.0	2.0
1325_1001018	2.0	2.0
1325_1001020	2.0	2.0
1325_1001027	2.0	2.0
1325_1001036	2.0	2.0
1325_1001037	2.0	2.0
1325_1001043	2.0	2.0
1325_1001047	2.0	2.0
1325_1001050	2.0	2.0
1325_1001055	2.0	2.0
1325_1001076	2.0	2.0
1325_1001089	2.0	2.0
1325_1001097	1.0	2.0
1325_1001109	2.0	2.0
1325_1001113	3.0	2.0
1325_1001121	2.0	1.0
1325_1001125	2.0	2.0
1325_1001126	2.0	2.0
1325_1001133	2.0	2.0
1325_1001134	2.0	2.0
1325_1001136	2.0	2.0
1325_1001143	2.0	2.0
1325_1001152	2.0	2.0
1325_1001154	3.0	2.0
1325_1001155	2.0	2.0
1325_1001160	2.0	2.0
1325_9000059	3.0	2.0
1325_9000095	2.0	2.0
1325_9000137	2.0	2.0
1325_9000140	3.0	2.0
1325_9000185	3.0	2.0
1325_9000186	3.0	2.0
1325_9000237	2.0	2.0
1325_9000241	3.0	2.0
1325_9000279	2.0	2.0
1325_9000304	2.0	2.0
1325_9000317	2.0	2.0
1325_9000319	2.0	2.0
1325_9000601	2.0	2.0
1325_9000677	2.0	2.0
1365_0100004	1.0	2.0
1365_0100006	2.0	2.0
1365_0100016	1.0	2.0
1365_0100017	2.0	2.0
1365_0100023	1.0	1.0
1365_0100028	2.0	2.0
1365_0100029	2.0	1.0
1365_0100058	2.0	1.0
1365_0100069	2.0	2.0
1365_0100080	2.0	2.0
1365_0100096	2.0	2.0
1365_0100117	2.0	2.0
1365_0100125	2.0	2.0
1365_0100151	2.0	2.0
1365_0100171	1.0	2.0
1365_0100188	1.0	2.0
1365_0100191	1.0	2.0
1365_0100200	2.0	2.0
1365_0100203	2.0	2.0
1365_0100222	2.0	2.0
1365_0100230	2.0	2.0
1365_0100253	1.0	2.0
1365_0100255	1.0	2.0
1365_0100258	2.0	2.0
1365_0100267	2.0	2.0
1365_0100275	2.0	2.0
1365_0100278	2.0	2.0
1365_0100281	2.0	2.0
1365_0100290	2.0	2.0
1365_0100458	2.0	2.0
1365_0100461	2.0	2.0
1365_0100470	2.0	2.0
1365_0100474	2.0	2.0
1365_0100476	2.0	2.0
1365_0100482	2.0	2.0
1385_0000013	0.0	1.0
1385_0000017	1.0	1.0
1385_0000020	2.0	2.0
1385_0000036	1.0	1.0
1385_0000049	2.0	2.0
1385_0000053	1.0	2.0
1385_0000054	2.0	1.0
1385_0000097	1.0	1.0
1385_0000098	1.0	1.0
1385_0000101	1.0	1.0
1385_0000114	2.0	2.0
1385_0001107	2.0	1.0
1385_0001111	2.0	1.0
1385_0001120	1.0	1.0
1385_0001123	1.0	1.0
1385_0001125	1.0	1.0
1385_0001126	1.0	1.0
1385_0001127	1.0	1.0
1385_0001137	1.0	1.0
1385_0001138	1.0	1.0
1385_0001148	1.0	1.0
1385_0001151	2.0	1.0
1385_0001153	2.0	1.0
1385_0001154	2.0	2.0
1385_0001156	2.0	2.0
1385_0001164	1.0	1.0
1385_0001166	1.0	1.0
1385_0001167	1.0	1.0
1385_0001189	0.0	1.0
1385_0001193	1.0	1.0
1385_0001195	2.0	1.0
1385_0001198	1.0	1.0
1385_0001522	1.0	1.0
1385_0001527	1.0	1.0
1385_0001715	0.0	1.0
1385_0001725	1.0	1.0
1385_0001736	1.0	1.0
1385_0001746	1.0	1.0
1385_0001747	0.0	1.0
1385_0001758	1.0	1.0
1385_0001759	1.0	1.0
1385_0001760	0.0	1.0
1385_0001765	0.0	1.0
1385_0001768	1.0	1.0
1385_0001771	1.0	1.0
1385_0001785	0.0	1.0
1385_0001789	1.0	2.0
1385_0001793	1.0	1.0
1385_0001800	1.0	1.0
1395_0000338	1.0	1.0
1395_0000340	1.0	1.0
1395_0000353	1.0	1.0
1395_0000354	0.0	1.0
1395_0000369	2.0	1.0
1395_0000376	2.0	1.0
1395_0000383	1.0	1.0
1395_0000415	1.0	1.0
1395_0000432	1.0	1.0
1395_0000451	2.0	1.0
1395_0000455	1.0	1.0
1395_0000469	1.0	1.0
1395_0000471	1.0	1.0
1395_0000513	1.0	2.0
1395_0000515	1.0	1.0
1395_0000526	1.0	1.0
1395_0000528	2.0	1.0
1395_0000547	1.0	1.0
1395_0000582	1.0	1.0
1395_0000591	0.0	1.0
1395_0000593	1.0	1.0
1395_0000595	0.0	1.0
1395_0000596	2.0	1.0
1395_0000598	1.0	1.0
1395_0000599	1.0	1.0
1395_0000630	1.0	1.0
1395_0001023	0.0	1.0
1395_0001034	1.0	1.0
1395_0001066	1.0	1.0
1395_0001108	1.0	1.0
1395_0001120	1.0	1.0
1395_0001126	1.0	1.0
1395_0001141	1.0	1.0
1395_0001150	1.0	1.0
1395_0001169	1.0	1.0
LANGUAGE: IT, 2th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.99
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.80
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.65      0.85      0.74        67
         2.0       0.75      0.73      0.74        74
         3.0       0.00      0.00      0.00         8

    accuracy                           0.69       160
   macro avg       0.35      0.40      0.37       160
weighted avg       0.62      0.69      0.65       160

[[ 0 11  0  0]
 [ 0 57 10  0]
 [ 0 20 54  0]
 [ 0  0  8  0]]
0.6501071586389747
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.77
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.82
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.70      0.79      0.74        67
         2.0       0.71      0.81      0.76        74
         3.0       0.00      0.00      0.00         8

    accuracy                           0.71       160
   macro avg       0.35      0.40      0.38       160
weighted avg       0.62      0.71      0.66       160

[[ 0  9  2  0]
 [ 0 53 14  0]
 [ 0 14 60  0]
 [ 0  0  8  0]]
0.661667920686908
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.71
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.85
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.72      0.69      0.70        67
         2.0       0.68      0.88      0.76        74
         3.0       0.00      0.00      0.00         8

    accuracy                           0.69       160
   macro avg       0.35      0.39      0.37       160
weighted avg       0.61      0.69      0.65       160

[[ 0  9  2  0]
 [ 0 46 21  0]
 [ 0  9 65  0]
 [ 0  0  8  0]]
0.6477604400538841
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.63
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.81
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.73      0.72      0.72        67
         2.0       0.69      0.88      0.77        74
         3.0       0.00      0.00      0.00         8

    accuracy                           0.71       160
   macro avg       0.35      0.40      0.37       160
weighted avg       0.62      0.71      0.66       160

[[ 0  9  2  0]
 [ 0 48 19  0]
 [ 0  9 65  0]
 [ 0  0  8  0]]
0.6601425438596491
160 160 160
Filename	True Label	Prediction
1325_1001009	2.0	2.0
1325_1001010	2.0	2.0
1325_1001011	3.0	2.0
1325_1001013	3.0	2.0
1325_1001029	2.0	2.0
1325_1001032	2.0	2.0
1325_1001051	1.0	2.0
1325_1001056	2.0	2.0
1325_1001062	2.0	2.0
1325_1001063	1.0	2.0
1325_1001077	2.0	2.0
1325_1001078	2.0	2.0
1325_1001080	2.0	2.0
1325_1001081	2.0	2.0
1325_1001084	2.0	2.0
1325_1001086	2.0	2.0
1325_1001092	2.0	2.0
1325_1001095	2.0	2.0
1325_1001096	2.0	2.0
1325_1001098	2.0	2.0
1325_1001100	2.0	2.0
1325_1001107	2.0	2.0
1325_1001120	2.0	2.0
1325_1001127	2.0	2.0
1325_1001162	2.0	2.0
1325_1001164	2.0	2.0
1325_1001166	2.0	2.0
1325_1001169	2.0	2.0
1325_9000106	2.0	2.0
1325_9000139	2.0	2.0
1325_9000152	2.0	2.0
1325_9000213	3.0	2.0
1325_9000314	2.0	2.0
1325_9000320	3.0	2.0
1325_9000321	3.0	2.0
1325_9000323	1.0	2.0
1325_9000503	3.0	2.0
1325_9000602	2.0	2.0
1325_9000675	2.0	2.0
1325_9000676	2.0	2.0
1325_9000685	3.0	2.0
1365_0100003	2.0	1.0
1365_0100013	2.0	2.0
1365_0100018	2.0	2.0
1365_0100063	2.0	2.0
1365_0100065	1.0	2.0
1365_0100066	1.0	2.0
1365_0100067	1.0	2.0
1365_0100073	2.0	2.0
1365_0100074	1.0	2.0
1365_0100106	1.0	2.0
1365_0100119	3.0	2.0
1365_0100133	2.0	2.0
1365_0100134	2.0	2.0
1365_0100136	2.0	2.0
1365_0100137	2.0	2.0
1365_0100145	2.0	2.0
1365_0100147	2.0	2.0
1365_0100148	1.0	2.0
1365_0100162	2.0	2.0
1365_0100163	2.0	2.0
1365_0100173	1.0	2.0
1365_0100176	2.0	2.0
1365_0100181	1.0	1.0
1365_0100199	2.0	2.0
1365_0100205	2.0	2.0
1365_0100219	2.0	2.0
1365_0100220	2.0	2.0
1365_0100223	2.0	2.0
1365_0100225	2.0	2.0
1365_0100251	2.0	2.0
1365_0100262	2.0	2.0
1365_0100266	1.0	2.0
1365_0100268	2.0	2.0
1365_0100270	2.0	2.0
1365_0100274	2.0	2.0
1365_0100280	1.0	2.0
1365_0100289	2.0	2.0
1365_0100299	1.0	2.0
1365_0100447	2.0	2.0
1365_0100456	2.0	2.0
1365_0100471	2.0	2.0
1365_0100477	2.0	2.0
1365_0100480	2.0	2.0
1385_0000011	1.0	1.0
1385_0000016	1.0	2.0
1385_0000033	1.0	1.0
1385_0000043	2.0	2.0
1385_0000052	2.0	1.0
1385_0000102	1.0	1.0
1385_0000104	1.0	1.0
1385_0000126	1.0	1.0
1385_0000128	1.0	1.0
1385_0001103	1.0	1.0
1385_0001112	1.0	1.0
1385_0001132	1.0	1.0
1385_0001135	1.0	1.0
1385_0001150	2.0	2.0
1385_0001152	2.0	2.0
1385_0001162	1.0	1.0
1385_0001175	1.0	1.0
1385_0001188	1.0	1.0
1385_0001191	1.0	1.0
1385_0001192	0.0	1.0
1385_0001197	1.0	1.0
1385_0001525	1.0	1.0
1385_0001712	1.0	2.0
1385_0001727	0.0	1.0
1385_0001732	1.0	2.0
1385_0001741	0.0	1.0
1385_0001748	1.0	2.0
1385_0001752	0.0	1.0
1385_0001754	0.0	1.0
1385_0001762	1.0	1.0
1385_0001772	0.0	1.0
1385_0001786	1.0	1.0
1385_0001787	0.0	2.0
1385_0001790	1.0	2.0
1385_0001796	0.0	2.0
1385_0001798	1.0	2.0
1395_0000337	1.0	1.0
1395_0000356	1.0	1.0
1395_0000361	1.0	1.0
1395_0000379	1.0	1.0
1395_0000390	1.0	1.0
1395_0000409	2.0	1.0
1395_0000413	1.0	1.0
1395_0000414	1.0	1.0
1395_0000447	1.0	1.0
1395_0000448	1.0	1.0
1395_0000465	1.0	1.0
1395_0000470	1.0	1.0
1395_0000516	1.0	1.0
1395_0000533	2.0	2.0
1395_0000537	1.0	1.0
1395_0000548	1.0	1.0
1395_0000551	2.0	1.0
1395_0000554	2.0	1.0
1395_0000556	1.0	1.0
1395_0000563	2.0	1.0
1395_0000575	1.0	1.0
1395_0000602	1.0	1.0
1395_0000604	0.0	1.0
1395_0000608	1.0	1.0
1395_0000610	2.0	1.0
1395_0000611	1.0	1.0
1395_0000612	1.0	1.0
1395_0001010	2.0	1.0
1395_0001017	1.0	1.0
1395_0001021	1.0	1.0
1395_0001022	1.0	1.0
1395_0001024	0.0	1.0
1395_0001065	1.0	1.0
1395_0001069	1.0	1.0
1395_0001078	1.0	1.0
1395_0001084	1.0	1.0
1395_0001116	2.0	1.0
1395_0001124	0.0	1.0
1395_0001145	1.0	1.0
1395_0001167	1.0	1.0
LANGUAGE: IT, 3th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 1.00
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.88
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.57      0.85      0.68        68
         2.0       0.72      0.57      0.64        74
         3.0       0.00      0.00      0.00         7

    accuracy                           0.62       160
   macro avg       0.32      0.36      0.33       160
weighted avg       0.58      0.62      0.58       160

[[ 0 11  0  0]
 [ 0 58 10  0]
 [ 0 32 42  0]
 [ 0  1  6  0]]
0.5843181818181818
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.81
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.82
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.70      0.75      0.72        68
         2.0       0.72      0.85      0.78        74
         3.0       0.00      0.00      0.00         7

    accuracy                           0.71       160
   macro avg       0.36      0.40      0.38       160
weighted avg       0.63      0.71      0.67       160

[[ 0 10  1  0]
 [ 0 51 17  0]
 [ 0 11 63  0]
 [ 0  1  6  0]]
0.6694033302497687
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.75
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.85
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.77      0.72      0.74        68
         2.0       0.69      0.89      0.78        74
         3.0       0.00      0.00      0.00         7

    accuracy                           0.72       160
   macro avg       0.36      0.40      0.38       160
weighted avg       0.64      0.72      0.67       160

[[ 0  7  4  0]
 [ 0 49 19  0]
 [ 0  8 66  0]
 [ 0  0  7  0]]
0.6746479500891265
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.69
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.83
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        11
         1.0       0.70      0.75      0.72        68
         2.0       0.70      0.82      0.76        74
         3.0       0.00      0.00      0.00         7

    accuracy                           0.70       160
   macro avg       0.35      0.39      0.37       160
weighted avg       0.62      0.70      0.66       160

[[ 0  9  2  0]
 [ 0 51 17  0]
 [ 0 13 61  0]
 [ 0  0  7  0]]
0.6579126470199551
160 160 160
Filename	True Label	Prediction
1325_1001008	2.0	2.0
1325_1001019	2.0	2.0
1325_1001023	2.0	2.0
1325_1001040	2.0	2.0
1325_1001041	3.0	2.0
1325_1001046	1.0	2.0
1325_1001053	2.0	2.0
1325_1001057	2.0	2.0
1325_1001059	2.0	2.0
1325_1001087	2.0	2.0
1325_1001088	2.0	2.0
1325_1001108	2.0	2.0
1325_1001123	2.0	2.0
1325_1001128	2.0	2.0
1325_1001130	2.0	2.0
1325_1001132	2.0	2.0
1325_1001135	2.0	2.0
1325_1001156	2.0	2.0
1325_1001161	2.0	2.0
1325_1001165	1.0	2.0
1325_1001168	1.0	2.0
1325_9000087	2.0	2.0
1325_9000099	2.0	2.0
1325_9000143	3.0	2.0
1325_9000144	3.0	2.0
1325_9000210	1.0	2.0
1325_9000215	2.0	2.0
1325_9000278	3.0	2.0
1325_9000318	2.0	2.0
1325_9000322	2.0	2.0
1325_9000505	2.0	2.0
1325_9000536	3.0	2.0
1325_9000674	3.0	2.0
1325_9000684	2.0	2.0
1325_9000686	2.0	2.0
1325_9000750	3.0	2.0
1365_0100002	1.0	2.0
1365_0100014	2.0	2.0
1365_0100015	2.0	1.0
1365_0100022	1.0	2.0
1365_0100056	1.0	2.0
1365_0100057	1.0	2.0
1365_0100064	2.0	2.0
1365_0100071	2.0	2.0
1365_0100092	2.0	2.0
1365_0100098	2.0	2.0
1365_0100101	2.0	2.0
1365_0100102	2.0	2.0
1365_0100104	1.0	2.0
1365_0100116	2.0	2.0
1365_0100118	2.0	2.0
1365_0100121	2.0	2.0
1365_0100138	2.0	2.0
1365_0100139	2.0	2.0
1365_0100146	2.0	2.0
1365_0100164	1.0	2.0
1365_0100165	2.0	2.0
1365_0100168	2.0	2.0
1365_0100177	2.0	2.0
1365_0100182	2.0	2.0
1365_0100184	2.0	2.0
1365_0100195	1.0	1.0
1365_0100196	1.0	2.0
1365_0100217	2.0	2.0
1365_0100221	2.0	2.0
1365_0100257	2.0	2.0
1365_0100259	2.0	2.0
1365_0100260	1.0	2.0
1365_0100279	2.0	2.0
1365_0100286	1.0	2.0
1365_0100288	2.0	2.0
1365_0100457	2.0	2.0
1365_0100469	2.0	2.0
1365_0100472	2.0	2.0
1365_0100475	2.0	2.0
1365_0100481	2.0	2.0
1385_0000038	2.0	2.0
1385_0000039	1.0	1.0
1385_0000044	2.0	2.0
1385_0000047	2.0	1.0
1385_0000099	1.0	1.0
1385_0000122	1.0	2.0
1385_0001105	1.0	1.0
1385_0001109	1.0	1.0
1385_0001110	1.0	1.0
1385_0001113	1.0	1.0
1385_0001118	1.0	1.0
1385_0001128	1.0	1.0
1385_0001136	1.0	1.0
1385_0001147	1.0	1.0
1385_0001149	2.0	2.0
1385_0001155	2.0	2.0
1385_0001159	2.0	1.0
1385_0001163	1.0	1.0
1385_0001178	0.0	1.0
1385_0001190	0.0	1.0
1385_0001194	1.0	1.0
1385_0001196	1.0	1.0
1385_0001501	0.0	2.0
1385_0001528	1.0	1.0
1385_0001718	1.0	1.0
1385_0001723	0.0	1.0
1385_0001724	1.0	1.0
1385_0001730	1.0	1.0
1385_0001733	1.0	2.0
1385_0001737	2.0	2.0
1385_0001738	0.0	1.0
1385_0001739	0.0	1.0
1385_0001740	1.0	1.0
1385_0001753	1.0	1.0
1385_0001757	1.0	1.0
1385_0001766	2.0	1.0
1385_0001767	1.0	1.0
1385_0001774	0.0	1.0
1385_0001775	1.0	1.0
1385_0001788	0.0	2.0
1385_0001795	1.0	1.0
1395_0000364	1.0	1.0
1395_0000388	1.0	1.0
1395_0000391	2.0	2.0
1395_0000396	1.0	1.0
1395_0000399	1.0	1.0
1395_0000402	1.0	1.0
1395_0000438	2.0	2.0
1395_0000446	2.0	1.0
1395_0000454	2.0	1.0
1395_0000460	1.0	1.0
1395_0000462	2.0	1.0
1395_0000499	1.0	1.0
1395_0000500	1.0	1.0
1395_0000504	1.0	1.0
1395_0000514	2.0	1.0
1395_0000529	2.0	1.0
1395_0000531	2.0	1.0
1395_0000549	1.0	2.0
1395_0000555	2.0	1.0
1395_0000557	2.0	1.0
1395_0000585	1.0	1.0
1395_0000607	0.0	1.0
1395_0000626	0.0	1.0
1395_0000627	1.0	1.0
1395_0000642	1.0	1.0
1395_0001020	1.0	1.0
1395_0001060	1.0	1.0
1395_0001061	1.0	2.0
1395_0001064	1.0	1.0
1395_0001073	1.0	1.0
1395_0001074	1.0	1.0
1395_0001076	1.0	1.0
1395_0001080	2.0	1.0
1395_0001090	1.0	1.0
1395_0001093	1.0	1.0
1395_0001109	1.0	1.0
1395_0001118	1.0	1.0
1395_0001121	0.0	1.0
1395_0001122	1.0	1.0
1395_0001132	1.0	1.0
1395_0001133	1.0	1.0
1395_0001158	1.0	1.0
1395_0001170	1.0	1.0
LANGUAGE: IT, 4th Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.96
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.86
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        12
         1.0       0.64      0.73      0.68        67
         2.0       0.70      0.78      0.74        74
         3.0       0.00      0.00      0.00         7

    accuracy                           0.67       160
   macro avg       0.33      0.38      0.35       160
weighted avg       0.59      0.67      0.63       160

[[ 0 12  0  0]
 [ 0 49 18  0]
 [ 0 16 58  0]
 [ 0  0  7  0]]
0.6267023841118189
160 160 160



======== Epoch 2 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.79
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.80
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        12
         1.0       0.64      0.70      0.67        67
         2.0       0.69      0.81      0.75        74
         3.0       0.00      0.00      0.00         7

    accuracy                           0.67       160
   macro avg       0.33      0.38      0.35       160
weighted avg       0.59      0.67      0.63       160

[[ 0 12  0  0]
 [ 0 47 20  0]
 [ 0 14 60  0]
 [ 0  0  7  0]]
0.6258812111801243
160 160 160



======== Epoch 3 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.73
  Training epoch took: 24
Running Validation...
  Average evaluation loss: 0.76
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        12
         1.0       0.70      0.72      0.71        67
         2.0       0.71      0.88      0.79        74
         3.0       0.00      0.00      0.00         7

    accuracy                           0.71       160
   macro avg       0.35      0.40      0.37       160
weighted avg       0.62      0.71      0.66       160

[[ 0 12  0  0]
 [ 0 48 19  0]
 [ 0  9 65  0]
 [ 0  0  7  0]]
0.6599821746880571
160 160 160



======== Epoch 4 / 4 ========
Training...
Elapsed time 12

  Average training loss: 0.65
  Training epoch took: 23
Running Validation...
  Average evaluation loss: 0.76
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        12
         1.0       0.73      0.76      0.74        67
         2.0       0.74      0.91      0.82        74
         3.0       0.00      0.00      0.00         7

    accuracy                           0.74       160
   macro avg       0.37      0.42      0.39       160
weighted avg       0.65      0.74      0.69       160

[[ 0 12  0  0]
 [ 0 51 16  0]
 [ 0  7 67  0]
 [ 0  0  7  0]]
0.6896664144561154
160 160 160
Filename	True Label	Prediction
1325_1001014	2.0	2.0
1325_1001024	2.0	2.0
1325_1001025	2.0	2.0
1325_1001028	1.0	2.0
1325_1001035	3.0	2.0
1325_1001039	2.0	2.0
1325_1001042	2.0	2.0
1325_1001044	2.0	2.0
1325_1001054	3.0	2.0
1325_1001075	2.0	2.0
1325_1001093	2.0	2.0
1325_1001111	3.0	2.0
1325_1001119	2.0	2.0
1325_1001129	1.0	2.0
1325_1001131	2.0	2.0
1325_1001141	1.0	2.0
1325_1001144	2.0	2.0
1325_1001153	2.0	2.0
1325_1001157	2.0	2.0
1325_1001163	1.0	2.0
1325_1001167	2.0	2.0
1325_1001170	2.0	2.0
1325_9000088	2.0	2.0
1325_9000089	2.0	2.0
1325_9000090	2.0	2.0
1325_9000104	2.0	2.0
1325_9000105	2.0	2.0
1325_9000107	3.0	2.0
1325_9000188	2.0	2.0
1325_9000209	2.0	2.0
1325_9000211	2.0	2.0
1325_9000214	2.0	2.0
1325_9000302	2.0	2.0
1325_9000303	2.0	2.0
1325_9000315	1.0	2.0
1325_9000504	2.0	2.0
1325_9000534	1.0	2.0
1325_9000554	2.0	2.0
1325_9000700	2.0	2.0
1365_0100007	1.0	1.0
1365_0100010	1.0	2.0
1365_0100011	2.0	1.0
1365_0100020	2.0	2.0
1365_0100021	2.0	2.0
1365_0100027	2.0	2.0
1365_0100031	1.0	2.0
1365_0100051	1.0	2.0
1365_0100061	3.0	2.0
1365_0100094	2.0	2.0
1365_0100095	2.0	2.0
1365_0100103	2.0	2.0
1365_0100107	2.0	2.0
1365_0100123	2.0	2.0
1365_0100135	1.0	2.0
1365_0100167	1.0	2.0
1365_0100169	2.0	2.0
1365_0100170	1.0	2.0
1365_0100172	2.0	2.0
1365_0100180	1.0	2.0
1365_0100183	2.0	2.0
1365_0100185	2.0	2.0
1365_0100186	2.0	2.0
1365_0100190	2.0	2.0
1365_0100192	3.0	2.0
1365_0100202	2.0	2.0
1365_0100204	1.0	2.0
1365_0100213	2.0	2.0
1365_0100218	2.0	2.0
1365_0100224	2.0	2.0
1365_0100226	2.0	2.0
1365_0100229	2.0	2.0
1365_0100231	2.0	2.0
1365_0100232	2.0	2.0
1365_0100252	2.0	2.0
1365_0100265	2.0	2.0
1365_0100277	2.0	2.0
1365_0100282	2.0	2.0
1365_0100287	2.0	2.0
1365_0100455	2.0	2.0
1365_0100473	2.0	2.0
1365_0100479	2.0	2.0
1385_0000022	0.0	1.0
1385_0000023	1.0	1.0
1385_0000040	1.0	1.0
1385_0000048	2.0	2.0
1385_0000050	2.0	2.0
1385_0000051	2.0	2.0
1385_0000058	2.0	2.0
1385_0000059	2.0	2.0
1385_0000095	1.0	1.0
1385_0000103	1.0	1.0
1385_0000119	2.0	2.0
1385_0001108	1.0	1.0
1385_0001119	1.0	1.0
1385_0001122	1.0	1.0
1385_0001131	1.0	1.0
1385_0001134	1.0	1.0
1385_0001157	2.0	1.0
1385_0001158	2.0	2.0
1385_0001165	1.0	1.0
1385_0001169	1.0	1.0
1385_0001173	0.0	1.0
1385_0001523	1.0	1.0
1385_0001526	0.0	1.0
1385_0001716	1.0	1.0
1385_0001717	1.0	1.0
1385_0001719	1.0	1.0
1385_0001720	0.0	1.0
1385_0001726	0.0	1.0
1385_0001729	1.0	1.0
1385_0001749	0.0	1.0
1385_0001750	0.0	1.0
1385_0001764	1.0	1.0
1385_0001791	1.0	1.0
1385_0001792	1.0	1.0
1395_0000333	1.0	1.0
1395_0000341	1.0	1.0
1395_0000355	1.0	1.0
1395_0000360	2.0	1.0
1395_0000365	2.0	1.0
1395_0000366	2.0	1.0
1395_0000368	0.0	1.0
1395_0000380	1.0	1.0
1395_0000387	3.0	2.0
1395_0000389	1.0	1.0
1395_0000392	1.0	1.0
1395_0000398	1.0	2.0
1395_0000404	1.0	1.0
1395_0000450	1.0	1.0
1395_0000518	2.0	1.0
1395_0000525	2.0	1.0
1395_0000534	1.0	1.0
1395_0000564	1.0	1.0
1395_0000572	1.0	1.0
1395_0000579	1.0	1.0
1395_0000581	1.0	1.0
1395_0000584	0.0	1.0
1395_0000597	1.0	1.0
1395_0000606	1.0	1.0
1395_0000635	1.0	1.0
1395_0000644	1.0	1.0
1395_0001013	1.0	1.0
1395_0001015	1.0	1.0
1395_0001028	1.0	1.0
1395_0001033	1.0	1.0
1395_0001058	1.0	1.0
1395_0001067	1.0	1.0
1395_0001068	0.0	1.0
1395_0001070	1.0	2.0
1395_0001071	1.0	1.0
1395_0001103	1.0	1.0
1395_0001114	1.0	1.0
1395_0001117	1.0	1.0
1395_0001123	0.0	1.0
1395_0001146	0.0	1.0
1395_0001149	1.0	1.0
1395_0001160	1.0	1.0
1395_0001161	1.0	1.0
1395_0001164	2.0	2.0
1395_0001171	1.0	1.0
Averaged weighted F1-scores 0.6681798977132708
MULTILINGUAL EXPERIMENTS
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
144.90552995391704 65.35717405024758
LANGUAGE: IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']
1 Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.84
  Training epoch took: 65
Running Validation...
  Average evaluation loss: 0.63
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.78      0.76      0.77       174
         2.0       0.72      0.79      0.75       177
         3.0       0.70      0.83      0.76        75
         4.0       0.00      0.00      0.00         9

    accuracy                           0.74       452
   macro avg       0.44      0.48      0.46       452
weighted avg       0.70      0.74      0.72       452

[[  0  16   1   0   0]
 [  0 133  41   0   0]
 [  0  21 139  17   0]
 [  0   0  13  62   0]
 [  0   0   0   9   0]]
0.7173284383067181
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.59
  Training epoch took: 65
Running Validation...
  Average evaluation loss: 0.67
              precision    recall  f1-score   support

         0.0       1.00      0.06      0.11        17
         1.0       0.79      0.79      0.79       174
         2.0       0.75      0.69      0.72       177
         3.0       0.63      0.97      0.76        75
         4.0       0.00      0.00      0.00         9

    accuracy                           0.74       452
   macro avg       0.63      0.50      0.48       452
weighted avg       0.74      0.74      0.72       452

[[  1  15   1   0   0]
 [  0 137  37   0   0]
 [  0  21 122  34   0]
 [  0   0   2  73   0]
 [  0   0   0   9   0]]
0.7168402092686657
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.45
  Training epoch took: 65
Running Validation...
  Average evaluation loss: 0.90
              precision    recall  f1-score   support

         0.0       0.50      0.06      0.11        17
         1.0       0.81      0.60      0.69       174
         2.0       0.66      0.78      0.71       177
         3.0       0.66      0.97      0.78        75
         4.0       0.00      0.00      0.00         9

    accuracy                           0.70       452
   macro avg       0.52      0.48      0.46       452
weighted avg       0.70      0.70      0.68       452

[[  1  15   1   0   0]
 [  1 104  69   0   0]
 [  0  10 138  29   0]
 [  0   0   2  73   0]
 [  0   0   0   9   0]]
0.6777401418587087
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.33
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.86
              precision    recall  f1-score   support

         0.0       0.67      0.12      0.20        17
         1.0       0.80      0.68      0.74       174
         2.0       0.69      0.79      0.74       177
         3.0       0.69      0.93      0.79        75
         4.0       0.00      0.00      0.00         9

    accuracy                           0.73       452
   macro avg       0.57      0.50      0.49       452
weighted avg       0.72      0.73      0.71       452

[[  2  14   1   0   0]
 [  1 118  55   0   0]
 [  0  15 139  23   0]
 [  0   0   5  70   0]
 [  0   0   0   9   0]]
0.7105468173503742
452 452 452
Filename	True Label	Prediction
1023_0001418	3.0	3.0
1023_0001419	3.0	3.0
1023_0001423	2.0	2.0
1023_0001575	3.0	3.0
1023_0101675	3.0	3.0
1023_0101684	2.0	3.0
1023_0101688	3.0	3.0
1023_0101694	3.0	3.0
1023_0101843	3.0	3.0
1023_0101844	2.0	3.0
1023_0101845	3.0	3.0
1023_0101856	2.0	3.0
1023_0101897	2.0	3.0
1023_0102118	3.0	3.0
1023_0103822	2.0	2.0
1023_0103826	3.0	3.0
1023_0103829	2.0	3.0
1023_0103833	4.0	3.0
1023_0103834	3.0	3.0
1023_0103836	3.0	3.0
1023_0103844	4.0	3.0
1023_0103883	3.0	3.0
1023_0107244	3.0	3.0
1023_0107726	3.0	3.0
1023_0108306	3.0	3.0
1023_0108648	3.0	3.0
1023_0108766	2.0	3.0
1023_0108886	3.0	3.0
1023_0108888	3.0	3.0
1023_0108993	3.0	3.0
1023_0109022	3.0	3.0
1023_0109395	2.0	3.0
1023_0109396	2.0	3.0
1023_0109402	2.0	3.0
1023_0109505	3.0	3.0
1023_0109518	2.0	2.0
1023_0109524	3.0	3.0
1023_0109528	3.0	3.0
1023_0109590	2.0	3.0
1023_0109606	3.0	2.0
1023_0109880	3.0	3.0
1023_0109945	3.0	3.0
1031_0001951	2.0	3.0
1031_0001997	3.0	3.0
1031_0002006	4.0	3.0
1031_0002036	4.0	3.0
1031_0002061	3.0	3.0
1031_0002086	3.0	3.0
1031_0002187	3.0	3.0
1031_0002196	3.0	3.0
1031_0002198	3.0	3.0
1031_0003053	3.0	3.0
1031_0003085	3.0	3.0
1031_0003095	3.0	3.0
1031_0003121	3.0	3.0
1031_0003132	3.0	3.0
1031_0003133	4.0	3.0
1031_0003140	3.0	3.0
1031_0003141	3.0	3.0
1031_0003146	4.0	3.0
1031_0003160	3.0	3.0
1031_0003166	2.0	3.0
1031_0003180	4.0	3.0
1031_0003191	3.0	3.0
1031_0003216	3.0	3.0
1031_0003217	4.0	3.0
1031_0003220	3.0	3.0
1031_0003224	3.0	3.0
1031_0003234	3.0	3.0
1031_0003236	3.0	3.0
1031_0003239	4.0	3.0
1031_0003240	2.0	3.0
1031_0003242	3.0	3.0
1031_0003249	3.0	3.0
1031_0003261	3.0	3.0
1031_0003331	3.0	3.0
1031_0003336	3.0	3.0
1031_0003355	3.0	3.0
1031_0003365	3.0	3.0
1031_0003368	3.0	3.0
1031_0003386	2.0	3.0
1031_0003389	3.0	3.0
1031_0003407	3.0	3.0
1061_0120282	0.0	1.0
1061_0120286	1.0	1.0
1061_0120295	0.0	2.0
1061_0120297	2.0	2.0
1061_0120298	2.0	2.0
1061_0120302	1.0	2.0
1061_0120311	3.0	2.0
1061_0120314	2.0	2.0
1061_0120318	2.0	2.0
1061_0120324	2.0	2.0
1061_0120328	1.0	2.0
1061_0120329	2.0	2.0
1061_0120371	3.0	2.0
1061_0120373	2.0	2.0
1061_0120375	2.0	1.0
1061_0120391	1.0	1.0
1061_0120403	3.0	2.0
1061_0120404	1.0	1.0
1061_0120430	2.0	2.0
1061_0120439	1.0	1.0
1061_0120460	2.0	2.0
1061_0120481	3.0	3.0
1061_0120482	2.0	2.0
1061_0120484	2.0	2.0
1061_0120491	2.0	2.0
1061_0120853	2.0	2.0
1061_0120855	1.0	2.0
1061_0120857	2.0	2.0
1061_0120881	2.0	3.0
1061_0120885	2.0	2.0
1061_1029119	1.0	2.0
1061_1202910	2.0	2.0
1061_1202911	1.0	2.0
1061_1202913	2.0	1.0
1071_0024680	2.0	2.0
1071_0024682	2.0	2.0
1071_0024686	2.0	2.0
1071_0024688	1.0	1.0
1071_0024690	1.0	2.0
1071_0024693	1.0	1.0
1071_0024694	1.0	2.0
1071_0024704	1.0	1.0
1071_0024705	1.0	2.0
1071_0024712	1.0	1.0
1071_0024714	2.0	1.0
1071_0024756	1.0	1.0
1071_0024765	0.0	1.0
1071_0024770	1.0	1.0
1071_0024779	1.0	1.0
1071_0024801	1.0	1.0
1071_0024813	0.0	1.0
1071_0024815	0.0	1.0
1071_0024825	1.0	1.0
1071_0024831	0.0	1.0
1071_0024840	1.0	1.0
1071_0024855	1.0	1.0
1071_0024856	1.0	1.0
1071_0024859	1.0	1.0
1071_0024874	1.0	1.0
1071_0024876	1.0	1.0
1071_0242022	0.0	1.0
1071_0242073	1.0	1.0
1071_0243581	1.0	1.0
1071_0248310	1.0	1.0
1071_0248317	0.0	0.0
1071_0248327	0.0	1.0
1071_0248329	1.0	1.0
1071_0248338	1.0	1.0
1071_0248347	1.0	1.0
1071_0248350	1.0	1.0
1091_0000001	1.0	2.0
1091_0000002	2.0	2.0
1091_0000009	0.0	1.0
1091_0000015	1.0	2.0
1091_0000016	0.0	1.0
1091_0000017	2.0	2.0
1091_0000019	1.0	1.0
1091_0000020	1.0	2.0
1091_0000021	2.0	2.0
1091_0000023	1.0	1.0
1091_0000027	2.0	1.0
1091_0000030	1.0	1.0
1091_0000035	1.0	1.0
1091_0000042	0.0	1.0
1091_0000043	1.0	2.0
1091_0000050	1.0	1.0
1091_0000054	1.0	1.0
1091_0000071	2.0	2.0
1091_0000116	2.0	2.0
1091_0000146	1.0	0.0
1091_0000151	1.0	1.0
1091_0000153	2.0	2.0
1091_0000155	2.0	2.0
1091_0000158	2.0	2.0
1091_0000159	2.0	2.0
1091_0000164	1.0	1.0
1091_0000165	1.0	2.0
1091_0000168	2.0	3.0
1091_0000172	2.0	1.0
1091_0000190	1.0	2.0
1091_0000195	1.0	1.0
1091_0000197	2.0	2.0
1091_0000204	1.0	2.0
1091_0000206	1.0	1.0
1091_0000207	2.0	2.0
1091_0000211	2.0	2.0
1091_0000213	1.0	2.0
1091_0000214	1.0	1.0
1091_0000215	2.0	2.0
1091_0000217	1.0	2.0
1091_0000218	1.0	1.0
1091_0000224	1.0	1.0
1091_0000233	1.0	2.0
1091_0000237	2.0	2.0
1091_0000248	1.0	1.0
1091_0000251	2.0	2.0
1091_0000252	2.0	2.0
1091_0000258	2.0	2.0
1091_0000262	1.0	2.0
1091_0000263	2.0	2.0
1091_0000264	1.0	2.0
1091_0000269	2.0	2.0
1091_0000272	1.0	2.0
1091_0000275	1.0	2.0
0611	2.0	2.0
0613	1.0	1.0
0623	1.0	2.0
0625	1.0	2.0
0633	2.0	2.0
0642	1.0	2.0
0644	1.0	2.0
0714	2.0	2.0
0715	2.0	2.0
0716	2.0	2.0
0719	2.0	2.0
0723	1.0	2.0
0802	1.0	1.0
0812	1.0	1.0
0813	1.0	1.0
0816	2.0	2.0
0825	1.0	2.0
0827	1.0	1.0
0924	1.0	1.0
0930	1.0	2.0
1003	1.0	1.0
1005	1.0	1.0
1014	2.0	2.0
1022	2.0	1.0
1023	1.0	2.0
BER0609003	2.0	2.0
LIB0611004A	1.0	1.0
MOS0509004	2.0	2.0
MOS0611015	2.0	3.0
PAR1011008A	1.0	1.0
PAR1011009A	1.0	1.0
PAR1011013	2.0	2.0
PAR1011014	2.0	3.0
PAR1011015	2.0	2.0
PAR1011016	3.0	3.0
PHA0111001A	1.0	1.0
PHA0111014	1.0	2.0
PHA0112003A	1.0	1.0
PHA0112003B	1.0	1.0
PHA0112006A	2.0	2.0
PHA0112006B	2.0	2.0
PHA0112009A	2.0	1.0
PHA0209013	1.0	1.0
PHA0209026	3.0	3.0
PHA0209034	2.0	3.0
PHA0209038	3.0	3.0
PHA0411008A	1.0	1.0
PHA0411011B	1.0	1.0
PHA0411012A	1.0	1.0
PHA0411030	3.0	3.0
PHA0411033	2.0	2.0
PHA0411036	3.0	2.0
PHA0411042	2.0	3.0
PHA0411045	2.0	2.0
PHA0411056	3.0	3.0
PHA0509017	2.0	3.0
PHA0509018	3.0	3.0
PHA0509019	2.0	2.0
PHA0509020	3.0	3.0
PHA0509022	3.0	3.0
PHA0509031	2.0	2.0
PHA0509044	2.0	3.0
PHA0510002A	1.0	1.0
PHA0510003A	1.0	1.0
PHA0510013B	1.0	1.0
PHA0510035	3.0	3.0
PHA0610005B	1.0	1.0
PHA0610006A	1.0	1.0
PHA0610017	3.0	3.0
PHA0710019	3.0	3.0
PHA0809010	2.0	2.0
PHA0810006	2.0	2.0
PHA0810008	2.0	3.0
PHA0811010	2.0	2.0
PHA0811014	2.0	2.0
PHA1109002	3.0	3.0
PHA1109007	2.0	2.0
PHA1110001A	1.0	1.0
PHA1110004A	1.0	1.0
PHA1111002A	1.0	1.0
PHA1111003A	1.0	1.0
PHA1111004B	1.0	1.0
PHA1111008B	1.0	1.0
VAR0909005	2.0	2.0
VAR0909008	2.0	2.0
VAR0909009	3.0	3.0
1325_1001014	2.0	2.0
1325_1001018	2.0	2.0
1325_1001024	2.0	2.0
1325_1001025	2.0	2.0
1325_1001035	2.0	2.0
1325_1001036	2.0	2.0
1325_1001040	2.0	2.0
1325_1001051	2.0	2.0
1325_1001057	2.0	2.0
1325_1001063	2.0	2.0
1325_1001075	1.0	2.0
1325_1001084	2.0	2.0
1325_1001085	2.0	2.0
1325_1001091	2.0	2.0
1325_1001093	2.0	2.0
1325_1001096	2.0	2.0
1325_1001123	2.0	2.0
1325_1001125	2.0	2.0
1325_1001127	2.0	2.0
1325_1001129	1.0	2.0
1325_1001134	2.0	2.0
1325_1001141	1.0	2.0
1325_1001159	2.0	2.0
1325_1001160	2.0	2.0
1325_1001162	2.0	2.0
1325_1001167	2.0	2.0
1325_1001170	2.0	2.0
1325_9000088	2.0	2.0
1325_9000089	2.0	2.0
1325_9000095	2.0	2.0
1325_9000105	2.0	2.0
1325_9000106	2.0	2.0
1325_9000137	2.0	2.0
1325_9000140	2.0	2.0
1325_9000211	2.0	2.0
1325_9000213	2.0	2.0
1325_9000214	2.0	2.0
1325_9000240	2.0	2.0
1325_9000296	2.0	2.0
1325_9000315	1.0	2.0
1325_9000316	2.0	2.0
1325_9000323	2.0	2.0
1325_9000505	2.0	2.0
1325_9000554	2.0	2.0
1325_9000675	2.0	2.0
1365_0100003	1.0	1.0
1365_0100004	1.0	2.0
1365_0100008	1.0	2.0
1365_0100009	1.0	1.0
1365_0100011	2.0	2.0
1365_0100014	2.0	2.0
1365_0100016	2.0	2.0
1365_0100017	2.0	2.0
1365_0100021	2.0	2.0
1365_0100024	1.0	2.0
1365_0100061	2.0	2.0
1365_0100065	1.0	2.0
1365_0100071	2.0	2.0
1365_0100079	2.0	2.0
1365_0100095	2.0	2.0
1365_0100096	2.0	2.0
1365_0100104	1.0	2.0
1365_0100106	1.0	2.0
1365_0100133	2.0	2.0
1365_0100139	2.0	2.0
1365_0100145	2.0	2.0
1365_0100169	2.0	2.0
1365_0100170	1.0	2.0
1365_0100191	1.0	2.0
1365_0100195	1.0	2.0
1365_0100196	1.0	2.0
1365_0100201	2.0	2.0
1365_0100202	1.0	2.0
1365_0100217	2.0	2.0
1365_0100226	2.0	2.0
1365_0100251	2.0	2.0
1365_0100255	1.0	2.0
1365_0100265	2.0	2.0
1365_0100270	2.0	2.0
1365_0100275	2.0	2.0
1365_0100277	2.0	2.0
1365_0100286	1.0	2.0
1365_0100447	2.0	2.0
1365_0100457	2.0	2.0
1365_0100458	2.0	2.0
1365_0100461	2.0	2.0
1365_0100477	1.0	2.0
1365_0100481	2.0	2.0
1385_0000021	1.0	1.0
1385_0000033	1.0	1.0
1385_0000039	1.0	1.0
1385_0000051	1.0	1.0
1385_0000057	1.0	1.0
1385_0000098	1.0	1.0
1385_0000101	1.0	1.0
1385_0000120	0.0	1.0
1385_0000127	1.0	1.0
1385_0000130	1.0	1.0
1385_0001111	1.0	1.0
1385_0001126	0.0	1.0
1385_0001130	1.0	1.0
1385_0001132	1.0	1.0
1385_0001149	1.0	1.0
1385_0001153	2.0	1.0
1385_0001158	1.0	1.0
1385_0001160	1.0	2.0
1385_0001161	1.0	1.0
1385_0001164	1.0	1.0
1385_0001178	0.0	1.0
1385_0001190	1.0	1.0
1385_0001522	1.0	1.0
1385_0001526	0.0	1.0
1385_0001712	1.0	1.0
1385_0001715	1.0	1.0
1385_0001730	2.0	1.0
1385_0001737	1.0	1.0
1385_0001742	0.0	0.0
1385_0001759	1.0	1.0
1385_0001766	1.0	2.0
1385_0001787	1.0	1.0
1385_0001789	1.0	1.0
1385_0001792	1.0	1.0
1395_0000353	1.0	1.0
1395_0000365	2.0	2.0
1395_0000366	2.0	1.0
1395_0000387	2.0	2.0
1395_0000388	2.0	1.0
1395_0000390	1.0	1.0
1395_0000403	1.0	1.0
1395_0000413	1.0	2.0
1395_0000449	2.0	1.0
1395_0000451	1.0	1.0
1395_0000452	1.0	1.0
1395_0000462	2.0	1.0
1395_0000512	1.0	2.0
1395_0000528	2.0	1.0
1395_0000552	1.0	2.0
1395_0000555	1.0	1.0
1395_0000564	1.0	1.0
1395_0000572	1.0	1.0
1395_0000597	1.0	2.0
1395_0000606	1.0	1.0
1395_0000607	1.0	1.0
1395_0000610	1.0	1.0
1395_0000636	1.0	1.0
1395_0000642	1.0	1.0
1395_0001010	2.0	1.0
1395_0001015	1.0	1.0
1395_0001023	1.0	1.0
1395_0001058	1.0	1.0
1395_0001070	2.0	2.0
1395_0001078	1.0	1.0
1395_0001080	1.0	1.0
1395_0001101	1.0	1.0
1395_0001103	1.0	1.0
1395_0001119	2.0	2.0
1395_0001141	1.0	1.0
1395_0001145	2.0	2.0
1395_0001171	1.0	1.0
2 Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.85
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.68
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.81      0.71      0.76       175
         2.0       0.70      0.72      0.71       177
         3.0       0.61      0.95      0.74        75
         4.0       0.00      0.00      0.00         8

    accuracy                           0.71       452
   macro avg       0.42      0.48      0.44       452
weighted avg       0.69      0.71      0.69       452

[[  0  17   0   0   0]
 [  0 125  50   0   0]
 [  0  13 127  37   0]
 [  0   0   4  71   0]
 [  0   0   0   8   0]]
0.6945042598805635
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.60
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.56
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.78      0.87      0.83       175
         2.0       0.78      0.76      0.77       177
         3.0       0.70      0.79      0.74        75
         4.0       0.00      0.00      0.00         8

    accuracy                           0.77       452
   macro avg       0.45      0.48      0.47       452
weighted avg       0.73      0.77      0.75       452

[[  0  17   0   0   0]
 [  0 153  22   0   0]
 [  0  25 135  17   0]
 [  0   0  16  59   0]
 [  0   0   0   8   0]]
0.745426912015658
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.44
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.61
              precision    recall  f1-score   support

         0.0       1.00      0.29      0.45        17
         1.0       0.84      0.86      0.85       175
         2.0       0.81      0.77      0.79       177
         3.0       0.69      0.91      0.78        75
         4.0       0.00      0.00      0.00         8

    accuracy                           0.80       452
   macro avg       0.67      0.57      0.58       452
weighted avg       0.79      0.80      0.79       452

[[  5  12   0   0   0]
 [  0 150  25   0   0]
 [  0  17 137  23   0]
 [  0   0   7  68   0]
 [  0   0   0   8   0]]
0.7850014981991401
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.32
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.66
              precision    recall  f1-score   support

         0.0       0.53      0.59      0.56        17
         1.0       0.86      0.74      0.80       175
         2.0       0.76      0.79      0.77       177
         3.0       0.69      0.91      0.78        75
         4.0       0.00      0.00      0.00         8

    accuracy                           0.77       452
   macro avg       0.57      0.60      0.58       452
weighted avg       0.76      0.77      0.76       452

[[ 10   7   0   0   0]
 [  9 130  36   0   0]
 [  0  15 139  23   0]
 [  0   0   7  68   0]
 [  0   0   0   8   0]]
0.7616657784045016
452 452 452
Filename	True Label	Prediction
1023_0001420	3.0	3.0
1023_0101683	3.0	3.0
1023_0101689	2.0	2.0
1023_0101690	2.0	2.0
1023_0101691	3.0	3.0
1023_0101693	3.0	3.0
1023_0101749	3.0	3.0
1023_0101847	3.0	3.0
1023_0101851	3.0	3.0
1023_0101893	3.0	3.0
1023_0101894	2.0	3.0
1023_0101907	3.0	3.0
1023_0103821	3.0	3.0
1023_0103823	3.0	3.0
1023_0103824	3.0	3.0
1023_0103830	3.0	3.0
1023_0103831	3.0	3.0
1023_0103839	3.0	3.0
1023_0107075	2.0	3.0
1023_0107682	2.0	1.0
1023_0107725	2.0	3.0
1023_0107729	3.0	3.0
1023_0107784	2.0	2.0
1023_0107787	2.0	3.0
1023_0108423	3.0	2.0
1023_0108426	2.0	3.0
1023_0108641	4.0	3.0
1023_0108815	3.0	3.0
1023_0108889	3.0	3.0
1023_0108931	3.0	3.0
1023_0108935	2.0	3.0
1023_0109248	2.0	3.0
1023_0109249	3.0	3.0
1023_0109267	2.0	3.0
1023_0109399	2.0	3.0
1023_0109495	3.0	3.0
1023_0109500	2.0	3.0
1023_0109516	3.0	3.0
1023_0109614	2.0	2.0
1023_0109717	3.0	3.0
1023_0109890	3.0	3.0
1023_0109891	3.0	3.0
1023_0109947	2.0	3.0
1023_0109951	2.0	3.0
1031_0001949	3.0	3.0
1031_0002003	3.0	3.0
1031_0002005	3.0	3.0
1031_0002032	3.0	3.0
1031_0002040	4.0	3.0
1031_0002043	4.0	3.0
1031_0002084	3.0	3.0
1031_0002089	3.0	3.0
1031_0002195	3.0	3.0
1031_0003029	3.0	3.0
1031_0003063	4.0	3.0
1031_0003065	3.0	3.0
1031_0003071	3.0	3.0
1031_0003072	3.0	3.0
1031_0003078	3.0	3.0
1031_0003092	2.0	3.0
1031_0003098	4.0	3.0
1031_0003099	3.0	3.0
1031_0003128	4.0	3.0
1031_0003131	3.0	3.0
1031_0003136	4.0	3.0
1031_0003155	3.0	3.0
1031_0003163	3.0	3.0
1031_0003182	4.0	3.0
1031_0003186	3.0	3.0
1031_0003211	2.0	3.0
1031_0003212	2.0	3.0
1031_0003221	2.0	3.0
1031_0003238	3.0	3.0
1031_0003245	3.0	3.0
1031_0003310	3.0	3.0
1031_0003338	3.0	3.0
1031_0003367	3.0	3.0
1031_0003384	2.0	2.0
1031_0003387	3.0	3.0
1031_0003393	3.0	3.0
1031_0003414	3.0	3.0
1061_0012029	3.0	2.0
1061_0120274	1.0	1.0
1061_0120275	2.0	2.0
1061_0120276	2.0	2.0
1061_0120277	1.0	2.0
1061_0120279	1.0	1.0
1061_0120280	1.0	1.0
1061_0120285	1.0	2.0
1061_0120290	1.0	2.0
1061_0120312	1.0	1.0
1061_0120327	2.0	2.0
1061_0120334	2.0	2.0
1061_0120335	2.0	3.0
1061_0120343	2.0	2.0
1061_0120347	1.0	1.0
1061_0120352	1.0	1.0
1061_0120354	1.0	2.0
1061_0120355	1.0	1.0
1061_0120358	1.0	1.0
1061_0120359	1.0	1.0
1061_0120366	3.0	2.0
1061_0120374	2.0	2.0
1061_0120388	1.0	2.0
1061_0120410	2.0	2.0
1061_0120421	2.0	2.0
1061_0120438	2.0	2.0
1061_0120443	0.0	0.0
1061_0120449	2.0	2.0
1061_0120457	2.0	2.0
1061_0120458	3.0	3.0
1061_0120486	2.0	2.0
1061_0120492	2.0	2.0
1061_0120494	2.0	2.0
1061_0120495	2.0	2.0
1061_0120497	3.0	3.0
1061_0120498	2.0	2.0
1061_0120500	2.0	2.0
1061_0120858	2.0	2.0
1061_0120874	2.0	2.0
1061_0120880	3.0	3.0
1061_0120887	1.0	2.0
1061_1029111	2.0	2.0
1061_1029114	1.0	1.0
1061_1029116	1.0	2.0
1061_1202912	2.0	2.0
1071_0020001	1.0	1.0
1071_0024683	0.0	1.0
1071_0024689	1.0	1.0
1071_0024702	1.0	2.0
1071_0024709	2.0	2.0
1071_0024759	0.0	1.0
1071_0024775	0.0	0.0
1071_0024777	1.0	1.0
1071_0024778	0.0	1.0
1071_0024783	0.0	0.0
1071_0024799	2.0	2.0
1071_0024810	1.0	1.0
1071_0024819	1.0	1.0
1071_0024820	0.0	1.0
1071_0024821	1.0	1.0
1071_0024822	0.0	1.0
1071_0024823	1.0	1.0
1071_0024827	1.0	1.0
1071_0024837	0.0	0.0
1071_0024852	0.0	0.0
1071_0024860	1.0	0.0
1071_0024866	2.0	2.0
1071_0024871	1.0	1.0
1071_0241831	1.0	2.0
1071_0242021	1.0	1.0
1071_0242091	1.0	0.0
1071_0243501	1.0	1.0
1071_0248304	1.0	1.0
1071_0248316	1.0	0.0
1071_0248318	0.0	0.0
1071_0248319	0.0	0.0
1071_0248322	1.0	1.0
1071_0248339	1.0	1.0
1071_0248343	1.0	1.0
1071_0248345	1.0	2.0
1091_0000004	1.0	0.0
1091_0000007	2.0	2.0
1091_0000012	1.0	1.0
1091_0000025	1.0	0.0
1091_0000031	1.0	1.0
1091_0000038	1.0	1.0
1091_0000044	1.0	2.0
1091_0000053	1.0	1.0
1091_0000055	2.0	2.0
1091_0000062	1.0	1.0
1091_0000068	2.0	2.0
1091_0000073	1.0	2.0
1091_0000075	2.0	2.0
1091_0000076	3.0	2.0
1091_0000078	1.0	1.0
1091_0000079	2.0	2.0
1091_0000102	1.0	1.0
1091_0000113	2.0	2.0
1091_0000123	2.0	2.0
1091_0000144	1.0	0.0
1091_0000145	1.0	0.0
1091_0000185	1.0	1.0
1091_0000191	2.0	1.0
1091_0000193	1.0	1.0
1091_0000208	2.0	1.0
1091_0000209	1.0	1.0
1091_0000210	1.0	1.0
1091_0000216	2.0	1.0
1091_0000225	1.0	1.0
1091_0000227	1.0	2.0
1091_0000229	2.0	2.0
1091_0000238	1.0	2.0
1091_0000240	1.0	1.0
1091_0000241	1.0	1.0
1091_0000254	1.0	1.0
1091_0000255	1.0	1.0
1091_0000260	2.0	2.0
1091_0000266	2.0	2.0
1091_0000274	2.0	1.0
0608	1.0	2.0
0610	1.0	2.0
0615	1.0	1.0
0618	1.0	2.0
0619	2.0	1.0
0622	1.0	1.0
0624	2.0	2.0
0627	2.0	2.0
0804	1.0	1.0
0806	1.0	2.0
0808	1.0	2.0
0809	2.0	2.0
0818	1.0	1.0
0819	3.0	2.0
0823	2.0	2.0
0824	1.0	2.0
0826	1.0	1.0
0828	2.0	1.0
0901	2.0	2.0
0906	2.0	2.0
0922	1.0	1.0
0923	2.0	2.0
0928	2.0	2.0
1001	1.0	2.0
1002	2.0	2.0
1007	2.0	2.0
1010	1.0	2.0
1117	1.0	1.0
BER0611007	2.0	2.0
KYJ0611005A	1.0	1.0
KYJ0611006A	1.0	1.0
KYJ0611006B	1.0	1.0
LIB0611001B	1.0	1.0
MOS0509001	2.0	2.0
PHA0111001B	1.0	1.0
PHA0111004B	1.0	1.0
PHA0111005A	1.0	1.0
PHA0111005B	1.0	1.0
PHA0111015	3.0	3.0
PHA0111016	3.0	3.0
PHA0112009B	2.0	1.0
PHA0112012B	1.0	1.0
PHA0209028	2.0	2.0
PHA0210007	1.0	1.0
PHA0411010B	1.0	1.0
PHA0411011A	1.0	1.0
PHA0411012B	1.0	1.0
PHA0411032	2.0	3.0
PHA0411037	2.0	2.0
PHA0411039	2.0	2.0
PHA0411043	2.0	2.0
PHA0411044	3.0	3.0
PHA0411055	3.0	3.0
PHA0411060	2.0	3.0
PHA0411061	3.0	3.0
PHA0509007	1.0	1.0
PHA0509025	3.0	3.0
PHA0509026	3.0	3.0
PHA0509027	2.0	2.0
PHA0509034	2.0	3.0
PHA0509036	3.0	3.0
PHA0509039	3.0	3.0
PHA0509042	3.0	3.0
PHA0510002B	1.0	1.0
PHA0510003B	1.0	1.0
PHA0510023	3.0	3.0
PHA0510030	2.0	2.0
PHA0510036	3.0	3.0
PHA0510046	2.0	2.0
PHA0510050	2.0	3.0
PHA0610005A	1.0	1.0
PHA0610019A	1.0	1.0
PHA0709008	3.0	2.0
PHA0710011	3.0	3.0
PHA0710016	3.0	2.0
PHA0809009	2.0	3.0
PHA0810003	2.0	3.0
PHA0810004	2.0	2.0
PHA0810015	3.0	3.0
PHA1110002B	2.0	1.0
PHA1110015	3.0	3.0
PHA1110021	2.0	2.0
PHA1111001B	1.0	1.0
VAR0909007	2.0	2.0
1325_1001013	2.0	2.0
1325_1001017	2.0	2.0
1325_1001019	2.0	2.0
1325_1001032	2.0	2.0
1325_1001033	2.0	2.0
1325_1001037	2.0	2.0
1325_1001043	2.0	2.0
1325_1001044	2.0	2.0
1325_1001048	2.0	2.0
1325_1001055	2.0	2.0
1325_1001080	2.0	2.0
1325_1001086	2.0	2.0
1325_1001088	2.0	2.0
1325_1001092	2.0	2.0
1325_1001100	2.0	2.0
1325_1001108	2.0	2.0
1325_1001121	2.0	2.0
1325_1001124	2.0	2.0
1325_1001126	2.0	2.0
1325_1001128	2.0	2.0
1325_1001131	2.0	2.0
1325_1001139	2.0	2.0
1325_1001142	2.0	2.0
1325_1001156	2.0	2.0
1325_1001163	2.0	2.0
1325_1001164	2.0	2.0
1325_1001165	2.0	2.0
1325_9000059	2.0	2.0
1325_9000090	2.0	2.0
1325_9000102	2.0	2.0
1325_9000144	2.0	2.0
1325_9000186	2.0	2.0
1325_9000187	2.0	2.0
1325_9000210	1.0	2.0
1325_9000302	2.0	2.0
1325_9000318	2.0	2.0
1325_9000319	2.0	2.0
1325_9000678	2.0	2.0
1325_9000750	2.0	1.0
1365_0100005	1.0	1.0
1365_0100015	1.0	1.0
1365_0100018	1.0	2.0
1365_0100023	1.0	2.0
1365_0100029	1.0	1.0
1365_0100031	2.0	1.0
1365_0100066	1.0	2.0
1365_0100067	1.0	2.0
1365_0100073	2.0	2.0
1365_0100098	1.0	2.0
1365_0100107	2.0	2.0
1365_0100136	2.0	2.0
1365_0100146	2.0	1.0
1365_0100147	2.0	2.0
1365_0100148	2.0	2.0
1365_0100162	2.0	2.0
1365_0100168	2.0	2.0
1365_0100177	2.0	2.0
1365_0100179	2.0	2.0
1365_0100183	2.0	2.0
1365_0100187	2.0	2.0
1365_0100203	2.0	2.0
1365_0100223	2.0	2.0
1365_0100224	2.0	2.0
1365_0100229	2.0	2.0
1365_0100230	2.0	2.0
1365_0100252	2.0	2.0
1365_0100259	2.0	2.0
1365_0100260	2.0	2.0
1365_0100263	2.0	2.0
1365_0100266	2.0	2.0
1365_0100267	2.0	2.0
1365_0100269	2.0	2.0
1365_0100278	2.0	2.0
1365_0100279	2.0	2.0
1365_0100280	1.0	1.0
1365_0100282	2.0	2.0
1365_0100285	2.0	2.0
1365_0100290	2.0	2.0
1365_0100451	2.0	2.0
1365_0100456	2.0	2.0
1365_0100470	2.0	2.0
1365_0100471	1.0	2.0
1365_0100472	2.0	2.0
1365_0100473	2.0	2.0
1365_0100480	2.0	2.0
1385_0000016	1.0	1.0
1385_0000023	1.0	1.0
1385_0000036	1.0	1.0
1385_0000040	1.0	1.0
1385_0000042	1.0	1.0
1385_0000043	1.0	1.0
1385_0000044	1.0	1.0
1385_0000054	1.0	1.0
1385_0000099	1.0	1.0
1385_0000125	1.0	1.0
1385_0000126	1.0	1.0
1385_0001105	1.0	1.0
1385_0001109	1.0	1.0
1385_0001110	1.0	1.0
1385_0001118	1.0	1.0
1385_0001124	1.0	0.0
1385_0001127	1.0	1.0
1385_0001131	1.0	1.0
1385_0001138	1.0	1.0
1385_0001154	1.0	1.0
1385_0001167	1.0	1.0
1385_0001171	1.0	0.0
1385_0001191	1.0	1.0
1385_0001195	1.0	2.0
1385_0001197	1.0	1.0
1385_0001524	1.0	1.0
1385_0001527	1.0	1.0
1385_0001716	1.0	1.0
1385_0001719	1.0	1.0
1385_0001725	1.0	1.0
1385_0001736	2.0	1.0
1385_0001738	0.0	0.0
1385_0001739	1.0	1.0
1385_0001746	1.0	1.0
1385_0001747	1.0	1.0
1385_0001749	1.0	1.0
1385_0001760	1.0	1.0
1385_0001774	0.0	1.0
1385_0001785	1.0	1.0
1385_0001790	1.0	1.0
1395_0000337	0.0	0.0
1395_0000354	1.0	1.0
1395_0000361	1.0	1.0
1395_0000364	1.0	2.0
1395_0000376	2.0	1.0
1395_0000391	2.0	2.0
1395_0000398	2.0	2.0
1395_0000402	1.0	1.0
1395_0000438	2.0	2.0
1395_0000450	1.0	1.0
1395_0000454	1.0	2.0
1395_0000460	1.0	1.0
1395_0000465	1.0	1.0
1395_0000471	1.0	1.0
1395_0000513	2.0	2.0
1395_0000514	2.0	2.0
1395_0000531	1.0	1.0
1395_0000534	1.0	2.0
1395_0000549	1.0	2.0
1395_0000551	2.0	1.0
1395_0000554	1.0	1.0
1395_0000556	1.0	2.0
1395_0000557	2.0	2.0
1395_0000565	1.0	1.0
1395_0000575	1.0	1.0
1395_0000583	1.0	1.0
1395_0000591	0.0	0.0
1395_0000604	1.0	1.0
1395_0000611	1.0	1.0
1395_0000627	1.0	1.0
1395_0000631	1.0	1.0
1395_0001019	1.0	1.0
1395_0001020	1.0	1.0
1395_0001034	1.0	1.0
1395_0001067	1.0	1.0
1395_0001069	1.0	2.0
1395_0001074	1.0	1.0
1395_0001075	1.0	1.0
1395_0001084	1.0	1.0
1395_0001121	0.0	1.0
1395_0001132	2.0	2.0
1395_0001149	1.0	1.0
1395_0001150	1.0	1.0
3 Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.82
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.66
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.81      0.77      0.79       175
         2.0       0.74      0.81      0.78       177
         3.0       0.70      0.88      0.78        75
         4.0       0.00      0.00      0.00         8

    accuracy                           0.76       452
   macro avg       0.45      0.49      0.47       452
weighted avg       0.72      0.76      0.74       452

[[  0  17   0   0   0]
 [  0 134  40   1   0]
 [  0  15 143  19   0]
 [  0   0   9  66   0]
 [  0   0   0   8   0]]
0.73739719119075
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.59
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.58
              precision    recall  f1-score   support

         0.0       1.00      0.06      0.11        17
         1.0       0.78      0.83      0.81       175
         2.0       0.79      0.76      0.78       177
         3.0       0.73      0.91      0.81        75
         4.0       0.00      0.00      0.00         8

    accuracy                           0.77       452
   macro avg       0.66      0.51      0.50       452
weighted avg       0.77      0.77      0.75       452

[[  1  16   0   0   0]
 [  0 146  29   0   0]
 [  0  25 135  17   0]
 [  0   0   7  68   0]
 [  0   0   0   8   0]]
0.7546260659392617
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.43
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.64
              precision    recall  f1-score   support

         0.0       1.00      0.18      0.30        17
         1.0       0.77      0.85      0.81       175
         2.0       0.81      0.72      0.76       177
         3.0       0.71      0.87      0.78        75
         4.0       0.20      0.12      0.15         8

    accuracy                           0.77       452
   macro avg       0.70      0.55      0.56       452
weighted avg       0.77      0.77      0.76       452

[[  3  14   0   0   0]
 [  0 149  25   1   0]
 [  0  30 128  19   0]
 [  0   0   6  65   4]
 [  0   0   0   7   1]]
0.7550511109349233
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.30
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.69
              precision    recall  f1-score   support

         0.0       1.00      0.18      0.30        17
         1.0       0.82      0.76      0.79       175
         2.0       0.77      0.81      0.79       177
         3.0       0.72      0.95      0.82        75
         4.0       0.00      0.00      0.00         8

    accuracy                           0.78       452
   macro avg       0.66      0.54      0.54       452
weighted avg       0.78      0.78      0.76       452

[[  3  14   0   0   0]
 [  0 133  41   1   0]
 [  0  15 144  18   0]
 [  0   0   3  71   1]
 [  0   0   0   8   0]]
0.7620609102045152
452 452 452
Filename	True Label	Prediction
1023_0101695	2.0	3.0
1023_0101700	3.0	3.0
1023_0101701	2.0	3.0
1023_0101841	2.0	3.0
1023_0101846	4.0	3.0
1023_0101848	2.0	2.0
1023_0101852	3.0	3.0
1023_0101854	2.0	2.0
1023_0101855	2.0	2.0
1023_0101898	3.0	3.0
1023_0101906	2.0	3.0
1023_0101909	4.0	3.0
1023_0103827	3.0	3.0
1023_0103841	3.0	3.0
1023_0103843	2.0	3.0
1023_0103880	3.0	3.0
1023_0103955	3.0	3.0
1023_0104203	3.0	3.0
1023_0104209	3.0	3.0
1023_0107672	2.0	3.0
1023_0107740	3.0	3.0
1023_0107780	3.0	3.0
1023_0107781	3.0	3.0
1023_0107788	3.0	3.0
1023_0108304	3.0	3.0
1023_0108307	3.0	3.0
1023_0108422	3.0	3.0
1023_0108649	3.0	3.0
1023_0108751	3.0	3.0
1023_0108811	3.0	3.0
1023_0108813	3.0	3.0
1023_0108885	2.0	3.0
1023_0108887	2.0	3.0
1023_0109029	1.0	2.0
1023_0109033	4.0	3.0
1023_0109038	3.0	3.0
1023_0109039	3.0	3.0
1023_0109096	3.0	3.0
1023_0109151	3.0	3.0
1023_0109247	3.0	3.0
1023_0109400	3.0	3.0
1023_0109522	3.0	3.0
1023_0109591	3.0	3.0
1023_0109651	3.0	3.0
1023_0109721	2.0	3.0
1023_0109917	3.0	3.0
1023_0109946	2.0	3.0
1031_0001703	4.0	3.0
1031_0001950	3.0	3.0
1031_0002088	3.0	3.0
1031_0002091	3.0	3.0
1031_0002092	4.0	3.0
1031_0002185	3.0	3.0
1031_0002199	3.0	3.0
1031_0003035	3.0	3.0
1031_0003043	4.0	3.0
1031_0003052	3.0	3.0
1031_0003054	3.0	3.0
1031_0003074	3.0	3.0
1031_0003076	4.0	3.0
1031_0003164	3.0	3.0
1031_0003170	3.0	3.0
1031_0003174	3.0	3.0
1031_0003185	3.0	3.0
1031_0003187	3.0	3.0
1031_0003189	3.0	4.0
1031_0003218	3.0	3.0
1031_0003237	3.0	3.0
1031_0003246	3.0	3.0
1031_0003273	3.0	3.0
1031_0003274	3.0	3.0
1031_0003313	3.0	3.0
1031_0003315	3.0	3.0
1031_0003327	2.0	3.0
1031_0003352	2.0	3.0
1031_0003356	3.0	3.0
1031_0003369	3.0	3.0
1031_0003388	3.0	3.0
1031_0003390	3.0	3.0
1031_0003392	3.0	3.0
1031_0003408	2.0	3.0
1031_0003409	4.0	3.0
1061_0120271	2.0	2.0
1061_0120278	2.0	2.0
1061_0120283	1.0	1.0
1061_0120287	1.0	2.0
1061_0120288	2.0	2.0
1061_0120291	1.0	1.0
1061_0120296	2.0	2.0
1061_0120299	2.0	2.0
1061_0120308	2.0	2.0
1061_0120313	2.0	1.0
1061_0120316	2.0	2.0
1061_0120317	2.0	2.0
1061_0120323	1.0	1.0
1061_0120326	2.0	2.0
1061_0120331	1.0	1.0
1061_0120336	1.0	2.0
1061_0120337	2.0	2.0
1061_0120346	2.0	2.0
1061_0120348	1.0	1.0
1061_0120356	2.0	2.0
1061_0120360	3.0	3.0
1061_0120367	3.0	3.0
1061_0120369	2.0	2.0
1061_0120370	2.0	2.0
1061_0120372	2.0	2.0
1061_0120383	2.0	3.0
1061_0120390	2.0	2.0
1061_0120411	3.0	3.0
1061_0120423	2.0	3.0
1061_0120441	2.0	2.0
1061_0120450	2.0	2.0
1061_0120455	2.0	2.0
1061_0120456	2.0	2.0
1061_0120478	2.0	2.0
1061_0120490	2.0	2.0
1061_0120499	2.0	2.0
1061_0120876	2.0	2.0
1061_0120877	2.0	2.0
1061_0120882	3.0	2.0
1061_0120883	2.0	2.0
1061_0120886	2.0	2.0
1061_0120889	1.0	1.0
1061_0120894	2.0	2.0
1061_1029117	1.0	2.0
1061_1029118	1.0	2.0
1061_1202914	1.0	1.0
1061_1202915	1.0	1.0
1061_1202918	1.0	2.0
1061_1202919	2.0	2.0
1071_0024678	1.0	1.0
1071_0024681	2.0	2.0
1071_0024685	2.0	2.0
1071_0024687	1.0	1.0
1071_0024692	2.0	2.0
1071_0024710	1.0	1.0
1071_0024711	1.0	1.0
1071_0024713	1.0	1.0
1071_0024757	2.0	2.0
1071_0024772	0.0	0.0
1071_0024802	1.0	2.0
1071_0024807	1.0	1.0
1071_0024809	1.0	1.0
1071_0024814	1.0	1.0
1071_0024816	1.0	1.0
1071_0024824	1.0	1.0
1071_0024826	1.0	1.0
1071_0024833	1.0	1.0
1071_0024834	2.0	2.0
1071_0024841	0.0	1.0
1071_0024850	1.0	1.0
1071_0024862	1.0	1.0
1071_0024864	0.0	0.0
1071_0024867	2.0	2.0
1071_0024873	1.0	1.0
1071_0024875	1.0	1.0
1071_0024878	2.0	2.0
1071_0024879	1.0	1.0
1071_0024881	2.0	1.0
1071_0241832	1.0	1.0
1071_0242011	1.0	1.0
1071_0242013	1.0	1.0
1071_0242041	1.0	1.0
1071_0242071	0.0	1.0
1071_0243582	1.0	2.0
1071_0248301	1.0	1.0
1071_0248302	1.0	1.0
1071_0248303	1.0	1.0
1071_0248308	1.0	1.0
1071_0248313	1.0	1.0
1071_0248320	0.0	0.0
1071_0248325	0.0	1.0
1071_0248326	1.0	1.0
1071_0248332	2.0	2.0
1071_0248333	2.0	2.0
1071_0248335	1.0	1.0
1071_0248342	1.0	1.0
1071_0248344	1.0	1.0
1091_0000003	2.0	2.0
1091_0000013	1.0	1.0
1091_0000029	1.0	2.0
1091_0000032	2.0	2.0
1091_0000033	1.0	1.0
1091_0000048	1.0	1.0
1091_0000051	0.0	1.0
1091_0000052	0.0	1.0
1091_0000060	2.0	2.0
1091_0000126	2.0	2.0
1091_0000127	1.0	1.0
1091_0000140	1.0	1.0
1091_0000154	2.0	3.0
1091_0000156	2.0	2.0
1091_0000160	1.0	3.0
1091_0000163	1.0	2.0
1091_0000170	2.0	1.0
1091_0000192	1.0	2.0
1091_0000199	2.0	2.0
1091_0000212	2.0	2.0
1091_0000220	2.0	1.0
1091_0000222	1.0	2.0
1091_0000223	2.0	2.0
1091_0000230	2.0	2.0
1091_0000231	2.0	2.0
1091_0000232	2.0	2.0
1091_0000234	2.0	2.0
1091_0000235	1.0	1.0
1091_0000239	2.0	2.0
1091_0000243	1.0	2.0
1091_0000245	2.0	2.0
1091_0000247	2.0	1.0
1091_0000249	2.0	2.0
1091_0000259	2.0	2.0
1091_0000270	1.0	1.0
0614	2.0	2.0
0620	1.0	2.0
0621	2.0	2.0
0626	2.0	1.0
0630	1.0	1.0
0631	2.0	1.0
0634	2.0	2.0
0636	2.0	2.0
0641	1.0	1.0
0643	2.0	2.0
0645	2.0	2.0
0720	1.0	2.0
0801	1.0	1.0
0807	2.0	1.0
0811	2.0	2.0
0814	1.0	1.0
0817	1.0	2.0
0820	1.0	1.0
0821	2.0	2.0
0822	1.0	2.0
0829	1.0	2.0
0902	2.0	2.0
0903	1.0	2.0
0904	1.0	1.0
0910	1.0	2.0
0911	1.0	2.0
0913	2.0	2.0
0915	2.0	2.0
0920	2.0	2.0
0921	1.0	1.0
0927	2.0	2.0
1006	2.0	2.0
1008	1.0	2.0
1009	2.0	2.0
1015	1.0	2.0
1018	1.0	2.0
1115	1.0	2.0
1116	1.0	2.0
BER0611005	2.0	2.0
KYJ0611003A	1.0	1.0
KYJ0611004A	1.0	2.0
KYJ0611009B	1.0	1.0
LIB0611001A	1.0	1.0
LIB0611002B	1.0	1.0
LIB0611003A	1.0	1.0
LIB0611004B	1.0	1.0
LON0610002A	1.0	1.0
LON0611002A	1.0	1.0
LON0611003	3.0	3.0
LON0611004A	1.0	1.0
LON0611004B	1.0	1.0
MOS0611012	2.0	2.0
PAR1011018	3.0	2.0
PHA0111018	2.0	2.0
PHA0112002A	1.0	1.0
PHA0112002B	1.0	1.0
PHA0411008B	1.0	1.0
PHA0411031	3.0	3.0
PHA0411054	3.0	2.0
PHA0509030	3.0	3.0
PHA0509033	1.0	2.0
PHA0509035	2.0	2.0
PHA0509037	3.0	3.0
PHA0509040	2.0	2.0
PHA0510010B	1.0	1.0
PHA0510031	2.0	2.0
PHA0510039	2.0	3.0
PHA0510047	2.0	2.0
PHA0610006B	1.0	1.0
PHA0610007A	1.0	1.0
PHA0710009	2.0	2.0
PHA0710013	3.0	3.0
PHA0810002	2.0	2.0
PHA0810012	2.0	2.0
PHA0811016	2.0	2.0
PHA0811017	3.0	3.0
PHA0811019	3.0	3.0
PHA1109004	3.0	3.0
PHA1109005	2.0	2.0
PHA1109027	3.0	3.0
PHA1110001B	1.0	1.0
PHA1110016	2.0	2.0
PHA1110022	3.0	3.0
PHA1111003B	1.0	1.0
PHA1111004A	1.0	1.0
PHA1111006A	1.0	1.0
PHA1111006B	1.0	1.0
PHA1111008A	1.0	1.0
ST071122B	1.0	1.0
VAR0909006	3.0	3.0
VAR0910005	2.0	2.0
VAR0910009	3.0	3.0
VAR0910011	2.0	3.0
1325_1001015	2.0	2.0
1325_1001028	2.0	2.0
1325_1001029	2.0	2.0
1325_1001042	2.0	2.0
1325_1001054	2.0	2.0
1325_1001058	2.0	2.0
1325_1001076	2.0	2.0
1325_1001077	2.0	2.0
1325_1001079	2.0	2.0
1325_1001089	2.0	2.0
1325_1001090	2.0	2.0
1325_1001095	2.0	2.0
1325_1001098	2.0	2.0
1325_1001101	2.0	2.0
1325_1001110	2.0	2.0
1325_1001113	2.0	2.0
1325_1001120	2.0	2.0
1325_1001132	2.0	2.0
1325_1001135	2.0	2.0
1325_1001152	2.0	2.0
1325_1001153	2.0	2.0
1325_1001154	2.0	2.0
1325_1001155	2.0	2.0
1325_1001157	2.0	2.0
1325_1001158	2.0	2.0
1325_1001166	2.0	2.0
1325_9000107	2.0	2.0
1325_9000279	2.0	2.0
1325_9000322	2.0	2.0
1325_9000504	2.0	2.0
1325_9000536	2.0	2.0
1325_9000602	2.0	2.0
1325_9000611	2.0	2.0
1365_0100010	1.0	1.0
1365_0100022	2.0	2.0
1365_0100026	1.0	1.0
1365_0100051	1.0	2.0
1365_0100057	2.0	2.0
1365_0100063	2.0	2.0
1365_0100064	2.0	2.0
1365_0100074	1.0	2.0
1365_0100093	2.0	2.0
1365_0100099	1.0	2.0
1365_0100105	2.0	2.0
1365_0100116	2.0	2.0
1365_0100117	2.0	2.0
1365_0100134	2.0	2.0
1365_0100135	2.0	2.0
1365_0100171	1.0	2.0
1365_0100182	2.0	2.0
1365_0100185	1.0	2.0
1365_0100205	2.0	1.0
1365_0100219	2.0	2.0
1365_0100221	2.0	2.0
1365_0100225	2.0	2.0
1365_0100253	1.0	2.0
1365_0100256	2.0	2.0
1365_0100262	2.0	2.0
1365_0100268	1.0	2.0
1365_0100274	2.0	2.0
1365_0100289	2.0	2.0
1365_0100455	2.0	2.0
1365_0100459	2.0	2.0
1365_0100469	2.0	2.0
1365_0100476	2.0	2.0
1365_0100479	2.0	2.0
1385_0000013	0.0	1.0
1385_0000017	1.0	1.0
1385_0000035	1.0	2.0
1385_0000041	1.0	1.0
1385_0000045	1.0	1.0
1385_0000100	1.0	1.0
1385_0000123	1.0	1.0
1385_0000128	1.0	1.0
1385_0001112	1.0	1.0
1385_0001121	1.0	1.0
1385_0001123	1.0	1.0
1385_0001128	0.0	1.0
1385_0001129	1.0	1.0
1385_0001137	1.0	1.0
1385_0001150	1.0	1.0
1385_0001155	1.0	1.0
1385_0001162	1.0	1.0
1385_0001163	1.0	1.0
1385_0001165	1.0	1.0
1385_0001166	1.0	1.0
1385_0001528	1.0	1.0
1385_0001718	1.0	1.0
1385_0001720	0.0	1.0
1385_0001723	0.0	1.0
1385_0001727	0.0	1.0
1385_0001728	1.0	2.0
1385_0001734	1.0	1.0
1385_0001750	0.0	1.0
1385_0001751	1.0	1.0
1385_0001758	1.0	1.0
1385_0001771	1.0	1.0
1385_0001772	1.0	1.0
1385_0001773	1.0	1.0
1385_0001791	1.0	1.0
1385_0001793	1.0	1.0
1385_0001796	1.0	1.0
1395_0000338	1.0	1.0
1395_0000340	1.0	1.0
1395_0000355	1.0	2.0
1395_0000359	1.0	2.0
1395_0000369	2.0	2.0
1395_0000378	1.0	1.0
1395_0000379	1.0	1.0
1395_0000380	1.0	2.0
1395_0000389	0.0	1.0
1395_0000396	1.0	1.0
1395_0000432	1.0	1.0
1395_0000448	1.0	1.0
1395_0000458	1.0	1.0
1395_0000499	1.0	1.0
1395_0000500	1.0	1.0
1395_0000518	2.0	2.0
1395_0000526	1.0	1.0
1395_0000535	1.0	1.0
1395_0000559	1.0	1.0
1395_0000563	1.0	1.0
1395_0000579	1.0	1.0
1395_0000582	0.0	1.0
1395_0000585	1.0	1.0
1395_0000587	0.0	1.0
1395_0000608	1.0	1.0
1395_0000649	2.0	1.0
1395_0001016	2.0	1.0
1395_0001017	1.0	1.0
1395_0001024	1.0	1.0
1395_0001040	1.0	1.0
1395_0001045	1.0	1.0
1395_0001073	2.0	1.0
1395_0001109	1.0	1.0
1395_0001114	1.0	1.0
1395_0001115	2.0	1.0
1395_0001117	1.0	1.0
1395_0001122	1.0	1.0
1395_0001123	1.0	1.0
1395_0001124	1.0	1.0
1395_0001160	2.0	1.0
1395_0001164	2.0	1.0
1395_0001167	1.0	1.0
1395_0001170	1.0	2.0
4 Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.83
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.95
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.77      0.43      0.55       175
         2.0       0.57      0.79      0.67       177
         3.0       0.65      0.95      0.77        75
         4.0       0.00      0.00      0.00         8

    accuracy                           0.63       452
   macro avg       0.40      0.43      0.40       452
weighted avg       0.63      0.63      0.60       452

[[  0  16   1   0   0]
 [  0  76  99   0   0]
 [  0   7 140  30   0]
 [  0   0   4  71   0]
 [  0   0   0   8   0]]
0.6032753135349163
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.62
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.66
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.80      0.74      0.77       175
         2.0       0.70      0.83      0.76       177
         3.0       0.72      0.75      0.73        75
         4.0       0.00      0.00      0.00         8

    accuracy                           0.74       452
   macro avg       0.44      0.46      0.45       452
weighted avg       0.70      0.74      0.72       452

[[  0  17   0   0   0]
 [  0 130  45   0   0]
 [  0  16 147  14   0]
 [  0   0  19  56   0]
 [  0   0   0   8   0]]
0.7160086329662152
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.47
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.71
              precision    recall  f1-score   support

         0.0       0.75      0.35      0.48        17
         1.0       0.82      0.73      0.77       175
         2.0       0.72      0.76      0.74       177
         3.0       0.67      0.91      0.77        75
         4.0       0.00      0.00      0.00         8

    accuracy                           0.74       452
   macro avg       0.59      0.55      0.55       452
weighted avg       0.74      0.74      0.73       452

[[  6  11   0   0   0]
 [  2 128  45   0   0]
 [  0  18 134  25   0]
 [  0   0   7  68   0]
 [  0   0   0   8   0]]
0.7339203328342991
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.35
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.69
              precision    recall  f1-score   support

         0.0       0.75      0.35      0.48        17
         1.0       0.81      0.74      0.77       175
         2.0       0.73      0.79      0.76       177
         3.0       0.72      0.91      0.80        75
         4.0       0.00      0.00      0.00         8

    accuracy                           0.76       452
   macro avg       0.60      0.56      0.56       452
weighted avg       0.75      0.76      0.75       452

[[  6  11   0   0   0]
 [  2 129  44   0   0]
 [  0  19 140  18   0]
 [  0   0   7  68   0]
 [  0   0   0   8   0]]
0.7486030605040708
452 452 452
Filename	True Label	Prediction
1023_0001416	3.0	3.0
1023_0101751	3.0	3.0
1023_0101752	3.0	3.0
1023_0101849	3.0	3.0
1023_0101853	2.0	3.0
1023_0101896	2.0	2.0
1023_0103832	2.0	2.0
1023_0103837	3.0	3.0
1023_0103838	3.0	3.0
1023_0104206	3.0	2.0
1023_0107042	3.0	3.0
1023_0107074	3.0	3.0
1023_0107783	3.0	2.0
1023_0108305	3.0	3.0
1023_0108650	3.0	3.0
1023_0108752	3.0	3.0
1023_0108753	2.0	2.0
1023_0108812	2.0	3.0
1023_0108814	3.0	3.0
1023_0108932	2.0	3.0
1023_0108933	3.0	3.0
1023_0108934	3.0	3.0
1023_0108955	3.0	3.0
1023_0108958	2.0	3.0
1023_0109027	2.0	2.0
1023_0109030	3.0	3.0
1023_0109391	2.0	3.0
1023_0109392	3.0	3.0
1023_0109401	3.0	3.0
1023_0109515	3.0	3.0
1023_0109588	3.0	3.0
1023_0109716	3.0	3.0
1023_0109878	2.0	3.0
1023_0109914	2.0	3.0
1031_0002004	3.0	3.0
1031_0002011	4.0	3.0
1031_0002042	3.0	3.0
1031_0002085	3.0	3.0
1031_0002200	2.0	3.0
1031_0003012	3.0	3.0
1031_0003013	4.0	3.0
1031_0003023	3.0	3.0
1031_0003048	4.0	3.0
1031_0003088	4.0	3.0
1031_0003097	3.0	3.0
1031_0003106	3.0	3.0
1031_0003135	3.0	3.0
1031_0003144	3.0	3.0
1031_0003145	3.0	3.0
1031_0003149	3.0	3.0
1031_0003156	3.0	3.0
1031_0003157	4.0	3.0
1031_0003161	3.0	3.0
1031_0003167	3.0	3.0
1031_0003172	3.0	3.0
1031_0003173	3.0	3.0
1031_0003179	3.0	3.0
1031_0003183	4.0	3.0
1031_0003190	3.0	3.0
1031_0003205	3.0	3.0
1031_0003219	3.0	3.0
1031_0003230	3.0	3.0
1031_0003232	2.0	3.0
1031_0003244	4.0	3.0
1031_0003309	3.0	3.0
1031_0003314	3.0	3.0
1031_0003330	3.0	3.0
1031_0003353	2.0	3.0
1031_0003366	3.0	3.0
1031_0003415	4.0	3.0
1031_0003419	3.0	3.0
1061_0120273	1.0	2.0
1061_0120281	1.0	1.0
1061_0120289	1.0	1.0
1061_0120303	1.0	2.0
1061_0120304	2.0	2.0
1061_0120309	1.0	1.0
1061_0120315	2.0	1.0
1061_0120325	2.0	2.0
1061_0120332	1.0	2.0
1061_0120338	1.0	2.0
1061_0120345	2.0	2.0
1061_0120350	2.0	2.0
1061_0120357	3.0	3.0
1061_0120368	2.0	2.0
1061_0120376	2.0	2.0
1061_0120386	1.0	2.0
1061_0120387	1.0	2.0
1061_0120394	2.0	2.0
1061_0120406	2.0	2.0
1061_0120413	1.0	1.0
1061_0120424	2.0	2.0
1061_0120431	2.0	2.0
1061_0120432	2.0	2.0
1061_0120442	2.0	2.0
1061_0120448	3.0	2.0
1061_0120459	2.0	2.0
1061_0120479	2.0	2.0
1061_0120480	2.0	2.0
1061_0120483	1.0	2.0
1061_0120489	2.0	2.0
1061_0120496	2.0	2.0
1061_0120856	1.0	2.0
1061_0120859	2.0	2.0
1061_0120875	3.0	3.0
1061_0120884	2.0	2.0
1061_1029112	3.0	3.0
1061_1029115	2.0	1.0
1061_1029120	1.0	1.0
1061_1202916	2.0	2.0
1071_0024701	2.0	2.0
1071_0024706	1.0	1.0
1071_0024715	1.0	2.0
1071_0024716	1.0	1.0
1071_0024758	2.0	2.0
1071_0024763	1.0	1.0
1071_0024766	1.0	1.0
1071_0024769	0.0	1.0
1071_0024774	0.0	0.0
1071_0024781	1.0	1.0
1071_0024782	0.0	0.0
1071_0024784	1.0	1.0
1071_0024798	0.0	1.0
1071_0024800	1.0	1.0
1071_0024804	1.0	1.0
1071_0024808	1.0	1.0
1071_0024818	2.0	1.0
1071_0024835	1.0	1.0
1071_0024843	1.0	1.0
1071_0024844	1.0	1.0
1071_0024851	1.0	1.0
1071_0024853	1.0	1.0
1071_0024857	1.0	1.0
1071_0024863	1.0	1.0
1071_0241833	1.0	1.0
1071_0242042	1.0	1.0
1071_0242043	0.0	1.0
1071_0242072	0.0	0.0
1071_0242092	0.0	0.0
1071_0242093	0.0	0.0
1071_0243593	1.0	1.0
1071_0243621	2.0	1.0
1071_0243622	1.0	0.0
1071_0243623	1.0	1.0
1071_0248309	2.0	1.0
1071_0248311	1.0	1.0
1071_0248312	1.0	1.0
1071_0248314	1.0	1.0
1071_0248323	1.0	1.0
1071_0248324	0.0	1.0
1071_0248328	0.0	1.0
1071_0248330	2.0	1.0
1071_0248337	1.0	2.0
1071_0248340	0.0	1.0
1091_0000010	2.0	2.0
1091_0000011	2.0	1.0
1091_0000018	2.0	2.0
1091_0000022	3.0	2.0
1091_0000046	1.0	1.0
1091_0000047	1.0	1.0
1091_0000049	1.0	1.0
1091_0000059	2.0	2.0
1091_0000061	1.0	1.0
1091_0000064	1.0	1.0
1091_0000070	1.0	1.0
1091_0000077	1.0	0.0
1091_0000087	1.0	1.0
1091_0000092	2.0	2.0
1091_0000095	2.0	1.0
1091_0000125	2.0	2.0
1091_0000148	1.0	1.0
1091_0000152	1.0	1.0
1091_0000157	2.0	2.0
1091_0000162	1.0	2.0
1091_0000167	2.0	2.0
1091_0000169	1.0	2.0
1091_0000171	2.0	2.0
1091_0000194	2.0	2.0
1091_0000196	2.0	2.0
1091_0000200	1.0	2.0
1091_0000201	1.0	2.0
1091_0000203	1.0	2.0
1091_0000221	1.0	1.0
1091_0000236	1.0	2.0
1091_0000246	2.0	2.0
1091_0000250	1.0	2.0
1091_0000253	0.0	1.0
1091_0000256	2.0	2.0
1091_0000257	2.0	2.0
1091_0000265	1.0	2.0
1091_0000267	1.0	2.0
1091_0000271	1.0	1.0
1091_0000273	2.0	2.0
1091_0000276	3.0	2.0
0601	1.0	1.0
0602	1.0	2.0
0604	2.0	2.0
0616	1.0	1.0
0632	1.0	1.0
0635	1.0	1.0
0637	2.0	2.0
0640	2.0	2.0
0717	1.0	1.0
0718	1.0	2.0
0724	2.0	2.0
0805	2.0	1.0
0815	2.0	2.0
0905	2.0	2.0
0907	2.0	2.0
0912	2.0	2.0
0914	1.0	2.0
0919	1.0	1.0
0925	2.0	2.0
0929	1.0	2.0
1016	1.0	2.0
1017	1.0	1.0
1019	1.0	2.0
1020	2.0	2.0
1021	1.0	2.0
1112	1.0	2.0
1113	1.0	2.0
9999	1.0	1.0
KYJ0611005B	1.0	1.0
KYJ0611009A	1.0	1.0
LON0610002B	1.0	1.0
LON0611002B	1.0	1.0
MOS0611013	2.0	2.0
PAR1011009B	1.0	1.0
PHA0111002A	2.0	1.0
PHA0111003A	1.0	1.0
PHA0111010	3.0	3.0
PHA0111012	2.0	2.0
PHA0112007A	1.0	1.0
PHA0209001	1.0	2.0
PHA0209008	1.0	1.0
PHA0209031	3.0	3.0
PHA0411009B	1.0	1.0
PHA0411010A	1.0	1.0
PHA0411034	1.0	2.0
PHA0411035	3.0	2.0
PHA0411038	3.0	3.0
PHA0411047	2.0	3.0
PHA0411062	2.0	3.0
PHA0509002	1.0	1.0
PHA0509021	2.0	2.0
PHA0509024	2.0	2.0
PHA0509028	3.0	3.0
PHA0509043	3.0	2.0
PHA0509045	2.0	2.0
PHA0510004A	1.0	1.0
PHA0510027	2.0	2.0
PHA0510034	3.0	3.0
PHA0510038	3.0	3.0
PHA0510040	2.0	3.0
PHA0510049	2.0	2.0
PHA0610007B	1.0	1.0
PHA0610015	2.0	3.0
PHA0610016	3.0	3.0
PHA0710010	2.0	3.0
PHA0710012	3.0	3.0
PHA0710014	3.0	3.0
PHA0710015	3.0	3.0
PHA0710017	3.0	3.0
PHA0710018	3.0	3.0
PHA0710021	3.0	3.0
PHA0810010	2.0	3.0
PHA0810011	2.0	2.0
PHA0811012	3.0	3.0
PHA0811013	3.0	3.0
PHA0811020	2.0	2.0
PHA1109003	2.0	2.0
PHA1109024	3.0	3.0
PHA1109026	3.0	3.0
PHA1110003A	1.0	1.0
VAR0209036	2.0	3.0
VAR0909004	2.0	2.0
VAR0910004	3.0	3.0
VAR0910006	2.0	3.0
1325_1001008	2.0	2.0
1325_1001010	2.0	2.0
1325_1001011	2.0	2.0
1325_1001012	2.0	2.0
1325_1001016	2.0	2.0
1325_1001020	2.0	2.0
1325_1001021	2.0	2.0
1325_1001022	2.0	2.0
1325_1001027	2.0	2.0
1325_1001045	2.0	2.0
1325_1001046	2.0	2.0
1325_1001047	2.0	2.0
1325_1001050	2.0	2.0
1325_1001052	2.0	2.0
1325_1001059	2.0	2.0
1325_1001081	2.0	2.0
1325_1001097	1.0	2.0
1325_1001107	2.0	2.0
1325_1001109	2.0	2.0
1325_1001119	2.0	2.0
1325_1001130	2.0	2.0
1325_1001144	2.0	2.0
1325_1001169	2.0	2.0
1325_9000087	2.0	2.0
1325_9000138	2.0	2.0
1325_9000152	2.0	2.0
1325_9000188	2.0	2.0
1325_9000209	2.0	2.0
1325_9000215	2.0	2.0
1325_9000241	2.0	2.0
1325_9000278	2.0	2.0
1325_9000303	2.0	2.0
1325_9000304	2.0	2.0
1325_9000314	2.0	2.0
1325_9000533	2.0	2.0
1325_9000534	2.0	2.0
1325_9000601	2.0	2.0
1325_9000676	2.0	2.0
1325_9000684	2.0	2.0
1325_9000685	2.0	2.0
1325_9000686	2.0	2.0
1365_0100002	2.0	2.0
1365_0100007	1.0	1.0
1365_0100012	2.0	1.0
1365_0100027	2.0	2.0
1365_0100028	2.0	2.0
1365_0100058	2.0	2.0
1365_0100069	2.0	2.0
1365_0100070	2.0	2.0
1365_0100072	2.0	2.0
1365_0100080	2.0	2.0
1365_0100094	2.0	2.0
1365_0100102	2.0	2.0
1365_0100118	2.0	2.0
1365_0100123	2.0	2.0
1365_0100137	2.0	2.0
1365_0100138	2.0	2.0
1365_0100151	1.0	1.0
1365_0100163	2.0	2.0
1365_0100165	2.0	2.0
1365_0100166	1.0	2.0
1365_0100167	1.0	2.0
1365_0100172	2.0	2.0
1365_0100173	2.0	2.0
1365_0100174	1.0	2.0
1365_0100175	2.0	2.0
1365_0100178	2.0	2.0
1365_0100180	1.0	1.0
1365_0100181	1.0	2.0
1365_0100184	2.0	2.0
1365_0100188	2.0	2.0
1365_0100204	2.0	2.0
1365_0100213	2.0	1.0
1365_0100215	2.0	2.0
1365_0100220	2.0	2.0
1365_0100222	2.0	2.0
1365_0100227	2.0	2.0
1365_0100257	2.0	2.0
1365_0100276	2.0	2.0
1365_0100281	2.0	2.0
1365_0100287	2.0	2.0
1365_0100288	2.0	2.0
1365_0100299	2.0	2.0
1365_0100474	2.0	2.0
1365_0100482	2.0	2.0
1385_0000012	1.0	1.0
1385_0000020	1.0	1.0
1385_0000037	1.0	1.0
1385_0000038	1.0	1.0
1385_0000049	1.0	1.0
1385_0000052	1.0	1.0
1385_0000053	1.0	1.0
1385_0000059	1.0	1.0
1385_0000097	1.0	1.0
1385_0000102	1.0	1.0
1385_0000103	1.0	1.0
1385_0000104	1.0	1.0
1385_0000114	1.0	1.0
1385_0000122	1.0	1.0
1385_0001103	1.0	1.0
1385_0001104	1.0	1.0
1385_0001108	1.0	1.0
1385_0001113	1.0	1.0
1385_0001120	1.0	1.0
1385_0001122	1.0	1.0
1385_0001134	1.0	1.0
1385_0001136	1.0	1.0
1385_0001151	1.0	1.0
1385_0001152	1.0	1.0
1385_0001156	1.0	1.0
1385_0001157	1.0	1.0
1385_0001159	1.0	1.0
1385_0001169	1.0	1.0
1385_0001173	0.0	1.0
1385_0001174	1.0	1.0
1385_0001194	1.0	1.0
1385_0001196	1.0	1.0
1385_0001198	2.0	1.0
1385_0001523	1.0	1.0
1385_0001714	1.0	1.0
1385_0001717	2.0	1.0
1385_0001726	1.0	1.0
1385_0001733	1.0	1.0
1385_0001744	0.0	1.0
1385_0001753	1.0	1.0
1385_0001754	1.0	1.0
1385_0001761	1.0	1.0
1385_0001762	1.0	1.0
1385_0001764	1.0	1.0
1385_0001765	0.0	0.0
1385_0001768	1.0	1.0
1385_0001775	1.0	1.0
1385_0001786	1.0	1.0
1385_0001794	1.0	1.0
1385_0001795	1.0	1.0
1395_0000333	1.0	1.0
1395_0000357	2.0	2.0
1395_0000360	2.0	1.0
1395_0000383	1.0	1.0
1395_0000399	1.0	1.0
1395_0000404	1.0	2.0
1395_0000415	1.0	1.0
1395_0000443	2.0	1.0
1395_0000446	2.0	2.0
1395_0000447	1.0	2.0
1395_0000470	1.0	1.0
1395_0000504	1.0	1.0
1395_0000515	2.0	1.0
1395_0000547	1.0	1.0
1395_0000548	1.0	2.0
1395_0000550	1.0	2.0
1395_0000584	0.0	1.0
1395_0000609	1.0	1.0
1395_0000626	2.0	2.0
1395_0000630	1.0	2.0
1395_0000635	1.0	1.0
1395_0000639	1.0	2.0
1395_0000644	1.0	2.0
1395_0000646	1.0	1.0
1395_0001013	1.0	1.0
1395_0001021	1.0	1.0
1395_0001033	1.0	1.0
1395_0001060	2.0	2.0
1395_0001064	1.0	2.0
1395_0001065	1.0	2.0
1395_0001066	1.0	1.0
1395_0001090	2.0	1.0
1395_0001104	1.0	1.0
1395_0001118	1.0	1.0
1395_0001126	1.0	1.0
1395_0001133	1.0	1.0
1395_0001146	0.0	1.0
1395_0001158	2.0	1.0
1395_0001161	1.0	1.0
5 Fold, Dimension = OverallCEFRrating

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.81
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.64
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        17
         1.0       0.72      0.95      0.82       175
         2.0       0.91      0.60      0.72       177
         3.0       0.67      0.95      0.79        74
         4.0       0.00      0.00      0.00         9

    accuracy                           0.76       452
   macro avg       0.46      0.50      0.47       452
weighted avg       0.75      0.76      0.73       452

[[  0  17   0   0   0]
 [  0 167   8   0   0]
 [  0  46 106  25   0]
 [  0   2   2  70   0]
 [  0   0   0   9   0]]
0.7298280977621026
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.57
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.58
              precision    recall  f1-score   support

         0.0       1.00      0.18      0.30        17
         1.0       0.78      0.82      0.80       175
         2.0       0.76      0.79      0.77       177
         3.0       0.73      0.81      0.77        74
         4.0       0.00      0.00      0.00         9

    accuracy                           0.76       452
   macro avg       0.65      0.52      0.53       452
weighted avg       0.76      0.76      0.75       452

[[  3  14   0   0   0]
 [  0 143  32   0   0]
 [  0  25 139  13   0]
 [  0   1  13  60   0]
 [  0   0   0   9   0]]
0.7480802109249871
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.45
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.71
              precision    recall  f1-score   support

         0.0       0.44      0.41      0.42        17
         1.0       0.85      0.73      0.78       175
         2.0       0.76      0.75      0.75       177
         3.0       0.64      0.95      0.76        74
         4.0       0.00      0.00      0.00         9

    accuracy                           0.75       452
   macro avg       0.54      0.57      0.54       452
weighted avg       0.74      0.75      0.74       452

[[  7  10   0   0   0]
 [  9 127  39   0   0]
 [  0  13 133  31   0]
 [  0   0   4  70   0]
 [  0   0   0   9   0]]
0.738191306074519
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.30
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.69
              precision    recall  f1-score   support

         0.0       0.50      0.29      0.37        17
         1.0       0.84      0.77      0.80       175
         2.0       0.77      0.80      0.78       177
         3.0       0.69      0.92      0.79        74
         4.0       0.00      0.00      0.00         9

    accuracy                           0.77       452
   macro avg       0.56      0.56      0.55       452
weighted avg       0.76      0.77      0.76       452

[[  5  12   0   0   0]
 [  5 134  36   0   0]
 [  0  14 141  22   0]
 [  0   0   6  68   0]
 [  0   0   0   9   0]]
0.759114395356019
452 452 452
Filename	True Label	Prediction
1023_0001422	2.0	2.0
1023_0101753	3.0	3.0
1023_0101895	3.0	3.0
1023_0101899	2.0	3.0
1023_0101900	3.0	3.0
1023_0101901	3.0	3.0
1023_0101904	2.0	2.0
1023_0102117	3.0	3.0
1023_0103825	3.0	3.0
1023_0103828	1.0	2.0
1023_0103840	3.0	3.0
1023_0104207	2.0	3.0
1023_0106816	3.0	3.0
1023_0107727	3.0	3.0
1023_0107773	2.0	3.0
1023_0108510	3.0	3.0
1023_0108518	3.0	3.0
1023_0108520	3.0	3.0
1023_0108810	3.0	3.0
1023_0108890	3.0	3.0
1023_0108908	3.0	3.0
1023_0108992	3.0	3.0
1023_0109026	2.0	3.0
1023_0109192	3.0	3.0
1023_0109250	2.0	3.0
1023_0109422	3.0	3.0
1023_0109496	3.0	3.0
1023_0109519	2.0	2.0
1023_0109520	2.0	3.0
1023_0109527	3.0	3.0
1023_0109609	2.0	2.0
1023_0109649	3.0	3.0
1023_0109671	3.0	3.0
1023_0109674	3.0	3.0
1023_0109915	2.0	2.0
1023_0109954	3.0	3.0
1023_0111896	2.0	2.0
1031_0001998	4.0	3.0
1031_0002002	2.0	3.0
1031_0002010	3.0	3.0
1031_0002079	4.0	3.0
1031_0002083	3.0	3.0
1031_0002087	3.0	3.0
1031_0002131	3.0	3.0
1031_0002184	3.0	3.0
1031_0002197	3.0	3.0
1031_0003042	3.0	3.0
1031_0003073	4.0	3.0
1031_0003077	3.0	3.0
1031_0003090	3.0	3.0
1031_0003091	2.0	3.0
1031_0003126	3.0	3.0
1031_0003127	4.0	3.0
1031_0003129	3.0	3.0
1031_0003130	4.0	3.0
1031_0003150	3.0	3.0
1031_0003154	3.0	3.0
1031_0003162	3.0	3.0
1031_0003165	2.0	3.0
1031_0003169	3.0	3.0
1031_0003181	4.0	3.0
1031_0003184	4.0	3.0
1031_0003203	2.0	3.0
1031_0003206	3.0	3.0
1031_0003207	4.0	3.0
1031_0003214	3.0	3.0
1031_0003225	3.0	3.0
1031_0003226	3.0	3.0
1031_0003231	3.0	3.0
1031_0003233	3.0	3.0
1031_0003235	3.0	3.0
1031_0003243	3.0	3.0
1031_0003260	3.0	3.0
1031_0003262	3.0	3.0
1031_0003272	3.0	2.0
1031_0003337	3.0	3.0
1031_0003339	3.0	3.0
1031_0003354	3.0	3.0
1031_0003357	3.0	3.0
1031_0003358	4.0	3.0
1031_0003359	2.0	3.0
1031_0003383	3.0	3.0
1031_0003391	2.0	3.0
1031_0003410	3.0	3.0
1061_0120272	1.0	1.0
1061_0120284	0.0	1.0
1061_0120300	2.0	2.0
1061_0120301	2.0	2.0
1061_0120306	3.0	2.0
1061_0120307	2.0	2.0
1061_0120310	2.0	2.0
1061_0120319	2.0	2.0
1061_0120320	3.0	3.0
1061_0120321	2.0	2.0
1061_0120330	2.0	2.0
1061_0120333	2.0	3.0
1061_0120341	1.0	1.0
1061_0120349	1.0	1.0
1061_0120351	2.0	2.0
1061_0120353	1.0	1.0
1061_0120361	2.0	2.0
1061_0120382	1.0	2.0
1061_0120384	1.0	1.0
1061_0120389	2.0	2.0
1061_0120405	2.0	2.0
1061_0120407	3.0	2.0
1061_0120408	2.0	2.0
1061_0120409	2.0	2.0
1061_0120414	2.0	2.0
1061_0120415	2.0	1.0
1061_0120425	2.0	2.0
1061_0120426	2.0	2.0
1061_0120427	2.0	2.0
1061_0120428	2.0	2.0
1061_0120429	2.0	2.0
1061_0120433	1.0	1.0
1061_0120440	1.0	1.0
1061_0120453	2.0	2.0
1061_0120485	2.0	2.0
1061_0120487	2.0	2.0
1061_0120488	2.0	2.0
1061_0120493	2.0	2.0
1061_0120878	1.0	1.0
1061_0120888	1.0	2.0
1061_0120890	1.0	1.0
1061_1029113	1.0	1.0
1061_1202917	1.0	1.0
1071_0024691	1.0	2.0
1071_0024699	1.0	1.0
1071_0024703	1.0	1.0
1071_0024708	1.0	1.0
1071_0024761	1.0	1.0
1071_0024762	1.0	0.0
1071_0024767	2.0	1.0
1071_0024768	1.0	1.0
1071_0024773	1.0	1.0
1071_0024776	0.0	0.0
1071_0024797	0.0	1.0
1071_0024803	1.0	1.0
1071_0024806	1.0	1.0
1071_0024811	1.0	1.0
1071_0024812	1.0	1.0
1071_0024817	1.0	1.0
1071_0024836	1.0	2.0
1071_0024838	0.0	0.0
1071_0024845	0.0	1.0
1071_0024846	1.0	1.0
1071_0024847	1.0	2.0
1071_0024848	1.0	2.0
1071_0024849	0.0	0.0
1071_0024854	0.0	1.0
1071_0024861	0.0	1.0
1071_0024865	2.0	2.0
1071_0024872	1.0	2.0
1071_0024877	1.0	1.0
1071_0242012	1.0	2.0
1071_0242023	1.0	1.0
1071_0243502	1.0	1.0
1071_0243591	1.0	1.0
1071_0243592	1.0	1.0
1071_0248305	0.0	1.0
1071_0248307	2.0	1.0
1071_0248315	0.0	0.0
1071_0248321	1.0	1.0
1071_0248331	1.0	1.0
1071_0248334	2.0	1.0
1071_0248336	1.0	1.0
1071_0248341	1.0	1.0
1071_0248346	1.0	1.0
1071_0248348	1.0	1.0
1071_0248349	1.0	0.0
1091_0000005	2.0	2.0
1091_0000006	0.0	1.0
1091_0000008	2.0	2.0
1091_0000014	0.0	1.0
1091_0000024	1.0	1.0
1091_0000026	1.0	1.0
1091_0000028	0.0	1.0
1091_0000034	1.0	1.0
1091_0000036	1.0	2.0
1091_0000037	1.0	1.0
1091_0000039	1.0	0.0
1091_0000041	1.0	1.0
1091_0000045	2.0	2.0
1091_0000056	2.0	2.0
1091_0000057	1.0	1.0
1091_0000058	2.0	2.0
1091_0000063	1.0	1.0
1091_0000065	2.0	1.0
1091_0000066	1.0	1.0
1091_0000067	2.0	1.0
1091_0000069	1.0	1.0
1091_0000072	2.0	2.0
1091_0000074	2.0	2.0
1091_0000086	1.0	2.0
1091_0000101	1.0	1.0
1091_0000114	2.0	2.0
1091_0000161	2.0	2.0
1091_0000166	2.0	1.0
1091_0000173	2.0	1.0
1091_0000174	1.0	0.0
1091_0000198	1.0	2.0
1091_0000202	2.0	2.0
1091_0000205	2.0	2.0
1091_0000219	2.0	2.0
1091_0000226	1.0	1.0
1091_0000228	2.0	2.0
1091_0000242	2.0	1.0
1091_0000244	2.0	2.0
1091_0000261	2.0	2.0
1091_0000268	2.0	2.0
0603	2.0	2.0
0605	2.0	2.0
0606	2.0	2.0
0607	2.0	2.0
0609	1.0	2.0
0612	1.0	1.0
0617	1.0	2.0
0628	2.0	2.0
0629	2.0	2.0
0638	1.0	2.0
0639	1.0	2.0
0721	2.0	2.0
0722	2.0	2.0
0725	1.0	2.0
0803	1.0	2.0
0810	1.0	2.0
0916	1.0	1.0
0917	1.0	2.0
0918	1.0	2.0
0926	2.0	2.0
1004	1.0	1.0
1111	1.0	1.0
1114	2.0	2.0
BER0611003	2.0	3.0
BER0611006	2.0	3.0
LIB0611002A	1.0	1.0
LIB0611011	1.0	2.0
MOS0611014	1.0	2.0
PAR1011017	3.0	2.0
PHA0111002B	2.0	2.0
PHA0111003B	1.0	1.0
PHA0111004A	1.0	1.0
PHA0111011	2.0	2.0
PHA0112007B	1.0	1.0
PHA0112012A	1.0	2.0
PHA0209024	2.0	2.0
PHA0209039	2.0	3.0
PHA0210001	1.0	1.0
PHA0210004	1.0	1.0
PHA0210008	1.0	1.0
PHA0411009A	1.0	2.0
PHA0411027	2.0	2.0
PHA0411028	2.0	2.0
PHA0411029	2.0	2.0
PHA0411041	3.0	3.0
PHA0411051	3.0	3.0
PHA0411053	3.0	3.0
PHA0411058	3.0	3.0
PHA0411059	3.0	3.0
PHA0509013	1.0	1.0
PHA0509015	3.0	2.0
PHA0509032	2.0	3.0
PHA0509038	2.0	2.0
PHA0509041	2.0	3.0
PHA0510004B	1.0	1.0
PHA0510010A	1.0	1.0
PHA0510013A	1.0	1.0
PHA0510029	3.0	3.0
PHA0510032	3.0	3.0
PHA0510037	2.0	2.0
PHA0510048	2.0	2.0
PHA0610018	3.0	3.0
PHA0610019B	1.0	1.0
PHA0610025	3.0	3.0
PHA0610026	3.0	3.0
PHA0810001	3.0	3.0
PHA0810009	3.0	3.0
PHA1109001	1.0	1.0
PHA1109006	2.0	2.0
PHA1109008	1.0	1.0
PHA1109023	1.0	1.0
PHA1109025	1.0	1.0
PHA1109028	2.0	3.0
PHA1110002A	2.0	2.0
PHA1110003B	1.0	1.0
PHA1110013	2.0	3.0
PHA1110014	2.0	3.0
PHA1110017	2.0	2.0
PHA1110019	2.0	2.0
PHA1111001A	1.0	1.0
PHA1111002B	1.0	1.0
PHA1111009A	1.0	1.0
TI071122B	1.0	1.0
VAR0909003	2.0	2.0
VAR0909010	2.0	2.0
VAR0910007	2.0	3.0
VAR0910010	3.0	2.0
1325_1001009	2.0	2.0
1325_1001023	2.0	2.0
1325_1001039	2.0	2.0
1325_1001041	2.0	2.0
1325_1001053	1.0	2.0
1325_1001056	2.0	2.0
1325_1001062	2.0	2.0
1325_1001078	2.0	2.0
1325_1001082	2.0	2.0
1325_1001083	2.0	2.0
1325_1001087	2.0	2.0
1325_1001094	2.0	2.0
1325_1001099	2.0	2.0
1325_1001111	2.0	2.0
1325_1001122	2.0	2.0
1325_1001133	2.0	2.0
1325_1001136	2.0	2.0
1325_1001138	2.0	2.0
1325_1001143	2.0	2.0
1325_1001161	2.0	2.0
1325_1001168	2.0	2.0
1325_9000099	2.0	2.0
1325_9000104	2.0	2.0
1325_9000136	2.0	2.0
1325_9000139	2.0	2.0
1325_9000143	2.0	2.0
1325_9000185	2.0	2.0
1325_9000237	2.0	2.0
1325_9000239	2.0	2.0
1325_9000317	2.0	2.0
1325_9000320	2.0	2.0
1325_9000321	2.0	2.0
1325_9000503	2.0	2.0
1325_9000612	1.0	2.0
1325_9000674	2.0	2.0
1325_9000677	2.0	2.0
1325_9000700	2.0	2.0
1365_0100006	2.0	2.0
1365_0100013	2.0	2.0
1365_0100019	1.0	1.0
1365_0100020	2.0	2.0
1365_0100030	1.0	2.0
1365_0100056	2.0	2.0
1365_0100092	2.0	2.0
1365_0100097	2.0	2.0
1365_0100100	2.0	2.0
1365_0100101	2.0	2.0
1365_0100103	2.0	2.0
1365_0100119	2.0	2.0
1365_0100120	2.0	2.0
1365_0100121	2.0	2.0
1365_0100125	2.0	2.0
1365_0100164	2.0	2.0
1365_0100176	2.0	2.0
1365_0100186	2.0	2.0
1365_0100190	2.0	2.0
1365_0100192	2.0	2.0
1365_0100194	2.0	2.0
1365_0100198	1.0	2.0
1365_0100199	2.0	2.0
1365_0100200	2.0	2.0
1365_0100211	2.0	2.0
1365_0100212	2.0	2.0
1365_0100218	2.0	2.0
1365_0100228	1.0	2.0
1365_0100231	2.0	2.0
1365_0100232	2.0	2.0
1365_0100233	2.0	2.0
1365_0100258	2.0	2.0
1365_0100261	2.0	2.0
1365_0100448	1.0	2.0
1365_0100475	2.0	2.0
1365_0100478	2.0	2.0
1385_0000011	0.0	1.0
1385_0000022	1.0	1.0
1385_0000034	1.0	1.0
1385_0000047	1.0	1.0
1385_0000048	1.0	1.0
1385_0000050	1.0	1.0
1385_0000058	1.0	1.0
1385_0000095	1.0	0.0
1385_0000119	1.0	1.0
1385_0000124	1.0	1.0
1385_0000129	1.0	1.0
1385_0001107	1.0	1.0
1385_0001119	1.0	1.0
1385_0001125	1.0	1.0
1385_0001133	1.0	1.0
1385_0001135	1.0	1.0
1385_0001147	1.0	1.0
1385_0001148	1.0	1.0
1385_0001170	1.0	1.0
1385_0001172	1.0	1.0
1385_0001175	1.0	1.0
1385_0001188	1.0	1.0
1385_0001189	1.0	1.0
1385_0001192	1.0	1.0
1385_0001193	2.0	1.0
1385_0001199	1.0	1.0
1385_0001501	1.0	1.0
1385_0001503	1.0	1.0
1385_0001525	1.0	1.0
1385_0001724	2.0	1.0
1385_0001729	1.0	2.0
1385_0001732	1.0	1.0
1385_0001740	1.0	1.0
1385_0001741	0.0	1.0
1385_0001748	1.0	1.0
1385_0001752	1.0	1.0
1385_0001756	1.0	1.0
1385_0001757	2.0	1.0
1385_0001767	1.0	1.0
1385_0001788	1.0	1.0
1385_0001798	1.0	1.0
1385_0001799	1.0	1.0
1385_0001800	1.0	1.0
1395_0000341	1.0	1.0
1395_0000356	1.0	1.0
1395_0000368	0.0	0.0
1395_0000392	1.0	1.0
1395_0000409	2.0	2.0
1395_0000414	1.0	1.0
1395_0000455	1.0	2.0
1395_0000469	1.0	1.0
1395_0000516	1.0	1.0
1395_0000525	2.0	1.0
1395_0000527	1.0	1.0
1395_0000529	1.0	1.0
1395_0000533	2.0	2.0
1395_0000537	1.0	2.0
1395_0000553	1.0	1.0
1395_0000560	1.0	1.0
1395_0000581	1.0	1.0
1395_0000593	1.0	1.0
1395_0000595	0.0	1.0
1395_0000596	2.0	1.0
1395_0000598	1.0	1.0
1395_0000599	1.0	1.0
1395_0000602	1.0	1.0
1395_0000612	1.0	1.0
1395_0000628	1.0	1.0
1395_0001022	1.0	1.0
1395_0001028	1.0	1.0
1395_0001061	2.0	2.0
1395_0001068	1.0	1.0
1395_0001071	1.0	1.0
1395_0001076	1.0	2.0
1395_0001093	1.0	1.0
1395_0001108	1.0	1.0
1395_0001116	1.0	1.0
1395_0001120	1.0	1.0
1395_0001131	1.0	1.0
1395_0001147	1.0	1.0
1395_0001169	1.0	2.0
Averaged weighted F1-scores 0.748398192363896
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
144.90552995391704 65.35717405024758
LANGUAGE: IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']
1 Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 1.07
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.60      0.09      0.16        33
         1.0       0.54      0.39      0.46       142
         2.0       0.50      0.68      0.58       176
         3.0       0.58      0.68      0.62        90
         4.0       0.00      0.00      0.00        11

    accuracy                           0.53       452
   macro avg       0.44      0.37      0.36       452
weighted avg       0.52      0.53      0.50       452

[[  3  25   5   0   0]
 [  1  56  84   1   0]
 [  1  23 119  33   0]
 [  0   0  29  61   0]
 [  0   0   0  11   0]]
0.5028873225590247
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 48
Elapsed time 60

  Average training loss: 0.88
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.96
              precision    recall  f1-score   support

         0.0       0.40      0.12      0.19        33
         1.0       0.58      0.64      0.61       142
         2.0       0.56      0.53      0.54       176
         3.0       0.56      0.72      0.63        90
         4.0       0.00      0.00      0.00        11

    accuracy                           0.56       452
   macro avg       0.42      0.40      0.39       452
weighted avg       0.54      0.56      0.54       452

[[ 4 25  4  0  0]
 [ 5 91 46  0  0]
 [ 1 40 94 41  0]
 [ 0  0 25 65  0]
 [ 0  0  0 11  0]]
0.542684408419762
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 48
Elapsed time 60

  Average training loss: 0.73
  Training epoch took: 68
Running Validation...
  Average evaluation loss: 1.03
              precision    recall  f1-score   support

         0.0       0.46      0.18      0.26        33
         1.0       0.56      0.43      0.49       142
         2.0       0.52      0.67      0.59       176
         3.0       0.61      0.70      0.65        90
         4.0       0.00      0.00      0.00        11

    accuracy                           0.55       452
   macro avg       0.43      0.40      0.40       452
weighted avg       0.53      0.55      0.53       452

[[  6  21   6   0   0]
 [  6  61  75   0   0]
 [  1  27 118  30   0]
 [  0   0  27  63   0]
 [  0   0   0  11   0]]
0.5296584232365018
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 48
Elapsed time 60

  Average training loss: 0.56
  Training epoch took: 68
Running Validation...
  Average evaluation loss: 1.13
              precision    recall  f1-score   support

         0.0       0.58      0.21      0.31        33
         1.0       0.55      0.46      0.50       142
         2.0       0.52      0.56      0.54       176
         3.0       0.56      0.82      0.67        90
         4.0       0.00      0.00      0.00        11

    accuracy                           0.54       452
   macro avg       0.44      0.41      0.41       452
weighted avg       0.53      0.54      0.53       452

[[ 7 21  5  0  0]
 [ 4 66 69  3  0]
 [ 1 33 99 43  0]
 [ 0  0 16 74  0]
 [ 0  0  0 11  0]]
0.525562206227656
452 452 452
Filename	True Label	Prediction
1023_0101694	3.0	3.0
1023_0101700	2.0	3.0
1023_0101701	2.0	3.0
1023_0101843	3.0	3.0
1023_0101847	3.0	3.0
1023_0101856	2.0	3.0
1023_0101893	3.0	3.0
1023_0101898	4.0	3.0
1023_0102118	3.0	3.0
1023_0103823	3.0	3.0
1023_0103826	3.0	3.0
1023_0103830	3.0	3.0
1023_0103831	3.0	3.0
1023_0103832	2.0	2.0
1023_0103837	3.0	3.0
1023_0103844	4.0	3.0
1023_0104206	3.0	2.0
1023_0104209	3.0	3.0
1023_0107672	2.0	3.0
1023_0107740	3.0	3.0
1023_0107773	2.0	3.0
1023_0107783	3.0	3.0
1023_0108426	2.0	3.0
1023_0108518	3.0	3.0
1023_0108751	2.0	2.0
1023_0108752	3.0	3.0
1023_0108815	2.0	3.0
1023_0108886	3.0	3.0
1023_0108934	2.0	3.0
1023_0109027	2.0	2.0
1023_0109038	3.0	3.0
1023_0109391	2.0	2.0
1023_0109401	2.0	2.0
1023_0109518	2.0	2.0
1023_0109522	3.0	3.0
1023_0109606	2.0	3.0
1023_0109674	2.0	3.0
1023_0109914	2.0	2.0
1023_0109915	2.0	2.0
1031_0001703	4.0	3.0
1031_0002002	2.0	3.0
1031_0002003	2.0	3.0
1031_0002011	3.0	3.0
1031_0002042	3.0	3.0
1031_0002043	3.0	3.0
1031_0002092	4.0	3.0
1031_0002196	4.0	3.0
1031_0002198	3.0	3.0
1031_0003023	3.0	3.0
1031_0003065	3.0	3.0
1031_0003073	4.0	3.0
1031_0003077	3.0	3.0
1031_0003092	2.0	3.0
1031_0003097	4.0	3.0
1031_0003133	4.0	3.0
1031_0003145	3.0	3.0
1031_0003155	3.0	3.0
1031_0003157	4.0	3.0
1031_0003162	3.0	3.0
1031_0003164	3.0	3.0
1031_0003173	3.0	3.0
1031_0003183	4.0	3.0
1031_0003207	4.0	3.0
1031_0003214	3.0	3.0
1031_0003220	2.0	3.0
1031_0003221	2.0	3.0
1031_0003230	2.0	3.0
1031_0003237	3.0	3.0
1031_0003272	3.0	2.0
1031_0003273	3.0	3.0
1031_0003274	3.0	3.0
1031_0003315	3.0	3.0
1031_0003327	2.0	3.0
1031_0003352	2.0	2.0
1031_0003359	3.0	3.0
1031_0003365	3.0	3.0
1031_0003389	3.0	3.0
1031_0003391	2.0	3.0
1031_0003410	3.0	3.0
1031_0003419	3.0	3.0
1061_0012029	3.0	2.0
1061_0120288	1.0	2.0
1061_0120296	1.0	1.0
1061_0120303	0.0	1.0
1061_0120307	2.0	2.0
1061_0120309	1.0	1.0
1061_0120310	2.0	2.0
1061_0120313	1.0	2.0
1061_0120314	1.0	2.0
1061_0120315	2.0	1.0
1061_0120330	2.0	2.0
1061_0120334	2.0	2.0
1061_0120352	1.0	1.0
1061_0120358	1.0	1.0
1061_0120371	3.0	3.0
1061_0120372	1.0	2.0
1061_0120374	2.0	2.0
1061_0120407	3.0	2.0
1061_0120408	2.0	2.0
1061_0120410	2.0	2.0
1061_0120413	1.0	1.0
1061_0120421	2.0	2.0
1061_0120424	2.0	2.0
1061_0120426	1.0	2.0
1061_0120431	2.0	2.0
1061_0120439	2.0	1.0
1061_0120453	2.0	2.0
1061_0120485	3.0	2.0
1061_0120489	2.0	2.0
1061_0120490	2.0	2.0
1061_0120492	2.0	2.0
1061_0120875	2.0	2.0
1061_0120880	2.0	2.0
1061_1202910	2.0	2.0
1061_1202911	0.0	2.0
1061_1202912	2.0	2.0
1061_1202918	1.0	2.0
1071_0024681	1.0	2.0
1071_0024687	0.0	1.0
1071_0024690	1.0	2.0
1071_0024692	2.0	2.0
1071_0024701	2.0	2.0
1071_0024714	2.0	1.0
1071_0024715	2.0	1.0
1071_0024757	2.0	1.0
1071_0024759	0.0	1.0
1071_0024772	0.0	0.0
1071_0024774	0.0	0.0
1071_0024775	0.0	0.0
1071_0024777	1.0	1.0
1071_0024779	1.0	1.0
1071_0024800	1.0	1.0
1071_0024802	2.0	2.0
1071_0024803	1.0	1.0
1071_0024809	0.0	1.0
1071_0024810	2.0	1.0
1071_0024815	0.0	1.0
1071_0024819	1.0	1.0
1071_0024822	0.0	1.0
1071_0024831	0.0	2.0
1071_0024837	0.0	0.0
1071_0024841	0.0	1.0
1071_0024843	0.0	1.0
1071_0024854	0.0	1.0
1071_0024859	1.0	1.0
1071_0024863	1.0	1.0
1071_0024871	1.0	1.0
1071_0024872	1.0	1.0
1071_0024874	1.0	1.0
1071_0024875	1.0	1.0
1071_0024878	1.0	1.0
1071_0241832	1.0	1.0
1071_0242012	1.0	2.0
1071_0242023	1.0	1.0
1071_0248304	1.0	1.0
1071_0248309	1.0	1.0
1071_0248324	0.0	0.0
1071_0248328	1.0	0.0
1071_0248333	2.0	1.0
1071_0248334	2.0	1.0
1071_0248344	1.0	0.0
1071_0248346	0.0	1.0
1071_0248348	2.0	1.0
1091_0000010	3.0	2.0
1091_0000028	1.0	1.0
1091_0000030	0.0	1.0
1091_0000036	1.0	2.0
1091_0000037	1.0	1.0
1091_0000039	1.0	0.0
1091_0000041	1.0	1.0
1091_0000042	1.0	0.0
1091_0000045	1.0	2.0
1091_0000046	2.0	1.0
1091_0000054	0.0	1.0
1091_0000055	1.0	2.0
1091_0000057	2.0	1.0
1091_0000067	2.0	1.0
1091_0000071	2.0	2.0
1091_0000077	2.0	0.0
1091_0000079	1.0	2.0
1091_0000087	2.0	2.0
1091_0000101	2.0	1.0
1091_0000114	1.0	2.0
1091_0000116	2.0	2.0
1091_0000125	2.0	2.0
1091_0000140	2.0	1.0
1091_0000154	1.0	2.0
1091_0000157	2.0	2.0
1091_0000159	2.0	2.0
1091_0000162	1.0	2.0
1091_0000163	1.0	1.0
1091_0000190	1.0	2.0
1091_0000193	2.0	1.0
1091_0000204	2.0	2.0
1091_0000209	2.0	1.0
1091_0000214	2.0	1.0
1091_0000217	2.0	1.0
1091_0000220	1.0	2.0
1091_0000221	2.0	1.0
1091_0000223	1.0	2.0
1091_0000227	0.0	2.0
1091_0000235	1.0	1.0
1091_0000236	2.0	2.0
1091_0000237	2.0	2.0
1091_0000243	1.0	2.0
1091_0000247	2.0	2.0
1091_0000250	1.0	2.0
1091_0000253	2.0	1.0
1091_0000261	1.0	2.0
1091_0000264	2.0	2.0
1091_0000267	1.0	2.0
1091_0000270	2.0	1.0
1091_0000275	2.0	1.0
0601	1.0	2.0
0608	1.0	2.0
0630	1.0	1.0
0635	1.0	2.0
0641	1.0	1.0
0723	2.0	2.0
0801	1.0	2.0
0810	2.0	2.0
0811	2.0	2.0
0816	2.0	2.0
0824	2.0	2.0
0901	2.0	2.0
0902	2.0	2.0
0911	1.0	2.0
0914	1.0	2.0
0915	2.0	2.0
0920	2.0	2.0
0924	1.0	1.0
0926	2.0	2.0
1001	1.0	2.0
1016	1.0	1.0
1017	1.0	2.0
1018	1.0	2.0
1115	1.0	2.0
1117	2.0	1.0
BER0611003	2.0	3.0
BER0611005	2.0	2.0
BER0611007	2.0	3.0
KYJ0611006A	1.0	2.0
KYJ0611009A	1.0	1.0
LIB0611002A	1.0	1.0
LIB0611004A	1.0	1.0
LIB0611004B	1.0	2.0
MOS0509004	1.0	1.0
PAR1011015	2.0	2.0
PHA0111001B	1.0	1.0
PHA0111003B	2.0	1.0
PHA0111004A	1.0	1.0
PHA0111004B	1.0	1.0
PHA0111012	1.0	2.0
PHA0112002A	2.0	1.0
PHA0112003A	1.0	1.0
PHA0112007A	1.0	1.0
PHA0112012B	1.0	2.0
PHA0209008	1.0	1.0
PHA0411011B	1.0	2.0
PHA0411028	2.0	1.0
PHA0411038	3.0	3.0
PHA0411041	3.0	3.0
PHA0411047	2.0	3.0
PHA0411051	3.0	3.0
PHA0411055	3.0	3.0
PHA0411056	3.0	3.0
PHA0411058	3.0	3.0
PHA0411062	3.0	3.0
PHA0509002	1.0	1.0
PHA0509013	1.0	1.0
PHA0509037	3.0	2.0
PHA0509042	3.0	3.0
PHA0510002A	2.0	1.0
PHA0510004B	0.0	1.0
PHA0510029	2.0	3.0
PHA0510034	3.0	3.0
PHA0610017	3.0	3.0
PHA0710012	3.0	2.0
PHA0710017	3.0	3.0
PHA0809009	2.0	2.0
PHA0810004	1.0	2.0
PHA0810015	3.0	3.0
PHA1109007	1.0	2.0
PHA1109008	1.0	1.0
PHA1109023	1.0	1.0
PHA1110003A	1.0	2.0
PHA1110003B	1.0	2.0
PHA1110017	1.0	2.0
PHA1111003B	1.0	1.0
PHA1111006B	1.0	1.0
PHA1111008B	1.0	1.0
ST071122B	1.0	1.0
VAR0909003	2.0	2.0
VAR0910004	3.0	3.0
VAR0910006	3.0	3.0
VAR0910009	3.0	3.0
VAR0910010	3.0	2.0
1325_1001012	2.0	3.0
1325_1001015	2.0	2.0
1325_1001028	2.0	3.0
1325_1001029	2.0	3.0
1325_1001054	2.0	2.0
1325_1001055	2.0	3.0
1325_1001058	2.0	2.0
1325_1001059	2.0	2.0
1325_1001077	2.0	2.0
1325_1001078	2.0	3.0
1325_1001084	2.0	2.0
1325_1001085	2.0	2.0
1325_1001099	3.0	3.0
1325_1001109	2.0	2.0
1325_1001122	2.0	2.0
1325_1001123	3.0	3.0
1325_1001131	3.0	2.0
1325_1001134	2.0	3.0
1325_1001138	2.0	3.0
1325_1001139	2.0	3.0
1325_1001144	3.0	3.0
1325_1001152	2.0	3.0
1325_1001154	3.0	3.0
1325_1001163	2.0	2.0
1325_1001166	2.0	2.0
1325_1001169	2.0	2.0
1325_9000087	2.0	2.0
1325_9000088	3.0	2.0
1325_9000089	2.0	2.0
1325_9000095	2.0	3.0
1325_9000107	2.0	3.0
1325_9000137	3.0	3.0
1325_9000139	2.0	2.0
1325_9000187	3.0	3.0
1325_9000209	2.0	3.0
1325_9000211	2.0	3.0
1325_9000278	3.0	3.0
1325_9000316	1.0	2.0
1325_9000321	3.0	3.0
1325_9000322	3.0	3.0
1325_9000504	3.0	3.0
1325_9000533	3.0	3.0
1325_9000611	2.0	3.0
1325_9000676	3.0	3.0
1325_9000678	3.0	3.0
1325_9000750	3.0	2.0
1365_0100003	1.0	2.0
1365_0100012	2.0	2.0
1365_0100014	2.0	3.0
1365_0100019	1.0	2.0
1365_0100024	1.0	3.0
1365_0100057	2.0	2.0
1365_0100063	3.0	2.0
1365_0100064	2.0	2.0
1365_0100069	1.0	2.0
1365_0100072	2.0	2.0
1365_0100093	2.0	2.0
1365_0100123	2.0	3.0
1365_0100125	3.0	3.0
1365_0100146	2.0	2.0
1365_0100163	3.0	3.0
1365_0100182	2.0	3.0
1365_0100184	2.0	2.0
1365_0100185	1.0	2.0
1365_0100192	3.0	3.0
1365_0100199	2.0	3.0
1365_0100200	3.0	2.0
1365_0100201	2.0	2.0
1365_0100204	2.0	2.0
1365_0100217	3.0	3.0
1365_0100228	1.0	2.0
1365_0100230	2.0	3.0
1365_0100251	2.0	3.0
1365_0100262	2.0	3.0
1365_0100263	3.0	3.0
1365_0100266	3.0	2.0
1365_0100287	2.0	2.0
1365_0100289	2.0	2.0
1365_0100448	1.0	2.0
1365_0100475	2.0	2.0
1365_0100477	1.0	3.0
1365_0100481	2.0	2.0
1385_0000011	0.0	1.0
1385_0000033	1.0	2.0
1385_0000038	1.0	2.0
1385_0000041	2.0	2.0
1385_0000059	2.0	2.0
1385_0000097	2.0	1.0
1385_0000098	1.0	1.0
1385_0000102	1.0	2.0
1385_0000124	2.0	2.0
1385_0001118	1.0	1.0
1385_0001127	2.0	1.0
1385_0001135	1.0	1.0
1385_0001138	1.0	1.0
1385_0001150	2.0	2.0
1385_0001159	1.0	2.0
1385_0001163	2.0	1.0
1385_0001164	1.0	2.0
1385_0001174	0.0	1.0
1385_0001178	0.0	0.0
1385_0001194	1.0	1.0
1385_0001197	1.0	2.0
1385_0001198	2.0	2.0
1385_0001523	1.0	2.0
1385_0001716	1.0	1.0
1385_0001717	2.0	2.0
1385_0001733	2.0	2.0
1385_0001734	1.0	2.0
1385_0001751	1.0	2.0
1385_0001754	1.0	1.0
1385_0001759	0.0	1.0
1385_0001761	0.0	1.0
1385_0001764	0.0	1.0
1385_0001771	0.0	2.0
1385_0001787	0.0	1.0
1385_0001794	0.0	2.0
1385_0001795	1.0	1.0
1385_0001796	2.0	2.0
1395_0000333	2.0	2.0
1395_0000357	3.0	2.0
1395_0000380	2.0	2.0
1395_0000399	1.0	1.0
1395_0000447	1.0	2.0
1395_0000469	1.0	1.0
1395_0000471	1.0	1.0
1395_0000504	1.0	1.0
1395_0000518	2.0	2.0
1395_0000525	2.0	1.0
1395_0000528	2.0	2.0
1395_0000531	2.0	1.0
1395_0000537	1.0	2.0
1395_0000554	2.0	2.0
1395_0000564	1.0	1.0
1395_0000582	0.0	0.0
1395_0000599	1.0	1.0
1395_0000608	1.0	2.0
1395_0000611	1.0	1.0
1395_0000639	1.0	2.0
1395_0000642	0.0	1.0
1395_0000644	1.0	2.0
1395_0001028	1.0	2.0
1395_0001045	2.0	1.0
1395_0001070	1.0	2.0
1395_0001073	2.0	2.0
1395_0001075	1.0	2.0
1395_0001101	1.0	1.0
1395_0001114	1.0	2.0
1395_0001122	1.0	1.0
1395_0001131	0.0	1.0
1395_0001132	1.0	3.0
1395_0001141	1.0	1.0
1395_0001160	2.0	2.0
1395_0001161	1.0	2.0
1395_0001169	2.0	2.0
2 Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 48
Elapsed time 59

  Average training loss: 1.11
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.98
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        33
         1.0       0.54      0.65      0.59       142
         2.0       0.59      0.55      0.56       176
         3.0       0.61      0.78      0.68        90
         4.0       0.00      0.00      0.00        11

    accuracy                           0.57       452
   macro avg       0.35      0.40      0.37       452
weighted avg       0.52      0.57      0.54       452

[[ 0 32  1  0  0]
 [ 0 93 48  1  0]
 [ 0 47 96 33  0]
 [ 0  1 19 70  0]
 [ 0  0  0 11  0]]
0.5413700641059359
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 48
Elapsed time 60

  Average training loss: 0.92
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.87
              precision    recall  f1-score   support

         0.0       0.57      0.12      0.20        33
         1.0       0.53      0.70      0.60       142
         2.0       0.58      0.57      0.57       176
         3.0       0.68      0.64      0.66        90
         4.0       0.00      0.00      0.00        11

    accuracy                           0.58       452
   macro avg       0.47      0.41      0.41       452
weighted avg       0.57      0.58      0.56       452

[[  4  29   0   0   0]
 [  2  99  41   0   0]
 [  1  59 100  16   0]
 [  0   1  31  58   0]
 [  0   0   0  11   0]]
0.5588640888152636
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 48
Elapsed time 60

  Average training loss: 0.77
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.89
              precision    recall  f1-score   support

         0.0       0.78      0.21      0.33        33
         1.0       0.57      0.61      0.59       142
         2.0       0.60      0.61      0.60       176
         3.0       0.65      0.81      0.72        90
         4.0       0.00      0.00      0.00        11

    accuracy                           0.60       452
   macro avg       0.52      0.45      0.45       452
weighted avg       0.60      0.60      0.59       452

[[  7  25   1   0   0]
 [  2  86  54   0   0]
 [  0  41 107  28   0]
 [  0   0  17  73   0]
 [  0   0   0  11   0]]
0.5867701538282194
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 48
Elapsed time 60

  Average training loss: 0.61
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.01
              precision    recall  f1-score   support

         0.0       0.67      0.24      0.36        33
         1.0       0.55      0.48      0.51       142
         2.0       0.56      0.64      0.60       176
         3.0       0.64      0.82      0.72        90
         4.0       1.00      0.18      0.31        11

    accuracy                           0.58       452
   macro avg       0.68      0.47      0.50       452
weighted avg       0.59      0.58      0.57       452

[[  8  22   3   0   0]
 [  4  68  69   1   0]
 [  0  32 112  32   0]
 [  0   1  15  74   0]
 [  0   0   0   9   2]]
0.5703192063218714
452 452 452
Filename	True Label	Prediction
1023_0001420	3.0	3.0
1023_0101690	2.0	2.0
1023_0101751	3.0	3.0
1023_0101841	2.0	3.0
1023_0101848	2.0	2.0
1023_0101853	2.0	3.0
1023_0101901	3.0	3.0
1023_0101906	2.0	3.0
1023_0101907	3.0	3.0
1023_0103834	4.0	3.0
1023_0107075	2.0	3.0
1023_0107244	2.0	3.0
1023_0107729	3.0	3.0
1023_0107788	2.0	3.0
1023_0108305	3.0	3.0
1023_0108510	2.0	3.0
1023_0108520	2.0	2.0
1023_0108811	3.0	2.0
1023_0108813	3.0	2.0
1023_0108889	3.0	3.0
1023_0108932	2.0	3.0
1023_0108935	2.0	3.0
1023_0109029	1.0	2.0
1023_0109030	3.0	3.0
1023_0109151	3.0	3.0
1023_0109250	2.0	3.0
1023_0109402	2.0	3.0
1023_0109496	3.0	3.0
1023_0109505	2.0	3.0
1023_0109524	3.0	3.0
1023_0109591	3.0	3.0
1023_0109614	2.0	2.0
1023_0109717	3.0	3.0
1023_0109880	3.0	3.0
1023_0109947	2.0	3.0
1031_0001998	4.0	3.0
1031_0002040	4.0	3.0
1031_0002079	4.0	3.0
1031_0002091	3.0	3.0
1031_0002131	3.0	3.0
1031_0002185	3.0	3.0
1031_0002195	3.0	3.0
1031_0002197	4.0	3.0
1031_0003013	4.0	4.0
1031_0003035	3.0	3.0
1031_0003042	3.0	3.0
1031_0003052	3.0	3.0
1031_0003071	3.0	3.0
1031_0003072	3.0	3.0
1031_0003085	3.0	3.0
1031_0003090	3.0	3.0
1031_0003126	3.0	3.0
1031_0003131	3.0	3.0
1031_0003135	3.0	3.0
1031_0003136	3.0	3.0
1031_0003140	3.0	3.0
1031_0003146	4.0	3.0
1031_0003154	3.0	3.0
1031_0003156	3.0	3.0
1031_0003161	3.0	3.0
1031_0003166	2.0	3.0
1031_0003169	3.0	3.0
1031_0003181	3.0	3.0
1031_0003184	4.0	3.0
1031_0003189	3.0	3.0
1031_0003191	3.0	3.0
1031_0003234	3.0	2.0
1031_0003242	3.0	3.0
1031_0003245	3.0	3.0
1031_0003261	3.0	3.0
1031_0003262	3.0	3.0
1031_0003338	3.0	3.0
1031_0003354	3.0	3.0
1031_0003355	3.0	3.0
1031_0003356	3.0	3.0
1031_0003357	3.0	3.0
1031_0003390	3.0	3.0
1031_0003392	3.0	3.0
1031_0003407	3.0	3.0
1031_0003414	3.0	3.0
1031_0003415	4.0	4.0
1061_0120271	2.0	2.0
1061_0120273	2.0	2.0
1061_0120274	1.0	1.0
1061_0120278	1.0	2.0
1061_0120284	0.0	1.0
1061_0120311	3.0	2.0
1061_0120316	2.0	1.0
1061_0120317	2.0	2.0
1061_0120326	2.0	2.0
1061_0120327	2.0	2.0
1061_0120331	1.0	2.0
1061_0120336	1.0	2.0
1061_0120350	2.0	2.0
1061_0120351	2.0	2.0
1061_0120357	3.0	2.0
1061_0120359	1.0	2.0
1061_0120367	2.0	2.0
1061_0120368	2.0	2.0
1061_0120375	2.0	1.0
1061_0120387	1.0	2.0
1061_0120389	2.0	2.0
1061_0120390	2.0	2.0
1061_0120394	2.0	2.0
1061_0120406	2.0	2.0
1061_0120409	2.0	2.0
1061_0120411	3.0	3.0
1061_0120414	2.0	2.0
1061_0120425	2.0	2.0
1061_0120450	2.0	2.0
1061_0120455	2.0	1.0
1061_0120456	2.0	2.0
1061_0120480	2.0	2.0
1061_0120481	2.0	3.0
1061_0120486	2.0	2.0
1061_0120493	1.0	2.0
1061_0120499	2.0	2.0
1061_0120500	1.0	2.0
1061_0120856	1.0	2.0
1061_0120877	2.0	2.0
1061_0120887	1.0	1.0
1061_1029111	2.0	2.0
1061_1029115	2.0	1.0
1061_1202914	1.0	2.0
1061_1202919	2.0	1.0
1071_0024694	1.0	2.0
1071_0024704	1.0	1.0
1071_0024706	1.0	1.0
1071_0024708	1.0	1.0
1071_0024710	1.0	2.0
1071_0024716	1.0	1.0
1071_0024766	1.0	1.0
1071_0024770	1.0	1.0
1071_0024783	0.0	0.0
1071_0024784	1.0	0.0
1071_0024812	1.0	0.0
1071_0024821	1.0	0.0
1071_0024823	1.0	1.0
1071_0024825	0.0	0.0
1071_0024835	0.0	1.0
1071_0024840	1.0	1.0
1071_0024844	1.0	1.0
1071_0024846	0.0	0.0
1071_0024851	2.0	1.0
1071_0024855	1.0	1.0
1071_0024857	0.0	1.0
1071_0024860	1.0	1.0
1071_0024862	1.0	2.0
1071_0024873	0.0	1.0
1071_0241831	1.0	2.0
1071_0242013	1.0	2.0
1071_0242021	1.0	2.0
1071_0242022	0.0	1.0
1071_0242072	0.0	0.0
1071_0242073	1.0	1.0
1071_0242091	1.0	1.0
1071_0243501	2.0	2.0
1071_0243581	0.0	1.0
1071_0243593	1.0	1.0
1071_0243623	1.0	1.0
1071_0248301	2.0	1.0
1071_0248314	1.0	1.0
1071_0248321	2.0	1.0
1071_0248326	1.0	1.0
1071_0248330	2.0	1.0
1071_0248338	1.0	1.0
1071_0248341	0.0	1.0
1071_0248343	2.0	1.0
1071_0248347	1.0	1.0
1091_0000001	1.0	1.0
1091_0000003	2.0	1.0
1091_0000004	1.0	1.0
1091_0000013	1.0	1.0
1091_0000014	0.0	1.0
1091_0000017	2.0	2.0
1091_0000025	1.0	1.0
1091_0000029	2.0	1.0
1091_0000032	1.0	2.0
1091_0000033	1.0	1.0
1091_0000044	0.0	1.0
1091_0000053	0.0	2.0
1091_0000061	2.0	1.0
1091_0000062	2.0	1.0
1091_0000063	1.0	1.0
1091_0000064	1.0	1.0
1091_0000065	2.0	1.0
1091_0000066	2.0	1.0
1091_0000069	2.0	1.0
1091_0000086	1.0	2.0
1091_0000095	1.0	2.0
1091_0000127	2.0	1.0
1091_0000148	1.0	1.0
1091_0000152	1.0	1.0
1091_0000158	2.0	2.0
1091_0000165	1.0	2.0
1091_0000166	1.0	2.0
1091_0000191	1.0	2.0
1091_0000192	1.0	2.0
1091_0000194	1.0	2.0
1091_0000205	1.0	2.0
1091_0000207	1.0	2.0
1091_0000208	1.0	2.0
1091_0000210	2.0	2.0
1091_0000219	1.0	2.0
1091_0000230	2.0	2.0
1091_0000231	1.0	2.0
1091_0000251	2.0	2.0
1091_0000257	1.0	2.0
1091_0000258	2.0	2.0
1091_0000266	2.0	2.0
0602	2.0	2.0
0603	2.0	2.0
0606	1.0	2.0
0607	2.0	2.0
0610	2.0	2.0
0618	1.0	2.0
0619	2.0	2.0
0627	2.0	2.0
0629	2.0	2.0
0638	2.0	2.0
0719	2.0	2.0
0721	2.0	2.0
0804	1.0	2.0
0817	1.0	2.0
0822	1.0	2.0
0825	1.0	2.0
0910	1.0	2.0
0921	1.0	2.0
0922	1.0	2.0
1005	1.0	1.0
1008	2.0	2.0
1112	1.0	2.0
1114	2.0	2.0
1116	1.0	2.0
BER0609003	2.0	2.0
KYJ0611004A	1.0	2.0
KYJ0611006B	1.0	1.0
LON0610002A	2.0	1.0
MOS0611012	2.0	2.0
MOS0611013	2.0	3.0
PAR1011009B	1.0	1.0
PAR1011013	2.0	3.0
PAR1011016	3.0	3.0
PHA0111002A	2.0	1.0
PHA0111016	3.0	3.0
PHA0209024	1.0	1.0
PHA0209034	2.0	3.0
PHA0209038	4.0	3.0
PHA0210004	1.0	1.0
PHA0210007	1.0	1.0
PHA0411008A	1.0	1.0
PHA0411008B	1.0	1.0
PHA0411011A	1.0	1.0
PHA0411012A	1.0	1.0
PHA0411029	2.0	2.0
PHA0411031	3.0	3.0
PHA0411042	2.0	3.0
PHA0411054	3.0	1.0
PHA0509007	1.0	1.0
PHA0509015	3.0	2.0
PHA0509020	3.0	3.0
PHA0509021	1.0	2.0
PHA0509025	3.0	3.0
PHA0509028	2.0	2.0
PHA0509031	1.0	3.0
PHA0509033	1.0	1.0
PHA0509038	1.0	2.0
PHA0510013B	1.0	1.0
PHA0510039	3.0	3.0
PHA0510050	2.0	3.0
PHA0610006A	1.0	1.0
PHA0610019A	2.0	1.0
PHA0610019B	2.0	1.0
PHA0710013	4.0	3.0
PHA0710019	3.0	3.0
PHA0710021	3.0	3.0
PHA0810006	2.0	2.0
PHA0810011	2.0	2.0
PHA0811010	2.0	2.0
PHA0811019	3.0	3.0
PHA0811020	1.0	2.0
PHA1109025	1.0	1.0
PHA1109028	3.0	3.0
PHA1110002B	2.0	1.0
PHA1110021	2.0	2.0
PHA1111004B	1.0	1.0
PHA1111006A	1.0	1.0
PHA1111009A	1.0	1.0
VAR0909004	2.0	2.0
VAR0909008	2.0	2.0
VAR0909010	1.0	2.0
VAR0910007	2.0	3.0
1325_1001008	2.0	2.0
1325_1001011	2.0	3.0
1325_1001016	2.0	2.0
1325_1001018	2.0	3.0
1325_1001019	2.0	3.0
1325_1001021	2.0	3.0
1325_1001023	2.0	2.0
1325_1001025	2.0	2.0
1325_1001033	3.0	3.0
1325_1001040	3.0	3.0
1325_1001042	2.0	2.0
1325_1001043	2.0	2.0
1325_1001045	2.0	2.0
1325_1001052	2.0	2.0
1325_1001080	2.0	3.0
1325_1001083	2.0	2.0
1325_1001086	2.0	2.0
1325_1001087	2.0	3.0
1325_1001101	3.0	3.0
1325_1001110	3.0	3.0
1325_1001121	2.0	2.0
1325_1001129	1.0	2.0
1325_1001153	2.0	2.0
1325_1001158	2.0	3.0
1325_1001159	3.0	3.0
1325_1001161	2.0	2.0
1325_1001170	3.0	3.0
1325_9000102	2.0	2.0
1325_9000104	2.0	3.0
1325_9000138	3.0	3.0
1325_9000140	3.0	3.0
1325_9000144	3.0	3.0
1325_9000279	3.0	3.0
1325_9000503	3.0	3.0
1325_9000601	3.0	3.0
1325_9000612	2.0	2.0
1325_9000674	2.0	3.0
1365_0100006	2.0	2.0
1365_0100011	1.0	2.0
1365_0100015	1.0	2.0
1365_0100016	2.0	3.0
1365_0100028	2.0	2.0
1365_0100030	1.0	2.0
1365_0100079	2.0	2.0
1365_0100080	2.0	2.0
1365_0100098	1.0	2.0
1365_0100105	3.0	3.0
1365_0100133	2.0	2.0
1365_0100137	2.0	2.0
1365_0100138	2.0	2.0
1365_0100139	2.0	2.0
1365_0100147	2.0	2.0
1365_0100167	1.0	2.0
1365_0100168	3.0	2.0
1365_0100172	2.0	2.0
1365_0100175	2.0	2.0
1365_0100181	1.0	2.0
1365_0100203	2.0	2.0
1365_0100205	3.0	2.0
1365_0100213	2.0	2.0
1365_0100218	3.0	2.0
1365_0100225	2.0	2.0
1365_0100226	2.0	2.0
1365_0100231	2.0	2.0
1365_0100233	3.0	2.0
1365_0100255	2.0	2.0
1365_0100256	2.0	2.0
1365_0100257	2.0	2.0
1365_0100259	2.0	2.0
1365_0100276	3.0	2.0
1365_0100286	1.0	2.0
1365_0100451	2.0	2.0
1365_0100456	2.0	3.0
1365_0100479	3.0	2.0
1365_0100482	2.0	2.0
1385_0000021	2.0	2.0
1385_0000034	2.0	1.0
1385_0000042	2.0	2.0
1385_0000043	2.0	2.0
1385_0000048	2.0	2.0
1385_0000050	2.0	2.0
1385_0000051	2.0	2.0
1385_0000057	1.0	1.0
1385_0000095	1.0	1.0
1385_0000119	2.0	2.0
1385_0000123	1.0	2.0
1385_0000126	2.0	1.0
1385_0000127	2.0	2.0
1385_0000129	2.0	1.0
1385_0001111	2.0	1.0
1385_0001120	1.0	1.0
1385_0001126	0.0	1.0
1385_0001128	0.0	2.0
1385_0001130	1.0	1.0
1385_0001132	1.0	1.0
1385_0001133	2.0	1.0
1385_0001136	1.0	1.0
1385_0001137	1.0	1.0
1385_0001158	1.0	2.0
1385_0001160	3.0	2.0
1385_0001173	0.0	1.0
1385_0001190	0.0	1.0
1385_0001191	1.0	1.0
1385_0001192	1.0	1.0
1385_0001503	1.0	2.0
1385_0001525	1.0	1.0
1385_0001526	0.0	0.0
1385_0001720	0.0	1.0
1385_0001725	1.0	1.0
1385_0001727	0.0	1.0
1385_0001739	0.0	2.0
1385_0001742	0.0	0.0
1385_0001746	0.0	1.0
1385_0001756	1.0	1.0
1385_0001760	1.0	1.0
1385_0001762	1.0	1.0
1385_0001765	0.0	1.0
1385_0001773	0.0	1.0
1385_0001775	0.0	1.0
1385_0001798	1.0	2.0
1395_0000353	1.0	1.0
1395_0000369	2.0	2.0
1395_0000383	2.0	1.0
1395_0000402	1.0	1.0
1395_0000403	2.0	2.0
1395_0000414	2.0	1.0
1395_0000443	2.0	2.0
1395_0000446	2.0	2.0
1395_0000448	1.0	1.0
1395_0000462	2.0	2.0
1395_0000512	2.0	2.0
1395_0000513	2.0	2.0
1395_0000514	3.0	2.0
1395_0000515	2.0	1.0
1395_0000529	2.0	1.0
1395_0000533	3.0	2.0
1395_0000549	2.0	2.0
1395_0000550	1.0	2.0
1395_0000555	1.0	2.0
1395_0000581	1.0	2.0
1395_0000583	1.0	2.0
1395_0000587	0.0	1.0
1395_0000606	1.0	0.0
1395_0000607	1.0	1.0
1395_0000628	0.0	1.0
1395_0000631	1.0	2.0
1395_0000636	0.0	1.0
1395_0001022	1.0	2.0
1395_0001040	0.0	0.0
1395_0001060	1.0	2.0
1395_0001065	1.0	2.0
1395_0001067	1.0	1.0
1395_0001069	2.0	2.0
1395_0001080	1.0	1.0
1395_0001115	1.0	2.0
1395_0001116	2.0	2.0
1395_0001118	0.0	1.0
1395_0001146	0.0	0.0
1395_0001167	1.0	2.0
1395_0001170	1.0	2.0
3 Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 48
Elapsed time 59

  Average training loss: 1.10
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.06
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        33
         1.0       0.47      0.21      0.29       142
         2.0       0.45      0.88      0.60       177
         3.0       0.60      0.30      0.40        90
         4.0       0.00      0.00      0.00        10

    accuracy                           0.47       452
   macro avg       0.30      0.28      0.26       452
weighted avg       0.44      0.47      0.40       452

[[  0  21  12   0   0]
 [  0  30 112   0   0]
 [  0  13 155   9   0]
 [  0   0  63  27   0]
 [  0   0   1   9   0]]
0.40459834971283554
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 48
Elapsed time 60

  Average training loss: 0.91
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.05
              precision    recall  f1-score   support

         0.0       1.00      0.03      0.06        33
         1.0       0.51      0.35      0.41       142
         2.0       0.50      0.67      0.57       177
         3.0       0.56      0.72      0.63        90
         4.0       0.00      0.00      0.00        10

    accuracy                           0.52       452
   macro avg       0.51      0.35      0.33       452
weighted avg       0.54      0.52      0.48       452

[[  1  30   2   0   0]
 [  0  49  93   0   0]
 [  0  17 118  42   0]
 [  0   1  24  65   0]
 [  0   0   0  10   0]]
0.48138807023950186
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.78
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.06
              precision    recall  f1-score   support

         0.0       0.52      0.33      0.41        33
         1.0       0.60      0.54      0.57       142
         2.0       0.54      0.52      0.53       177
         3.0       0.53      0.80      0.64        90
         4.0       0.00      0.00      0.00        10

    accuracy                           0.56       452
   macro avg       0.44      0.44      0.43       452
weighted avg       0.55      0.56      0.54       452

[[11 21  1  0  0]
 [ 6 76 60  0  0]
 [ 4 28 92 53  0]
 [ 0  2 16 72  0]
 [ 0  0  0 10  0]]
0.5429414386949462
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.63
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.06
              precision    recall  f1-score   support

         0.0       0.67      0.30      0.42        33
         1.0       0.62      0.56      0.59       142
         2.0       0.59      0.64      0.61       177
         3.0       0.59      0.76      0.66        90
         4.0       0.00      0.00      0.00        10

    accuracy                           0.60       452
   macro avg       0.49      0.45      0.46       452
weighted avg       0.59      0.60      0.59       452

[[ 10  22   1   0   0]
 [  3  80  58   1   0]
 [  2  26 113  36   0]
 [  0   2  20  68   0]
 [  0   0   0  10   0]]
0.5871531701391974
452 452 452
Filename	True Label	Prediction
1023_0101675	3.0	3.0
1023_0101684	2.0	3.0
1023_0101749	4.0	3.0
1023_0101852	3.0	3.0
1023_0101894	2.0	3.0
1023_0101896	2.0	2.0
1023_0101897	2.0	2.0
1023_0103824	3.0	3.0
1023_0103828	1.0	2.0
1023_0103829	2.0	3.0
1023_0103840	3.0	3.0
1023_0103883	2.0	3.0
1023_0104207	2.0	2.0
1023_0106816	3.0	3.0
1023_0107042	3.0	3.0
1023_0107074	3.0	3.0
1023_0107781	2.0	3.0
1023_0107784	1.0	2.0
1023_0107787	2.0	3.0
1023_0108304	3.0	3.0
1023_0108641	3.0	3.0
1023_0108753	2.0	2.0
1023_0108766	2.0	3.0
1023_0108810	3.0	3.0
1023_0108812	2.0	3.0
1023_0108885	2.0	2.0
1023_0108933	2.0	2.0
1023_0108955	3.0	3.0
1023_0109096	3.0	3.0
1023_0109248	2.0	3.0
1023_0109392	2.0	2.0
1023_0109399	2.0	2.0
1023_0109500	2.0	3.0
1023_0109515	3.0	3.0
1023_0109520	2.0	3.0
1023_0109588	3.0	3.0
1023_0109649	2.0	3.0
1023_0109651	3.0	3.0
1023_0109716	3.0	2.0
1023_0109721	2.0	2.0
1023_0109890	3.0	3.0
1023_0109917	3.0	3.0
1023_0109945	4.0	3.0
1023_0109954	3.0	3.0
1031_0001949	3.0	3.0
1031_0002061	3.0	3.0
1031_0002083	2.0	3.0
1031_0002086	3.0	3.0
1031_0002088	3.0	3.0
1031_0002187	3.0	3.0
1031_0003012	3.0	3.0
1031_0003043	4.0	3.0
1031_0003048	4.0	3.0
1031_0003053	3.0	3.0
1031_0003063	4.0	3.0
1031_0003099	3.0	3.0
1031_0003128	3.0	3.0
1031_0003129	3.0	3.0
1031_0003132	3.0	3.0
1031_0003144	3.0	3.0
1031_0003160	3.0	3.0
1031_0003180	3.0	3.0
1031_0003185	3.0	3.0
1031_0003186	3.0	3.0
1031_0003203	2.0	2.0
1031_0003212	2.0	3.0
1031_0003217	3.0	3.0
1031_0003219	3.0	3.0
1031_0003224	3.0	3.0
1031_0003225	3.0	3.0
1031_0003232	2.0	3.0
1031_0003233	2.0	2.0
1031_0003236	3.0	3.0
1031_0003239	4.0	3.0
1031_0003243	3.0	3.0
1031_0003310	3.0	3.0
1031_0003313	4.0	3.0
1031_0003314	3.0	3.0
1031_0003330	3.0	3.0
1031_0003353	3.0	3.0
1031_0003358	4.0	3.0
1031_0003368	3.0	3.0
1031_0003369	3.0	3.0
1031_0003386	2.0	3.0
1031_0003409	4.0	3.0
1061_0120277	1.0	2.0
1061_0120279	1.0	1.0
1061_0120280	1.0	1.0
1061_0120282	0.0	1.0
1061_0120283	0.0	1.0
1061_0120285	1.0	1.0
1061_0120298	1.0	1.0
1061_0120300	2.0	1.0
1061_0120319	2.0	2.0
1061_0120320	3.0	3.0
1061_0120321	2.0	2.0
1061_0120329	2.0	2.0
1061_0120341	1.0	1.0
1061_0120343	2.0	2.0
1061_0120345	2.0	2.0
1061_0120348	1.0	1.0
1061_0120353	1.0	1.0
1061_0120361	2.0	3.0
1061_0120373	2.0	3.0
1061_0120376	2.0	2.0
1061_0120388	1.0	2.0
1061_0120423	2.0	3.0
1061_0120432	1.0	2.0
1061_0120438	2.0	2.0
1061_0120440	1.0	1.0
1061_0120441	2.0	2.0
1061_0120449	3.0	2.0
1061_0120478	2.0	2.0
1061_0120479	2.0	2.0
1061_0120484	1.0	2.0
1061_0120488	2.0	2.0
1061_0120496	1.0	2.0
1061_0120497	2.0	3.0
1061_0120498	2.0	2.0
1061_0120855	1.0	1.0
1061_0120857	2.0	2.0
1061_0120859	2.0	2.0
1061_0120882	2.0	2.0
1061_0120883	1.0	2.0
1061_0120886	2.0	2.0
1061_1029113	2.0	1.0
1061_1029119	1.0	2.0
1061_1029120	1.0	2.0
1061_1202913	2.0	1.0
1061_1202916	2.0	2.0
1071_0024685	1.0	2.0
1071_0024688	1.0	1.0
1071_0024689	1.0	1.0
1071_0024691	1.0	2.0
1071_0024711	1.0	1.0
1071_0024713	1.0	1.0
1071_0024756	1.0	1.0
1071_0024758	2.0	2.0
1071_0024767	2.0	1.0
1071_0024781	0.0	1.0
1071_0024782	0.0	0.0
1071_0024797	0.0	1.0
1071_0024799	2.0	2.0
1071_0024807	0.0	1.0
1071_0024814	1.0	1.0
1071_0024818	2.0	0.0
1071_0024834	2.0	2.0
1071_0024836	1.0	1.0
1071_0024845	0.0	1.0
1071_0024852	0.0	0.0
1071_0024861	0.0	1.0
1071_0024864	0.0	0.0
1071_0024865	2.0	2.0
1071_0024867	1.0	1.0
1071_0024879	1.0	1.0
1071_0242041	1.0	1.0
1071_0242093	0.0	0.0
1071_0243502	1.0	1.0
1071_0243592	1.0	1.0
1071_0248305	0.0	1.0
1071_0248311	1.0	1.0
1071_0248320	0.0	0.0
1071_0248323	0.0	1.0
1071_0248332	2.0	2.0
1071_0248336	0.0	1.0
1071_0248339	2.0	1.0
1071_0248340	0.0	0.0
1071_0248345	1.0	1.0
1091_0000008	2.0	1.0
1091_0000009	0.0	1.0
1091_0000016	0.0	1.0
1091_0000019	1.0	1.0
1091_0000022	1.0	2.0
1091_0000023	2.0	1.0
1091_0000024	3.0	1.0
1091_0000026	1.0	1.0
1091_0000027	0.0	1.0
1091_0000047	2.0	1.0
1091_0000049	1.0	1.0
1091_0000051	1.0	1.0
1091_0000058	2.0	1.0
1091_0000072	1.0	2.0
1091_0000074	1.0	2.0
1091_0000076	2.0	2.0
1091_0000078	2.0	1.0
1091_0000113	1.0	2.0
1091_0000151	0.0	1.0
1091_0000155	1.0	3.0
1091_0000156	2.0	2.0
1091_0000160	2.0	3.0
1091_0000161	2.0	2.0
1091_0000164	1.0	1.0
1091_0000168	2.0	2.0
1091_0000169	3.0	1.0
1091_0000170	2.0	1.0
1091_0000172	2.0	1.0
1091_0000174	2.0	1.0
1091_0000197	1.0	2.0
1091_0000211	1.0	1.0
1091_0000212	1.0	2.0
1091_0000218	2.0	2.0
1091_0000225	2.0	1.0
1091_0000229	1.0	2.0
1091_0000234	3.0	2.0
1091_0000245	1.0	2.0
1091_0000246	2.0	1.0
1091_0000248	2.0	1.0
1091_0000263	3.0	2.0
1091_0000268	2.0	2.0
1091_0000269	1.0	2.0
1091_0000273	1.0	2.0
0604	2.0	2.0
0612	1.0	2.0
0614	1.0	2.0
0624	2.0	2.0
0625	2.0	2.0
0632	1.0	1.0
0639	1.0	2.0
0643	2.0	2.0
0644	1.0	2.0
0722	2.0	2.0
0803	2.0	2.0
0807	2.0	2.0
0808	1.0	2.0
0809	2.0	2.0
0813	1.0	1.0
0818	1.0	2.0
0820	1.0	1.0
0827	1.0	1.0
0904	1.0	1.0
0905	2.0	2.0
0907	2.0	2.0
0913	2.0	2.0
0919	1.0	1.0
0927	1.0	1.0
0930	2.0	2.0
1009	2.0	2.0
1019	1.0	2.0
1020	2.0	2.0
1113	1.0	2.0
9999	1.0	1.0
KYJ0611005B	1.0	2.0
KYJ0611009B	1.0	1.0
LIB0611003A	1.0	1.0
LIB0611011	1.0	2.0
LON0610002B	1.0	1.0
LON0611002A	1.0	1.0
LON0611002B	1.0	1.0
PHA0111005A	1.0	1.0
PHA0111010	3.0	3.0
PHA0111014	1.0	2.0
PHA0112009B	2.0	1.0
PHA0112012A	1.0	2.0
PHA0209031	4.0	3.0
PHA0210008	1.0	1.0
PHA0411009A	2.0	2.0
PHA0411034	1.0	1.0
PHA0411036	3.0	2.0
PHA0411045	3.0	2.0
PHA0411053	3.0	3.0
PHA0509017	2.0	3.0
PHA0509019	3.0	2.0
PHA0509026	3.0	3.0
PHA0509040	2.0	2.0
PHA0509044	2.0	3.0
PHA0510002B	2.0	1.0
PHA0510010B	0.0	1.0
PHA0510013A	2.0	1.0
PHA0510023	3.0	3.0
PHA0510027	1.0	2.0
PHA0510035	3.0	3.0
PHA0510036	3.0	3.0
PHA0510037	1.0	2.0
PHA0510049	3.0	3.0
PHA0610005B	0.0	1.0
PHA0610007A	1.0	1.0
PHA0610018	2.0	3.0
PHA0610025	2.0	3.0
PHA0709008	3.0	2.0
PHA0710011	3.0	3.0
PHA0710016	3.0	2.0
PHA0810008	3.0	3.0
PHA0810009	3.0	3.0
PHA1109002	3.0	3.0
PHA1109003	1.0	1.0
PHA1109004	3.0	3.0
PHA1110001B	1.0	1.0
PHA1110013	2.0	3.0
PHA1110016	1.0	2.0
PHA1110022	3.0	3.0
PHA1111001A	1.0	2.0
PHA1111002A	2.0	1.0
PHA1111002B	1.0	1.0
PHA1111004A	1.0	1.0
VAR0910005	3.0	2.0
1325_1001010	2.0	2.0
1325_1001020	2.0	2.0
1325_1001024	2.0	2.0
1325_1001027	3.0	2.0
1325_1001037	2.0	2.0
1325_1001041	3.0	3.0
1325_1001047	1.0	2.0
1325_1001048	1.0	2.0
1325_1001053	1.0	2.0
1325_1001062	2.0	3.0
1325_1001079	3.0	3.0
1325_1001088	2.0	2.0
1325_1001098	2.0	3.0
1325_1001111	3.0	3.0
1325_1001119	3.0	3.0
1325_1001156	2.0	2.0
1325_1001157	2.0	2.0
1325_1001164	2.0	2.0
1325_1001165	2.0	2.0
1325_9000090	2.0	3.0
1325_9000106	2.0	2.0
1325_9000188	2.0	3.0
1325_9000296	2.0	2.0
1325_9000303	3.0	2.0
1325_9000314	2.0	3.0
1325_9000319	2.0	2.0
1325_9000320	3.0	2.0
1325_9000677	3.0	3.0
1325_9000684	3.0	3.0
1325_9000700	2.0	3.0
1365_0100002	2.0	2.0
1365_0100007	1.0	1.0
1365_0100018	1.0	2.0
1365_0100020	2.0	2.0
1365_0100021	2.0	2.0
1365_0100022	2.0	2.0
1365_0100031	2.0	2.0
1365_0100051	1.0	1.0
1365_0100067	2.0	2.0
1365_0100070	2.0	2.0
1365_0100071	2.0	2.0
1365_0100073	2.0	2.0
1365_0100092	2.0	2.0
1365_0100096	2.0	2.0
1365_0100097	2.0	2.0
1365_0100100	2.0	2.0
1365_0100104	2.0	2.0
1365_0100106	2.0	2.0
1365_0100116	2.0	2.0
1365_0100117	3.0	3.0
1365_0100119	3.0	2.0
1365_0100135	2.0	2.0
1365_0100162	2.0	2.0
1365_0100170	1.0	2.0
1365_0100173	2.0	2.0
1365_0100176	2.0	2.0
1365_0100179	2.0	2.0
1365_0100183	1.0	2.0
1365_0100211	2.0	3.0
1365_0100220	2.0	2.0
1365_0100221	2.0	2.0
1365_0100232	3.0	3.0
1365_0100252	2.0	2.0
1365_0100268	2.0	2.0
1365_0100290	2.0	2.0
1365_0100299	3.0	2.0
1365_0100455	3.0	2.0
1365_0100458	2.0	3.0
1365_0100469	2.0	2.0
1365_0100474	2.0	2.0
1365_0100476	2.0	3.0
1365_0100478	2.0	3.0
1385_0000016	2.0	1.0
1385_0000036	1.0	2.0
1385_0000052	1.0	2.0
1385_0000104	2.0	1.0
1385_0000114	2.0	2.0
1385_0001103	2.0	0.0
1385_0001104	1.0	0.0
1385_0001105	2.0	1.0
1385_0001112	2.0	1.0
1385_0001113	1.0	1.0
1385_0001124	1.0	0.0
1385_0001134	1.0	0.0
1385_0001152	2.0	2.0
1385_0001153	3.0	2.0
1385_0001154	2.0	2.0
1385_0001169	1.0	1.0
1385_0001172	0.0	1.0
1385_0001196	1.0	1.0
1385_0001501	1.0	1.0
1385_0001524	0.0	1.0
1385_0001724	1.0	2.0
1385_0001726	1.0	1.0
1385_0001728	1.0	2.0
1385_0001729	1.0	2.0
1385_0001740	1.0	1.0
1385_0001748	1.0	2.0
1385_0001750	0.0	0.0
1385_0001757	2.0	1.0
1385_0001800	1.0	1.0
1395_0000337	0.0	0.0
1395_0000338	1.0	1.0
1395_0000340	2.0	2.0
1395_0000354	1.0	1.0
1395_0000359	2.0	2.0
1395_0000360	3.0	2.0
1395_0000361	2.0	2.0
1395_0000365	2.0	2.0
1395_0000366	2.0	2.0
1395_0000376	2.0	2.0
1395_0000379	1.0	1.0
1395_0000387	3.0	2.0
1395_0000389	0.0	0.0
1395_0000391	3.0	2.0
1395_0000392	2.0	2.0
1395_0000396	2.0	2.0
1395_0000398	2.0	2.0
1395_0000409	2.0	2.0
1395_0000452	1.0	1.0
1395_0000454	2.0	2.0
1395_0000455	1.0	2.0
1395_0000460	1.0	1.0
1395_0000465	1.0	1.0
1395_0000499	1.0	1.0
1395_0000500	1.0	1.0
1395_0000516	1.0	1.0
1395_0000548	2.0	2.0
1395_0000560	2.0	2.0
1395_0000565	1.0	1.0
1395_0000575	1.0	1.0
1395_0000584	0.0	1.0
1395_0000585	1.0	2.0
1395_0000591	0.0	0.0
1395_0000593	1.0	1.0
1395_0000597	1.0	2.0
1395_0000602	1.0	1.0
1395_0000610	2.0	1.0
1395_0000630	1.0	2.0
1395_0000646	1.0	1.0
1395_0001013	1.0	2.0
1395_0001015	1.0	1.0
1395_0001017	1.0	1.0
1395_0001019	0.0	1.0
1395_0001020	1.0	2.0
1395_0001058	1.0	1.0
1395_0001074	1.0	1.0
1395_0001103	1.0	1.0
1395_0001109	0.0	1.0
1395_0001117	1.0	1.0
1395_0001119	1.0	2.0
1395_0001121	0.0	2.0
1395_0001126	1.0	1.0
1395_0001133	0.0	1.0
1395_0001145	2.0	2.0
1395_0001147	1.0	2.0
1395_0001164	2.0	2.0
4 Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 1.08
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.93
              precision    recall  f1-score   support

         0.0       0.38      0.18      0.24        33
         1.0       0.54      0.58      0.56       142
         2.0       0.54      0.64      0.58       176
         3.0       0.61      0.49      0.55        91
         4.0       0.00      0.00      0.00        10

    accuracy                           0.54       452
   macro avg       0.41      0.38      0.39       452
weighted avg       0.53      0.54      0.53       452

[[  6  25   2   0   0]
 [ 10  83  48   1   0]
 [  0  46 112  18   0]
 [  0   1  45  45   0]
 [  0   0   0  10   0]]
0.5310170918252038
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.88
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.90
              precision    recall  f1-score   support

         0.0       0.60      0.09      0.16        33
         1.0       0.59      0.70      0.64       142
         2.0       0.58      0.64      0.61       176
         3.0       0.61      0.56      0.59        91
         4.0       0.00      0.00      0.00        10

    accuracy                           0.59       452
   macro avg       0.48      0.40      0.40       452
weighted avg       0.58      0.59      0.57       452

[[  3  28   2   0   0]
 [  2  99  40   1   0]
 [  0  42 113  21   0]
 [  0   0  40  51   0]
 [  0   0   0  10   0]]
0.5667553907122147
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.73
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.90
              precision    recall  f1-score   support

         0.0       0.41      0.21      0.28        33
         1.0       0.60      0.65      0.63       142
         2.0       0.63      0.56      0.59       176
         3.0       0.59      0.79      0.67        91
         4.0       0.00      0.00      0.00        10

    accuracy                           0.60       452
   macro avg       0.45      0.44      0.43       452
weighted avg       0.58      0.60      0.58       452

[[ 7 25  1  0  0]
 [ 9 93 39  1  0]
 [ 1 36 99 40  0]
 [ 0  0 19 72  0]
 [ 0  0  0 10  0]]
0.5841564246257968
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 48
Elapsed time 60

  Average training loss: 0.57
  Training epoch took: 68
Running Validation...
  Average evaluation loss: 0.97
              precision    recall  f1-score   support

         0.0       0.56      0.30      0.39        33
         1.0       0.66      0.65      0.65       142
         2.0       0.66      0.61      0.64       176
         3.0       0.59      0.85      0.69        91
         4.0       0.00      0.00      0.00        10

    accuracy                           0.63       452
   macro avg       0.49      0.48      0.48       452
weighted avg       0.62      0.63      0.62       452

[[ 10  21   2   0   0]
 [  8  92  40   2   0]
 [  0  26 108  42   0]
 [  0   0  14  77   0]
 [  0   0   0  10   0]]
0.6213744993546253
452 452 452
Filename	True Label	Prediction
1023_0001416	4.0	3.0
1023_0001419	3.0	3.0
1023_0001575	3.0	2.0
1023_0101689	1.0	1.0
1023_0101693	3.0	3.0
1023_0101695	2.0	2.0
1023_0101753	3.0	3.0
1023_0101846	4.0	3.0
1023_0101849	3.0	2.0
1023_0101851	3.0	3.0
1023_0101854	2.0	2.0
1023_0101855	2.0	2.0
1023_0101899	3.0	3.0
1023_0101900	3.0	3.0
1023_0101909	3.0	3.0
1023_0103821	3.0	3.0
1023_0103836	3.0	3.0
1023_0103839	3.0	3.0
1023_0107725	2.0	2.0
1023_0107726	2.0	3.0
1023_0107780	3.0	3.0
1023_0108307	3.0	3.0
1023_0108423	2.0	1.0
1023_0108649	3.0	3.0
1023_0108650	3.0	3.0
1023_0108814	3.0	3.0
1023_0108887	2.0	2.0
1023_0108888	3.0	3.0
1023_0108908	2.0	3.0
1023_0108931	3.0	3.0
1023_0108958	2.0	3.0
1023_0108992	3.0	3.0
1023_0108993	3.0	3.0
1023_0109026	2.0	2.0
1023_0109033	3.0	3.0
1023_0109192	2.0	3.0
1023_0109249	2.0	3.0
1023_0109395	2.0	2.0
1023_0109400	3.0	2.0
1023_0109422	3.0	3.0
1023_0109528	3.0	3.0
1023_0109590	3.0	2.0
1023_0109671	3.0	3.0
1023_0109946	2.0	3.0
1031_0001950	3.0	3.0
1031_0002010	2.0	3.0
1031_0002032	3.0	3.0
1031_0002084	3.0	3.0
1031_0002085	3.0	3.0
1031_0002087	3.0	3.0
1031_0002184	3.0	3.0
1031_0002199	3.0	3.0
1031_0002200	2.0	3.0
1031_0003029	3.0	3.0
1031_0003054	3.0	3.0
1031_0003074	3.0	3.0
1031_0003076	4.0	3.0
1031_0003098	4.0	3.0
1031_0003121	4.0	3.0
1031_0003127	4.0	3.0
1031_0003130	4.0	3.0
1031_0003149	3.0	3.0
1031_0003165	2.0	3.0
1031_0003167	3.0	3.0
1031_0003170	2.0	3.0
1031_0003174	4.0	3.0
1031_0003179	3.0	3.0
1031_0003190	3.0	3.0
1031_0003216	3.0	3.0
1031_0003218	3.0	3.0
1031_0003231	3.0	3.0
1031_0003235	3.0	3.0
1031_0003238	3.0	3.0
1031_0003240	2.0	3.0
1031_0003246	3.0	3.0
1031_0003249	3.0	3.0
1031_0003309	3.0	3.0
1031_0003331	2.0	3.0
1031_0003336	2.0	3.0
1031_0003337	3.0	3.0
1031_0003339	3.0	3.0
1031_0003367	4.0	3.0
1031_0003384	2.0	2.0
1031_0003393	3.0	3.0
1061_0120275	2.0	2.0
1061_0120287	1.0	1.0
1061_0120308	3.0	3.0
1061_0120312	1.0	1.0
1061_0120325	2.0	2.0
1061_0120328	1.0	1.0
1061_0120332	1.0	2.0
1061_0120333	3.0	2.0
1061_0120335	3.0	3.0
1061_0120337	2.0	2.0
1061_0120338	2.0	2.0
1061_0120346	2.0	2.0
1061_0120355	1.0	1.0
1061_0120366	3.0	2.0
1061_0120369	1.0	1.0
1061_0120370	2.0	2.0
1061_0120383	2.0	3.0
1061_0120384	1.0	1.0
1061_0120386	0.0	1.0
1061_0120415	2.0	1.0
1061_0120427	1.0	2.0
1061_0120428	2.0	2.0
1061_0120429	2.0	2.0
1061_0120430	1.0	1.0
1061_0120457	3.0	2.0
1061_0120460	2.0	1.0
1061_0120482	1.0	2.0
1061_0120487	2.0	2.0
1061_0120491	2.0	2.0
1061_0120853	1.0	2.0
1061_0120884	1.0	1.0
1061_0120885	2.0	2.0
1061_0120888	1.0	2.0
1061_0120890	1.0	1.0
1061_0120894	2.0	2.0
1061_1029114	1.0	1.0
1061_1029116	1.0	2.0
1061_1029117	1.0	2.0
1061_1029118	1.0	1.0
1061_1202915	1.0	1.0
1071_0020001	1.0	1.0
1071_0024680	2.0	1.0
1071_0024683	0.0	1.0
1071_0024693	1.0	1.0
1071_0024699	1.0	2.0
1071_0024709	2.0	2.0
1071_0024769	0.0	1.0
1071_0024773	1.0	1.0
1071_0024776	0.0	0.0
1071_0024778	0.0	0.0
1071_0024804	1.0	1.0
1071_0024806	1.0	1.0
1071_0024808	0.0	1.0
1071_0024811	1.0	1.0
1071_0024813	0.0	1.0
1071_0024816	1.0	1.0
1071_0024820	0.0	0.0
1071_0024824	1.0	1.0
1071_0024833	1.0	1.0
1071_0024838	0.0	0.0
1071_0024847	1.0	2.0
1071_0024866	2.0	1.0
1071_0242043	0.0	1.0
1071_0242092	0.0	0.0
1071_0243622	1.0	0.0
1071_0248302	0.0	0.0
1071_0248303	0.0	1.0
1071_0248307	2.0	1.0
1071_0248310	0.0	1.0
1071_0248312	1.0	1.0
1071_0248319	0.0	0.0
1071_0248325	0.0	1.0
1071_0248327	0.0	1.0
1071_0248335	1.0	1.0
1071_0248349	1.0	0.0
1091_0000002	2.0	2.0
1091_0000005	2.0	2.0
1091_0000011	1.0	1.0
1091_0000012	1.0	1.0
1091_0000015	2.0	2.0
1091_0000018	2.0	2.0
1091_0000020	1.0	2.0
1091_0000021	1.0	2.0
1091_0000034	2.0	1.0
1091_0000038	1.0	1.0
1091_0000043	1.0	1.0
1091_0000059	1.0	2.0
1091_0000060	2.0	2.0
1091_0000075	2.0	1.0
1091_0000123	2.0	2.0
1091_0000145	1.0	1.0
1091_0000146	1.0	0.0
1091_0000167	2.0	2.0
1091_0000171	1.0	2.0
1091_0000185	2.0	1.0
1091_0000195	1.0	1.0
1091_0000199	2.0	2.0
1091_0000201	2.0	2.0
1091_0000202	1.0	1.0
1091_0000216	1.0	2.0
1091_0000222	2.0	1.0
1091_0000224	1.0	1.0
1091_0000226	1.0	1.0
1091_0000238	1.0	1.0
1091_0000244	2.0	2.0
1091_0000255	0.0	1.0
1091_0000260	1.0	2.0
1091_0000271	2.0	2.0
1091_0000272	1.0	2.0
1091_0000274	1.0	1.0
1091_0000276	2.0	1.0
0611	2.0	1.0
0613	1.0	1.0
0615	1.0	2.0
0616	1.0	1.0
0617	1.0	1.0
0620	1.0	2.0
0631	2.0	2.0
0633	2.0	2.0
0642	1.0	2.0
0645	2.0	2.0
0715	2.0	1.0
0716	2.0	2.0
0724	2.0	2.0
0802	2.0	2.0
0805	2.0	2.0
0806	1.0	2.0
0819	3.0	2.0
0823	2.0	2.0
0829	1.0	2.0
0903	1.0	2.0
0916	1.0	1.0
0923	2.0	2.0
0925	2.0	2.0
0928	1.0	2.0
0929	0.0	1.0
1003	1.0	1.0
1006	2.0	2.0
1007	2.0	2.0
1010	1.0	1.0
1014	2.0	2.0
KYJ0611003A	1.0	1.0
LIB0611001B	1.0	1.0
LON0611004B	1.0	1.0
MOS0611014	1.0	1.0
MOS0611015	2.0	3.0
PAR1011008A	1.0	1.0
PAR1011009A	2.0	2.0
PAR1011014	2.0	3.0
PAR1011018	3.0	3.0
PHA0111001A	1.0	1.0
PHA0111005B	2.0	1.0
PHA0111011	2.0	2.0
PHA0112002B	1.0	1.0
PHA0112003B	1.0	1.0
PHA0112006A	3.0	2.0
PHA0112007B	1.0	1.0
PHA0209026	2.0	3.0
PHA0209028	2.0	2.0
PHA0210001	1.0	1.0
PHA0411010A	0.0	1.0
PHA0411012B	1.0	1.0
PHA0411032	1.0	3.0
PHA0411033	2.0	2.0
PHA0411035	3.0	3.0
PHA0411037	2.0	3.0
PHA0411039	2.0	3.0
PHA0411060	2.0	3.0
PHA0411061	3.0	3.0
PHA0509022	4.0	3.0
PHA0509024	2.0	3.0
PHA0509027	1.0	2.0
PHA0509030	2.0	3.0
PHA0509032	2.0	3.0
PHA0509034	1.0	2.0
PHA0509039	3.0	3.0
PHA0509041	2.0	3.0
PHA0510003A	1.0	1.0
PHA0510003B	1.0	1.0
PHA0510004A	1.0	1.0
PHA0510010A	1.0	1.0
PHA0510030	2.0	3.0
PHA0510031	2.0	2.0
PHA0510032	3.0	3.0
PHA0510038	3.0	3.0
PHA0510040	2.0	3.0
PHA0510048	1.0	2.0
PHA0610005A	1.0	1.0
PHA0610006B	1.0	1.0
PHA0610007B	1.0	1.0
PHA0710010	2.0	3.0
PHA0710014	3.0	3.0
PHA0710015	2.0	3.0
PHA0810002	1.0	2.0
PHA0810012	3.0	3.0
PHA0811014	1.0	3.0
PHA1109006	2.0	2.0
PHA1110015	3.0	3.0
PHA1110019	2.0	3.0
VAR0910011	3.0	3.0
1325_1001009	2.0	2.0
1325_1001013	2.0	2.0
1325_1001014	3.0	2.0
1325_1001022	2.0	2.0
1325_1001035	3.0	3.0
1325_1001036	2.0	2.0
1325_1001039	3.0	3.0
1325_1001051	2.0	2.0
1325_1001057	2.0	2.0
1325_1001081	3.0	2.0
1325_1001082	2.0	2.0
1325_1001093	2.0	3.0
1325_1001094	2.0	3.0
1325_1001095	2.0	2.0
1325_1001096	2.0	2.0
1325_1001097	1.0	1.0
1325_1001100	2.0	3.0
1325_1001107	3.0	3.0
1325_1001108	3.0	3.0
1325_1001120	3.0	3.0
1325_1001126	2.0	2.0
1325_1001128	2.0	3.0
1325_1001130	2.0	2.0
1325_1001133	2.0	2.0
1325_1001135	2.0	2.0
1325_1001141	2.0	2.0
1325_1001167	3.0	3.0
1325_1001168	2.0	2.0
1325_9000059	2.0	3.0
1325_9000099	2.0	2.0
1325_9000105	1.0	2.0
1325_9000136	2.0	3.0
1325_9000210	2.0	2.0
1325_9000213	3.0	3.0
1325_9000239	3.0	3.0
1325_9000240	2.0	2.0
1325_9000302	2.0	2.0
1325_9000315	2.0	2.0
1325_9000317	3.0	3.0
1325_9000318	3.0	3.0
1325_9000505	2.0	2.0
1325_9000536	2.0	3.0
1325_9000602	3.0	3.0
1325_9000675	3.0	2.0
1365_0100017	2.0	2.0
1365_0100027	2.0	2.0
1365_0100029	1.0	1.0
1365_0100056	2.0	2.0
1365_0100058	2.0	3.0
1365_0100065	1.0	2.0
1365_0100066	1.0	2.0
1365_0100094	2.0	2.0
1365_0100099	2.0	2.0
1365_0100118	3.0	3.0
1365_0100120	3.0	3.0
1365_0100121	3.0	3.0
1365_0100134	2.0	2.0
1365_0100136	2.0	2.0
1365_0100145	2.0	3.0
1365_0100164	2.0	2.0
1365_0100178	2.0	2.0
1365_0100180	1.0	2.0
1365_0100186	2.0	2.0
1365_0100190	2.0	2.0
1365_0100191	2.0	2.0
1365_0100194	3.0	2.0
1365_0100195	1.0	2.0
1365_0100212	3.0	3.0
1365_0100215	2.0	2.0
1365_0100219	2.0	3.0
1365_0100223	2.0	3.0
1365_0100258	2.0	2.0
1365_0100260	2.0	2.0
1365_0100261	2.0	2.0
1365_0100265	3.0	3.0
1365_0100269	2.0	3.0
1365_0100270	2.0	2.0
1365_0100275	2.0	2.0
1365_0100281	1.0	2.0
1365_0100282	2.0	2.0
1365_0100285	2.0	2.0
1365_0100447	3.0	3.0
1365_0100457	2.0	3.0
1365_0100461	2.0	2.0
1365_0100480	2.0	2.0
1385_0000012	2.0	2.0
1385_0000020	2.0	1.0
1385_0000023	2.0	1.0
1385_0000035	2.0	2.0
1385_0000037	1.0	1.0
1385_0000040	1.0	0.0
1385_0000044	2.0	2.0
1385_0000049	2.0	1.0
1385_0000053	2.0	1.0
1385_0000054	2.0	2.0
1385_0000099	0.0	1.0
1385_0000100	1.0	0.0
1385_0000122	2.0	2.0
1385_0000128	1.0	1.0
1385_0001107	2.0	1.0
1385_0001109	2.0	1.0
1385_0001110	2.0	2.0
1385_0001119	1.0	1.0
1385_0001121	1.0	1.0
1385_0001122	1.0	1.0
1385_0001125	1.0	1.0
1385_0001129	1.0	1.0
1385_0001148	2.0	1.0
1385_0001151	2.0	2.0
1385_0001157	1.0	2.0
1385_0001161	2.0	2.0
1385_0001165	1.0	2.0
1385_0001166	0.0	1.0
1385_0001170	1.0	0.0
1385_0001171	1.0	0.0
1385_0001195	1.0	2.0
1385_0001199	1.0	1.0
1385_0001723	0.0	0.0
1385_0001741	0.0	1.0
1385_0001744	0.0	1.0
1385_0001749	1.0	1.0
1385_0001752	0.0	2.0
1385_0001758	1.0	0.0
1385_0001772	0.0	1.0
1385_0001785	0.0	0.0
1385_0001786	1.0	1.0
1385_0001788	1.0	1.0
1385_0001792	0.0	2.0
1385_0001793	1.0	1.0
1395_0000355	2.0	2.0
1395_0000364	1.0	1.0
1395_0000368	0.0	0.0
1395_0000378	1.0	1.0
1395_0000390	1.0	1.0
1395_0000413	2.0	2.0
1395_0000415	1.0	1.0
1395_0000432	2.0	2.0
1395_0000449	2.0	2.0
1395_0000458	2.0	1.0
1395_0000527	1.0	1.0
1395_0000534	2.0	1.0
1395_0000535	1.0	1.0
1395_0000547	2.0	1.0
1395_0000551	2.0	1.0
1395_0000556	1.0	2.0
1395_0000557	3.0	2.0
1395_0000563	1.0	1.0
1395_0000572	1.0	1.0
1395_0000579	1.0	1.0
1395_0000598	1.0	1.0
1395_0000612	1.0	1.0
1395_0000626	1.0	1.0
1395_0000627	1.0	1.0
1395_0000635	0.0	1.0
1395_0000649	2.0	2.0
1395_0001016	2.0	1.0
1395_0001023	1.0	1.0
1395_0001024	1.0	1.0
1395_0001033	1.0	2.0
1395_0001061	1.0	2.0
1395_0001064	2.0	2.0
1395_0001078	0.0	1.0
1395_0001084	1.0	1.0
1395_0001093	1.0	1.0
1395_0001120	1.0	1.0
1395_0001123	1.0	2.0
1395_0001149	0.0	1.0
1395_0001171	1.0	1.0
5 Fold, Dimension = Grammaticalaccuracy

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 1.10
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.01
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        33
         1.0       0.52      0.84      0.64       143
         2.0       0.61      0.32      0.42       176
         3.0       0.54      0.76      0.63        90
         4.0       0.00      0.00      0.00        10

    accuracy                           0.54       452
   macro avg       0.33      0.38      0.34       452
weighted avg       0.51      0.54      0.49       452

[[  0  33   0   0   0]
 [  0 120  18   5   0]
 [  0  75  57  44   0]
 [  0   3  19  68   0]
 [  0   0   0  10   0]]
0.49221536935261306
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.92
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.95
              precision    recall  f1-score   support

         0.0       0.62      0.15      0.24        33
         1.0       0.57      0.67      0.62       143
         2.0       0.59      0.49      0.54       176
         3.0       0.53      0.76      0.62        90
         4.0       0.00      0.00      0.00        10

    accuracy                           0.57       452
   macro avg       0.46      0.41      0.40       452
weighted avg       0.56      0.57      0.55       452

[[ 5 28  0  0  0]
 [ 2 96 41  4  0]
 [ 1 41 87 47  0]
 [ 0  2 20 68  0]
 [ 0  0  0 10  0]]
0.5465164865059987
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.77
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.94
              precision    recall  f1-score   support

         0.0       0.68      0.45      0.55        33
         1.0       0.60      0.69      0.64       143
         2.0       0.59      0.55      0.57       176
         3.0       0.56      0.63      0.59        90
         4.0       0.00      0.00      0.00        10

    accuracy                           0.59       452
   macro avg       0.49      0.47      0.47       452
weighted avg       0.58      0.59      0.58       452

[[15 18  0  0  0]
 [ 6 99 37  1  0]
 [ 1 44 97 34  0]
 [ 0  3 30 57  0]
 [ 0  0  0 10  0]]
0.5842677930112404
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.61
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.01
              precision    recall  f1-score   support

         0.0       0.67      0.30      0.42        33
         1.0       0.58      0.63      0.61       143
         2.0       0.56      0.54      0.55       176
         3.0       0.54      0.68      0.60        90
         4.0       0.00      0.00      0.00        10

    accuracy                           0.57       452
   macro avg       0.47      0.43      0.43       452
weighted avg       0.56      0.57      0.56       452

[[10 23  0  0  0]
 [ 4 90 48  1  0]
 [ 1 38 95 42  0]
 [ 0  3 26 61  0]
 [ 0  0  0 10  0]]
0.5556808236953882
452 452 452
Filename	True Label	Prediction
1023_0001418	2.0	3.0
1023_0001422	3.0	3.0
1023_0001423	2.0	3.0
1023_0101683	3.0	3.0
1023_0101688	3.0	3.0
1023_0101691	3.0	3.0
1023_0101752	2.0	3.0
1023_0101844	2.0	3.0
1023_0101845	2.0	3.0
1023_0101895	4.0	3.0
1023_0101904	2.0	3.0
1023_0102117	2.0	3.0
1023_0103822	2.0	2.0
1023_0103825	3.0	3.0
1023_0103827	3.0	3.0
1023_0103833	4.0	3.0
1023_0103838	3.0	3.0
1023_0103841	3.0	3.0
1023_0103843	2.0	3.0
1023_0103880	3.0	3.0
1023_0103955	3.0	3.0
1023_0104203	2.0	3.0
1023_0107682	2.0	2.0
1023_0107727	4.0	3.0
1023_0108306	3.0	3.0
1023_0108422	3.0	3.0
1023_0108648	3.0	3.0
1023_0108890	3.0	3.0
1023_0109022	2.0	3.0
1023_0109039	3.0	3.0
1023_0109247	3.0	3.0
1023_0109267	2.0	2.0
1023_0109396	2.0	3.0
1023_0109495	2.0	3.0
1023_0109516	3.0	3.0
1023_0109519	2.0	3.0
1023_0109527	3.0	3.0
1023_0109609	2.0	2.0
1023_0109878	2.0	3.0
1023_0109891	3.0	3.0
1023_0109951	2.0	3.0
1023_0111896	2.0	2.0
1031_0001951	2.0	3.0
1031_0001997	3.0	3.0
1031_0002004	3.0	3.0
1031_0002005	3.0	3.0
1031_0002006	4.0	3.0
1031_0002036	4.0	3.0
1031_0002089	3.0	3.0
1031_0003078	3.0	3.0
1031_0003088	4.0	3.0
1031_0003091	2.0	3.0
1031_0003095	2.0	3.0
1031_0003106	3.0	3.0
1031_0003141	3.0	3.0
1031_0003150	3.0	3.0
1031_0003163	3.0	3.0
1031_0003172	3.0	3.0
1031_0003182	4.0	3.0
1031_0003187	3.0	3.0
1031_0003205	3.0	3.0
1031_0003206	3.0	3.0
1031_0003211	2.0	3.0
1031_0003226	3.0	3.0
1031_0003244	4.0	3.0
1031_0003260	4.0	3.0
1031_0003366	3.0	3.0
1031_0003383	3.0	3.0
1031_0003387	3.0	3.0
1031_0003388	3.0	3.0
1031_0003408	2.0	3.0
1061_0120272	1.0	1.0
1061_0120276	2.0	2.0
1061_0120281	1.0	1.0
1061_0120286	0.0	1.0
1061_0120289	1.0	2.0
1061_0120290	1.0	2.0
1061_0120291	1.0	1.0
1061_0120295	0.0	1.0
1061_0120297	1.0	1.0
1061_0120299	2.0	2.0
1061_0120301	2.0	1.0
1061_0120302	1.0	1.0
1061_0120304	2.0	1.0
1061_0120306	3.0	2.0
1061_0120318	2.0	2.0
1061_0120323	1.0	1.0
1061_0120324	2.0	2.0
1061_0120347	2.0	1.0
1061_0120349	1.0	1.0
1061_0120354	1.0	1.0
1061_0120356	2.0	2.0
1061_0120360	3.0	2.0
1061_0120382	1.0	2.0
1061_0120391	1.0	1.0
1061_0120403	3.0	2.0
1061_0120404	1.0	1.0
1061_0120405	2.0	2.0
1061_0120433	1.0	1.0
1061_0120442	2.0	3.0
1061_0120443	0.0	1.0
1061_0120448	3.0	2.0
1061_0120458	3.0	2.0
1061_0120459	2.0	2.0
1061_0120483	1.0	2.0
1061_0120494	1.0	1.0
1061_0120495	2.0	2.0
1061_0120858	2.0	2.0
1061_0120874	2.0	2.0
1061_0120876	2.0	2.0
1061_0120878	1.0	1.0
1061_0120881	2.0	2.0
1061_0120889	1.0	1.0
1061_1029112	3.0	2.0
1061_1202917	1.0	2.0
1071_0024678	1.0	1.0
1071_0024682	2.0	2.0
1071_0024686	2.0	1.0
1071_0024702	1.0	1.0
1071_0024703	1.0	1.0
1071_0024705	1.0	1.0
1071_0024712	1.0	2.0
1071_0024761	2.0	1.0
1071_0024762	0.0	1.0
1071_0024763	1.0	1.0
1071_0024765	0.0	0.0
1071_0024768	1.0	1.0
1071_0024798	1.0	1.0
1071_0024801	1.0	1.0
1071_0024817	1.0	1.0
1071_0024826	1.0	1.0
1071_0024827	1.0	1.0
1071_0024848	1.0	1.0
1071_0024849	0.0	0.0
1071_0024850	0.0	0.0
1071_0024853	1.0	0.0
1071_0024856	1.0	1.0
1071_0024876	1.0	1.0
1071_0024877	1.0	1.0
1071_0024881	2.0	1.0
1071_0241833	1.0	1.0
1071_0242011	2.0	1.0
1071_0242042	0.0	0.0
1071_0242071	0.0	1.0
1071_0243582	0.0	1.0
1071_0243591	1.0	1.0
1071_0243621	2.0	1.0
1071_0248308	1.0	1.0
1071_0248313	1.0	1.0
1071_0248315	0.0	0.0
1071_0248316	1.0	0.0
1071_0248317	0.0	0.0
1071_0248318	0.0	0.0
1071_0248322	1.0	2.0
1071_0248329	1.0	1.0
1071_0248331	0.0	1.0
1071_0248337	1.0	1.0
1071_0248342	0.0	1.0
1071_0248350	2.0	0.0
1091_0000006	1.0	1.0
1091_0000007	3.0	1.0
1091_0000031	1.0	1.0
1091_0000035	1.0	1.0
1091_0000048	1.0	1.0
1091_0000050	1.0	1.0
1091_0000052	0.0	1.0
1091_0000056	1.0	1.0
1091_0000068	2.0	1.0
1091_0000070	2.0	1.0
1091_0000073	3.0	1.0
1091_0000092	1.0	1.0
1091_0000102	1.0	1.0
1091_0000126	2.0	1.0
1091_0000144	1.0	0.0
1091_0000153	1.0	2.0
1091_0000173	2.0	2.0
1091_0000196	2.0	1.0
1091_0000198	2.0	1.0
1091_0000200	1.0	2.0
1091_0000203	1.0	1.0
1091_0000206	1.0	1.0
1091_0000213	2.0	2.0
1091_0000215	2.0	2.0
1091_0000228	1.0	1.0
1091_0000232	2.0	1.0
1091_0000233	2.0	1.0
1091_0000239	2.0	2.0
1091_0000240	1.0	1.0
1091_0000241	2.0	1.0
1091_0000242	1.0	2.0
1091_0000249	2.0	2.0
1091_0000252	1.0	2.0
1091_0000254	2.0	1.0
1091_0000256	1.0	2.0
1091_0000259	2.0	2.0
1091_0000262	2.0	1.0
1091_0000265	2.0	2.0
0605	2.0	2.0
0609	1.0	2.0
0621	2.0	2.0
0622	1.0	1.0
0623	2.0	2.0
0626	2.0	2.0
0628	2.0	2.0
0634	2.0	2.0
0636	2.0	2.0
0637	2.0	2.0
0640	2.0	2.0
0714	2.0	2.0
0717	1.0	1.0
0718	1.0	2.0
0720	1.0	2.0
0725	1.0	2.0
0812	2.0	1.0
0814	1.0	1.0
0815	2.0	2.0
0821	2.0	1.0
0826	1.0	2.0
0828	2.0	2.0
0906	2.0	2.0
0912	2.0	2.0
0917	1.0	2.0
0918	1.0	2.0
1002	2.0	2.0
1004	1.0	2.0
1015	1.0	2.0
1021	1.0	1.0
1022	2.0	2.0
1023	1.0	2.0
1111	1.0	2.0
BER0611006	2.0	3.0
KYJ0611005A	1.0	1.0
LIB0611001A	1.0	1.0
LIB0611002B	2.0	1.0
LON0611003	2.0	3.0
LON0611004A	1.0	1.0
MOS0509001	1.0	2.0
PAR1011017	3.0	2.0
PHA0111002B	3.0	1.0
PHA0111003A	1.0	1.0
PHA0111015	3.0	3.0
PHA0111018	1.0	3.0
PHA0112006B	2.0	1.0
PHA0112009A	2.0	1.0
PHA0209001	1.0	2.0
PHA0209013	1.0	1.0
PHA0209039	2.0	3.0
PHA0411009B	1.0	1.0
PHA0411010B	0.0	1.0
PHA0411027	2.0	2.0
PHA0411030	3.0	3.0
PHA0411043	1.0	2.0
PHA0411044	3.0	3.0
PHA0411059	3.0	3.0
PHA0509018	3.0	3.0
PHA0509035	2.0	3.0
PHA0509036	3.0	3.0
PHA0509043	2.0	2.0
PHA0509045	1.0	2.0
PHA0510046	2.0	3.0
PHA0510047	1.0	2.0
PHA0610015	2.0	3.0
PHA0610016	2.0	3.0
PHA0610026	3.0	3.0
PHA0710009	3.0	2.0
PHA0710018	3.0	2.0
PHA0809010	2.0	2.0
PHA0810001	3.0	3.0
PHA0810003	3.0	3.0
PHA0810010	2.0	3.0
PHA0811012	3.0	3.0
PHA0811013	3.0	3.0
PHA0811016	1.0	2.0
PHA0811017	3.0	3.0
PHA1109001	1.0	1.0
PHA1109005	1.0	1.0
PHA1109024	4.0	3.0
PHA1109026	3.0	3.0
PHA1109027	3.0	2.0
PHA1110001A	2.0	1.0
PHA1110002A	2.0	2.0
PHA1110004A	1.0	1.0
PHA1110014	2.0	3.0
PHA1111001B	1.0	1.0
PHA1111003A	2.0	1.0
PHA1111008A	2.0	1.0
TI071122B	1.0	1.0
VAR0209036	3.0	2.0
VAR0909005	1.0	2.0
VAR0909006	3.0	2.0
VAR0909007	2.0	2.0
VAR0909009	3.0	2.0
1325_1001017	2.0	2.0
1325_1001032	2.0	3.0
1325_1001044	3.0	2.0
1325_1001046	2.0	2.0
1325_1001050	2.0	3.0
1325_1001056	2.0	3.0
1325_1001063	2.0	2.0
1325_1001075	1.0	2.0
1325_1001076	3.0	2.0
1325_1001089	2.0	2.0
1325_1001090	2.0	2.0
1325_1001091	3.0	2.0
1325_1001092	2.0	2.0
1325_1001113	3.0	3.0
1325_1001124	2.0	2.0
1325_1001125	2.0	2.0
1325_1001127	3.0	2.0
1325_1001132	2.0	2.0
1325_1001136	2.0	2.0
1325_1001142	2.0	3.0
1325_1001143	2.0	3.0
1325_1001155	2.0	2.0
1325_1001160	2.0	2.0
1325_1001162	2.0	2.0
1325_9000143	3.0	3.0
1325_9000152	3.0	3.0
1325_9000185	3.0	2.0
1325_9000186	3.0	3.0
1325_9000214	2.0	2.0
1325_9000215	3.0	3.0
1325_9000237	3.0	3.0
1325_9000241	3.0	3.0
1325_9000304	2.0	3.0
1325_9000323	2.0	2.0
1325_9000534	2.0	2.0
1325_9000554	2.0	2.0
1325_9000685	3.0	3.0
1325_9000686	3.0	2.0
1365_0100004	2.0	2.0
1365_0100005	2.0	1.0
1365_0100008	1.0	2.0
1365_0100009	2.0	1.0
1365_0100010	1.0	1.0
1365_0100013	3.0	2.0
1365_0100023	2.0	2.0
1365_0100026	1.0	1.0
1365_0100061	3.0	2.0
1365_0100074	2.0	2.0
1365_0100095	2.0	2.0
1365_0100101	3.0	3.0
1365_0100102	3.0	2.0
1365_0100103	3.0	2.0
1365_0100107	3.0	3.0
1365_0100148	2.0	3.0
1365_0100151	2.0	2.0
1365_0100165	3.0	3.0
1365_0100166	2.0	2.0
1365_0100169	2.0	2.0
1365_0100171	2.0	2.0
1365_0100174	2.0	2.0
1365_0100177	2.0	2.0
1365_0100187	2.0	2.0
1365_0100188	2.0	2.0
1365_0100196	1.0	2.0
1365_0100198	2.0	2.0
1365_0100202	1.0	2.0
1365_0100222	1.0	2.0
1365_0100224	2.0	3.0
1365_0100227	3.0	2.0
1365_0100229	2.0	2.0
1365_0100253	2.0	2.0
1365_0100267	2.0	3.0
1365_0100274	2.0	3.0
1365_0100277	3.0	2.0
1365_0100278	2.0	2.0
1365_0100279	1.0	2.0
1365_0100280	1.0	1.0
1365_0100288	2.0	2.0
1365_0100459	2.0	2.0
1365_0100470	2.0	3.0
1365_0100471	2.0	3.0
1365_0100472	2.0	2.0
1365_0100473	2.0	2.0
1385_0000013	1.0	2.0
1385_0000017	1.0	1.0
1385_0000022	1.0	1.0
1385_0000039	1.0	2.0
1385_0000045	2.0	2.0
1385_0000047	1.0	1.0
1385_0000058	2.0	2.0
1385_0000101	1.0	1.0
1385_0000103	1.0	1.0
1385_0000120	0.0	1.0
1385_0000125	1.0	1.0
1385_0000130	2.0	1.0
1385_0001108	2.0	1.0
1385_0001123	2.0	1.0
1385_0001131	1.0	1.0
1385_0001147	1.0	0.0
1385_0001149	2.0	2.0
1385_0001155	2.0	2.0
1385_0001156	2.0	2.0
1385_0001162	1.0	1.0
1385_0001167	1.0	1.0
1385_0001175	0.0	1.0
1385_0001188	1.0	1.0
1385_0001189	0.0	1.0
1385_0001193	2.0	2.0
1385_0001522	0.0	0.0
1385_0001527	1.0	1.0
1385_0001528	1.0	2.0
1385_0001712	1.0	2.0
1385_0001714	1.0	1.0
1385_0001715	1.0	1.0
1385_0001718	0.0	1.0
1385_0001719	0.0	1.0
1385_0001730	1.0	2.0
1385_0001732	1.0	2.0
1385_0001736	2.0	2.0
1385_0001737	2.0	1.0
1385_0001738	0.0	1.0
1385_0001747	1.0	1.0
1385_0001753	1.0	1.0
1385_0001766	2.0	2.0
1385_0001767	0.0	1.0
1385_0001768	1.0	1.0
1385_0001774	0.0	0.0
1385_0001789	1.0	2.0
1385_0001790	2.0	1.0
1385_0001791	0.0	1.0
1385_0001799	1.0	2.0
1395_0000341	1.0	1.0
1395_0000356	1.0	1.0
1395_0000388	2.0	2.0
1395_0000404	2.0	2.0
1395_0000438	3.0	2.0
1395_0000450	1.0	1.0
1395_0000451	2.0	2.0
1395_0000470	1.0	1.0
1395_0000526	1.0	1.0
1395_0000552	2.0	2.0
1395_0000553	2.0	1.0
1395_0000559	2.0	1.0
1395_0000595	0.0	0.0
1395_0000596	2.0	1.0
1395_0000604	1.0	1.0
1395_0000609	0.0	1.0
1395_0001010	1.0	2.0
1395_0001021	1.0	1.0
1395_0001034	1.0	1.0
1395_0001066	1.0	2.0
1395_0001068	1.0	2.0
1395_0001071	1.0	2.0
1395_0001076	0.0	1.0
1395_0001090	2.0	1.0
1395_0001104	0.0	1.0
1395_0001108	0.0	1.0
1395_0001124	0.0	1.0
1395_0001150	1.0	1.0
1395_0001158	2.0	1.0
Averaged weighted F1-scores 0.5720179811477476
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
144.90552995391704 65.35717405024758
LANGUAGE: IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
1 Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 1.30
  Training epoch took: 65
Running Validation...
  Average evaluation loss: 1.13
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.00      0.00      0.00        67
         2.0       0.50      0.71      0.59       162
         3.0       0.44      0.60      0.51       106
         4.0       0.62      0.56      0.59        86
         5.0       0.00      0.00      0.00        23

    accuracy                           0.50       452
   macro avg       0.26      0.31      0.28       452
weighted avg       0.40      0.50      0.44       452

[[  0   0   8   0   0   0]
 [  0   0  66   0   1   0]
 [  0   0 115  43   4   0]
 [  0   0  30  64  12   0]
 [  0   0   8  30  48   0]
 [  0   0   2   9  12   0]]
0.44200374831149325
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 48
Elapsed time 60

  Average training loss: 1.04
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 1.01
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.51      0.63      0.56        67
         2.0       0.65      0.68      0.66       162
         3.0       0.48      0.56      0.52       106
         4.0       0.67      0.60      0.63        86
         5.0       0.00      0.00      0.00        23

    accuracy                           0.58       452
   macro avg       0.38      0.41      0.40       452
weighted avg       0.55      0.58      0.56       452

[[  0   8   0   0   0   0]
 [  0  42  23   2   0   0]
 [  0  27 110  22   3   0]
 [  0   4  31  59  12   0]
 [  0   1   2  31  52   0]
 [  0   0   3   9  11   0]]
0.5632792052980525
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.89
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.11
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.52      0.18      0.27        67
         2.0       0.57      0.64      0.60       162
         3.0       0.48      0.66      0.56       106
         4.0       0.64      0.76      0.70        86
         5.0       0.00      0.00      0.00        23

    accuracy                           0.56       452
   macro avg       0.37      0.37      0.35       452
weighted avg       0.52      0.56      0.52       452

[[  0   5   3   0   0   0]
 [  0  12  52   3   0   0]
 [  0   5 104  50   3   0]
 [  0   1  20  70  15   0]
 [  0   0   2  19  65   0]
 [  0   0   2   3  18   0]]
0.5186854737098188
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.74
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.06
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.59      0.40      0.48        67
         2.0       0.61      0.70      0.65       162
         3.0       0.54      0.49      0.51       106
         4.0       0.60      0.86      0.70        86
         5.0       0.00      0.00      0.00        23

    accuracy                           0.59       452
   macro avg       0.39      0.41      0.39       452
weighted avg       0.55      0.59      0.56       452

[[  0   7   1   0   0   0]
 [  0  27  38   2   0   0]
 [  0  11 113  34   4   0]
 [  0   1  28  52  25   0]
 [  0   0   3   9  74   0]
 [  0   0   2   0  21   0]]
0.5585016108387418
452 452 452
Filename	True Label	Prediction
1023_0001416	4.0	4.0
1023_0101683	3.0	3.0
1023_0101689	2.0	2.0
1023_0101691	3.0	4.0
1023_0101701	3.0	3.0
1023_0101841	3.0	3.0
1023_0101846	4.0	4.0
1023_0101847	2.0	4.0
1023_0101852	4.0	3.0
1023_0101896	2.0	2.0
1023_0101898	3.0	3.0
1023_0101899	3.0	4.0
1023_0103824	4.0	4.0
1023_0103830	3.0	4.0
1023_0103838	4.0	3.0
1023_0103839	3.0	4.0
1023_0103840	4.0	4.0
1023_0103841	5.0	4.0
1023_0104203	4.0	4.0
1023_0107244	4.0	4.0
1023_0107672	3.0	4.0
1023_0107725	3.0	3.0
1023_0108306	4.0	4.0
1023_0108648	4.0	4.0
1023_0108751	3.0	3.0
1023_0108752	5.0	4.0
1023_0108810	3.0	3.0
1023_0108811	3.0	3.0
1023_0108814	4.0	3.0
1023_0108815	4.0	4.0
1023_0108885	2.0	3.0
1023_0108886	4.0	3.0
1023_0108890	4.0	4.0
1023_0108931	4.0	4.0
1023_0108955	5.0	4.0
1023_0108992	3.0	3.0
1023_0109027	2.0	2.0
1023_0109029	2.0	2.0
1023_0109038	5.0	4.0
1023_0109249	3.0	3.0
1023_0109496	3.0	4.0
1023_0109516	5.0	4.0
1023_0109527	5.0	4.0
1023_0109891	4.0	4.0
1023_0109914	3.0	3.0
1023_0109915	2.0	3.0
1031_0002011	5.0	4.0
1031_0002032	4.0	4.0
1031_0002043	4.0	4.0
1031_0002083	3.0	4.0
1031_0002198	4.0	4.0
1031_0002199	4.0	4.0
1031_0002200	3.0	3.0
1031_0003012	3.0	3.0
1031_0003073	5.0	4.0
1031_0003088	4.0	4.0
1031_0003128	4.0	4.0
1031_0003144	3.0	3.0
1031_0003154	3.0	4.0
1031_0003180	4.0	4.0
1031_0003185	3.0	3.0
1031_0003187	3.0	3.0
1031_0003191	3.0	3.0
1031_0003216	4.0	4.0
1031_0003226	5.0	4.0
1031_0003238	4.0	4.0
1031_0003249	3.0	4.0
1031_0003310	4.0	3.0
1031_0003315	3.0	3.0
1031_0003355	3.0	3.0
1031_0003356	4.0	4.0
1031_0003365	3.0	3.0
1031_0003368	3.0	3.0
1031_0003384	2.0	2.0
1031_0003386	3.0	4.0
1031_0003388	3.0	4.0
1031_0003392	3.0	3.0
1031_0003414	3.0	4.0
1061_0120272	2.0	2.0
1061_0120274	2.0	2.0
1061_0120289	2.0	2.0
1061_0120309	2.0	2.0
1061_0120319	2.0	2.0
1061_0120325	2.0	3.0
1061_0120329	2.0	3.0
1061_0120331	1.0	2.0
1061_0120333	2.0	3.0
1061_0120334	3.0	3.0
1061_0120338	2.0	3.0
1061_0120348	1.0	1.0
1061_0120368	3.0	2.0
1061_0120369	2.0	2.0
1061_0120370	2.0	2.0
1061_0120374	3.0	3.0
1061_0120382	2.0	2.0
1061_0120391	1.0	2.0
1061_0120427	3.0	3.0
1061_0120433	1.0	2.0
1061_0120441	2.0	2.0
1061_0120457	3.0	3.0
1061_0120458	4.0	3.0
1061_0120483	2.0	2.0
1061_0120485	3.0	3.0
1061_0120487	2.0	3.0
1061_0120489	3.0	3.0
1061_0120490	3.0	3.0
1061_0120853	3.0	4.0
1061_0120855	2.0	3.0
1061_0120874	3.0	3.0
1061_0120875	3.0	3.0
1061_0120880	4.0	3.0
1061_0120883	3.0	3.0
1061_0120889	2.0	2.0
1061_1029112	3.0	3.0
1061_1029113	2.0	3.0
1061_1029116	2.0	3.0
1061_1029119	3.0	3.0
1061_1202912	3.0	2.0
1061_1202915	2.0	2.0
1071_0024686	2.0	2.0
1071_0024692	4.0	3.0
1071_0024701	3.0	2.0
1071_0024703	2.0	2.0
1071_0024704	2.0	2.0
1071_0024715	1.0	2.0
1071_0024758	3.0	2.0
1071_0024765	1.0	1.0
1071_0024779	2.0	2.0
1071_0024781	1.0	2.0
1071_0024800	1.0	1.0
1071_0024801	1.0	2.0
1071_0024802	2.0	2.0
1071_0024824	2.0	2.0
1071_0024826	2.0	2.0
1071_0024840	2.0	2.0
1071_0024848	1.0	2.0
1071_0024851	1.0	1.0
1071_0024863	1.0	2.0
1071_0024871	3.0	2.0
1071_0024872	2.0	3.0
1071_0242012	2.0	2.0
1071_0242023	1.0	2.0
1071_0242041	1.0	2.0
1071_0243501	2.0	2.0
1071_0243502	2.0	1.0
1071_0243592	2.0	2.0
1071_0243623	2.0	2.0
1071_0248302	1.0	1.0
1071_0248310	1.0	1.0
1071_0248314	1.0	1.0
1071_0248316	1.0	1.0
1071_0248318	0.0	1.0
1071_0248319	1.0	1.0
1071_0248320	0.0	1.0
1071_0248343	1.0	1.0
1071_0248344	1.0	1.0
1071_0248347	1.0	1.0
1071_0248350	1.0	1.0
1091_0000002	3.0	2.0
1091_0000005	3.0	2.0
1091_0000013	1.0	1.0
1091_0000016	1.0	2.0
1091_0000021	1.0	3.0
1091_0000022	2.0	2.0
1091_0000026	2.0	2.0
1091_0000034	3.0	2.0
1091_0000035	2.0	2.0
1091_0000037	2.0	1.0
1091_0000042	1.0	1.0
1091_0000049	2.0	2.0
1091_0000051	1.0	1.0
1091_0000056	2.0	2.0
1091_0000061	3.0	1.0
1091_0000067	3.0	2.0
1091_0000070	3.0	2.0
1091_0000076	2.0	3.0
1091_0000095	2.0	2.0
1091_0000101	2.0	2.0
1091_0000113	2.0	3.0
1091_0000114	2.0	3.0
1091_0000153	2.0	3.0
1091_0000155	3.0	2.0
1091_0000157	3.0	2.0
1091_0000158	2.0	2.0
1091_0000169	3.0	2.0
1091_0000173	2.0	2.0
1091_0000185	2.0	2.0
1091_0000190	1.0	2.0
1091_0000191	2.0	3.0
1091_0000199	3.0	3.0
1091_0000200	3.0	3.0
1091_0000203	2.0	2.0
1091_0000213	2.0	2.0
1091_0000214	3.0	2.0
1091_0000215	2.0	3.0
1091_0000221	3.0	2.0
1091_0000236	3.0	2.0
1091_0000238	2.0	2.0
1091_0000243	1.0	2.0
1091_0000257	2.0	3.0
1091_0000258	2.0	2.0
1091_0000262	2.0	2.0
1091_0000273	1.0	3.0
1091_0000274	2.0	2.0
0606	2.0	2.0
0612	2.0	2.0
0615	2.0	2.0
0624	3.0	2.0
0633	3.0	2.0
0634	3.0	2.0
0637	2.0	3.0
0644	2.0	2.0
0645	3.0	2.0
0720	2.0	2.0
0723	2.0	3.0
0724	3.0	3.0
0811	3.0	3.0
0812	2.0	2.0
0815	3.0	2.0
0817	2.0	2.0
0819	3.0	3.0
0820	1.0	2.0
0825	2.0	2.0
0827	2.0	2.0
0829	2.0	3.0
0903	2.0	3.0
0907	2.0	2.0
0910	2.0	2.0
0915	2.0	3.0
0919	2.0	2.0
0923	2.0	3.0
0925	2.0	2.0
0926	3.0	2.0
0929	1.0	2.0
1008	1.0	2.0
1009	2.0	2.0
1019	2.0	2.0
1022	2.0	2.0
9999	1.0	2.0
KYJ0611006A	1.0	2.0
KYJ0611009B	1.0	2.0
LIB0611001B	2.0	2.0
LON0610002B	2.0	2.0
MOS0509004	2.0	2.0
PHA0111002B	2.0	2.0
PHA0111004B	2.0	2.0
PHA0111005B	2.0	2.0
PHA0111016	3.0	3.0
PHA0209026	3.0	3.0
PHA0209039	2.0	3.0
PHA0411011A	3.0	2.0
PHA0411012A	2.0	2.0
PHA0411029	2.0	2.0
PHA0411037	2.0	3.0
PHA0411043	2.0	2.0
PHA0411045	2.0	2.0
PHA0509007	2.0	2.0
PHA0509013	1.0	1.0
PHA0509020	3.0	3.0
PHA0509021	3.0	2.0
PHA0509032	3.0	3.0
PHA0509034	2.0	3.0
PHA0509036	3.0	3.0
PHA0509041	3.0	3.0
PHA0509045	2.0	2.0
PHA0510013A	2.0	2.0
PHA0510013B	2.0	2.0
PHA0510027	2.0	2.0
PHA0510035	3.0	3.0
PHA0510039	2.0	3.0
PHA0510040	3.0	3.0
PHA0510048	2.0	2.0
PHA0610007A	2.0	1.0
PHA0610016	3.0	3.0
PHA0610019B	2.0	2.0
PHA0710011	3.0	3.0
PHA0710016	3.0	2.0
PHA0710019	2.0	3.0
PHA0809009	2.0	3.0
PHA0810004	2.0	2.0
PHA0810008	2.0	3.0
PHA0811012	4.0	3.0
PHA1109001	2.0	2.0
PHA1109003	2.0	2.0
PHA1109028	2.0	3.0
PHA1110004A	2.0	2.0
PHA1110013	3.0	3.0
PHA1110017	2.0	2.0
PHA1110022	3.0	3.0
PHA1111001B	1.0	2.0
PHA1111002B	1.0	2.0
VAR0209036	3.0	3.0
VAR0909005	2.0	2.0
VAR0909008	2.0	2.0
VAR0909010	2.0	2.0
1325_1001009	5.0	4.0
1325_1001010	5.0	4.0
1325_1001013	4.0	4.0
1325_1001016	5.0	4.0
1325_1001021	5.0	4.0
1325_1001024	4.0	4.0
1325_1001028	4.0	4.0
1325_1001032	4.0	4.0
1325_1001050	4.0	4.0
1325_1001055	5.0	4.0
1325_1001058	4.0	4.0
1325_1001077	4.0	4.0
1325_1001091	4.0	4.0
1325_1001094	4.0	4.0
1325_1001095	5.0	4.0
1325_1001098	4.0	4.0
1325_1001122	4.0	4.0
1325_1001130	4.0	4.0
1325_1001142	4.0	4.0
1325_1001144	4.0	4.0
1325_1001155	4.0	4.0
1325_1001163	4.0	4.0
1325_1001168	4.0	4.0
1325_1001170	4.0	4.0
1325_9000095	4.0	4.0
1325_9000104	3.0	4.0
1325_9000137	4.0	4.0
1325_9000144	4.0	4.0
1325_9000185	4.0	4.0
1325_9000209	4.0	4.0
1325_9000211	4.0	4.0
1325_9000302	3.0	4.0
1325_9000304	3.0	4.0
1325_9000314	4.0	4.0
1325_9000315	3.0	4.0
1325_9000316	3.0	4.0
1325_9000323	4.0	4.0
1325_9000505	4.0	4.0
1325_9000601	4.0	4.0
1325_9000602	4.0	4.0
1325_9000700	4.0	4.0
1365_0100004	3.0	4.0
1365_0100011	4.0	4.0
1365_0100013	4.0	4.0
1365_0100017	4.0	4.0
1365_0100018	3.0	4.0
1365_0100023	2.0	4.0
1365_0100030	4.0	4.0
1365_0100056	4.0	4.0
1365_0100067	3.0	4.0
1365_0100070	4.0	4.0
1365_0100099	3.0	4.0
1365_0100102	4.0	4.0
1365_0100135	4.0	4.0
1365_0100136	4.0	4.0
1365_0100146	4.0	2.0
1365_0100147	4.0	2.0
1365_0100162	4.0	4.0
1365_0100172	5.0	4.0
1365_0100178	4.0	4.0
1365_0100179	4.0	4.0
1365_0100186	5.0	4.0
1365_0100195	4.0	4.0
1365_0100196	4.0	4.0
1365_0100198	4.0	4.0
1365_0100199	4.0	4.0
1365_0100212	5.0	4.0
1365_0100221	3.0	4.0
1365_0100222	3.0	4.0
1365_0100227	4.0	4.0
1365_0100233	4.0	4.0
1365_0100262	4.0	4.0
1365_0100266	3.0	4.0
1365_0100279	4.0	4.0
1365_0100290	4.0	4.0
1365_0100457	2.0	4.0
1365_0100469	5.0	4.0
1365_0100480	5.0	4.0
1385_0000017	1.0	2.0
1385_0000035	2.0	1.0
1385_0000043	2.0	2.0
1385_0000057	1.0	2.0
1385_0000058	2.0	2.0
1385_0000102	2.0	2.0
1385_0000104	2.0	2.0
1385_0001104	1.0	1.0
1385_0001105	2.0	1.0
1385_0001120	2.0	1.0
1385_0001124	1.0	1.0
1385_0001129	1.0	2.0
1385_0001130	1.0	1.0
1385_0001134	2.0	2.0
1385_0001136	2.0	1.0
1385_0001157	2.0	1.0
1385_0001169	2.0	1.0
1385_0001173	0.0	1.0
1385_0001188	2.0	2.0
1385_0001197	2.0	2.0
1385_0001712	1.0	2.0
1385_0001714	1.0	2.0
1385_0001715	1.0	2.0
1385_0001728	2.0	2.0
1385_0001730	2.0	2.0
1385_0001733	1.0	2.0
1385_0001742	0.0	1.0
1385_0001758	1.0	1.0
1385_0001759	1.0	2.0
1385_0001761	1.0	1.0
1385_0001786	2.0	2.0
1385_0001794	1.0	2.0
1395_0000341	2.0	2.0
1395_0000353	2.0	1.0
1395_0000361	4.0	4.0
1395_0000376	5.0	2.0
1395_0000378	2.0	2.0
1395_0000379	1.0	2.0
1395_0000387	5.0	4.0
1395_0000392	5.0	2.0
1395_0000399	2.0	2.0
1395_0000409	3.0	2.0
1395_0000447	2.0	2.0
1395_0000450	2.0	2.0
1395_0000499	2.0	2.0
1395_0000512	3.0	2.0
1395_0000518	4.0	2.0
1395_0000534	2.0	2.0
1395_0000535	2.0	2.0
1395_0000537	2.0	2.0
1395_0000547	3.0	2.0
1395_0000548	2.0	3.0
1395_0000553	2.0	2.0
1395_0000555	2.0	2.0
1395_0000556	2.0	2.0
1395_0000575	2.0	2.0
1395_0000585	1.0	2.0
1395_0000587	0.0	1.0
1395_0000593	0.0	2.0
1395_0000602	1.0	1.0
1395_0000604	1.0	1.0
1395_0000606	0.0	1.0
1395_0000612	0.0	1.0
1395_0000626	2.0	2.0
1395_0000627	1.0	2.0
1395_0000635	1.0	2.0
1395_0001022	2.0	2.0
1395_0001023	2.0	2.0
1395_0001028	2.0	3.0
1395_0001067	1.0	2.0
1395_0001070	2.0	4.0
1395_0001073	1.0	2.0
1395_0001090	2.0	2.0
1395_0001101	2.0	1.0
1395_0001104	1.0	1.0
1395_0001108	1.0	2.0
1395_0001120	1.0	1.0
1395_0001133	1.0	2.0
1395_0001149	1.0	1.0
2 Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 1.29
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.03
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.41      0.13      0.20        68
         2.0       0.60      0.77      0.68       162
         3.0       0.63      0.56      0.59       106
         4.0       0.55      0.84      0.67        86
         5.0       0.00      0.00      0.00        22

    accuracy                           0.59       452
   macro avg       0.37      0.38      0.36       452
weighted avg       0.53      0.59      0.54       452

[[  0   5   3   0   0   0]
 [  0   9  56   0   3   0]
 [  0   7 125  23   7   0]
 [  0   0  19  59  28   0]
 [  0   1   3  10  72   0]
 [  0   0   1   1  20   0]]
0.5388135005555152
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 1.00
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.04
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.48      0.43      0.45        68
         2.0       0.66      0.67      0.67       162
         3.0       0.54      0.43      0.48       106
         4.0       0.51      0.84      0.63        86
         5.0       0.00      0.00      0.00        22

    accuracy                           0.57       452
   macro avg       0.37      0.40      0.37       452
weighted avg       0.53      0.57      0.54       452

[[  0   7   1   0   0   0]
 [  0  29  37   0   2   0]
 [  0  19 109  27   7   0]
 [  0   5  14  46  41   0]
 [  0   0   4  10  72   0]
 [  0   0   0   2  20   0]]
0.5402342502139842
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.86
  Training epoch took: 65
Running Validation...
  Average evaluation loss: 1.04
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.50      0.31      0.38        68
         2.0       0.63      0.72      0.67       162
         3.0       0.58      0.58      0.58       106
         4.0       0.56      0.78      0.65        86
         5.0       0.00      0.00      0.00        22

    accuracy                           0.59       452
   macro avg       0.38      0.40      0.38       452
weighted avg       0.54      0.59      0.56       452

[[  0   6   2   0   0   0]
 [  0  21  45   1   1   0]
 [  0  12 117  26   7   0]
 [  0   3  17  61  25   0]
 [  0   0   3  16  67   0]
 [  0   0   1   1  20   0]]
0.5584943368027712
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.69
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.08
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.53      0.37      0.43        68
         2.0       0.65      0.69      0.67       162
         3.0       0.52      0.54      0.53       106
         4.0       0.53      0.76      0.62        86
         5.0       0.00      0.00      0.00        22

    accuracy                           0.57       452
   macro avg       0.37      0.39      0.38       452
weighted avg       0.54      0.57      0.55       452

[[  0   7   1   0   0   0]
 [  0  25  40   2   1   0]
 [  0  12 112  31   7   0]
 [  0   3  16  57  30   0]
 [  0   0   3  18  65   0]
 [  0   0   0   2  20   0]]
0.5478964411262883
452 452 452
Filename	True Label	Prediction
1023_0001422	2.0	3.0
1023_0101684	3.0	3.0
1023_0101752	3.0	3.0
1023_0101849	2.0	3.0
1023_0101853	3.0	4.0
1023_0101904	2.0	3.0
1023_0103822	3.0	4.0
1023_0103827	3.0	4.0
1023_0103832	3.0	3.0
1023_0103833	5.0	4.0
1023_0103843	3.0	4.0
1023_0103955	3.0	4.0
1023_0104207	3.0	4.0
1023_0107042	3.0	3.0
1023_0107729	3.0	4.0
1023_0107773	3.0	3.0
1023_0107780	4.0	3.0
1023_0107784	3.0	3.0
1023_0107788	3.0	4.0
1023_0108305	3.0	4.0
1023_0108423	3.0	2.0
1023_0108520	3.0	3.0
1023_0108812	3.0	4.0
1023_0108935	4.0	4.0
1023_0109022	3.0	3.0
1023_0109026	3.0	3.0
1023_0109033	4.0	4.0
1023_0109039	4.0	4.0
1023_0109096	4.0	4.0
1023_0109247	4.0	4.0
1023_0109250	3.0	4.0
1023_0109392	3.0	3.0
1023_0109515	4.0	4.0
1023_0109524	4.0	3.0
1023_0109614	3.0	3.0
1023_0109651	4.0	3.0
1023_0109717	3.0	4.0
1023_0109880	4.0	4.0
1023_0109917	4.0	3.0
1023_0109951	3.0	3.0
1023_0111896	3.0	3.0
1031_0001703	4.0	3.0
1031_0001998	4.0	3.0
1031_0002002	3.0	3.0
1031_0002003	3.0	3.0
1031_0002079	4.0	4.0
1031_0002187	4.0	4.0
1031_0003013	5.0	4.0
1031_0003023	3.0	3.0
1031_0003029	3.0	3.0
1031_0003053	3.0	4.0
1031_0003076	4.0	4.0
1031_0003090	4.0	3.0
1031_0003097	3.0	3.0
1031_0003098	4.0	3.0
1031_0003099	3.0	3.0
1031_0003106	3.0	3.0
1031_0003126	3.0	3.0
1031_0003127	4.0	3.0
1031_0003132	3.0	4.0
1031_0003133	4.0	4.0
1031_0003136	4.0	3.0
1031_0003165	3.0	3.0
1031_0003166	3.0	3.0
1031_0003169	3.0	3.0
1031_0003170	4.0	3.0
1031_0003174	3.0	3.0
1031_0003181	4.0	4.0
1031_0003189	4.0	4.0
1031_0003220	4.0	4.0
1031_0003236	4.0	4.0
1031_0003274	4.0	4.0
1031_0003327	2.0	3.0
1031_0003331	3.0	3.0
1031_0003339	5.0	3.0
1061_0012029	4.0	4.0
1061_0120278	2.0	3.0
1061_0120281	2.0	2.0
1061_0120283	1.0	2.0
1061_0120296	2.0	2.0
1061_0120299	3.0	3.0
1061_0120312	2.0	2.0
1061_0120313	2.0	2.0
1061_0120326	3.0	4.0
1061_0120327	3.0	4.0
1061_0120335	3.0	4.0
1061_0120336	2.0	3.0
1061_0120356	2.0	3.0
1061_0120359	2.0	3.0
1061_0120371	3.0	3.0
1061_0120373	3.0	3.0
1061_0120386	1.0	2.0
1061_0120424	3.0	3.0
1061_0120440	2.0	2.0
1061_0120448	4.0	3.0
1061_0120450	2.0	3.0
1061_0120456	2.0	3.0
1061_0120459	3.0	3.0
1061_0120460	2.0	2.0
1061_0120478	3.0	3.0
1061_0120479	3.0	3.0
1061_0120488	2.0	3.0
1061_0120492	3.0	3.0
1061_0120500	2.0	2.0
1061_0120856	1.0	2.0
1061_0120876	2.0	3.0
1061_0120878	2.0	2.0
1061_0120885	3.0	3.0
1061_0120888	2.0	3.0
1061_1029114	2.0	2.0
1061_1202918	2.0	3.0
1071_0024681	2.0	2.0
1071_0024683	1.0	1.0
1071_0024685	2.0	2.0
1071_0024702	2.0	2.0
1071_0024710	1.0	2.0
1071_0024714	2.0	2.0
1071_0024716	3.0	1.0
1071_0024759	1.0	1.0
1071_0024777	1.0	2.0
1071_0024778	0.0	1.0
1071_0024783	0.0	1.0
1071_0024784	1.0	1.0
1071_0024799	3.0	3.0
1071_0024806	1.0	2.0
1071_0024812	1.0	1.0
1071_0024814	1.0	2.0
1071_0024816	1.0	2.0
1071_0024822	1.0	2.0
1071_0024834	3.0	3.0
1071_0024836	2.0	2.0
1071_0024845	1.0	2.0
1071_0024847	2.0	2.0
1071_0024853	2.0	1.0
1071_0024857	1.0	1.0
1071_0024862	2.0	2.0
1071_0024866	5.0	3.0
1071_0024873	2.0	2.0
1071_0024881	3.0	2.0
1071_0241831	1.0	2.0
1071_0241833	2.0	2.0
1071_0242043	1.0	1.0
1071_0242072	0.0	1.0
1071_0242073	1.0	2.0
1071_0243581	1.0	1.0
1071_0243621	2.0	2.0
1071_0248308	2.0	2.0
1071_0248309	2.0	2.0
1071_0248322	2.0	2.0
1071_0248324	0.0	1.0
1071_0248326	2.0	2.0
1071_0248328	1.0	1.0
1071_0248331	2.0	2.0
1071_0248334	2.0	2.0
1071_0248335	2.0	2.0
1071_0248342	1.0	2.0
1091_0000004	1.0	2.0
1091_0000009	1.0	1.0
1091_0000010	3.0	2.0
1091_0000011	2.0	2.0
1091_0000015	2.0	2.0
1091_0000017	3.0	3.0
1091_0000020	3.0	2.0
1091_0000029	3.0	2.0
1091_0000036	3.0	2.0
1091_0000038	2.0	1.0
1091_0000039	2.0	1.0
1091_0000043	2.0	2.0
1091_0000055	2.0	2.0
1091_0000057	3.0	1.0
1091_0000063	2.0	1.0
1091_0000065	1.0	2.0
1091_0000068	2.0	2.0
1091_0000069	3.0	1.0
1091_0000086	2.0	2.0
1091_0000127	2.0	2.0
1091_0000140	2.0	1.0
1091_0000145	1.0	1.0
1091_0000148	2.0	2.0
1091_0000168	2.0	3.0
1091_0000174	2.0	1.0
1091_0000193	2.0	2.0
1091_0000196	3.0	2.0
1091_0000209	3.0	2.0
1091_0000216	2.0	3.0
1091_0000220	2.0	2.0
1091_0000223	2.0	2.0
1091_0000226	3.0	2.0
1091_0000229	2.0	2.0
1091_0000240	2.0	2.0
1091_0000246	3.0	2.0
1091_0000252	2.0	2.0
1091_0000253	2.0	1.0
1091_0000254	3.0	2.0
1091_0000255	1.0	2.0
1091_0000263	4.0	3.0
1091_0000265	3.0	2.0
1091_0000268	2.0	2.0
1091_0000269	2.0	2.0
1091_0000271	2.0	2.0
1091_0000272	1.0	2.0
0613	2.0	2.0
0619	2.0	2.0
0622	2.0	2.0
0628	2.0	3.0
0632	2.0	2.0
0640	2.0	3.0
0716	2.0	2.0
0725	2.0	2.0
0803	2.0	3.0
0808	2.0	3.0
0816	3.0	2.0
0818	2.0	2.0
0822	2.0	2.0
0901	3.0	3.0
0904	2.0	2.0
0913	2.0	2.0
0916	2.0	2.0
0921	2.0	2.0
0928	3.0	3.0
1001	2.0	2.0
1002	2.0	2.0
1003	1.0	2.0
1004	2.0	2.0
1006	2.0	2.0
1007	2.0	2.0
1015	2.0	2.0
1016	2.0	2.0
1021	2.0	3.0
1112	2.0	2.0
LIB0611002B	2.0	2.0
LON0610002A	2.0	2.0
LON0611002A	2.0	2.0
LON0611004B	1.0	1.0
MOS0611014	1.0	2.0
MOS0611015	3.0	3.0
PAR1011015	3.0	3.0
PHA0111001B	2.0	2.0
PHA0111010	3.0	3.0
PHA0111012	2.0	2.0
PHA0112002A	2.0	2.0
PHA0209001	2.0	2.0
PHA0209008	2.0	2.0
PHA0209013	2.0	1.0
PHA0209024	1.0	2.0
PHA0209031	3.0	3.0
PHA0411009B	1.0	2.0
PHA0411012B	2.0	2.0
PHA0411030	4.0	4.0
PHA0411031	3.0	3.0
PHA0411039	2.0	3.0
PHA0411044	4.0	3.0
PHA0411054	3.0	2.0
PHA0509019	2.0	3.0
PHA0509028	3.0	3.0
PHA0509030	3.0	3.0
PHA0509037	3.0	3.0
PHA0509039	3.0	3.0
PHA0510002B	2.0	2.0
PHA0510034	3.0	3.0
PHA0510036	3.0	3.0
PHA0510046	2.0	3.0
PHA0510047	2.0	3.0
PHA0510050	3.0	3.0
PHA0610006A	1.0	2.0
PHA0610025	3.0	4.0
PHA0709008	2.0	2.0
PHA0710009	2.0	2.0
PHA0710013	3.0	3.0
PHA0710015	3.0	3.0
PHA0710021	3.0	3.0
PHA0810002	2.0	2.0
PHA0810015	4.0	4.0
PHA0811014	2.0	2.0
PHA0811020	2.0	3.0
PHA1109002	3.0	3.0
PHA1109024	3.0	3.0
PHA1110001A	2.0	2.0
PHA1110003A	2.0	2.0
PHA1110021	2.0	2.0
PHA1111006B	1.0	2.0
VAR0909007	2.0	2.0
VAR0909009	2.0	3.0
VAR0910004	2.0	3.0
VAR0910005	1.0	2.0
VAR0910006	2.0	3.0
VAR0910007	2.0	3.0
VAR0910009	2.0	3.0
1325_1001008	5.0	4.0
1325_1001019	4.0	4.0
1325_1001022	4.0	4.0
1325_1001023	4.0	4.0
1325_1001040	4.0	4.0
1325_1001044	3.0	4.0
1325_1001045	5.0	4.0
1325_1001046	4.0	4.0
1325_1001062	4.0	4.0
1325_1001075	3.0	4.0
1325_1001081	4.0	3.0
1325_1001083	5.0	4.0
1325_1001092	4.0	4.0
1325_1001097	1.0	3.0
1325_1001108	4.0	4.0
1325_1001124	3.0	4.0
1325_1001127	4.0	4.0
1325_1001128	4.0	4.0
1325_1001131	3.0	4.0
1325_1001135	3.0	4.0
1325_1001136	3.0	4.0
1325_1001138	4.0	4.0
1325_1001139	4.0	4.0
1325_1001153	4.0	4.0
1325_1001159	5.0	4.0
1325_1001162	4.0	4.0
1325_9000059	5.0	4.0
1325_9000087	5.0	4.0
1325_9000099	5.0	4.0
1325_9000105	4.0	4.0
1325_9000138	4.0	4.0
1325_9000140	4.0	4.0
1325_9000213	4.0	4.0
1325_9000214	4.0	4.0
1325_9000240	2.0	4.0
1325_9000318	4.0	4.0
1325_9000321	4.0	4.0
1325_9000504	4.0	4.0
1325_9000612	2.0	4.0
1325_9000677	4.0	4.0
1325_9000750	4.0	3.0
1365_0100002	4.0	4.0
1365_0100006	4.0	4.0
1365_0100015	4.0	4.0
1365_0100027	4.0	3.0
1365_0100028	4.0	4.0
1365_0100051	2.0	2.0
1365_0100061	5.0	4.0
1365_0100069	4.0	4.0
1365_0100071	4.0	4.0
1365_0100073	3.0	4.0
1365_0100079	5.0	4.0
1365_0100080	4.0	4.0
1365_0100094	5.0	4.0
1365_0100096	4.0	4.0
1365_0100100	4.0	4.0
1365_0100105	5.0	4.0
1365_0100116	4.0	4.0
1365_0100117	5.0	4.0
1365_0100125	4.0	3.0
1365_0100137	5.0	4.0
1365_0100145	5.0	4.0
1365_0100166	3.0	4.0
1365_0100167	2.0	4.0
1365_0100170	4.0	4.0
1365_0100177	5.0	4.0
1365_0100180	5.0	4.0
1365_0100181	3.0	4.0
1365_0100184	4.0	4.0
1365_0100203	4.0	4.0
1365_0100213	4.0	4.0
1365_0100215	4.0	4.0
1365_0100219	4.0	4.0
1365_0100220	4.0	4.0
1365_0100252	4.0	4.0
1365_0100255	3.0	4.0
1365_0100257	3.0	4.0
1365_0100269	4.0	4.0
1365_0100274	4.0	4.0
1365_0100281	5.0	4.0
1365_0100286	3.0	4.0
1365_0100447	5.0	4.0
1365_0100448	2.0	4.0
1365_0100455	2.0	4.0
1365_0100475	4.0	4.0
1365_0100481	4.0	4.0
1365_0100482	4.0	4.0
1385_0000013	1.0	1.0
1385_0000021	1.0	2.0
1385_0000023	2.0	1.0
1385_0000034	2.0	2.0
1385_0000097	2.0	2.0
1385_0000128	1.0	2.0
1385_0000130	2.0	2.0
1385_0001109	2.0	2.0
1385_0001122	2.0	1.0
1385_0001126	1.0	1.0
1385_0001131	2.0	1.0
1385_0001135	2.0	2.0
1385_0001147	2.0	1.0
1385_0001149	2.0	2.0
1385_0001154	2.0	2.0
1385_0001160	1.0	4.0
1385_0001172	1.0	1.0
1385_0001174	1.0	1.0
1385_0001190	1.0	2.0
1385_0001191	2.0	2.0
1385_0001194	2.0	2.0
1385_0001195	2.0	2.0
1385_0001196	2.0	2.0
1385_0001503	2.0	2.0
1385_0001522	1.0	1.0
1385_0001528	2.0	2.0
1385_0001718	1.0	1.0
1385_0001719	1.0	2.0
1385_0001724	2.0	4.0
1385_0001727	1.0	2.0
1385_0001734	2.0	2.0
1385_0001736	2.0	2.0
1385_0001741	1.0	1.0
1385_0001746	1.0	2.0
1385_0001747	0.0	1.0
1385_0001748	1.0	3.0
1385_0001762	2.0	2.0
1385_0001767	1.0	2.0
1385_0001768	2.0	2.0
1385_0001789	2.0	2.0
1385_0001793	1.0	2.0
1385_0001795	1.0	1.0
1385_0001796	1.0	1.0
1385_0001799	2.0	2.0
1385_0001800	1.0	1.0
1395_0000333	2.0	2.0
1395_0000360	3.0	2.0
1395_0000365	3.0	2.0
1395_0000369	4.0	2.0
1395_0000451	2.0	2.0
1395_0000462	4.0	2.0
1395_0000504	2.0	2.0
1395_0000513	4.0	2.0
1395_0000526	2.0	2.0
1395_0000527	1.0	1.0
1395_0000582	0.0	1.0
1395_0000583	1.0	2.0
1395_0000595	0.0	1.0
1395_0000611	1.0	1.0
1395_0000630	1.0	2.0
1395_0000636	1.0	2.0
1395_0001015	2.0	2.0
1395_0001017	2.0	2.0
1395_0001020	1.0	2.0
1395_0001024	1.0	2.0
1395_0001033	2.0	2.0
1395_0001069	2.0	2.0
1395_0001074	1.0	2.0
1395_0001080	2.0	2.0
1395_0001093	2.0	2.0
1395_0001103	1.0	2.0
1395_0001109	0.0	2.0
1395_0001116	2.0	2.0
1395_0001117	1.0	2.0
1395_0001118	1.0	1.0
1395_0001164	2.0	4.0
1395_0001171	1.0	1.0
3 Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 1.28
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.04
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.00      0.00      0.00        68
         2.0       0.57      0.91      0.70       162
         3.0       0.60      0.50      0.55       105
         4.0       0.58      0.70      0.63        86
         5.0       0.00      0.00      0.00        23

    accuracy                           0.58       452
   macro avg       0.29      0.35      0.31       452
weighted avg       0.45      0.58      0.50       452

[[  0   0   8   0   0   0]
 [  0   0  68   0   0   0]
 [  0   0 148  11   3   0]
 [  0   0  29  53  23   0]
 [  0   0   4  22  60   0]
 [  0   0   2   3  18   0]]
0.49908663943361503
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 1.04
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.07
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.42      0.59      0.49        68
         2.0       0.64      0.54      0.59       162
         3.0       0.51      0.36      0.42       105
         4.0       0.53      0.92      0.68        86
         5.0       0.00      0.00      0.00        23

    accuracy                           0.54       452
   macro avg       0.35      0.40      0.36       452
weighted avg       0.52      0.54      0.51       452

[[ 0  7  1  0  0  0]
 [ 0 40 27  1  0  0]
 [ 0 41 87 31  3  0]
 [ 0  6 16 38 45  0]
 [ 0  0  3  4 79  0]
 [ 0  1  1  0 21  0]]
0.5109130656922423
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.90
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.97
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.59      0.32      0.42        68
         2.0       0.62      0.90      0.73       162
         3.0       0.66      0.49      0.56       105
         4.0       0.60      0.73      0.66        86
         5.0       0.00      0.00      0.00        23

    accuracy                           0.62       452
   macro avg       0.41      0.41      0.40       452
weighted avg       0.58      0.62      0.58       452

[[  0   6   2   0   0   0]
 [  0  22  46   0   0   0]
 [  0   8 145   7   2   0]
 [  0   1  33  51  20   0]
 [  0   0   5  18  63   0]
 [  0   0   2   1  20   0]]
0.5818829316268698
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.75
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.05
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.54      0.28      0.37        68
         2.0       0.62      0.72      0.67       162
         3.0       0.57      0.48      0.52       105
         4.0       0.54      0.91      0.68        86
         5.0       0.00      0.00      0.00        23

    accuracy                           0.58       452
   macro avg       0.38      0.40      0.37       452
weighted avg       0.54      0.58      0.54       452

[[  0   6   1   1   0   0]
 [  0  19  47   2   0   0]
 [  0   9 116  28   9   0]
 [  0   1  18  50  36   0]
 [  0   0   2   6  78   0]
 [  0   0   2   0  21   0]]
0.5444807819101447
452 452 452
Filename	True Label	Prediction
1023_0001423	4.0	4.0
1023_0101693	3.0	4.0
1023_0101694	3.0	4.0
1023_0101749	4.0	4.0
1023_0101843	4.0	4.0
1023_0101844	3.0	3.0
1023_0101845	3.0	3.0
1023_0101900	3.0	4.0
1023_0102117	4.0	3.0
1023_0103823	5.0	4.0
1023_0103834	3.0	4.0
1023_0103844	4.0	4.0
1023_0104206	3.0	2.0
1023_0104209	3.0	4.0
1023_0107726	3.0	3.0
1023_0107740	3.0	3.0
1023_0107781	4.0	4.0
1023_0107783	3.0	3.0
1023_0108518	4.0	4.0
1023_0108641	5.0	4.0
1023_0108766	3.0	3.0
1023_0108934	4.0	4.0
1023_0108958	3.0	4.0
1023_0108993	4.0	4.0
1023_0109192	4.0	4.0
1023_0109248	3.0	4.0
1023_0109267	3.0	3.0
1023_0109396	3.0	4.0
1023_0109400	3.0	3.0
1023_0109518	3.0	3.0
1023_0109519	4.0	3.0
1023_0109522	3.0	3.0
1023_0109588	3.0	4.0
1023_0109591	3.0	4.0
1023_0109609	3.0	3.0
1023_0109716	5.0	4.0
1023_0109890	4.0	4.0
1023_0109954	3.0	3.0
1031_0002005	3.0	3.0
1031_0002006	4.0	4.0
1031_0002010	4.0	4.0
1031_0002040	5.0	4.0
1031_0002061	4.0	4.0
1031_0002131	3.0	4.0
1031_0002184	4.0	4.0
1031_0002185	4.0	4.0
1031_0002196	4.0	4.0
1031_0003063	4.0	4.0
1031_0003071	3.0	3.0
1031_0003077	3.0	3.0
1031_0003091	2.0	3.0
1031_0003140	4.0	4.0
1031_0003141	3.0	4.0
1031_0003149	4.0	4.0
1031_0003155	3.0	4.0
1031_0003173	3.0	4.0
1031_0003179	4.0	4.0
1031_0003205	4.0	4.0
1031_0003211	3.0	4.0
1031_0003224	3.0	4.0
1031_0003232	3.0	4.0
1031_0003240	3.0	4.0
1031_0003242	3.0	4.0
1031_0003246	3.0	3.0
1031_0003272	3.0	3.0
1031_0003314	3.0	4.0
1031_0003330	3.0	4.0
1031_0003337	3.0	3.0
1031_0003358	5.0	4.0
1031_0003359	2.0	4.0
1031_0003369	4.0	4.0
1031_0003383	3.0	3.0
1031_0003415	4.0	4.0
1031_0003419	4.0	4.0
1061_0120276	3.0	2.0
1061_0120280	2.0	2.0
1061_0120285	2.0	3.0
1061_0120295	0.0	3.0
1061_0120301	2.0	2.0
1061_0120310	2.0	2.0
1061_0120311	3.0	3.0
1061_0120330	4.0	3.0
1061_0120354	1.0	2.0
1061_0120355	2.0	2.0
1061_0120360	3.0	3.0
1061_0120366	3.0	3.0
1061_0120367	3.0	3.0
1061_0120372	3.0	3.0
1061_0120376	2.0	3.0
1061_0120388	2.0	2.0
1061_0120408	3.0	3.0
1061_0120411	4.0	3.0
1061_0120423	3.0	3.0
1061_0120426	2.0	3.0
1061_0120429	2.0	2.0
1061_0120430	2.0	2.0
1061_0120438	4.0	3.0
1061_0120453	3.0	3.0
1061_0120455	3.0	3.0
1061_0120484	3.0	3.0
1061_0120486	2.0	3.0
1061_0120495	3.0	4.0
1061_0120496	2.0	2.0
1061_0120859	3.0	3.0
1061_1029111	3.0	3.0
1061_1029117	2.0	3.0
1061_1029118	2.0	2.0
1061_1029120	1.0	2.0
1061_1202910	3.0	3.0
1061_1202911	1.0	2.0
1061_1202914	2.0	2.0
1061_1202916	2.0	3.0
1061_1202917	2.0	3.0
1071_0024682	3.0	2.0
1071_0024689	1.0	2.0
1071_0024690	2.0	3.0
1071_0024699	1.0	2.0
1071_0024705	2.0	2.0
1071_0024757	2.0	2.0
1071_0024761	2.0	2.0
1071_0024762	1.0	1.0
1071_0024763	1.0	2.0
1071_0024766	1.0	1.0
1071_0024767	2.0	2.0
1071_0024768	1.0	2.0
1071_0024772	1.0	1.0
1071_0024774	0.0	1.0
1071_0024775	0.0	1.0
1071_0024782	0.0	1.0
1071_0024797	1.0	1.0
1071_0024807	1.0	2.0
1071_0024811	1.0	2.0
1071_0024813	1.0	1.0
1071_0024818	3.0	1.0
1071_0024820	1.0	2.0
1071_0024841	1.0	1.0
1071_0024846	0.0	1.0
1071_0024856	1.0	2.0
1071_0024861	1.0	2.0
1071_0024874	2.0	1.0
1071_0024876	2.0	2.0
1071_0241832	1.0	2.0
1071_0242021	2.0	2.0
1071_0242022	1.0	1.0
1071_0243582	1.0	2.0
1071_0243622	1.0	1.0
1071_0248301	2.0	1.0
1071_0248304	1.0	2.0
1071_0248311	2.0	2.0
1071_0248321	3.0	2.0
1071_0248327	0.0	2.0
1071_0248330	2.0	2.0
1071_0248333	2.0	2.0
1071_0248336	2.0	1.0
1071_0248337	2.0	2.0
1071_0248339	2.0	2.0
1071_0248345	2.0	2.0
1071_0248346	1.0	1.0
1071_0248348	2.0	2.0
1091_0000001	2.0	2.0
1091_0000007	3.0	2.0
1091_0000008	3.0	2.0
1091_0000019	2.0	2.0
1091_0000025	1.0	1.0
1091_0000027	1.0	2.0
1091_0000028	1.0	2.0
1091_0000030	1.0	3.0
1091_0000045	2.0	3.0
1091_0000052	1.0	1.0
1091_0000053	1.0	2.0
1091_0000054	1.0	2.0
1091_0000062	3.0	2.0
1091_0000066	1.0	1.0
1091_0000078	3.0	2.0
1091_0000092	2.0	2.0
1091_0000144	2.0	1.0
1091_0000154	2.0	2.0
1091_0000162	2.0	3.0
1091_0000164	2.0	2.0
1091_0000192	2.0	2.0
1091_0000194	2.0	2.0
1091_0000195	2.0	2.0
1091_0000202	2.0	3.0
1091_0000206	1.0	2.0
1091_0000211	2.0	2.0
1091_0000212	2.0	3.0
1091_0000219	2.0	2.0
1091_0000227	1.0	3.0
1091_0000228	2.0	2.0
1091_0000233	2.0	3.0
1091_0000235	2.0	2.0
1091_0000237	2.0	2.0
1091_0000239	3.0	2.0
1091_0000241	2.0	2.0
1091_0000250	2.0	2.0
1091_0000256	2.0	2.0
1091_0000259	2.0	3.0
1091_0000261	2.0	2.0
1091_0000266	2.0	3.0
1091_0000267	2.0	2.0
1091_0000275	2.0	2.0
1091_0000276	2.0	2.0
0609	2.0	2.0
0611	2.0	2.0
0620	2.0	3.0
0623	2.0	2.0
0627	2.0	3.0
0629	2.0	2.0
0630	2.0	2.0
0631	2.0	3.0
0639	2.0	2.0
0802	2.0	2.0
0807	3.0	3.0
0823	3.0	2.0
0905	2.0	2.0
0906	3.0	3.0
0911	2.0	2.0
0914	2.0	2.0
1005	2.0	2.0
1014	3.0	2.0
1111	2.0	2.0
1116	2.0	3.0
BER0609003	2.0	2.0
BER0611005	3.0	3.0
BER0611007	1.0	2.0
KYJ0611005A	1.0	2.0
KYJ0611006B	1.0	2.0
KYJ0611009A	2.0	2.0
LIB0611002A	2.0	2.0
LIB0611004B	2.0	2.0
LON0611002B	1.0	2.0
LON0611003	3.0	3.0
MOS0611013	3.0	3.0
PAR1011009B	2.0	2.0
PAR1011018	2.0	3.0
PHA0111014	1.0	2.0
PHA0111018	2.0	2.0
PHA0112003B	1.0	2.0
PHA0112007A	2.0	2.0
PHA0112009B	2.0	2.0
PHA0112012A	2.0	2.0
PHA0209028	3.0	2.0
PHA0209034	2.0	3.0
PHA0210001	2.0	2.0
PHA0411008B	2.0	2.0
PHA0411009A	2.0	2.0
PHA0411033	2.0	3.0
PHA0411034	1.0	2.0
PHA0411038	3.0	3.0
PHA0411041	2.0	3.0
PHA0411042	3.0	3.0
PHA0411047	2.0	3.0
PHA0411051	3.0	3.0
PHA0509002	1.0	1.0
PHA0509022	3.0	3.0
PHA0509025	4.0	4.0
PHA0509031	2.0	3.0
PHA0509044	2.0	3.0
PHA0510002A	2.0	2.0
PHA0510030	3.0	3.0
PHA0510031	2.0	2.0
PHA0610007B	2.0	2.0
PHA0610015	3.0	3.0
PHA0610019A	1.0	2.0
PHA0710012	3.0	3.0
PHA0810006	2.0	2.0
PHA0810012	2.0	2.0
PHA0811013	3.0	3.0
PHA0811016	2.0	2.0
PHA0811017	3.0	3.0
PHA1109005	2.0	2.0
PHA1109007	2.0	2.0
PHA1110002A	2.0	3.0
PHA1110002B	2.0	2.0
PHA1110003B	2.0	2.0
PHA1110015	3.0	3.0
PHA1111002A	2.0	2.0
PHA1111003A	3.0	2.0
PHA1111003B	2.0	2.0
PHA1111004A	1.0	2.0
PHA1111006A	2.0	2.0
TI071122B	2.0	2.0
VAR0909003	2.0	2.0
VAR0909006	3.0	3.0
1325_1001025	4.0	4.0
1325_1001027	4.0	4.0
1325_1001033	5.0	4.0
1325_1001035	5.0	4.0
1325_1001037	5.0	4.0
1325_1001041	4.0	4.0
1325_1001048	4.0	4.0
1325_1001051	4.0	4.0
1325_1001056	4.0	4.0
1325_1001063	4.0	4.0
1325_1001082	5.0	4.0
1325_1001084	4.0	4.0
1325_1001087	4.0	4.0
1325_1001090	4.0	4.0
1325_1001099	4.0	4.0
1325_1001109	4.0	4.0
1325_1001113	4.0	4.0
1325_1001119	4.0	4.0
1325_1001120	5.0	4.0
1325_1001123	4.0	4.0
1325_1001134	4.0	4.0
1325_1001141	2.0	4.0
1325_1001156	4.0	4.0
1325_1001157	4.0	4.0
1325_1001160	4.0	4.0
1325_1001165	4.0	4.0
1325_1001167	4.0	4.0
1325_1001169	4.0	4.0
1325_9000088	4.0	4.0
1325_9000102	3.0	4.0
1325_9000136	4.0	4.0
1325_9000143	4.0	4.0
1325_9000187	4.0	4.0
1325_9000215	4.0	4.0
1325_9000303	4.0	4.0
1325_9000320	3.0	4.0
1325_9000554	3.0	3.0
1325_9000611	3.0	4.0
1325_9000674	4.0	4.0
1325_9000685	4.0	4.0
1365_0100005	3.0	4.0
1365_0100014	4.0	4.0
1365_0100022	4.0	4.0
1365_0100024	4.0	4.0
1365_0100026	2.0	2.0
1365_0100058	4.0	4.0
1365_0100072	4.0	4.0
1365_0100092	4.0	3.0
1365_0100097	4.0	4.0
1365_0100101	4.0	4.0
1365_0100106	3.0	4.0
1365_0100121	4.0	4.0
1365_0100148	4.0	4.0
1365_0100151	4.0	4.0
1365_0100163	5.0	4.0
1365_0100165	4.0	4.0
1365_0100169	5.0	4.0
1365_0100173	5.0	4.0
1365_0100176	4.0	4.0
1365_0100185	3.0	4.0
1365_0100188	5.0	4.0
1365_0100211	4.0	4.0
1365_0100217	5.0	4.0
1365_0100218	4.0	4.0
1365_0100224	3.0	4.0
1365_0100226	5.0	4.0
1365_0100228	3.0	4.0
1365_0100229	5.0	4.0
1365_0100232	4.0	4.0
1365_0100251	3.0	4.0
1365_0100261	5.0	4.0
1365_0100268	3.0	4.0
1365_0100270	3.0	4.0
1365_0100275	4.0	4.0
1365_0100276	4.0	4.0
1365_0100287	4.0	4.0
1365_0100288	4.0	4.0
1365_0100289	5.0	4.0
1365_0100299	5.0	4.0
1365_0100458	4.0	4.0
1365_0100461	2.0	4.0
1365_0100472	3.0	4.0
1365_0100479	4.0	4.0
1385_0000011	1.0	1.0
1385_0000020	2.0	2.0
1385_0000038	2.0	2.0
1385_0000042	2.0	2.0
1385_0000044	2.0	1.0
1385_0000048	2.0	2.0
1385_0000049	2.0	2.0
1385_0000050	2.0	2.0
1385_0000099	1.0	2.0
1385_0000100	2.0	1.0
1385_0000101	1.0	2.0
1385_0000103	2.0	2.0
1385_0000127	2.0	2.0
1385_0001112	2.0	2.0
1385_0001113	1.0	2.0
1385_0001121	2.0	2.0
1385_0001123	2.0	2.0
1385_0001138	2.0	1.0
1385_0001148	2.0	2.0
1385_0001151	2.0	2.0
1385_0001161	2.0	2.0
1385_0001163	1.0	2.0
1385_0001166	2.0	2.0
1385_0001189	1.0	2.0
1385_0001192	1.0	2.0
1385_0001198	2.0	2.0
1385_0001523	2.0	2.0
1385_0001525	2.0	2.0
1385_0001720	1.0	2.0
1385_0001726	1.0	2.0
1385_0001740	2.0	2.0
1385_0001749	1.0	2.0
1385_0001757	2.0	2.0
1385_0001766	2.0	4.0
1395_0000354	1.0	1.0
1395_0000356	2.0	2.0
1395_0000357	3.0	3.0
1395_0000359	3.0	4.0
1395_0000364	2.0	4.0
1395_0000366	3.0	2.0
1395_0000368	1.0	1.0
1395_0000383	5.0	2.0
1395_0000388	4.0	2.0
1395_0000389	1.0	1.0
1395_0000390	1.0	2.0
1395_0000391	3.0	4.0
1395_0000414	2.0	2.0
1395_0000438	5.0	4.0
1395_0000443	5.0	2.0
1395_0000449	4.0	4.0
1395_0000452	2.0	1.0
1395_0000455	2.0	4.0
1395_0000465	2.0	2.0
1395_0000469	2.0	2.0
1395_0000471	2.0	1.0
1395_0000515	4.0	2.0
1395_0000516	1.0	1.0
1395_0000533	4.0	4.0
1395_0000550	2.0	4.0
1395_0000551	3.0	2.0
1395_0000552	3.0	2.0
1395_0000554	2.0	2.0
1395_0000560	3.0	2.0
1395_0000564	2.0	2.0
1395_0000565	2.0	2.0
1395_0000584	0.0	1.0
1395_0000596	3.0	2.0
1395_0000610	2.0	2.0
1395_0000628	1.0	1.0
1395_0000631	1.0	2.0
1395_0000644	2.0	4.0
1395_0000646	2.0	2.0
1395_0001013	2.0	4.0
1395_0001016	2.0	2.0
1395_0001058	2.0	2.0
1395_0001066	1.0	2.0
1395_0001071	2.0	2.0
1395_0001075	1.0	2.0
1395_0001114	1.0	2.0
1395_0001122	1.0	2.0
1395_0001123	1.0	2.0
1395_0001146	0.0	1.0
1395_0001150	1.0	2.0
1395_0001158	2.0	2.0
1395_0001170	1.0	2.0
4 Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 1.21
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.08
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.42      0.62      0.50        68
         2.0       0.61      0.65      0.63       162
         3.0       0.51      0.56      0.53       106
         4.0       0.73      0.56      0.64        85
         5.0       0.00      0.00      0.00        23

    accuracy                           0.56       452
   macro avg       0.38      0.40      0.38       452
weighted avg       0.54      0.56      0.55       452

[[  0   8   0   0   0   0]
 [  0  42  25   1   0   0]
 [  0  41 105  14   2   0]
 [  0   7  33  59   7   0]
 [  0   1   5  31  48   0]
 [  0   0   4  10   9   0]]
0.5457896026697611
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 1.00
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.03
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.63      0.38      0.48        68
         2.0       0.64      0.77      0.70       162
         3.0       0.59      0.42      0.49       106
         4.0       0.51      0.85      0.64        85
         5.0       0.00      0.00      0.00        23

    accuracy                           0.59       452
   macro avg       0.40      0.40      0.38       452
weighted avg       0.56      0.59      0.56       452

[[  0   6   2   0   0   0]
 [  0  26  39   0   3   0]
 [  0   6 125  19  12   0]
 [  0   2  26  44  34   0]
 [  0   1   2  10  72   0]
 [  0   0   2   2  19   0]]
0.5564264827318556
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.86
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.15
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.58      0.37      0.45        68
         2.0       0.65      0.62      0.63       162
         3.0       0.47      0.43      0.45       106
         4.0       0.50      0.93      0.65        85
         5.0       0.00      0.00      0.00        23

    accuracy                           0.55       452
   macro avg       0.37      0.39      0.36       452
weighted avg       0.53      0.55      0.52       452

[[  0   7   1   0   0   0]
 [  0  25  37   4   2   0]
 [  0   8 100  42  12   0]
 [  0   2  12  46  46   0]
 [  0   1   2   3  79   0]
 [  0   0   2   2  19   0]]
0.5231618911043807
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.70
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.18
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.60      0.37      0.45        68
         2.0       0.66      0.62      0.64       162
         3.0       0.49      0.51      0.50       106
         4.0       0.51      0.89      0.65        85
         5.0       0.00      0.00      0.00        23

    accuracy                           0.56       452
   macro avg       0.38      0.40      0.37       452
weighted avg       0.54      0.56      0.54       452

[[  0   7   1   0   0   0]
 [  0  25  36   5   2   0]
 [  0   7 100  42  13   0]
 [  0   2  12  54  38   0]
 [  0   1   2   6  76   0]
 [  0   0   1   3  19   0]]
0.5366027611984759
452 452 452
Filename	True Label	Prediction
1023_0001418	3.0	4.0
1023_0001575	5.0	3.0
1023_0101675	4.0	4.0
1023_0101751	3.0	3.0
1023_0101753	4.0	4.0
1023_0101848	3.0	3.0
1023_0101854	2.0	2.0
1023_0101856	2.0	3.0
1023_0101895	3.0	4.0
1023_0101897	3.0	3.0
1023_0102118	3.0	3.0
1023_0103825	4.0	4.0
1023_0103826	4.0	4.0
1023_0103828	2.0	2.0
1023_0103831	4.0	4.0
1023_0103836	5.0	4.0
1023_0103837	5.0	4.0
1023_0103880	4.0	4.0
1023_0103883	3.0	4.0
1023_0106816	5.0	4.0
1023_0107075	2.0	3.0
1023_0107682	2.0	2.0
1023_0107727	5.0	4.0
1023_0107787	3.0	4.0
1023_0108307	5.0	3.0
1023_0108422	4.0	4.0
1023_0108510	4.0	4.0
1023_0108650	3.0	4.0
1023_0108753	4.0	3.0
1023_0108887	3.0	3.0
1023_0108932	3.0	4.0
1023_0109151	2.0	3.0
1023_0109391	4.0	3.0
1023_0109395	3.0	3.0
1023_0109500	4.0	4.0
1023_0109505	4.0	4.0
1023_0109520	3.0	4.0
1023_0109528	3.0	4.0
1023_0109590	3.0	3.0
1023_0109674	3.0	4.0
1023_0109878	3.0	3.0
1023_0109947	3.0	4.0
1031_0001949	4.0	4.0
1031_0001950	4.0	4.0
1031_0001951	3.0	4.0
1031_0002004	3.0	4.0
1031_0002036	5.0	4.0
1031_0002042	4.0	3.0
1031_0002088	3.0	4.0
1031_0002092	4.0	4.0
1031_0002197	3.0	4.0
1031_0003052	4.0	4.0
1031_0003065	3.0	3.0
1031_0003078	3.0	3.0
1031_0003092	2.0	3.0
1031_0003129	3.0	3.0
1031_0003135	4.0	4.0
1031_0003146	4.0	4.0
1031_0003150	4.0	4.0
1031_0003160	3.0	4.0
1031_0003161	4.0	4.0
1031_0003164	4.0	3.0
1031_0003167	3.0	4.0
1031_0003183	4.0	4.0
1031_0003186	5.0	4.0
1031_0003190	3.0	4.0
1031_0003206	3.0	4.0
1031_0003212	2.0	4.0
1031_0003217	4.0	4.0
1031_0003219	3.0	4.0
1031_0003231	3.0	4.0
1031_0003233	3.0	3.0
1031_0003243	3.0	4.0
1031_0003244	4.0	4.0
1031_0003261	3.0	3.0
1031_0003309	3.0	4.0
1031_0003336	5.0	4.0
1031_0003357	3.0	3.0
1031_0003387	3.0	3.0
1031_0003409	4.0	3.0
1061_0120271	3.0	3.0
1061_0120277	2.0	3.0
1061_0120282	1.0	2.0
1061_0120284	1.0	1.0
1061_0120288	3.0	3.0
1061_0120290	2.0	3.0
1061_0120298	2.0	2.0
1061_0120304	2.0	2.0
1061_0120308	3.0	4.0
1061_0120314	3.0	3.0
1061_0120316	2.0	3.0
1061_0120317	3.0	4.0
1061_0120320	3.0	3.0
1061_0120321	2.0	3.0
1061_0120328	2.0	2.0
1061_0120332	2.0	2.0
1061_0120337	3.0	3.0
1061_0120350	4.0	4.0
1061_0120351	2.0	3.0
1061_0120357	3.0	3.0
1061_0120358	2.0	2.0
1061_0120361	3.0	3.0
1061_0120375	2.0	1.0
1061_0120389	3.0	3.0
1061_0120390	2.0	3.0
1061_0120394	3.0	4.0
1061_0120403	4.0	3.0
1061_0120406	3.0	3.0
1061_0120413	2.0	2.0
1061_0120432	3.0	3.0
1061_0120442	2.0	3.0
1061_0120443	1.0	2.0
1061_0120449	4.0	4.0
1061_0120480	3.0	3.0
1061_0120482	2.0	3.0
1061_0120498	3.0	4.0
1061_0120858	2.0	3.0
1061_0120877	3.0	3.0
1061_0120881	3.0	3.0
1061_0120886	2.0	3.0
1061_0120887	2.0	2.0
1061_1202913	2.0	2.0
1071_0024678	2.0	1.0
1071_0024691	2.0	3.0
1071_0024693	2.0	2.0
1071_0024708	2.0	2.0
1071_0024711	1.0	1.0
1071_0024713	2.0	2.0
1071_0024756	2.0	2.0
1071_0024769	1.0	2.0
1071_0024773	1.0	1.0
1071_0024808	1.0	2.0
1071_0024817	1.0	2.0
1071_0024819	1.0	2.0
1071_0024821	1.0	1.0
1071_0024823	2.0	2.0
1071_0024827	2.0	1.0
1071_0024837	1.0	1.0
1071_0024838	1.0	1.0
1071_0024843	1.0	2.0
1071_0024850	1.0	1.0
1071_0024859	2.0	2.0
1071_0024865	2.0	2.0
1071_0024875	3.0	2.0
1071_0024877	2.0	2.0
1071_0024879	3.0	1.0
1071_0242011	2.0	2.0
1071_0242013	2.0	2.0
1071_0242091	1.0	1.0
1071_0242093	0.0	1.0
1071_0248305	0.0	1.0
1071_0248307	3.0	2.0
1071_0248312	1.0	2.0
1071_0248315	0.0	1.0
1071_0248317	1.0	1.0
1071_0248325	1.0	2.0
1071_0248329	2.0	2.0
1071_0248338	2.0	2.0
1071_0248340	1.0	1.0
1071_0248341	1.0	1.0
1071_0248349	1.0	1.0
1091_0000003	2.0	2.0
1091_0000006	1.0	1.0
1091_0000018	3.0	3.0
1091_0000023	4.0	1.0
1091_0000031	2.0	2.0
1091_0000032	2.0	3.0
1091_0000041	1.0	1.0
1091_0000044	1.0	3.0
1091_0000046	2.0	2.0
1091_0000047	3.0	2.0
1091_0000048	1.0	1.0
1091_0000050	1.0	2.0
1091_0000058	3.0	2.0
1091_0000060	3.0	3.0
1091_0000064	2.0	2.0
1091_0000071	2.0	2.0
1091_0000072	1.0	3.0
1091_0000074	2.0	3.0
1091_0000077	3.0	1.0
1091_0000087	3.0	2.0
1091_0000116	3.0	3.0
1091_0000126	3.0	3.0
1091_0000146	1.0	1.0
1091_0000151	1.0	2.0
1091_0000156	2.0	3.0
1091_0000160	3.0	3.0
1091_0000163	2.0	2.0
1091_0000167	2.0	3.0
1091_0000170	3.0	2.0
1091_0000201	3.0	3.0
1091_0000207	2.0	4.0
1091_0000224	2.0	1.0
1091_0000230	2.0	2.0
1091_0000231	3.0	3.0
1091_0000234	2.0	3.0
1091_0000245	2.0	2.0
1091_0000260	2.0	3.0
1091_0000264	2.0	2.0
1091_0000270	2.0	2.0
0601	2.0	2.0
0605	2.0	3.0
0607	3.0	3.0
0608	1.0	2.0
0610	2.0	3.0
0614	2.0	3.0
0616	2.0	2.0
0625	2.0	3.0
0636	3.0	3.0
0638	2.0	2.0
0714	2.0	3.0
0717	2.0	2.0
0719	2.0	2.0
0722	2.0	3.0
0801	2.0	3.0
0804	2.0	2.0
0806	2.0	3.0
0810	2.0	3.0
0813	2.0	2.0
0814	2.0	2.0
0824	2.0	3.0
0826	2.0	2.0
0828	2.0	3.0
0912	3.0	3.0
0918	2.0	3.0
0920	3.0	2.0
0924	2.0	2.0
0927	3.0	2.0
1017	2.0	2.0
1018	2.0	2.0
1020	2.0	2.0
1113	2.0	2.0
1115	2.0	2.0
1117	2.0	2.0
BER0611003	3.0	3.0
KYJ0611003A	2.0	2.0
KYJ0611004A	2.0	2.0
LIB0611001A	2.0	2.0
LIB0611003A	1.0	2.0
MOS0611012	3.0	3.0
PAR1011009A	2.0	2.0
PAR1011017	3.0	3.0
PHA0111001A	2.0	2.0
PHA0111002A	3.0	2.0
PHA0111003A	1.0	2.0
PHA0111004A	3.0	2.0
PHA0111011	2.0	2.0
PHA0111015	3.0	3.0
PHA0112006A	3.0	3.0
PHA0112007B	1.0	2.0
PHA0209038	3.0	3.0
PHA0210008	1.0	1.0
PHA0411035	3.0	3.0
PHA0411058	3.0	3.0
PHA0411059	3.0	4.0
PHA0411061	3.0	4.0
PHA0509017	2.0	3.0
PHA0509018	3.0	3.0
PHA0509024	2.0	2.0
PHA0509026	3.0	3.0
PHA0509038	2.0	2.0
PHA0509040	2.0	2.0
PHA0510003B	2.0	2.0
PHA0510004B	1.0	2.0
PHA0510010A	1.0	2.0
PHA0510010B	1.0	1.0
PHA0510023	3.0	4.0
PHA0510037	2.0	2.0
PHA0510038	3.0	3.0
PHA0610005B	1.0	1.0
PHA0610006B	1.0	2.0
PHA0610017	3.0	3.0
PHA0710014	3.0	3.0
PHA0810003	2.0	3.0
PHA0810010	2.0	3.0
PHA0811019	3.0	3.0
PHA1109004	2.0	3.0
PHA1109025	1.0	2.0
PHA1109027	2.0	3.0
PHA1110001B	2.0	2.0
PHA1110019	3.0	2.0
PHA1111001A	2.0	2.0
VAR0909004	2.0	2.0
1325_1001012	4.0	4.0
1325_1001014	4.0	4.0
1325_1001036	5.0	4.0
1325_1001042	5.0	4.0
1325_1001052	5.0	4.0
1325_1001053	5.0	4.0
1325_1001054	4.0	4.0
1325_1001057	4.0	4.0
1325_1001078	4.0	4.0
1325_1001079	4.0	4.0
1325_1001080	4.0	4.0
1325_1001085	4.0	4.0
1325_1001089	3.0	4.0
1325_1001093	4.0	4.0
1325_1001096	4.0	4.0
1325_1001100	4.0	4.0
1325_1001101	4.0	4.0
1325_1001107	4.0	4.0
1325_1001125	4.0	4.0
1325_1001132	4.0	4.0
1325_1001154	4.0	4.0
1325_1001164	3.0	4.0
1325_1001166	4.0	4.0
1325_9000089	4.0	4.0
1325_9000090	4.0	4.0
1325_9000106	4.0	4.0
1325_9000107	4.0	4.0
1325_9000139	4.0	4.0
1325_9000186	4.0	4.0
1325_9000188	4.0	4.0
1325_9000237	3.0	4.0
1325_9000239	3.0	4.0
1325_9000241	4.0	4.0
1325_9000296	3.0	4.0
1325_9000319	4.0	4.0
1325_9000322	4.0	4.0
1325_9000533	4.0	4.0
1325_9000534	4.0	4.0
1325_9000536	4.0	4.0
1325_9000678	4.0	4.0
1325_9000684	4.0	4.0
1365_0100003	2.0	2.0
1365_0100008	4.0	4.0
1365_0100009	3.0	3.0
1365_0100010	1.0	3.0
1365_0100031	5.0	2.0
1365_0100063	4.0	4.0
1365_0100064	4.0	4.0
1365_0100074	3.0	4.0
1365_0100093	4.0	4.0
1365_0100095	4.0	4.0
1365_0100103	5.0	4.0
1365_0100104	3.0	4.0
1365_0100118	4.0	4.0
1365_0100119	5.0	4.0
1365_0100123	4.0	4.0
1365_0100134	5.0	4.0
1365_0100138	5.0	4.0
1365_0100139	4.0	4.0
1365_0100174	3.0	4.0
1365_0100190	4.0	2.0
1365_0100194	4.0	4.0
1365_0100200	5.0	4.0
1365_0100202	3.0	4.0
1365_0100223	4.0	4.0
1365_0100231	4.0	4.0
1365_0100253	3.0	2.0
1365_0100258	5.0	4.0
1365_0100259	4.0	4.0
1365_0100260	4.0	4.0
1365_0100265	4.0	4.0
1365_0100277	4.0	4.0
1365_0100278	4.0	4.0
1365_0100280	2.0	4.0
1365_0100282	5.0	4.0
1365_0100451	2.0	4.0
1365_0100470	4.0	4.0
1365_0100473	4.0	4.0
1365_0100477	4.0	4.0
1385_0000012	2.0	1.0
1385_0000033	1.0	2.0
1385_0000039	2.0	2.0
1385_0000047	2.0	2.0
1385_0000051	2.0	2.0
1385_0000054	2.0	2.0
1385_0000114	2.0	2.0
1385_0000119	2.0	2.0
1385_0000122	2.0	2.0
1385_0000123	2.0	2.0
1385_0000126	2.0	2.0
1385_0000129	2.0	2.0
1385_0001103	2.0	1.0
1385_0001107	2.0	2.0
1385_0001110	2.0	2.0
1385_0001111	2.0	2.0
1385_0001125	2.0	2.0
1385_0001132	2.0	2.0
1385_0001137	2.0	2.0
1385_0001152	2.0	3.0
1385_0001155	2.0	2.0
1385_0001158	2.0	2.0
1385_0001164	1.0	2.0
1385_0001171	1.0	1.0
1385_0001178	1.0	1.0
1385_0001526	0.0	1.0
1385_0001716	2.0	2.0
1385_0001717	2.0	2.0
1385_0001723	0.0	1.0
1385_0001725	1.0	1.0
1385_0001729	2.0	2.0
1385_0001732	1.0	2.0
1385_0001738	0.0	1.0
1385_0001739	1.0	2.0
1385_0001744	0.0	2.0
1385_0001750	0.0	1.0
1385_0001751	1.0	2.0
1385_0001752	1.0	2.0
1385_0001753	1.0	2.0
1385_0001754	1.0	2.0
1385_0001756	2.0	2.0
1385_0001772	1.0	2.0
1385_0001774	1.0	1.0
1385_0001775	2.0	2.0
1385_0001787	1.0	2.0
1385_0001790	2.0	2.0
1385_0001792	1.0	2.0
1395_0000338	2.0	2.0
1395_0000340	2.0	4.0
1395_0000355	2.0	2.0
1395_0000380	5.0	3.0
1395_0000396	2.0	4.0
1395_0000402	2.0	1.0
1395_0000403	5.0	4.0
1395_0000404	4.0	4.0
1395_0000413	2.0	4.0
1395_0000415	2.0	2.0
1395_0000446	3.0	3.0
1395_0000454	2.0	4.0
1395_0000458	2.0	2.0
1395_0000460	2.0	2.0
1395_0000500	2.0	2.0
1395_0000528	4.0	2.0
1395_0000529	2.0	2.0
1395_0000549	2.0	4.0
1395_0000557	4.0	4.0
1395_0000559	2.0	3.0
1395_0000572	2.0	4.0
1395_0000579	1.0	1.0
1395_0000598	1.0	2.0
1395_0000608	1.0	2.0
1395_0000609	1.0	1.0
1395_0000639	1.0	4.0
1395_0001010	2.0	3.0
1395_0001019	2.0	2.0
1395_0001021	2.0	2.0
1395_0001034	2.0	2.0
1395_0001061	2.0	4.0
1395_0001064	2.0	4.0
1395_0001065	1.0	4.0
1395_0001068	1.0	2.0
1395_0001076	1.0	2.0
1395_0001078	2.0	2.0
1395_0001115	2.0	4.0
1395_0001126	1.0	2.0
1395_0001132	1.0	3.0
1395_0001160	2.0	3.0
1395_0001161	1.0	2.0
1395_0001167	1.0	3.0
1395_0001169	2.0	2.0
5 Fold, Dimension = Orthography

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 1.26
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.19
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.67      0.03      0.06        68
         2.0       0.56      0.70      0.62       162
         3.0       0.42      0.70      0.53       106
         4.0       0.60      0.49      0.54        85
         5.0       0.00      0.00      0.00        23

    accuracy                           0.51       452
   macro avg       0.37      0.32      0.29       452
weighted avg       0.51      0.51      0.46       452

[[  0   1   7   0   0   0]
 [  0   2  60   4   2   0]
 [  0   0 114  46   2   0]
 [  0   0  17  74  15   0]
 [  0   0   3  40  42   0]
 [  0   0   3  11   9   0]]
0.45717415116387694
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 1.00
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.98
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.57      0.40      0.47        68
         2.0       0.61      0.78      0.68       162
         3.0       0.56      0.45      0.50       106
         4.0       0.59      0.76      0.67        85
         5.0       0.00      0.00      0.00        23

    accuracy                           0.59       452
   macro avg       0.39      0.40      0.39       452
weighted avg       0.55      0.59      0.56       452

[[  0   6   2   0   0   0]
 [  0  27  41   0   0   0]
 [  0  14 127  19   2   0]
 [  0   0  31  48  27   0]
 [  0   0   4  16  65   0]
 [  0   0   4   3  16   0]]
0.5586463554473747
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.85
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.04
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.65      0.38      0.48        68
         2.0       0.66      0.73      0.69       162
         3.0       0.55      0.49      0.52       106
         4.0       0.55      0.89      0.68        85
         5.0       0.00      0.00      0.00        23

    accuracy                           0.60       452
   macro avg       0.40      0.42      0.40       452
weighted avg       0.57      0.60      0.57       452

[[  0   6   2   0   0   0]
 [  0  26  39   2   1   0]
 [  0   6 118  33   5   0]
 [  0   1  18  52  35   0]
 [  0   0   1   8  76   0]
 [  0   1   1   0  21   0]]
0.5700022868777588
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.70
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.02
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00         8
         1.0       0.73      0.35      0.48        68
         2.0       0.65      0.77      0.70       162
         3.0       0.56      0.54      0.55       106
         4.0       0.58      0.85      0.69        85
         5.0       0.00      0.00      0.00        23

    accuracy                           0.62       452
   macro avg       0.42      0.42      0.40       452
weighted avg       0.58      0.62      0.58       452

[[  0   5   3   0   0   0]
 [  0  24  42   2   0   0]
 [  0   3 125  30   4   0]
 [  0   1  21  57  27   0]
 [  0   0   1  12  72   0]
 [  0   0   1   1  21   0]]
0.5819957652057768
452 452 452
Filename	True Label	Prediction
1023_0001419	4.0	4.0
1023_0001420	3.0	3.0
1023_0101688	4.0	4.0
1023_0101690	2.0	3.0
1023_0101695	4.0	3.0
1023_0101700	4.0	4.0
1023_0101851	3.0	3.0
1023_0101855	2.0	3.0
1023_0101893	3.0	4.0
1023_0101894	2.0	3.0
1023_0101901	4.0	4.0
1023_0101906	3.0	3.0
1023_0101907	4.0	4.0
1023_0101909	4.0	4.0
1023_0103821	4.0	4.0
1023_0103829	4.0	4.0
1023_0107074	3.0	4.0
1023_0108304	4.0	4.0
1023_0108426	3.0	4.0
1023_0108649	4.0	4.0
1023_0108813	4.0	4.0
1023_0108888	3.0	4.0
1023_0108889	3.0	3.0
1023_0108908	4.0	3.0
1023_0108933	4.0	4.0
1023_0109030	3.0	3.0
1023_0109399	3.0	3.0
1023_0109401	3.0	3.0
1023_0109402	3.0	3.0
1023_0109422	5.0	4.0
1023_0109495	3.0	3.0
1023_0109606	5.0	4.0
1023_0109649	3.0	3.0
1023_0109671	3.0	4.0
1023_0109721	3.0	3.0
1023_0109945	4.0	4.0
1023_0109946	2.0	3.0
1031_0001997	3.0	3.0
1031_0002084	4.0	4.0
1031_0002085	4.0	4.0
1031_0002086	3.0	3.0
1031_0002087	3.0	3.0
1031_0002089	4.0	4.0
1031_0002091	3.0	4.0
1031_0002195	3.0	3.0
1031_0003035	3.0	3.0
1031_0003042	3.0	4.0
1031_0003043	5.0	4.0
1031_0003048	4.0	4.0
1031_0003054	3.0	4.0
1031_0003072	5.0	4.0
1031_0003074	5.0	4.0
1031_0003085	4.0	4.0
1031_0003095	3.0	3.0
1031_0003121	5.0	4.0
1031_0003130	5.0	4.0
1031_0003131	3.0	4.0
1031_0003145	4.0	4.0
1031_0003156	3.0	4.0
1031_0003157	4.0	4.0
1031_0003162	4.0	4.0
1031_0003163	3.0	3.0
1031_0003172	3.0	4.0
1031_0003182	5.0	4.0
1031_0003184	4.0	4.0
1031_0003203	2.0	2.0
1031_0003207	4.0	4.0
1031_0003214	3.0	4.0
1031_0003218	4.0	4.0
1031_0003221	2.0	4.0
1031_0003225	5.0	4.0
1031_0003230	4.0	4.0
1031_0003234	3.0	3.0
1031_0003235	4.0	3.0
1031_0003237	4.0	4.0
1031_0003239	5.0	4.0
1031_0003245	3.0	3.0
1031_0003260	3.0	3.0
1031_0003262	3.0	4.0
1031_0003273	3.0	3.0
1031_0003313	3.0	3.0
1031_0003338	4.0	4.0
1031_0003352	3.0	3.0
1031_0003353	3.0	3.0
1031_0003354	3.0	4.0
1031_0003366	3.0	3.0
1031_0003367	4.0	4.0
1031_0003389	4.0	3.0
1031_0003390	5.0	4.0
1031_0003391	3.0	3.0
1031_0003393	4.0	4.0
1031_0003407	3.0	3.0
1031_0003408	2.0	3.0
1031_0003410	4.0	4.0
1061_0120273	2.0	2.0
1061_0120275	2.0	2.0
1061_0120279	2.0	2.0
1061_0120286	1.0	2.0
1061_0120287	2.0	2.0
1061_0120291	2.0	2.0
1061_0120297	3.0	3.0
1061_0120300	3.0	2.0
1061_0120302	2.0	2.0
1061_0120303	1.0	3.0
1061_0120306	4.0	3.0
1061_0120307	3.0	2.0
1061_0120315	2.0	1.0
1061_0120318	3.0	3.0
1061_0120323	2.0	2.0
1061_0120324	2.0	2.0
1061_0120341	2.0	2.0
1061_0120343	3.0	3.0
1061_0120345	3.0	2.0
1061_0120346	3.0	3.0
1061_0120347	1.0	2.0
1061_0120349	2.0	2.0
1061_0120352	1.0	1.0
1061_0120353	1.0	1.0
1061_0120383	4.0	3.0
1061_0120384	2.0	2.0
1061_0120387	2.0	3.0
1061_0120404	1.0	1.0
1061_0120405	2.0	2.0
1061_0120407	3.0	2.0
1061_0120409	3.0	3.0
1061_0120410	3.0	2.0
1061_0120414	4.0	3.0
1061_0120415	2.0	2.0
1061_0120421	3.0	3.0
1061_0120425	2.0	2.0
1061_0120428	3.0	3.0
1061_0120431	3.0	3.0
1061_0120439	2.0	2.0
1061_0120481	4.0	3.0
1061_0120491	4.0	2.0
1061_0120493	3.0	2.0
1061_0120494	2.0	2.0
1061_0120497	4.0	4.0
1061_0120499	4.0	3.0
1061_0120857	3.0	3.0
1061_0120882	4.0	3.0
1061_0120884	3.0	3.0
1061_0120890	2.0	2.0
1061_0120894	2.0	3.0
1061_1029115	3.0	3.0
1061_1202919	2.0	2.0
1071_0020001	2.0	1.0
1071_0024680	2.0	2.0
1071_0024687	1.0	1.0
1071_0024688	1.0	2.0
1071_0024694	2.0	2.0
1071_0024706	2.0	2.0
1071_0024709	3.0	2.0
1071_0024712	2.0	2.0
1071_0024770	1.0	1.0
1071_0024776	1.0	1.0
1071_0024798	1.0	1.0
1071_0024803	1.0	2.0
1071_0024804	1.0	2.0
1071_0024809	1.0	1.0
1071_0024810	2.0	2.0
1071_0024815	1.0	2.0
1071_0024825	1.0	1.0
1071_0024831	1.0	2.0
1071_0024833	2.0	2.0
1071_0024835	2.0	2.0
1071_0024844	1.0	1.0
1071_0024849	0.0	1.0
1071_0024852	1.0	1.0
1071_0024854	1.0	2.0
1071_0024855	2.0	2.0
1071_0024860	2.0	2.0
1071_0024864	1.0	1.0
1071_0024867	3.0	2.0
1071_0024878	2.0	3.0
1071_0242042	1.0	2.0
1071_0242071	0.0	1.0
1071_0242092	0.0	1.0
1071_0243591	2.0	2.0
1071_0243593	2.0	2.0
1071_0248303	1.0	1.0
1071_0248313	2.0	3.0
1071_0248323	2.0	2.0
1071_0248332	3.0	3.0
1091_0000012	2.0	2.0
1091_0000014	1.0	2.0
1091_0000024	3.0	1.0
1091_0000033	2.0	2.0
1091_0000059	2.0	3.0
1091_0000073	3.0	2.0
1091_0000075	2.0	3.0
1091_0000079	1.0	2.0
1091_0000102	2.0	2.0
1091_0000123	3.0	3.0
1091_0000125	3.0	3.0
1091_0000152	2.0	2.0
1091_0000159	2.0	3.0
1091_0000161	2.0	2.0
1091_0000165	1.0	2.0
1091_0000166	2.0	2.0
1091_0000171	2.0	2.0
1091_0000172	2.0	2.0
1091_0000197	1.0	2.0
1091_0000198	2.0	2.0
1091_0000204	2.0	2.0
1091_0000205	2.0	2.0
1091_0000208	1.0	2.0
1091_0000210	2.0	2.0
1091_0000217	3.0	2.0
1091_0000218	3.0	2.0
1091_0000222	2.0	2.0
1091_0000225	3.0	2.0
1091_0000232	2.0	2.0
1091_0000242	1.0	2.0
1091_0000244	2.0	3.0
1091_0000247	2.0	2.0
1091_0000248	2.0	2.0
1091_0000249	2.0	2.0
1091_0000251	2.0	2.0
0602	2.0	3.0
0603	2.0	2.0
0604	2.0	2.0
0617	2.0	2.0
0618	2.0	2.0
0621	2.0	2.0
0626	3.0	2.0
0635	2.0	2.0
0641	1.0	2.0
0642	2.0	3.0
0643	2.0	2.0
0715	2.0	2.0
0718	2.0	2.0
0721	3.0	2.0
0805	2.0	2.0
0809	2.0	2.0
0821	2.0	2.0
0902	3.0	2.0
0917	2.0	2.0
0922	2.0	2.0
0930	2.0	2.0
1010	2.0	2.0
1023	2.0	2.0
1114	2.0	2.0
BER0611006	3.0	3.0
KYJ0611005B	1.0	2.0
LIB0611004A	3.0	2.0
LIB0611011	1.0	2.0
LON0611004A	2.0	2.0
MOS0509001	2.0	3.0
PAR1011008A	2.0	2.0
PAR1011013	3.0	3.0
PAR1011014	2.0	3.0
PAR1011016	3.0	3.0
PHA0111003B	2.0	2.0
PHA0111005A	3.0	2.0
PHA0112002B	2.0	2.0
PHA0112003A	2.0	2.0
PHA0112006B	3.0	3.0
PHA0112009A	2.0	2.0
PHA0112012B	2.0	2.0
PHA0210004	2.0	2.0
PHA0210007	2.0	2.0
PHA0411008A	2.0	2.0
PHA0411010A	2.0	2.0
PHA0411010B	2.0	2.0
PHA0411011B	2.0	2.0
PHA0411027	2.0	3.0
PHA0411028	2.0	2.0
PHA0411032	2.0	3.0
PHA0411036	2.0	3.0
PHA0411053	3.0	3.0
PHA0411055	3.0	3.0
PHA0411056	3.0	3.0
PHA0411060	3.0	3.0
PHA0411062	2.0	3.0
PHA0509015	2.0	2.0
PHA0509027	2.0	2.0
PHA0509033	2.0	2.0
PHA0509035	2.0	2.0
PHA0509042	3.0	3.0
PHA0509043	2.0	3.0
PHA0510003A	2.0	2.0
PHA0510004A	1.0	2.0
PHA0510029	3.0	3.0
PHA0510032	3.0	4.0
PHA0510049	3.0	2.0
PHA0610005A	2.0	2.0
PHA0610018	3.0	3.0
PHA0610026	2.0	3.0
PHA0710010	3.0	3.0
PHA0710017	2.0	3.0
PHA0710018	3.0	3.0
PHA0809010	2.0	2.0
PHA0810001	3.0	2.0
PHA0810009	2.0	3.0
PHA0810011	2.0	3.0
PHA0811010	2.0	3.0
PHA1109006	3.0	3.0
PHA1109008	1.0	1.0
PHA1109023	2.0	2.0
PHA1109026	2.0	3.0
PHA1110014	2.0	3.0
PHA1110016	1.0	2.0
PHA1111004B	1.0	2.0
PHA1111008A	2.0	2.0
PHA1111008B	2.0	2.0
PHA1111009A	1.0	2.0
ST071122B	2.0	2.0
VAR0910010	2.0	2.0
VAR0910011	2.0	2.0
1325_1001011	4.0	4.0
1325_1001015	4.0	3.0
1325_1001017	4.0	4.0
1325_1001018	4.0	4.0
1325_1001020	4.0	4.0
1325_1001029	4.0	4.0
1325_1001039	5.0	4.0
1325_1001043	4.0	4.0
1325_1001047	4.0	3.0
1325_1001059	4.0	4.0
1325_1001076	5.0	4.0
1325_1001086	4.0	4.0
1325_1001088	4.0	4.0
1325_1001110	4.0	4.0
1325_1001111	4.0	4.0
1325_1001121	4.0	4.0
1325_1001126	3.0	4.0
1325_1001129	3.0	4.0
1325_1001133	5.0	4.0
1325_1001143	4.0	4.0
1325_1001152	4.0	4.0
1325_1001158	4.0	4.0
1325_1001161	4.0	4.0
1325_9000152	4.0	4.0
1325_9000210	3.0	4.0
1325_9000278	3.0	4.0
1325_9000279	3.0	4.0
1325_9000317	4.0	4.0
1325_9000503	4.0	4.0
1325_9000675	4.0	4.0
1325_9000676	3.0	4.0
1325_9000686	3.0	4.0
1365_0100007	2.0	3.0
1365_0100012	4.0	4.0
1365_0100016	4.0	4.0
1365_0100019	4.0	4.0
1365_0100020	3.0	4.0
1365_0100021	3.0	3.0
1365_0100029	1.0	2.0
1365_0100057	4.0	4.0
1365_0100065	2.0	4.0
1365_0100066	3.0	2.0
1365_0100098	2.0	2.0
1365_0100107	4.0	4.0
1365_0100120	5.0	4.0
1365_0100133	5.0	4.0
1365_0100164	3.0	4.0
1365_0100168	4.0	4.0
1365_0100171	4.0	4.0
1365_0100175	4.0	4.0
1365_0100182	4.0	4.0
1365_0100183	4.0	4.0
1365_0100187	5.0	4.0
1365_0100191	3.0	4.0
1365_0100192	4.0	4.0
1365_0100201	5.0	4.0
1365_0100204	5.0	4.0
1365_0100205	5.0	4.0
1365_0100225	4.0	4.0
1365_0100230	4.0	4.0
1365_0100256	4.0	4.0
1365_0100263	4.0	4.0
1365_0100267	4.0	4.0
1365_0100285	3.0	4.0
1365_0100456	2.0	4.0
1365_0100459	4.0	4.0
1365_0100471	3.0	4.0
1365_0100474	4.0	4.0
1365_0100476	4.0	4.0
1365_0100478	4.0	4.0
1385_0000016	1.0	1.0
1385_0000022	0.0	2.0
1385_0000036	2.0	2.0
1385_0000037	2.0	2.0
1385_0000040	1.0	1.0
1385_0000041	2.0	2.0
1385_0000045	2.0	2.0
1385_0000052	1.0	2.0
1385_0000053	1.0	2.0
1385_0000059	2.0	2.0
1385_0000095	1.0	1.0
1385_0000098	2.0	2.0
1385_0000120	1.0	1.0
1385_0000124	2.0	4.0
1385_0000125	2.0	2.0
1385_0001108	2.0	2.0
1385_0001118	2.0	2.0
1385_0001119	2.0	2.0
1385_0001127	2.0	2.0
1385_0001128	1.0	2.0
1385_0001133	2.0	2.0
1385_0001150	2.0	2.0
1385_0001153	2.0	2.0
1385_0001156	2.0	2.0
1385_0001159	1.0	2.0
1385_0001162	1.0	2.0
1385_0001165	2.0	2.0
1385_0001167	2.0	2.0
1385_0001170	1.0	1.0
1385_0001175	1.0	2.0
1385_0001193	2.0	2.0
1385_0001199	2.0	2.0
1385_0001501	1.0	2.0
1385_0001524	1.0	2.0
1385_0001527	2.0	2.0
1385_0001737	2.0	2.0
1385_0001760	0.0	2.0
1385_0001764	1.0	2.0
1385_0001765	0.0	1.0
1385_0001771	1.0	2.0
1385_0001773	1.0	2.0
1385_0001785	1.0	1.0
1385_0001788	1.0	2.0
1385_0001791	1.0	2.0
1385_0001798	1.0	2.0
1395_0000337	1.0	1.0
1395_0000398	5.0	4.0
1395_0000432	5.0	3.0
1395_0000448	2.0	2.0
1395_0000470	2.0	2.0
1395_0000514	4.0	4.0
1395_0000525	5.0	2.0
1395_0000531	2.0	1.0
1395_0000563	2.0	2.0
1395_0000581	1.0	3.0
1395_0000591	0.0	1.0
1395_0000597	1.0	2.0
1395_0000599	1.0	2.0
1395_0000607	1.0	1.0
1395_0000642	1.0	2.0
1395_0000649	2.0	2.0
1395_0001040	1.0	1.0
1395_0001045	2.0	2.0
1395_0001060	2.0	2.0
1395_0001084	2.0	2.0
1395_0001119	2.0	2.0
1395_0001121	1.0	2.0
1395_0001124	0.0	2.0
1395_0001131	1.0	1.0
1395_0001141	2.0	2.0
1395_0001145	3.0	2.0
1395_0001147	1.0	2.0
Averaged weighted F1-scores 0.5538954720558855
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
144.90552995391704 65.35717405024758
LANGUAGE: IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
1 Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.98
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.78
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        19
         1.0       0.64      0.53      0.58       118
         2.0       0.64      0.74      0.69       163
         3.0       0.74      0.79      0.76       121
         4.0       0.61      0.73      0.67        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.67       452
   macro avg       0.44      0.47      0.45       452
weighted avg       0.64      0.67      0.65       452

[[  0  18   1   0   0   0]
 [  0  63  55   0   0   0]
 [  0  16 121  26   0   0]
 [  0   1  11  96  13   0]
 [  0   0   0   8  22   0]
 [  0   0   0   0   1   0]]
0.6499399748659348
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.73
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.87
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        19
         1.0       0.64      0.51      0.57       118
         2.0       0.63      0.72      0.67       163
         3.0       0.72      0.75      0.74       121
         4.0       0.57      0.87      0.68        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.65       452
   macro avg       0.43      0.47      0.44       452
weighted avg       0.62      0.65      0.63       452

[[  0  18   1   0   0   0]
 [  0  60  58   0   0   0]
 [  0  15 117  31   0   0]
 [  0   1  10  91  19   0]
 [  0   0   0   4  26   0]
 [  0   0   0   0   1   0]]
0.6322258000825371
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.58
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.76
              precision    recall  f1-score   support

         0.0       0.67      0.32      0.43        19
         1.0       0.66      0.73      0.69       118
         2.0       0.72      0.68      0.70       163
         3.0       0.75      0.83      0.78       121
         4.0       0.62      0.50      0.56        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.70       452
   macro avg       0.57      0.51      0.53       452
weighted avg       0.70      0.70      0.70       452

[[  6  12   1   0   0   0]
 [  2  86  30   0   0   0]
 [  1  32 111  19   0   0]
 [  0   1  12 100   8   0]
 [  0   0   0  15  15   0]
 [  0   0   0   0   1   0]]
0.6977277529045126
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.43
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.81
              precision    recall  f1-score   support

         0.0       0.71      0.26      0.38        19
         1.0       0.68      0.66      0.67       118
         2.0       0.69      0.69      0.69       163
         3.0       0.74      0.83      0.79       121
         4.0       0.68      0.70      0.69        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.70       452
   macro avg       0.58      0.52      0.54       452
weighted avg       0.70      0.70      0.69       452

[[  5  12   2   0   0   0]
 [  1  78  39   0   0   0]
 [  1  24 112  26   0   0]
 [  0   1  10 101   9   0]
 [  0   0   0   9  21   0]
 [  0   0   0   0   1   0]]
0.6948512836811119
452 452 452
Filename	True Label	Prediction
1023_0101693	4.0	3.0
1023_0101694	3.0	3.0
1023_0101752	3.0	3.0
1023_0101841	3.0	3.0
1023_0101848	2.0	3.0
1023_0101855	3.0	3.0
1023_0101907	4.0	3.0
1023_0103827	3.0	3.0
1023_0103837	3.0	3.0
1023_0103841	3.0	3.0
1023_0104203	3.0	3.0
1023_0107075	3.0	3.0
1023_0107727	3.0	3.0
1023_0108306	3.0	3.0
1023_0108510	3.0	3.0
1023_0108641	4.0	3.0
1023_0108812	3.0	3.0
1023_0108815	3.0	3.0
1023_0108886	3.0	3.0
1023_0108887	2.0	3.0
1023_0108955	3.0	3.0
1023_0108958	3.0	3.0
1023_0108992	3.0	3.0
1023_0109022	3.0	3.0
1023_0109027	3.0	3.0
1023_0109250	3.0	3.0
1023_0109401	3.0	3.0
1023_0109422	3.0	3.0
1023_0109505	3.0	3.0
1023_0109590	3.0	3.0
1023_0109649	3.0	3.0
1023_0109717	3.0	3.0
1023_0109878	3.0	3.0
1023_0109880	4.0	3.0
1031_0001949	4.0	4.0
1031_0002002	3.0	4.0
1031_0002011	4.0	4.0
1031_0002042	4.0	4.0
1031_0002089	4.0	4.0
1031_0002091	4.0	4.0
1031_0002092	4.0	4.0
1031_0002197	4.0	4.0
1031_0003012	4.0	4.0
1031_0003035	4.0	4.0
1031_0003043	5.0	4.0
1031_0003048	4.0	4.0
1031_0003091	3.0	3.0
1031_0003092	3.0	4.0
1031_0003131	4.0	4.0
1031_0003132	4.0	4.0
1031_0003136	4.0	3.0
1031_0003160	3.0	4.0
1031_0003161	4.0	4.0
1031_0003165	3.0	3.0
1031_0003166	3.0	3.0
1031_0003167	3.0	4.0
1031_0003169	3.0	4.0
1031_0003170	3.0	3.0
1031_0003182	4.0	4.0
1031_0003212	3.0	3.0
1031_0003217	4.0	4.0
1031_0003218	4.0	4.0
1031_0003226	4.0	4.0
1031_0003234	3.0	3.0
1031_0003238	4.0	4.0
1031_0003240	3.0	4.0
1031_0003244	4.0	4.0
1031_0003245	4.0	4.0
1031_0003260	4.0	3.0
1031_0003331	3.0	4.0
1031_0003352	3.0	3.0
1031_0003354	3.0	4.0
1031_0003355	4.0	3.0
1031_0003386	3.0	4.0
1031_0003410	4.0	4.0
1061_0120273	1.0	2.0
1061_0120286	1.0	1.0
1061_0120289	2.0	2.0
1061_0120290	2.0	2.0
1061_0120300	2.0	2.0
1061_0120304	2.0	2.0
1061_0120315	2.0	2.0
1061_0120327	2.0	2.0
1061_0120332	2.0	2.0
1061_0120336	1.0	2.0
1061_0120337	2.0	2.0
1061_0120341	2.0	2.0
1061_0120343	2.0	2.0
1061_0120358	1.0	2.0
1061_0120370	2.0	3.0
1061_0120376	2.0	2.0
1061_0120384	2.0	2.0
1061_0120390	2.0	2.0
1061_0120391	1.0	2.0
1061_0120408	3.0	2.0
1061_0120426	2.0	3.0
1061_0120438	2.0	2.0
1061_0120441	2.0	2.0
1061_0120442	2.0	2.0
1061_0120443	0.0	1.0
1061_0120460	2.0	2.0
1061_0120479	2.0	2.0
1061_0120482	2.0	2.0
1061_0120485	2.0	2.0
1061_0120497	3.0	3.0
1061_0120498	2.0	2.0
1061_0120880	3.0	3.0
1061_0120882	3.0	3.0
1061_0120886	2.0	2.0
1061_0120889	1.0	2.0
1061_1029114	1.0	2.0
1061_1029117	2.0	2.0
1061_1029119	2.0	2.0
1061_1029120	2.0	2.0
1071_0020001	1.0	1.0
1071_0024678	2.0	1.0
1071_0024709	2.0	2.0
1071_0024710	1.0	1.0
1071_0024715	2.0	2.0
1071_0024758	2.0	2.0
1071_0024759	0.0	2.0
1071_0024773	1.0	1.0
1071_0024778	1.0	1.0
1071_0024782	0.0	0.0
1071_0024800	0.0	1.0
1071_0024802	2.0	2.0
1071_0024806	1.0	1.0
1071_0024808	1.0	2.0
1071_0024809	1.0	1.0
1071_0024814	1.0	1.0
1071_0024819	2.0	2.0
1071_0024838	0.0	0.0
1071_0024857	1.0	2.0
1071_0024860	1.0	1.0
1071_0024876	1.0	2.0
1071_0024877	1.0	2.0
1071_0024881	2.0	2.0
1071_0241833	1.0	1.0
1071_0242011	1.0	1.0
1071_0242013	1.0	1.0
1071_0242022	0.0	0.0
1071_0242043	0.0	1.0
1071_0243592	1.0	1.0
1071_0243623	1.0	1.0
1071_0248304	1.0	1.0
1071_0248310	1.0	1.0
1071_0248315	0.0	0.0
1071_0248317	0.0	0.0
1071_0248321	1.0	1.0
1071_0248322	1.0	1.0
1071_0248330	2.0	1.0
1071_0248334	2.0	2.0
1071_0248336	1.0	1.0
1071_0248338	1.0	2.0
1071_0248339	1.0	1.0
1071_0248344	2.0	1.0
1071_0248346	1.0	1.0
1071_0248349	0.0	1.0
1091_0000007	2.0	2.0
1091_0000009	0.0	1.0
1091_0000012	1.0	1.0
1091_0000014	0.0	1.0
1091_0000020	1.0	2.0
1091_0000029	2.0	1.0
1091_0000034	2.0	1.0
1091_0000038	2.0	1.0
1091_0000052	1.0	1.0
1091_0000055	1.0	2.0
1091_0000058	2.0	2.0
1091_0000061	2.0	1.0
1091_0000063	2.0	1.0
1091_0000068	1.0	2.0
1091_0000070	2.0	2.0
1091_0000077	2.0	0.0
1091_0000078	3.0	1.0
1091_0000113	1.0	2.0
1091_0000116	3.0	2.0
1091_0000125	2.0	2.0
1091_0000127	2.0	2.0
1091_0000144	2.0	1.0
1091_0000156	3.0	2.0
1091_0000163	2.0	2.0
1091_0000168	2.0	2.0
1091_0000193	2.0	1.0
1091_0000203	2.0	2.0
1091_0000217	2.0	2.0
1091_0000218	2.0	2.0
1091_0000230	2.0	2.0
1091_0000232	2.0	2.0
1091_0000236	2.0	2.0
1091_0000242	1.0	2.0
1091_0000243	1.0	2.0
1091_0000244	2.0	2.0
1091_0000247	2.0	2.0
1091_0000254	2.0	2.0
1091_0000255	0.0	2.0
1091_0000258	2.0	2.0
1091_0000263	2.0	2.0
1091_0000269	1.0	2.0
1091_0000270	2.0	2.0
1091_0000273	1.0	2.0
1091_0000276	2.0	2.0
0605	2.0	2.0
0610	2.0	2.0
0615	2.0	2.0
0618	2.0	2.0
0619	2.0	2.0
0622	1.0	2.0
0624	2.0	2.0
0625	2.0	1.0
0633	2.0	2.0
0642	2.0	2.0
0807	2.0	2.0
0808	1.0	2.0
0817	2.0	2.0
0820	1.0	1.0
0822	1.0	2.0
0823	2.0	2.0
0826	1.0	2.0
0828	2.0	2.0
0829	2.0	2.0
0923	1.0	2.0
0924	2.0	2.0
1014	2.0	2.0
1016	1.0	2.0
1019	2.0	2.0
1114	2.0	2.0
1117	1.0	2.0
KYJ0611004A	1.0	1.0
KYJ0611005A	1.0	1.0
KYJ0611009B	1.0	2.0
LIB0611002A	1.0	1.0
LON0610002B	1.0	1.0
LON0611002B	1.0	1.0
LON0611004B	1.0	1.0
MOS0611014	2.0	3.0
PAR1011008A	2.0	1.0
PHA0111015	3.0	3.0
PHA0112002B	1.0	1.0
PHA0112009A	2.0	1.0
PHA0209001	1.0	2.0
PHA0209026	3.0	3.0
PHA0209028	3.0	3.0
PHA0209031	4.0	3.0
PHA0411009A	1.0	2.0
PHA0411012A	1.0	1.0
PHA0411012B	1.0	1.0
PHA0411027	2.0	3.0
PHA0411029	3.0	3.0
PHA0411030	3.0	3.0
PHA0411031	3.0	3.0
PHA0411034	2.0	2.0
PHA0411043	3.0	3.0
PHA0411058	3.0	3.0
PHA0411060	2.0	3.0
PHA0509017	3.0	3.0
PHA0509026	3.0	3.0
PHA0509027	3.0	3.0
PHA0509030	3.0	3.0
PHA0509031	2.0	3.0
PHA0509038	2.0	2.0
PHA0509041	3.0	3.0
PHA0510002A	1.0	1.0
PHA0510002B	1.0	1.0
PHA0510003B	1.0	1.0
PHA0510035	3.0	3.0
PHA0510037	2.0	3.0
PHA0510039	3.0	3.0
PHA0510047	2.0	3.0
PHA0510048	3.0	3.0
PHA0510050	3.0	3.0
PHA0610006A	2.0	1.0
PHA0610007A	1.0	1.0
PHA0709008	3.0	3.0
PHA0810012	3.0	3.0
PHA0811010	3.0	3.0
PHA0811014	3.0	3.0
PHA0811016	3.0	3.0
PHA1109007	2.0	3.0
PHA1109028	3.0	3.0
PHA1110015	3.0	3.0
PHA1110021	3.0	3.0
PHA1111002A	1.0	1.0
PHA1111002B	1.0	1.0
PHA1111004B	1.0	1.0
VAR0909005	3.0	3.0
VAR0909006	3.0	3.0
VAR0909010	3.0	3.0
1325_1001009	3.0	3.0
1325_1001018	3.0	3.0
1325_1001027	3.0	3.0
1325_1001028	3.0	3.0
1325_1001037	2.0	2.0
1325_1001040	3.0	3.0
1325_1001042	2.0	3.0
1325_1001045	3.0	3.0
1325_1001058	2.0	3.0
1325_1001077	3.0	3.0
1325_1001079	3.0	3.0
1325_1001082	3.0	3.0
1325_1001087	2.0	3.0
1325_1001090	2.0	3.0
1325_1001095	2.0	3.0
1325_1001096	3.0	3.0
1325_1001109	2.0	3.0
1325_1001122	3.0	3.0
1325_1001130	3.0	2.0
1325_1001135	3.0	3.0
1325_1001136	3.0	3.0
1325_1001138	2.0	3.0
1325_1001139	3.0	3.0
1325_1001144	3.0	2.0
1325_1001160	3.0	3.0
1325_1001166	3.0	3.0
1325_9000059	3.0	3.0
1325_9000102	3.0	2.0
1325_9000105	2.0	3.0
1325_9000107	3.0	3.0
1325_9000143	3.0	3.0
1325_9000144	3.0	3.0
1325_9000186	3.0	3.0
1325_9000210	2.0	3.0
1325_9000237	3.0	3.0
1325_9000320	3.0	3.0
1325_9000321	3.0	3.0
1325_9000322	3.0	3.0
1325_9000503	3.0	3.0
1325_9000601	3.0	3.0
1325_9000602	4.0	3.0
1325_9000674	3.0	3.0
1325_9000677	3.0	3.0
1325_9000684	3.0	3.0
1325_9000685	3.0	3.0
1325_9000700	3.0	3.0
1325_9000750	3.0	2.0
1365_0100003	2.0	2.0
1365_0100006	2.0	2.0
1365_0100009	2.0	2.0
1365_0100026	2.0	1.0
1365_0100031	2.0	2.0
1365_0100057	2.0	3.0
1365_0100070	2.0	2.0
1365_0100071	3.0	2.0
1365_0100096	2.0	2.0
1365_0100104	2.0	2.0
1365_0100120	3.0	3.0
1365_0100121	2.0	3.0
1365_0100123	2.0	2.0
1365_0100136	2.0	2.0
1365_0100164	2.0	3.0
1365_0100167	2.0	2.0
1365_0100175	2.0	2.0
1365_0100179	2.0	2.0
1365_0100184	2.0	2.0
1365_0100185	2.0	2.0
1365_0100191	2.0	3.0
1365_0100196	2.0	3.0
1365_0100203	2.0	2.0
1365_0100211	3.0	3.0
1365_0100215	2.0	2.0
1365_0100218	2.0	2.0
1365_0100251	2.0	3.0
1365_0100252	2.0	2.0
1365_0100263	3.0	3.0
1365_0100268	2.0	2.0
1365_0100278	3.0	2.0
1365_0100288	2.0	2.0
1365_0100289	2.0	2.0
1365_0100448	2.0	2.0
1365_0100478	2.0	2.0
1385_0000021	1.0	1.0
1385_0000022	1.0	2.0
1385_0000033	1.0	1.0
1385_0000034	1.0	1.0
1385_0000040	1.0	1.0
1385_0000041	1.0	1.0
1385_0000042	1.0	1.0
1385_0000044	2.0	2.0
1385_0000047	1.0	1.0
1385_0000057	1.0	1.0
1385_0000102	2.0	1.0
1385_0000103	1.0	1.0
1385_0000119	1.0	1.0
1385_0000120	0.0	1.0
1385_0001104	1.0	1.0
1385_0001107	1.0	1.0
1385_0001121	2.0	1.0
1385_0001123	2.0	1.0
1385_0001130	1.0	0.0
1385_0001138	1.0	1.0
1385_0001152	2.0	2.0
1385_0001157	1.0	1.0
1385_0001167	1.0	1.0
1385_0001195	2.0	2.0
1385_0001522	0.0	1.0
1385_0001727	0.0	1.0
1385_0001729	1.0	2.0
1385_0001734	1.0	1.0
1385_0001746	1.0	1.0
1385_0001750	0.0	1.0
1385_0001757	1.0	1.0
1385_0001760	1.0	1.0
1385_0001761	1.0	1.0
1385_0001788	1.0	1.0
1385_0001789	1.0	1.0
1385_0001793	1.0	1.0
1385_0001798	1.0	1.0
1395_0000340	2.0	2.0
1395_0000353	1.0	1.0
1395_0000366	2.0	2.0
1395_0000398	2.0	2.0
1395_0000403	2.0	2.0
1395_0000409	2.0	2.0
1395_0000438	3.0	2.0
1395_0000447	2.0	2.0
1395_0000449	2.0	2.0
1395_0000451	2.0	1.0
1395_0000460	1.0	1.0
1395_0000500	1.0	1.0
1395_0000526	1.0	1.0
1395_0000537	2.0	2.0
1395_0000547	2.0	2.0
1395_0000555	1.0	1.0
1395_0000556	1.0	2.0
1395_0000557	2.0	2.0
1395_0000559	2.0	1.0
1395_0000563	2.0	1.0
1395_0000575	1.0	1.0
1395_0000582	0.0	1.0
1395_0000583	1.0	2.0
1395_0000587	0.0	1.0
1395_0000608	1.0	1.0
1395_0000610	2.0	1.0
1395_0000626	2.0	2.0
1395_0000631	1.0	2.0
1395_0001010	1.0	2.0
1395_0001020	1.0	1.0
1395_0001060	1.0	2.0
1395_0001061	2.0	2.0
1395_0001074	1.0	2.0
1395_0001080	2.0	1.0
1395_0001084	1.0	2.0
1395_0001093	1.0	1.0
1395_0001101	2.0	1.0
1395_0001104	1.0	1.0
1395_0001108	1.0	1.0
1395_0001109	1.0	2.0
1395_0001119	2.0	2.0
1395_0001120	1.0	1.0
1395_0001147	2.0	2.0
1395_0001158	1.0	1.0
1395_0001171	1.0	1.0
2 Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.99
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.75
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        19
         1.0       0.67      0.66      0.66       118
         2.0       0.71      0.71      0.71       163
         3.0       0.73      0.93      0.81       121
         4.0       0.80      0.53      0.64        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.71       452
   macro avg       0.48      0.47      0.47       452
weighted avg       0.68      0.71      0.69       452

[[  0  19   0   0   0   0]
 [  0  78  40   0   0   0]
 [  0  20 115  28   0   0]
 [  0   0   6 112   3   0]
 [  0   0   0  14  16   0]
 [  0   0   0   0   1   0]]
0.6898269722053694
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.75
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.69
              precision    recall  f1-score   support

         0.0       1.00      0.05      0.10        19
         1.0       0.66      0.62      0.64       118
         2.0       0.69      0.76      0.72       163
         3.0       0.80      0.84      0.82       121
         4.0       0.71      0.80      0.75        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.72       452
   macro avg       0.64      0.51      0.51       452
weighted avg       0.73      0.72      0.70       452

[[  1  18   0   0   0   0]
 [  0  73  45   0   0   0]
 [  0  19 124  19   1   0]
 [  0   0  11 102   8   0]
 [  0   0   0   6  24   0]
 [  0   0   0   0   1   0]]
0.7020970995100351
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.62
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.69
              precision    recall  f1-score   support

         0.0       1.00      0.11      0.19        19
         1.0       0.70      0.62      0.65       118
         2.0       0.68      0.81      0.74       163
         3.0       0.80      0.78      0.79       121
         4.0       0.66      0.77      0.71        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.72       452
   macro avg       0.64      0.51      0.51       452
weighted avg       0.73      0.72      0.70       452

[[  2  17   0   0   0   0]
 [  0  73  45   0   0   0]
 [  0  15 132  16   0   0]
 [  0   0  16  94  11   0]
 [  0   0   0   7  23   0]
 [  0   0   0   0   1   0]]
0.7047824306595971
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.49
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.73
              precision    recall  f1-score   support

         0.0       1.00      0.32      0.48        19
         1.0       0.72      0.64      0.68       118
         2.0       0.67      0.80      0.73       163
         3.0       0.80      0.75      0.77       121
         4.0       0.67      0.73      0.70        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.72       452
   macro avg       0.64      0.54      0.56       452
weighted avg       0.73      0.72      0.71       452

[[  6  12   1   0   0   0]
 [  0  75  43   0   0   0]
 [  0  17 131  15   0   0]
 [  0   0  20  91  10   0]
 [  0   0   0   8  22   0]
 [  0   0   0   0   1   0]]
0.714166486224069
452 452 452
Filename	True Label	Prediction
1023_0101689	2.0	2.0
1023_0101691	3.0	3.0
1023_0101695	3.0	3.0
1023_0101700	3.0	2.0
1023_0101751	3.0	3.0
1023_0101845	3.0	3.0
1023_0101856	3.0	3.0
1023_0101894	3.0	3.0
1023_0101895	3.0	3.0
1023_0101898	4.0	3.0
1023_0102118	3.0	3.0
1023_0103821	3.0	3.0
1023_0103824	3.0	4.0
1023_0103831	3.0	3.0
1023_0103832	3.0	2.0
1023_0103838	3.0	3.0
1023_0103840	3.0	3.0
1023_0103843	3.0	2.0
1023_0103883	3.0	4.0
1023_0107740	3.0	3.0
1023_0107780	3.0	3.0
1023_0107783	3.0	3.0
1023_0107784	2.0	2.0
1023_0108518	3.0	3.0
1023_0108649	3.0	3.0
1023_0108752	3.0	3.0
1023_0108766	3.0	3.0
1023_0108810	3.0	3.0
1023_0108885	3.0	3.0
1023_0108908	3.0	3.0
1023_0108931	3.0	3.0
1023_0109033	4.0	3.0
1023_0109247	3.0	3.0
1023_0109392	3.0	3.0
1023_0109400	3.0	3.0
1023_0109515	3.0	3.0
1023_0109516	3.0	3.0
1023_0109522	3.0	3.0
1023_0109524	3.0	3.0
1023_0109591	3.0	3.0
1023_0109606	3.0	3.0
1023_0109609	3.0	3.0
1023_0109614	2.0	3.0
1023_0109951	3.0	3.0
1031_0001950	4.0	4.0
1031_0001998	4.0	4.0
1031_0002084	4.0	4.0
1031_0002184	4.0	4.0
1031_0002198	4.0	4.0
1031_0003053	4.0	4.0
1031_0003071	4.0	4.0
1031_0003073	4.0	4.0
1031_0003098	5.0	4.0
1031_0003126	4.0	3.0
1031_0003140	4.0	4.0
1031_0003155	4.0	4.0
1031_0003156	4.0	4.0
1031_0003180	4.0	4.0
1031_0003181	4.0	4.0
1031_0003185	3.0	2.0
1031_0003190	4.0	4.0
1031_0003203	2.0	3.0
1031_0003206	3.0	4.0
1031_0003207	4.0	4.0
1031_0003211	3.0	4.0
1031_0003272	3.0	3.0
1031_0003273	3.0	4.0
1031_0003274	3.0	4.0
1031_0003310	4.0	4.0
1031_0003314	4.0	4.0
1031_0003315	3.0	4.0
1031_0003337	4.0	4.0
1031_0003357	4.0	4.0
1031_0003365	4.0	3.0
1031_0003367	4.0	4.0
1031_0003368	3.0	4.0
1031_0003369	4.0	4.0
1031_0003384	3.0	3.0
1031_0003389	3.0	4.0
1031_0003393	4.0	4.0
1031_0003414	3.0	4.0
1061_0120282	0.0	1.0
1061_0120297	2.0	2.0
1061_0120302	1.0	2.0
1061_0120307	2.0	2.0
1061_0120318	2.0	2.0
1061_0120321	2.0	2.0
1061_0120323	2.0	2.0
1061_0120324	2.0	2.0
1061_0120325	2.0	2.0
1061_0120333	3.0	3.0
1061_0120345	2.0	2.0
1061_0120360	3.0	2.0
1061_0120368	2.0	2.0
1061_0120369	2.0	2.0
1061_0120382	2.0	2.0
1061_0120407	3.0	2.0
1061_0120409	2.0	2.0
1061_0120423	3.0	3.0
1061_0120431	2.0	2.0
1061_0120439	2.0	2.0
1061_0120440	1.0	2.0
1061_0120458	3.0	2.0
1061_0120480	2.0	2.0
1061_0120483	2.0	2.0
1061_0120489	2.0	2.0
1061_0120856	2.0	2.0
1061_0120878	1.0	2.0
1061_0120884	2.0	2.0
1061_0120887	2.0	2.0
1061_0120890	1.0	2.0
1061_1029111	2.0	2.0
1061_1029118	2.0	2.0
1061_1202917	2.0	2.0
1071_0024680	2.0	2.0
1071_0024682	2.0	2.0
1071_0024688	2.0	2.0
1071_0024691	2.0	2.0
1071_0024693	1.0	2.0
1071_0024699	2.0	2.0
1071_0024703	1.0	1.0
1071_0024711	2.0	2.0
1071_0024762	1.0	1.0
1071_0024763	1.0	1.0
1071_0024769	0.0	1.0
1071_0024770	1.0	1.0
1071_0024775	0.0	1.0
1071_0024776	0.0	0.0
1071_0024784	1.0	1.0
1071_0024803	1.0	1.0
1071_0024807	1.0	1.0
1071_0024810	1.0	1.0
1071_0024811	1.0	1.0
1071_0024812	1.0	1.0
1071_0024821	1.0	1.0
1071_0024823	1.0	1.0
1071_0024824	1.0	1.0
1071_0024833	2.0	2.0
1071_0024837	0.0	0.0
1071_0024841	1.0	1.0
1071_0024843	1.0	1.0
1071_0024844	1.0	1.0
1071_0024848	1.0	2.0
1071_0024851	2.0	1.0
1071_0024865	2.0	2.0
1071_0024874	1.0	1.0
1071_0024878	2.0	2.0
1071_0241832	1.0	1.0
1071_0242023	1.0	1.0
1071_0243621	2.0	2.0
1071_0248308	1.0	1.0
1071_0248311	2.0	2.0
1071_0248318	0.0	0.0
1071_0248320	0.0	0.0
1071_0248323	1.0	1.0
1071_0248335	1.0	1.0
1071_0248341	1.0	1.0
1071_0248348	1.0	1.0
1091_0000001	1.0	1.0
1091_0000006	1.0	1.0
1091_0000008	2.0	2.0
1091_0000010	2.0	2.0
1091_0000013	1.0	1.0
1091_0000022	2.0	2.0
1091_0000036	1.0	2.0
1091_0000044	1.0	2.0
1091_0000053	1.0	2.0
1091_0000057	2.0	1.0
1091_0000059	1.0	2.0
1091_0000064	1.0	2.0
1091_0000071	1.0	2.0
1091_0000074	2.0	2.0
1091_0000076	2.0	2.0
1091_0000095	2.0	2.0
1091_0000102	2.0	2.0
1091_0000114	2.0	2.0
1091_0000126	3.0	2.0
1091_0000145	1.0	1.0
1091_0000148	1.0	1.0
1091_0000153	1.0	2.0
1091_0000158	2.0	2.0
1091_0000159	2.0	2.0
1091_0000160	3.0	3.0
1091_0000161	2.0	2.0
1091_0000162	2.0	2.0
1091_0000167	1.0	2.0
1091_0000172	2.0	1.0
1091_0000190	1.0	1.0
1091_0000195	1.0	1.0
1091_0000198	2.0	2.0
1091_0000201	2.0	2.0
1091_0000205	2.0	2.0
1091_0000206	1.0	2.0
1091_0000207	2.0	2.0
1091_0000221	2.0	1.0
1091_0000223	2.0	2.0
1091_0000226	1.0	2.0
1091_0000231	2.0	2.0
1091_0000238	2.0	2.0
1091_0000245	1.0	2.0
1091_0000248	2.0	2.0
1091_0000252	2.0	2.0
1091_0000257	2.0	2.0
1091_0000261	2.0	2.0
0603	2.0	2.0
0607	2.0	2.0
0614	2.0	2.0
0627	2.0	2.0
0628	1.0	2.0
0636	2.0	2.0
0641	1.0	1.0
0645	2.0	2.0
0718	1.0	2.0
0722	2.0	2.0
0801	2.0	2.0
0805	1.0	2.0
0810	2.0	2.0
0813	2.0	2.0
0902	1.0	2.0
0906	2.0	2.0
0910	1.0	2.0
0925	1.0	2.0
0929	1.0	2.0
0930	1.0	2.0
1003	2.0	2.0
1020	2.0	2.0
1022	2.0	2.0
BER0611006	2.0	3.0
KYJ0611005B	1.0	2.0
KYJ0611006B	0.0	1.0
LIB0611004A	1.0	1.0
LIB0611011	2.0	2.0
MOS0509001	2.0	2.0
MOS0509004	3.0	2.0
MOS0611012	3.0	3.0
MOS0611015	3.0	3.0
PAR1011009B	1.0	1.0
PAR1011014	2.0	3.0
PAR1011018	4.0	3.0
PHA0111002B	2.0	2.0
PHA0111005B	1.0	1.0
PHA0111012	2.0	3.0
PHA0112006A	2.0	2.0
PHA0112006B	2.0	2.0
PHA0112012B	1.0	2.0
PHA0209013	1.0	1.0
PHA0210001	1.0	1.0
PHA0210004	1.0	1.0
PHA0411008A	1.0	1.0
PHA0411032	3.0	3.0
PHA0411037	3.0	3.0
PHA0411044	4.0	3.0
PHA0411062	3.0	3.0
PHA0509007	1.0	1.0
PHA0509013	1.0	1.0
PHA0509015	3.0	2.0
PHA0509019	3.0	3.0
PHA0509025	3.0	3.0
PHA0509028	3.0	3.0
PHA0509036	3.0	3.0
PHA0509042	3.0	3.0
PHA0509044	3.0	3.0
PHA0509045	3.0	2.0
PHA0510003A	1.0	1.0
PHA0510013A	1.0	1.0
PHA0510027	3.0	2.0
PHA0510032	3.0	3.0
PHA0510034	3.0	3.0
PHA0610015	3.0	3.0
PHA0610016	3.0	3.0
PHA0610017	3.0	3.0
PHA0710009	3.0	3.0
PHA0710011	3.0	3.0
PHA0710019	3.0	3.0
PHA0809010	2.0	2.0
PHA0810006	3.0	3.0
PHA0810010	3.0	3.0
PHA0811019	4.0	3.0
PHA1109001	1.0	1.0
PHA1109002	3.0	3.0
PHA1109027	3.0	3.0
PHA1110002B	1.0	1.0
PHA1110004A	1.0	1.0
PHA1110014	3.0	3.0
PHA1110016	3.0	2.0
PHA1111006B	1.0	1.0
PHA1111008A	1.0	1.0
PHA1111008B	1.0	1.0
VAR0209036	2.0	2.0
VAR0910004	3.0	3.0
VAR0910005	3.0	3.0
1325_1001008	3.0	3.0
1325_1001010	3.0	3.0
1325_1001019	3.0	3.0
1325_1001023	3.0	3.0
1325_1001032	3.0	3.0
1325_1001035	3.0	3.0
1325_1001048	2.0	3.0
1325_1001052	2.0	3.0
1325_1001054	3.0	2.0
1325_1001085	3.0	3.0
1325_1001086	3.0	3.0
1325_1001093	2.0	3.0
1325_1001100	2.0	3.0
1325_1001101	3.0	3.0
1325_1001111	3.0	3.0
1325_1001124	3.0	3.0
1325_1001125	3.0	3.0
1325_1001142	3.0	3.0
1325_1001157	3.0	3.0
1325_1001161	3.0	3.0
1325_1001164	3.0	3.0
1325_9000088	2.0	3.0
1325_9000106	3.0	2.0
1325_9000137	3.0	3.0
1325_9000138	4.0	3.0
1325_9000140	3.0	3.0
1325_9000185	3.0	3.0
1325_9000239	3.0	3.0
1325_9000304	3.0	3.0
1325_9000317	3.0	3.0
1325_9000534	3.0	3.0
1325_9000675	3.0	3.0
1325_9000686	3.0	3.0
1365_0100010	2.0	2.0
1365_0100012	2.0	2.0
1365_0100017	2.0	3.0
1365_0100018	2.0	2.0
1365_0100020	2.0	2.0
1365_0100021	2.0	2.0
1365_0100029	1.0	2.0
1365_0100066	2.0	2.0
1365_0100073	2.0	2.0
1365_0100080	2.0	2.0
1365_0100094	2.0	2.0
1365_0100095	2.0	2.0
1365_0100098	2.0	2.0
1365_0100103	2.0	3.0
1365_0100106	2.0	3.0
1365_0100134	2.0	2.0
1365_0100138	2.0	2.0
1365_0100147	2.0	2.0
1365_0100163	3.0	3.0
1365_0100165	3.0	2.0
1365_0100170	2.0	3.0
1365_0100172	2.0	2.0
1365_0100176	2.0	2.0
1365_0100178	2.0	2.0
1365_0100180	2.0	2.0
1365_0100188	2.0	2.0
1365_0100198	2.0	2.0
1365_0100229	2.0	2.0
1365_0100230	2.0	2.0
1365_0100253	2.0	2.0
1365_0100259	2.0	2.0
1365_0100265	2.0	2.0
1365_0100267	2.0	2.0
1365_0100270	2.0	2.0
1365_0100277	3.0	2.0
1365_0100286	2.0	2.0
1365_0100299	2.0	2.0
1365_0100447	2.0	2.0
1365_0100459	3.0	2.0
1365_0100461	2.0	3.0
1365_0100469	2.0	2.0
1365_0100470	2.0	2.0
1365_0100473	2.0	2.0
1365_0100474	2.0	2.0
1365_0100482	2.0	2.0
1385_0000036	1.0	2.0
1385_0000038	1.0	1.0
1385_0000045	2.0	1.0
1385_0000058	1.0	1.0
1385_0000059	1.0	1.0
1385_0000095	1.0	1.0
1385_0000128	1.0	1.0
1385_0001103	1.0	1.0
1385_0001110	2.0	1.0
1385_0001120	2.0	1.0
1385_0001122	2.0	1.0
1385_0001124	1.0	1.0
1385_0001132	1.0	1.0
1385_0001133	2.0	1.0
1385_0001136	1.0	1.0
1385_0001137	1.0	1.0
1385_0001147	1.0	1.0
1385_0001153	2.0	1.0
1385_0001154	1.0	1.0
1385_0001161	1.0	1.0
1385_0001166	1.0	1.0
1385_0001173	0.0	0.0
1385_0001175	0.0	1.0
1385_0001192	1.0	1.0
1385_0001193	1.0	2.0
1385_0001526	0.0	1.0
1385_0001715	1.0	1.0
1385_0001718	0.0	1.0
1385_0001728	1.0	2.0
1385_0001733	1.0	2.0
1385_0001738	0.0	1.0
1385_0001742	0.0	1.0
1385_0001744	0.0	2.0
1385_0001752	1.0	1.0
1385_0001762	1.0	1.0
1385_0001768	2.0	1.0
1385_0001772	1.0	1.0
1395_0000333	1.0	2.0
1395_0000355	2.0	1.0
1395_0000356	1.0	1.0
1395_0000357	3.0	2.0
1395_0000365	2.0	2.0
1395_0000368	0.0	1.0
1395_0000376	2.0	2.0
1395_0000388	2.0	2.0
1395_0000391	3.0	2.0
1395_0000404	2.0	2.0
1395_0000413	2.0	2.0
1395_0000432	2.0	2.0
1395_0000454	2.0	2.0
1395_0000455	2.0	2.0
1395_0000462	2.0	1.0
1395_0000529	2.0	1.0
1395_0000549	2.0	2.0
1395_0000550	2.0	2.0
1395_0000551	2.0	2.0
1395_0000552	2.0	2.0
1395_0000553	2.0	1.0
1395_0000593	1.0	2.0
1395_0000596	2.0	2.0
1395_0000606	0.0	1.0
1395_0000609	1.0	1.0
1395_0000627	1.0	1.0
1395_0000628	1.0	2.0
1395_0000639	1.0	2.0
1395_0000644	1.0	2.0
1395_0000646	1.0	2.0
1395_0000649	1.0	2.0
1395_0001015	1.0	2.0
1395_0001024	2.0	1.0
1395_0001028	1.0	2.0
1395_0001034	1.0	1.0
1395_0001040	0.0	0.0
1395_0001058	1.0	1.0
1395_0001064	2.0	2.0
1395_0001067	1.0	1.0
1395_0001070	2.0	2.0
1395_0001073	1.0	2.0
1395_0001116	2.0	1.0
1395_0001126	1.0	1.0
1395_0001146	0.0	1.0
1395_0001164	2.0	2.0
1395_0001170	1.0	2.0
3 Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 1.03
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        19
         1.0       0.63      0.64      0.64       118
         2.0       0.64      0.75      0.69       164
         3.0       0.76      0.74      0.75       120
         4.0       0.65      0.50      0.57        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.67       452
   macro avg       0.45      0.44      0.44       452
weighted avg       0.64      0.67      0.65       452

[[  0  17   2   0   0   0]
 [  0  76  42   0   0   0]
 [  0  28 123  13   0   0]
 [  0   0  24  89   7   0]
 [  0   0   0  15  15   0]
 [  0   0   0   0   1   0]]
0.6544221457546249
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.76
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.73
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        19
         1.0       0.62      0.85      0.72       118
         2.0       0.74      0.65      0.69       164
         3.0       0.75      0.77      0.76       120
         4.0       0.65      0.57      0.61        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.70       452
   macro avg       0.46      0.47      0.46       452
weighted avg       0.67      0.70      0.68       452

[[  0  19   0   0   0   0]
 [  0 100  18   0   0   0]
 [  0  41 106  17   0   0]
 [  0   1  19  92   8   0]
 [  0   0   0  13  17   0]
 [  0   0   0   0   1   0]]
0.6798508475400279
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.63
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.75
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        19
         1.0       0.66      0.71      0.68       118
         2.0       0.70      0.72      0.71       164
         3.0       0.76      0.75      0.76       120
         4.0       0.62      0.77      0.69        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.70       452
   macro avg       0.46      0.49      0.47       452
weighted avg       0.67      0.70      0.68       452

[[  0  19   0   0   0   0]
 [  0  84  34   0   0   0]
 [  0  25 118  21   0   0]
 [  0   0  17  90  13   0]
 [  0   0   0   7  23   0]
 [  0   0   0   0   1   0]]
0.6817852024961688
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.51
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.78
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        19
         1.0       0.65      0.65      0.65       118
         2.0       0.69      0.73      0.71       164
         3.0       0.76      0.82      0.79       120
         4.0       0.66      0.70      0.68        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.70       452
   macro avg       0.46      0.48      0.47       452
weighted avg       0.67      0.70      0.68       452

[[  0  18   1   0   0   0]
 [  0  77  41   0   0   0]
 [  0  23 119  22   0   0]
 [  0   0  12  98  10   0]
 [  0   0   0   9  21   0]
 [  0   0   0   0   1   0]]
0.6805362177598138
452 452 452
Filename	True Label	Prediction
1023_0001422	3.0	3.0
1023_0101675	3.0	3.0
1023_0101688	3.0	3.0
1023_0101690	2.0	3.0
1023_0101701	3.0	3.0
1023_0101753	3.0	3.0
1023_0101843	3.0	3.0
1023_0101847	3.0	3.0
1023_0101849	3.0	3.0
1023_0101851	3.0	3.0
1023_0101852	3.0	3.0
1023_0101899	3.0	3.0
1023_0101900	4.0	3.0
1023_0101901	4.0	3.0
1023_0101906	3.0	3.0
1023_0101909	4.0	4.0
1023_0103825	3.0	3.0
1023_0103829	2.0	3.0
1023_0103833	4.0	4.0
1023_0103844	4.0	3.0
1023_0106816	3.0	3.0
1023_0107074	4.0	3.0
1023_0107244	3.0	3.0
1023_0107729	3.0	3.0
1023_0107787	2.0	3.0
1023_0107788	3.0	3.0
1023_0108307	3.0	3.0
1023_0108426	3.0	3.0
1023_0108648	3.0	3.0
1023_0108933	3.0	3.0
1023_0108993	3.0	3.0
1023_0109026	2.0	3.0
1023_0109030	3.0	3.0
1023_0109038	3.0	3.0
1023_0109039	3.0	3.0
1023_0109096	3.0	3.0
1023_0109248	2.0	3.0
1023_0109396	3.0	3.0
1023_0109495	3.0	3.0
1023_0109500	2.0	3.0
1023_0109518	2.0	3.0
1023_0109520	3.0	3.0
1023_0109528	3.0	3.0
1023_0109588	3.0	3.0
1023_0109651	3.0	3.0
1023_0109891	3.0	3.0
1023_0109945	4.0	3.0
1023_0109947	3.0	3.0
1031_0002004	4.0	4.0
1031_0002083	3.0	4.0
1031_0002131	3.0	4.0
1031_0002196	4.0	4.0
1031_0002199	4.0	4.0
1031_0003023	4.0	4.0
1031_0003029	4.0	4.0
1031_0003076	4.0	4.0
1031_0003078	4.0	4.0
1031_0003090	4.0	4.0
1031_0003095	3.0	4.0
1031_0003097	4.0	4.0
1031_0003099	3.0	4.0
1031_0003130	5.0	4.0
1031_0003135	4.0	4.0
1031_0003144	3.0	4.0
1031_0003154	4.0	4.0
1031_0003174	4.0	4.0
1031_0003214	3.0	4.0
1031_0003216	3.0	4.0
1031_0003220	3.0	3.0
1031_0003224	3.0	4.0
1031_0003231	4.0	3.0
1031_0003232	3.0	4.0
1031_0003233	3.0	3.0
1031_0003309	3.0	4.0
1031_0003313	4.0	4.0
1031_0003330	4.0	4.0
1031_0003339	4.0	4.0
1031_0003356	3.0	3.0
1031_0003387	4.0	4.0
1031_0003388	4.0	4.0
1031_0003392	4.0	4.0
1031_0003419	4.0	4.0
1061_0120271	2.0	2.0
1061_0120278	2.0	2.0
1061_0120281	2.0	2.0
1061_0120285	2.0	2.0
1061_0120288	2.0	2.0
1061_0120291	1.0	1.0
1061_0120298	2.0	2.0
1061_0120301	2.0	2.0
1061_0120303	1.0	2.0
1061_0120308	3.0	2.0
1061_0120309	2.0	1.0
1061_0120317	3.0	3.0
1061_0120326	2.0	2.0
1061_0120328	2.0	2.0
1061_0120329	2.0	2.0
1061_0120331	1.0	2.0
1061_0120347	2.0	2.0
1061_0120349	1.0	1.0
1061_0120353	1.0	1.0
1061_0120354	2.0	2.0
1061_0120357	3.0	3.0
1061_0120373	2.0	2.0
1061_0120383	3.0	3.0
1061_0120386	1.0	2.0
1061_0120387	2.0	2.0
1061_0120424	2.0	2.0
1061_0120427	2.0	2.0
1061_0120428	2.0	2.0
1061_0120429	3.0	3.0
1061_0120448	3.0	2.0
1061_0120453	2.0	2.0
1061_0120455	2.0	2.0
1061_0120457	2.0	2.0
1061_0120484	2.0	2.0
1061_0120493	2.0	2.0
1061_0120495	2.0	2.0
1061_0120499	2.0	2.0
1061_0120500	2.0	2.0
1061_0120877	2.0	2.0
1061_0120881	3.0	3.0
1061_0120885	2.0	2.0
1061_0120888	2.0	2.0
1061_1029113	2.0	2.0
1061_1029116	1.0	2.0
1061_1202910	2.0	2.0
1061_1202915	1.0	2.0
1071_0024683	0.0	1.0
1071_0024685	2.0	2.0
1071_0024686	2.0	2.0
1071_0024690	2.0	2.0
1071_0024694	2.0	2.0
1071_0024701	2.0	2.0
1071_0024712	1.0	2.0
1071_0024713	2.0	2.0
1071_0024714	2.0	2.0
1071_0024766	1.0	1.0
1071_0024774	0.0	1.0
1071_0024777	1.0	1.0
1071_0024781	1.0	1.0
1071_0024797	0.0	1.0
1071_0024798	0.0	1.0
1071_0024801	1.0	2.0
1071_0024804	1.0	1.0
1071_0024815	1.0	1.0
1071_0024816	1.0	1.0
1071_0024818	2.0	1.0
1071_0024820	1.0	1.0
1071_0024822	1.0	1.0
1071_0024825	1.0	1.0
1071_0024836	2.0	2.0
1071_0024840	1.0	1.0
1071_0024847	2.0	2.0
1071_0024850	1.0	1.0
1071_0024852	0.0	1.0
1071_0024859	2.0	2.0
1071_0024863	2.0	1.0
1071_0024864	0.0	1.0
1071_0024873	0.0	1.0
1071_0242012	2.0	2.0
1071_0242041	1.0	1.0
1071_0242073	1.0	1.0
1071_0242093	0.0	1.0
1071_0243581	1.0	1.0
1071_0243582	1.0	1.0
1071_0243593	1.0	2.0
1071_0248301	2.0	1.0
1071_0248307	2.0	1.0
1071_0248316	1.0	1.0
1071_0248325	0.0	1.0
1071_0248328	0.0	1.0
1071_0248332	2.0	2.0
1091_0000002	2.0	2.0
1091_0000003	2.0	2.0
1091_0000015	1.0	2.0
1091_0000019	1.0	1.0
1091_0000024	2.0	1.0
1091_0000033	1.0	2.0
1091_0000042	1.0	1.0
1091_0000047	2.0	1.0
1091_0000060	2.0	2.0
1091_0000069	2.0	1.0
1091_0000075	1.0	2.0
1091_0000086	1.0	2.0
1091_0000087	2.0	2.0
1091_0000123	2.0	2.0
1091_0000164	2.0	1.0
1091_0000171	2.0	2.0
1091_0000192	2.0	2.0
1091_0000199	2.0	2.0
1091_0000202	2.0	2.0
1091_0000210	2.0	2.0
1091_0000212	1.0	2.0
1091_0000219	1.0	2.0
1091_0000229	2.0	2.0
1091_0000233	2.0	2.0
1091_0000237	1.0	2.0
1091_0000239	2.0	2.0
1091_0000241	2.0	2.0
1091_0000251	2.0	2.0
1091_0000265	2.0	2.0
1091_0000268	2.0	2.0
1091_0000272	1.0	2.0
0602	2.0	3.0
0616	1.0	2.0
0621	2.0	2.0
0623	2.0	2.0
0626	2.0	2.0
0630	1.0	1.0
0632	1.0	2.0
0634	2.0	2.0
0635	1.0	1.0
0637	2.0	2.0
0639	1.0	2.0
0715	2.0	2.0
0719	2.0	1.0
0720	2.0	2.0
0812	2.0	1.0
0814	1.0	1.0
0818	2.0	2.0
0819	2.0	2.0
0825	1.0	2.0
0827	1.0	2.0
0901	2.0	2.0
0904	2.0	2.0
0911	2.0	2.0
0913	2.0	2.0
0914	2.0	2.0
0917	1.0	2.0
0927	1.0	2.0
0928	2.0	2.0
1005	2.0	2.0
1018	1.0	2.0
1111	2.0	2.0
1112	2.0	2.0
1116	2.0	2.0
9999	0.0	1.0
BER0609003	3.0	3.0
BER0611003	2.0	3.0
LIB0611001A	1.0	1.0
LIB0611004B	1.0	1.0
LON0611004A	1.0	1.0
PAR1011016	3.0	3.0
PHA0111001B	1.0	1.0
PHA0111003A	1.0	1.0
PHA0111004A	1.0	1.0
PHA0111016	3.0	3.0
PHA0111018	3.0	3.0
PHA0112007B	1.0	1.0
PHA0209038	4.0	3.0
PHA0210008	1.0	1.0
PHA0411035	2.0	3.0
PHA0411038	3.0	3.0
PHA0411041	3.0	3.0
PHA0411051	4.0	3.0
PHA0411059	3.0	3.0
PHA0509018	3.0	3.0
PHA0510010A	2.0	1.0
PHA0510013B	1.0	1.0
PHA0510023	3.0	3.0
PHA0510029	3.0	3.0
PHA0610005A	1.0	1.0
PHA0610007B	1.0	1.0
PHA0610019A	2.0	1.0
PHA0610019B	1.0	2.0
PHA0610026	3.0	3.0
PHA0710010	3.0	3.0
PHA0710013	4.0	3.0
PHA0710014	3.0	3.0
PHA0710016	3.0	3.0
PHA0810002	3.0	3.0
PHA0810003	3.0	3.0
PHA0810008	3.0	3.0
PHA0810011	3.0	3.0
PHA1109003	2.0	2.0
PHA1109006	2.0	3.0
PHA1109024	3.0	3.0
PHA1110001A	1.0	2.0
PHA1110002A	2.0	2.0
PHA1110013	3.0	3.0
PHA1110017	3.0	3.0
PHA1111006A	1.0	1.0
ST071122B	1.0	1.0
VAR0909003	3.0	3.0
VAR0909004	3.0	3.0
VAR0909007	3.0	3.0
VAR0910006	3.0	3.0
VAR0910009	3.0	3.0
VAR0910011	3.0	3.0
1325_1001015	3.0	3.0
1325_1001025	2.0	3.0
1325_1001036	3.0	3.0
1325_1001051	3.0	2.0
1325_1001053	2.0	2.0
1325_1001055	3.0	3.0
1325_1001084	3.0	3.0
1325_1001089	3.0	2.0
1325_1001091	3.0	3.0
1325_1001092	2.0	2.0
1325_1001094	2.0	3.0
1325_1001097	1.0	2.0
1325_1001098	3.0	3.0
1325_1001110	3.0	3.0
1325_1001120	3.0	3.0
1325_1001126	2.0	3.0
1325_1001127	3.0	3.0
1325_1001129	2.0	3.0
1325_1001132	3.0	3.0
1325_1001133	3.0	3.0
1325_1001141	2.0	3.0
1325_1001154	3.0	3.0
1325_1001155	3.0	3.0
1325_1001156	3.0	3.0
1325_1001163	2.0	3.0
1325_1001165	2.0	3.0
1325_1001167	3.0	3.0
1325_1001169	3.0	3.0
1325_9000087	2.0	3.0
1325_9000089	2.0	3.0
1325_9000104	3.0	3.0
1325_9000214	3.0	3.0
1325_9000279	3.0	3.0
1325_9000296	3.0	3.0
1325_9000302	3.0	2.0
1325_9000303	3.0	3.0
1325_9000316	3.0	3.0
1325_9000318	3.0	3.0
1325_9000319	3.0	3.0
1325_9000323	3.0	3.0
1325_9000533	3.0	3.0
1325_9000611	3.0	3.0
1365_0100004	2.0	2.0
1365_0100015	2.0	2.0
1365_0100019	2.0	2.0
1365_0100022	2.0	2.0
1365_0100024	2.0	2.0
1365_0100027	2.0	2.0
1365_0100063	3.0	2.0
1365_0100065	1.0	2.0
1365_0100069	2.0	2.0
1365_0100072	2.0	2.0
1365_0100100	2.0	3.0
1365_0100116	3.0	2.0
1365_0100119	3.0	3.0
1365_0100133	2.0	2.0
1365_0100148	2.0	2.0
1365_0100162	2.0	2.0
1365_0100168	2.0	2.0
1365_0100181	2.0	2.0
1365_0100183	2.0	2.0
1365_0100200	3.0	2.0
1365_0100221	2.0	2.0
1365_0100222	3.0	3.0
1365_0100225	2.0	2.0
1365_0100226	3.0	2.0
1365_0100227	3.0	2.0
1365_0100228	2.0	2.0
1365_0100255	2.0	2.0
1365_0100282	2.0	2.0
1365_0100287	2.0	2.0
1365_0100456	2.0	3.0
1365_0100476	2.0	2.0
1365_0100481	2.0	2.0
1385_0000016	1.0	1.0
1385_0000035	1.0	1.0
1385_0000039	1.0	1.0
1385_0000048	1.0	1.0
1385_0000050	1.0	1.0
1385_0000051	2.0	2.0
1385_0000098	2.0	1.0
1385_0000100	1.0	1.0
1385_0000114	2.0	1.0
1385_0000124	2.0	2.0
1385_0001105	1.0	1.0
1385_0001119	2.0	1.0
1385_0001134	1.0	1.0
1385_0001135	1.0	2.0
1385_0001148	2.0	1.0
1385_0001150	1.0	1.0
1385_0001151	2.0	2.0
1385_0001159	1.0	1.0
1385_0001169	1.0	1.0
1385_0001170	1.0	1.0
1385_0001189	1.0	1.0
1385_0001196	1.0	1.0
1385_0001197	1.0	1.0
1385_0001523	1.0	2.0
1385_0001527	2.0	1.0
1385_0001717	1.0	1.0
1385_0001723	0.0	1.0
1385_0001741	0.0	1.0
1385_0001747	1.0	1.0
1385_0001751	1.0	2.0
1385_0001753	1.0	1.0
1385_0001754	1.0	1.0
1385_0001756	1.0	1.0
1385_0001766	2.0	2.0
1385_0001767	1.0	1.0
1385_0001771	1.0	1.0
1385_0001774	0.0	1.0
1385_0001786	1.0	1.0
1385_0001791	1.0	1.0
1385_0001795	0.0	1.0
1385_0001796	1.0	1.0
1385_0001800	1.0	1.0
1395_0000337	1.0	1.0
1395_0000360	3.0	2.0
1395_0000361	2.0	2.0
1395_0000369	2.0	2.0
1395_0000378	2.0	2.0
1395_0000389	1.0	1.0
1395_0000390	1.0	1.0
1395_0000402	2.0	1.0
1395_0000448	2.0	1.0
1395_0000452	1.0	1.0
1395_0000499	2.0	1.0
1395_0000512	2.0	2.0
1395_0000514	3.0	2.0
1395_0000516	1.0	1.0
1395_0000518	2.0	2.0
1395_0000527	1.0	1.0
1395_0000533	2.0	2.0
1395_0000554	2.0	2.0
1395_0000560	2.0	2.0
1395_0000564	2.0	2.0
1395_0000572	1.0	1.0
1395_0000579	1.0	1.0
1395_0000584	1.0	1.0
1395_0000591	0.0	1.0
1395_0000597	1.0	1.0
1395_0000598	1.0	1.0
1395_0000630	1.0	2.0
1395_0000635	1.0	2.0
1395_0000642	1.0	2.0
1395_0001016	1.0	2.0
1395_0001021	1.0	1.0
1395_0001022	1.0	2.0
1395_0001065	1.0	2.0
1395_0001071	2.0	1.0
1395_0001075	1.0	2.0
1395_0001076	1.0	2.0
1395_0001114	0.0	2.0
1395_0001115	2.0	2.0
1395_0001122	0.0	1.0
1395_0001124	1.0	1.0
1395_0001131	0.0	1.0
1395_0001141	2.0	1.0
1395_0001145	2.0	2.0
1395_0001149	1.0	1.0
1395_0001160	1.0	2.0
1395_0001161	1.0	2.0
4 Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.98
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        20
         1.0       0.65      0.74      0.69       117
         2.0       0.66      0.70      0.68       164
         3.0       0.77      0.69      0.73       120
         4.0       0.68      0.83      0.75        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.69       452
   macro avg       0.46      0.49      0.48       452
weighted avg       0.66      0.69      0.67       452

[[  0  17   3   0   0   0]
 [  0  87  30   0   0   0]
 [  0  29 115  20   0   0]
 [  0   1  25  83  11   0]
 [  0   0   0   5  25   0]
 [  0   0   0   0   1   0]]
0.6698956501283827
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.74
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.83
              precision    recall  f1-score   support

         0.0       0.57      0.20      0.30        20
         1.0       0.71      0.67      0.69       117
         2.0       0.68      0.66      0.67       164
         3.0       0.70      0.74      0.72       120
         4.0       0.57      0.93      0.71        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.68       452
   macro avg       0.54      0.53      0.51       452
weighted avg       0.68      0.68      0.67       452

[[  4  14   2   0   0   0]
 [  2  78  37   0   0   0]
 [  1  18 108  36   1   0]
 [  0   0  12  89  19   0]
 [  0   0   0   2  28   0]
 [  0   0   0   0   1   0]]
0.6720057399850832
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.60
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.84
              precision    recall  f1-score   support

         0.0       0.75      0.15      0.25        20
         1.0       0.70      0.54      0.61       117
         2.0       0.63      0.77      0.69       164
         3.0       0.76      0.74      0.75       120
         4.0       0.68      0.93      0.79        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.68       452
   macro avg       0.59      0.52      0.52       452
weighted avg       0.69      0.68      0.67       452

[[  3  15   2   0   0   0]
 [  1  63  53   0   0   0]
 [  0  12 126  26   0   0]
 [  0   0  19  89  12   0]
 [  0   0   0   2  28   0]
 [  0   0   0   0   1   0]]
0.6715584224607882
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.47
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.82
              precision    recall  f1-score   support

         0.0       0.50      0.20      0.29        20
         1.0       0.69      0.61      0.65       117
         2.0       0.66      0.73      0.69       164
         3.0       0.76      0.74      0.75       120
         4.0       0.67      0.93      0.78        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.69       452
   macro avg       0.55      0.54      0.53       452
weighted avg       0.69      0.69      0.68       452

[[  4  14   2   0   0   0]
 [  4  71  42   0   0   0]
 [  0  18 120  26   0   0]
 [  0   0  18  89  13   0]
 [  0   0   0   2  28   0]
 [  0   0   0   0   1   0]]
0.6824106384464775
452 452 452
Filename	True Label	Prediction
1023_0001416	3.0	3.0
1023_0001423	2.0	3.0
1023_0001575	3.0	3.0
1023_0101683	3.0	3.0
1023_0101854	2.0	3.0
1023_0101896	3.0	3.0
1023_0101897	3.0	3.0
1023_0101904	2.0	3.0
1023_0102117	3.0	3.0
1023_0103822	2.0	2.0
1023_0103826	3.0	3.0
1023_0103830	3.0	3.0
1023_0103834	3.0	4.0
1023_0103839	3.0	3.0
1023_0103880	3.0	3.0
1023_0107042	3.0	3.0
1023_0107672	2.0	3.0
1023_0107773	3.0	3.0
1023_0107781	3.0	3.0
1023_0108304	3.0	3.0
1023_0108305	3.0	3.0
1023_0108422	3.0	3.0
1023_0108520	3.0	3.0
1023_0108751	3.0	3.0
1023_0108753	3.0	3.0
1023_0108811	3.0	3.0
1023_0108813	3.0	3.0
1023_0108814	3.0	3.0
1023_0108890	3.0	3.0
1023_0108932	3.0	3.0
1023_0108935	2.0	3.0
1023_0109029	2.0	2.0
1023_0109192	3.0	3.0
1023_0109249	3.0	3.0
1023_0109391	2.0	3.0
1023_0109399	2.0	3.0
1023_0109402	3.0	3.0
1023_0109496	3.0	3.0
1023_0109527	3.0	3.0
1023_0109671	3.0	3.0
1023_0109721	3.0	3.0
1023_0109915	2.0	2.0
1023_0109917	3.0	3.0
1023_0109954	3.0	3.0
1023_0111896	3.0	3.0
1031_0002005	4.0	4.0
1031_0002010	3.0	4.0
1031_0002032	3.0	4.0
1031_0002036	4.0	4.0
1031_0002043	4.0	4.0
1031_0002086	4.0	4.0
1031_0002185	4.0	4.0
1031_0003013	4.0	4.0
1031_0003054	4.0	4.0
1031_0003063	5.0	4.0
1031_0003077	4.0	4.0
1031_0003121	4.0	4.0
1031_0003128	4.0	4.0
1031_0003129	4.0	4.0
1031_0003145	4.0	4.0
1031_0003146	4.0	4.0
1031_0003149	4.0	4.0
1031_0003150	4.0	4.0
1031_0003162	4.0	4.0
1031_0003172	3.0	4.0
1031_0003173	4.0	4.0
1031_0003179	4.0	4.0
1031_0003184	4.0	4.0
1031_0003186	4.0	4.0
1031_0003187	4.0	4.0
1031_0003191	4.0	4.0
1031_0003219	3.0	4.0
1031_0003221	3.0	4.0
1031_0003225	3.0	4.0
1031_0003230	4.0	4.0
1031_0003235	4.0	4.0
1031_0003236	3.0	4.0
1031_0003239	4.0	4.0
1031_0003261	3.0	4.0
1031_0003262	3.0	4.0
1031_0003336	3.0	4.0
1031_0003338	4.0	4.0
1031_0003359	3.0	4.0
1031_0003383	4.0	4.0
1031_0003391	3.0	3.0
1031_0003408	3.0	4.0
1031_0003415	4.0	4.0
1061_0120274	1.0	2.0
1061_0120277	1.0	2.0
1061_0120287	1.0	2.0
1061_0120295	0.0	2.0
1061_0120306	2.0	2.0
1061_0120310	3.0	2.0
1061_0120312	1.0	1.0
1061_0120313	2.0	2.0
1061_0120316	2.0	2.0
1061_0120319	3.0	2.0
1061_0120335	2.0	3.0
1061_0120338	2.0	2.0
1061_0120346	2.0	2.0
1061_0120350	2.0	2.0
1061_0120352	1.0	1.0
1061_0120356	2.0	2.0
1061_0120366	3.0	2.0
1061_0120367	3.0	2.0
1061_0120372	2.0	2.0
1061_0120374	3.0	3.0
1061_0120375	2.0	2.0
1061_0120389	2.0	2.0
1061_0120394	2.0	2.0
1061_0120403	2.0	2.0
1061_0120404	2.0	2.0
1061_0120410	2.0	2.0
1061_0120411	3.0	3.0
1061_0120413	1.0	2.0
1061_0120421	2.0	2.0
1061_0120430	2.0	2.0
1061_0120432	2.0	2.0
1061_0120433	1.0	2.0
1061_0120449	2.0	2.0
1061_0120478	2.0	2.0
1061_0120481	3.0	3.0
1061_0120486	2.0	2.0
1061_0120855	2.0	2.0
1061_0120874	2.0	2.0
1061_0120894	2.0	3.0
1061_1029112	3.0	3.0
1061_1202911	1.0	2.0
1061_1202914	2.0	2.0
1061_1202918	2.0	2.0
1061_1202919	2.0	2.0
1071_0024681	2.0	2.0
1071_0024687	1.0	2.0
1071_0024692	2.0	2.0
1071_0024702	2.0	2.0
1071_0024704	1.0	1.0
1071_0024756	2.0	1.0
1071_0024757	2.0	2.0
1071_0024767	2.0	2.0
1071_0024768	1.0	1.0
1071_0024779	2.0	1.0
1071_0024783	0.0	1.0
1071_0024799	2.0	2.0
1071_0024813	0.0	1.0
1071_0024817	1.0	1.0
1071_0024826	2.0	2.0
1071_0024831	1.0	1.0
1071_0024845	0.0	1.0
1071_0024853	1.0	1.0
1071_0024854	0.0	1.0
1071_0024855	1.0	2.0
1071_0024861	0.0	1.0
1071_0024866	2.0	2.0
1071_0024867	2.0	2.0
1071_0024871	1.0	1.0
1071_0024872	1.0	2.0
1071_0241831	1.0	2.0
1071_0242071	0.0	0.0
1071_0242091	1.0	0.0
1071_0242092	0.0	0.0
1071_0243502	1.0	0.0
1071_0248303	1.0	1.0
1071_0248313	2.0	1.0
1071_0248327	1.0	1.0
1071_0248331	1.0	1.0
1071_0248333	2.0	1.0
1071_0248345	2.0	2.0
1071_0248347	1.0	1.0
1071_0248350	2.0	1.0
1091_0000005	2.0	2.0
1091_0000016	0.0	1.0
1091_0000017	2.0	2.0
1091_0000018	2.0	2.0
1091_0000023	2.0	1.0
1091_0000031	1.0	1.0
1091_0000035	2.0	1.0
1091_0000037	1.0	0.0
1091_0000045	2.0	2.0
1091_0000050	1.0	1.0
1091_0000065	1.0	1.0
1091_0000066	2.0	1.0
1091_0000079	1.0	2.0
1091_0000092	1.0	2.0
1091_0000140	2.0	1.0
1091_0000146	1.0	1.0
1091_0000152	1.0	2.0
1091_0000154	2.0	3.0
1091_0000155	2.0	3.0
1091_0000169	3.0	2.0
1091_0000170	3.0	2.0
1091_0000173	2.0	2.0
1091_0000194	1.0	2.0
1091_0000204	2.0	2.0
1091_0000209	2.0	2.0
1091_0000215	2.0	2.0
1091_0000222	2.0	2.0
1091_0000224	2.0	1.0
1091_0000234	3.0	2.0
1091_0000235	1.0	1.0
1091_0000240	2.0	1.0
1091_0000256	1.0	2.0
1091_0000260	2.0	2.0
1091_0000264	2.0	2.0
1091_0000266	2.0	2.0
1091_0000271	2.0	2.0
1091_0000274	1.0	2.0
1091_0000275	2.0	2.0
0601	2.0	2.0
0604	2.0	2.0
0606	2.0	2.0
0612	1.0	2.0
0629	1.0	2.0
0638	1.0	2.0
0640	2.0	2.0
0643	2.0	2.0
0716	2.0	2.0
0723	1.0	2.0
0724	2.0	2.0
0725	2.0	2.0
0802	1.0	1.0
0804	1.0	2.0
0809	2.0	2.0
0821	2.0	2.0
0824	2.0	2.0
0903	2.0	2.0
0907	2.0	2.0
0918	2.0	2.0
0919	1.0	2.0
0920	2.0	2.0
0922	2.0	2.0
0926	2.0	2.0
1001	2.0	2.0
1002	2.0	2.0
1004	2.0	2.0
1006	2.0	2.0
1017	1.0	2.0
1113	2.0	2.0
BER0611007	3.0	3.0
KYJ0611003A	1.0	2.0
LIB0611001B	1.0	1.0
LON0610002A	1.0	1.0
LON0611002A	1.0	1.0
MOS0611013	3.0	3.0
PAR1011013	3.0	3.0
PAR1011015	3.0	3.0
PAR1011017	3.0	3.0
PHA0111001A	1.0	1.0
PHA0111002A	1.0	1.0
PHA0111005A	1.0	1.0
PHA0112003B	1.0	1.0
PHA0112009B	1.0	1.0
PHA0209008	1.0	1.0
PHA0209039	3.0	3.0
PHA0210007	1.0	2.0
PHA0411008B	2.0	1.0
PHA0411009B	1.0	1.0
PHA0411010A	0.0	1.0
PHA0411011B	1.0	1.0
PHA0411033	3.0	2.0
PHA0411036	3.0	3.0
PHA0411053	4.0	3.0
PHA0411055	3.0	3.0
PHA0509002	1.0	1.0
PHA0509022	4.0	3.0
PHA0509024	3.0	3.0
PHA0509037	2.0	2.0
PHA0510010B	0.0	1.0
PHA0510036	3.0	3.0
PHA0510040	3.0	3.0
PHA0510049	2.0	3.0
PHA0610006B	1.0	1.0
PHA0610018	3.0	3.0
PHA0710018	3.0	3.0
PHA0810001	3.0	3.0
PHA0810004	3.0	3.0
PHA1109005	2.0	2.0
PHA1109008	1.0	1.0
PHA1109025	2.0	1.0
PHA1110001B	1.0	1.0
PHA1110019	3.0	3.0
PHA1110022	3.0	3.0
PHA1111001B	1.0	1.0
PHA1111003B	1.0	1.0
PHA1111004A	1.0	1.0
VAR0909008	3.0	2.0
VAR0909009	3.0	3.0
VAR0910007	3.0	3.0
1325_1001012	3.0	3.0
1325_1001016	2.0	3.0
1325_1001017	3.0	3.0
1325_1001020	3.0	3.0
1325_1001021	3.0	3.0
1325_1001024	3.0	3.0
1325_1001029	3.0	3.0
1325_1001033	3.0	3.0
1325_1001039	3.0	3.0
1325_1001041	3.0	3.0
1325_1001046	2.0	3.0
1325_1001047	3.0	2.0
1325_1001063	2.0	2.0
1325_1001075	2.0	3.0
1325_1001076	3.0	3.0
1325_1001078	3.0	3.0
1325_1001081	3.0	3.0
1325_1001088	2.0	2.0
1325_1001107	3.0	3.0
1325_1001119	3.0	3.0
1325_1001128	3.0	3.0
1325_1001134	2.0	3.0
1325_1001152	3.0	3.0
1325_1001153	2.0	3.0
1325_1001159	3.0	3.0
1325_1001168	3.0	3.0
1325_9000090	2.0	3.0
1325_9000099	2.0	3.0
1325_9000136	3.0	3.0
1325_9000152	3.0	3.0
1325_9000187	3.0	3.0
1325_9000209	3.0	3.0
1325_9000211	3.0	3.0
1325_9000213	3.0	2.0
1325_9000241	3.0	3.0
1325_9000278	3.0	3.0
1325_9000314	3.0	2.0
1325_9000315	2.0	2.0
1325_9000536	3.0	3.0
1325_9000676	3.0	3.0
1365_0100005	2.0	2.0
1365_0100007	1.0	2.0
1365_0100008	2.0	2.0
1365_0100011	2.0	2.0
1365_0100014	2.0	2.0
1365_0100028	2.0	2.0
1365_0100051	2.0	2.0
1365_0100061	3.0	2.0
1365_0100064	2.0	2.0
1365_0100067	2.0	2.0
1365_0100074	2.0	3.0
1365_0100079	2.0	2.0
1365_0100092	2.0	2.0
1365_0100097	2.0	2.0
1365_0100099	2.0	2.0
1365_0100101	3.0	2.0
1365_0100102	3.0	2.0
1365_0100105	3.0	3.0
1365_0100107	2.0	3.0
1365_0100137	2.0	2.0
1365_0100139	2.0	2.0
1365_0100169	2.0	2.0
1365_0100171	2.0	2.0
1365_0100173	2.0	2.0
1365_0100174	2.0	2.0
1365_0100190	2.0	3.0
1365_0100199	2.0	3.0
1365_0100204	2.0	2.0
1365_0100213	2.0	2.0
1365_0100217	3.0	3.0
1365_0100223	2.0	3.0
1365_0100224	3.0	3.0
1365_0100231	2.0	2.0
1365_0100233	2.0	2.0
1365_0100260	2.0	2.0
1365_0100262	3.0	2.0
1365_0100269	2.0	2.0
1365_0100275	3.0	2.0
1365_0100276	3.0	3.0
1365_0100285	2.0	2.0
1365_0100290	2.0	2.0
1365_0100455	2.0	2.0
1365_0100457	2.0	3.0
1365_0100458	2.0	2.0
1365_0100471	2.0	3.0
1365_0100475	2.0	2.0
1365_0100479	2.0	2.0
1385_0000011	0.0	1.0
1385_0000013	1.0	1.0
1385_0000023	1.0	1.0
1385_0000037	1.0	1.0
1385_0000049	1.0	1.0
1385_0000052	1.0	1.0
1385_0000053	1.0	1.0
1385_0000099	1.0	1.0
1385_0000104	2.0	1.0
1385_0000123	1.0	1.0
1385_0000125	2.0	1.0
1385_0000126	1.0	1.0
1385_0001113	1.0	1.0
1385_0001126	0.0	0.0
1385_0001128	1.0	1.0
1385_0001156	1.0	1.0
1385_0001163	1.0	1.0
1385_0001164	1.0	1.0
1385_0001171	0.0	0.0
1385_0001172	1.0	1.0
1385_0001174	1.0	1.0
1385_0001178	1.0	1.0
1385_0001190	1.0	1.0
1385_0001501	1.0	1.0
1385_0001503	1.0	1.0
1385_0001524	1.0	1.0
1385_0001525	1.0	2.0
1385_0001712	1.0	2.0
1385_0001714	0.0	1.0
1385_0001719	1.0	1.0
1385_0001725	1.0	1.0
1385_0001726	1.0	1.0
1385_0001730	1.0	2.0
1385_0001732	1.0	2.0
1385_0001736	1.0	2.0
1385_0001737	1.0	1.0
1385_0001739	1.0	2.0
1385_0001740	1.0	1.0
1385_0001758	1.0	1.0
1385_0001764	1.0	1.0
1385_0001773	0.0	1.0
1385_0001775	1.0	2.0
1385_0001787	0.0	1.0
1385_0001790	1.0	1.0
1385_0001792	1.0	2.0
1385_0001799	2.0	2.0
1395_0000338	2.0	2.0
1395_0000341	2.0	1.0
1395_0000359	2.0	2.0
1395_0000380	2.0	2.0
1395_0000383	2.0	2.0
1395_0000387	3.0	2.0
1395_0000392	2.0	2.0
1395_0000396	2.0	2.0
1395_0000415	2.0	2.0
1395_0000443	2.0	2.0
1395_0000450	1.0	1.0
1395_0000458	1.0	2.0
1395_0000471	2.0	1.0
1395_0000504	2.0	1.0
1395_0000513	2.0	2.0
1395_0000534	2.0	2.0
1395_0000581	1.0	2.0
1395_0000595	1.0	0.0
1395_0000599	1.0	1.0
1395_0000602	1.0	1.0
1395_0000612	0.0	2.0
1395_0001013	1.0	2.0
1395_0001017	1.0	2.0
1395_0001023	1.0	1.0
1395_0001033	1.0	2.0
1395_0001068	1.0	2.0
1395_0001078	0.0	1.0
1395_0001117	1.0	1.0
1395_0001118	1.0	1.0
1395_0001123	1.0	2.0
1395_0001150	0.0	1.0
1395_0001167	1.0	2.0
5 Fold, Dimension = Vocabularyrange

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.98
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.77
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        20
         1.0       0.57      0.91      0.70       117
         2.0       0.78      0.55      0.64       164
         3.0       0.73      0.85      0.78       120
         4.0       0.60      0.20      0.30        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.67       452
   macro avg       0.45      0.42      0.40       452
weighted avg       0.66      0.67      0.64       452

[[  0  19   1   0   0   0]
 [  0 106  11   0   0   0]
 [  0  61  90  13   0   0]
 [  0   0  14 102   4   0]
 [  0   0   0  24   6   0]
 [  0   0   0   1   0   0]]
0.6425747869934324
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.74
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        20
         1.0       0.66      0.50      0.57       117
         2.0       0.64      0.74      0.69       164
         3.0       0.72      0.82      0.77       120
         4.0       0.57      0.70      0.63        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.66       452
   macro avg       0.43      0.46      0.44       452
weighted avg       0.63      0.66      0.64       452

[[  0  17   3   0   0   0]
 [  0  59  58   0   0   0]
 [  0  13 122  29   0   0]
 [  0   0   7  98  15   0]
 [  0   0   0   9  21   0]
 [  0   0   0   0   1   0]]
0.6432299461229661
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.61
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.86
              precision    recall  f1-score   support

         0.0       0.75      0.15      0.25        20
         1.0       0.66      0.48      0.55       117
         2.0       0.63      0.71      0.66       164
         3.0       0.70      0.79      0.75       120
         4.0       0.53      0.77      0.63        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.65       452
   macro avg       0.55      0.48      0.47       452
weighted avg       0.65      0.65      0.64       452

[[  3  13   4   0   0   0]
 [  1  56  59   1   0   0]
 [  0  16 116  32   0   0]
 [  0   0   6  95  19   0]
 [  0   0   0   7  23   0]
 [  0   0   0   0   1   0]]
0.6354142007305899
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.49
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.75
              precision    recall  f1-score   support

         0.0       0.55      0.30      0.39        20
         1.0       0.67      0.68      0.68       117
         2.0       0.73      0.74      0.74       164
         3.0       0.80      0.75      0.77       120
         4.0       0.53      0.77      0.63        30
         5.0       0.00      0.00      0.00         1

    accuracy                           0.71       452
   macro avg       0.55      0.54      0.53       452
weighted avg       0.71      0.71      0.71       452

[[  6  13   1   0   0   0]
 [  5  80  32   0   0   0]
 [  0  26 122  16   0   0]
 [  0   0  11  90  19   0]
 [  0   0   0   7  23   0]
 [  0   0   0   0   1   0]]
0.7078153710016978
452 452 452
Filename	True Label	Prediction
1023_0001418	3.0	3.0
1023_0001419	3.0	3.0
1023_0001420	3.0	3.0
1023_0101684	3.0	2.0
1023_0101749	4.0	3.0
1023_0101844	3.0	3.0
1023_0101846	4.0	3.0
1023_0101853	3.0	3.0
1023_0101893	3.0	3.0
1023_0103823	4.0	3.0
1023_0103828	2.0	2.0
1023_0103836	3.0	3.0
1023_0103955	4.0	3.0
1023_0104206	3.0	3.0
1023_0104207	3.0	3.0
1023_0104209	3.0	3.0
1023_0107682	2.0	3.0
1023_0107725	3.0	3.0
1023_0107726	3.0	3.0
1023_0108423	3.0	3.0
1023_0108650	3.0	3.0
1023_0108888	3.0	3.0
1023_0108889	3.0	3.0
1023_0108934	3.0	3.0
1023_0109151	4.0	3.0
1023_0109267	3.0	3.0
1023_0109395	3.0	3.0
1023_0109519	2.0	3.0
1023_0109674	3.0	3.0
1023_0109716	3.0	3.0
1023_0109890	4.0	3.0
1023_0109914	2.0	3.0
1023_0109946	3.0	3.0
1031_0001703	4.0	4.0
1031_0001951	3.0	4.0
1031_0001997	4.0	4.0
1031_0002003	3.0	4.0
1031_0002006	4.0	4.0
1031_0002040	4.0	4.0
1031_0002061	3.0	4.0
1031_0002079	5.0	4.0
1031_0002085	4.0	4.0
1031_0002087	4.0	4.0
1031_0002088	4.0	4.0
1031_0002187	4.0	4.0
1031_0002195	4.0	4.0
1031_0002200	3.0	4.0
1031_0003042	3.0	4.0
1031_0003052	4.0	4.0
1031_0003065	3.0	4.0
1031_0003072	3.0	4.0
1031_0003074	4.0	4.0
1031_0003085	3.0	4.0
1031_0003088	4.0	4.0
1031_0003106	4.0	4.0
1031_0003127	4.0	4.0
1031_0003133	4.0	4.0
1031_0003141	3.0	4.0
1031_0003157	4.0	4.0
1031_0003163	3.0	3.0
1031_0003164	4.0	4.0
1031_0003183	4.0	4.0
1031_0003189	4.0	4.0
1031_0003205	4.0	4.0
1031_0003237	3.0	4.0
1031_0003242	3.0	4.0
1031_0003243	3.0	4.0
1031_0003246	3.0	4.0
1031_0003249	3.0	4.0
1031_0003327	3.0	4.0
1031_0003353	3.0	4.0
1031_0003358	4.0	4.0
1031_0003366	3.0	4.0
1031_0003390	4.0	4.0
1031_0003407	3.0	4.0
1031_0003409	4.0	4.0
1061_0012029	3.0	2.0
1061_0120272	2.0	1.0
1061_0120275	2.0	2.0
1061_0120276	2.0	2.0
1061_0120279	2.0	2.0
1061_0120280	1.0	1.0
1061_0120283	1.0	1.0
1061_0120284	0.0	1.0
1061_0120296	2.0	2.0
1061_0120299	2.0	2.0
1061_0120311	3.0	2.0
1061_0120314	2.0	2.0
1061_0120320	3.0	3.0
1061_0120330	2.0	3.0
1061_0120334	2.0	2.0
1061_0120348	1.0	1.0
1061_0120351	2.0	2.0
1061_0120355	1.0	1.0
1061_0120359	2.0	2.0
1061_0120361	3.0	2.0
1061_0120371	3.0	3.0
1061_0120388	2.0	2.0
1061_0120405	3.0	2.0
1061_0120406	2.0	2.0
1061_0120414	2.0	2.0
1061_0120415	2.0	2.0
1061_0120425	2.0	2.0
1061_0120450	2.0	2.0
1061_0120456	2.0	2.0
1061_0120459	2.0	2.0
1061_0120487	2.0	2.0
1061_0120488	2.0	2.0
1061_0120490	2.0	2.0
1061_0120491	2.0	2.0
1061_0120492	2.0	2.0
1061_0120494	2.0	2.0
1061_0120496	2.0	2.0
1061_0120853	2.0	2.0
1061_0120857	2.0	2.0
1061_0120858	2.0	2.0
1061_0120859	2.0	3.0
1061_0120875	3.0	3.0
1061_0120876	2.0	2.0
1061_0120883	2.0	2.0
1061_1029115	2.0	2.0
1061_1202912	2.0	2.0
1061_1202913	2.0	2.0
1061_1202916	2.0	2.0
1071_0024689	1.0	2.0
1071_0024705	2.0	2.0
1071_0024706	1.0	2.0
1071_0024708	1.0	1.0
1071_0024716	1.0	0.0
1071_0024761	2.0	1.0
1071_0024765	0.0	0.0
1071_0024772	0.0	0.0
1071_0024827	1.0	1.0
1071_0024834	2.0	2.0
1071_0024835	1.0	1.0
1071_0024846	1.0	1.0
1071_0024849	0.0	0.0
1071_0024856	1.0	1.0
1071_0024862	2.0	2.0
1071_0024875	1.0	1.0
1071_0024879	1.0	1.0
1071_0242021	1.0	1.0
1071_0242042	1.0	1.0
1071_0242072	0.0	0.0
1071_0243501	1.0	1.0
1071_0243591	1.0	1.0
1071_0243622	1.0	0.0
1071_0248302	1.0	1.0
1071_0248305	0.0	1.0
1071_0248309	2.0	1.0
1071_0248312	1.0	1.0
1071_0248314	1.0	1.0
1071_0248319	1.0	0.0
1071_0248324	0.0	1.0
1071_0248326	1.0	1.0
1071_0248329	1.0	1.0
1071_0248337	1.0	2.0
1071_0248340	0.0	0.0
1071_0248342	1.0	1.0
1071_0248343	1.0	1.0
1091_0000004	1.0	1.0
1091_0000011	2.0	1.0
1091_0000021	2.0	2.0
1091_0000025	1.0	1.0
1091_0000026	1.0	1.0
1091_0000027	0.0	2.0
1091_0000028	1.0	1.0
1091_0000030	0.0	1.0
1091_0000032	1.0	2.0
1091_0000039	1.0	0.0
1091_0000041	1.0	1.0
1091_0000043	1.0	2.0
1091_0000046	1.0	1.0
1091_0000048	1.0	1.0
1091_0000049	1.0	2.0
1091_0000051	1.0	2.0
1091_0000054	0.0	1.0
1091_0000056	1.0	2.0
1091_0000062	2.0	1.0
1091_0000067	2.0	1.0
1091_0000072	1.0	2.0
1091_0000073	2.0	1.0
1091_0000101	2.0	2.0
1091_0000151	1.0	1.0
1091_0000157	3.0	2.0
1091_0000165	2.0	1.0
1091_0000166	1.0	2.0
1091_0000174	2.0	1.0
1091_0000185	2.0	1.0
1091_0000191	1.0	2.0
1091_0000196	2.0	2.0
1091_0000197	1.0	2.0
1091_0000200	2.0	2.0
1091_0000208	2.0	2.0
1091_0000211	1.0	2.0
1091_0000213	2.0	2.0
1091_0000214	2.0	2.0
1091_0000216	1.0	2.0
1091_0000220	1.0	2.0
1091_0000225	2.0	1.0
1091_0000227	1.0	2.0
1091_0000228	2.0	2.0
1091_0000246	2.0	2.0
1091_0000249	2.0	2.0
1091_0000250	2.0	2.0
1091_0000253	2.0	1.0
1091_0000259	2.0	2.0
1091_0000262	2.0	2.0
1091_0000267	2.0	2.0
0608	1.0	2.0
0609	1.0	2.0
0611	2.0	2.0
0613	1.0	2.0
0617	1.0	2.0
0620	2.0	2.0
0631	2.0	2.0
0644	2.0	2.0
0714	2.0	2.0
0717	2.0	2.0
0721	2.0	2.0
0803	1.0	2.0
0806	1.0	2.0
0811	2.0	2.0
0815	2.0	2.0
0816	2.0	2.0
0905	2.0	2.0
0912	2.0	2.0
0915	2.0	2.0
0916	1.0	2.0
0921	2.0	1.0
1007	2.0	2.0
1008	2.0	2.0
1009	2.0	2.0
1010	2.0	2.0
1015	2.0	2.0
1021	2.0	2.0
1023	2.0	2.0
1115	2.0	3.0
BER0611005	2.0	2.0
KYJ0611006A	0.0	1.0
KYJ0611009A	1.0	1.0
LIB0611002B	1.0	1.0
LIB0611003A	1.0	1.0
LON0611003	3.0	3.0
PAR1011009A	1.0	1.0
PHA0111003B	1.0	1.0
PHA0111004B	1.0	1.0
PHA0111010	2.0	3.0
PHA0111011	3.0	3.0
PHA0111014	2.0	3.0
PHA0112002A	1.0	1.0
PHA0112003A	1.0	1.0
PHA0112007A	1.0	1.0
PHA0112012A	2.0	2.0
PHA0209024	3.0	3.0
PHA0209034	3.0	3.0
PHA0411010B	0.0	1.0
PHA0411011A	1.0	1.0
PHA0411028	2.0	2.0
PHA0411039	3.0	3.0
PHA0411042	3.0	3.0
PHA0411045	3.0	3.0
PHA0411047	3.0	3.0
PHA0411054	3.0	3.0
PHA0411056	3.0	3.0
PHA0411061	3.0	3.0
PHA0509020	3.0	3.0
PHA0509021	2.0	2.0
PHA0509032	3.0	3.0
PHA0509033	2.0	3.0
PHA0509034	3.0	3.0
PHA0509035	3.0	3.0
PHA0509039	3.0	3.0
PHA0509040	3.0	3.0
PHA0509043	3.0	3.0
PHA0510004A	1.0	1.0
PHA0510004B	1.0	1.0
PHA0510030	3.0	3.0
PHA0510031	3.0	3.0
PHA0510038	3.0	3.0
PHA0510046	3.0	3.0
PHA0610005B	0.0	1.0
PHA0610025	3.0	4.0
PHA0710012	3.0	3.0
PHA0710015	3.0	3.0
PHA0710017	3.0	3.0
PHA0710021	3.0	3.0
PHA0809009	3.0	3.0
PHA0810009	3.0	3.0
PHA0810015	3.0	3.0
PHA0811012	3.0	3.0
PHA0811013	4.0	3.0
PHA0811017	3.0	3.0
PHA0811020	3.0	3.0
PHA1109004	3.0	3.0
PHA1109023	2.0	1.0
PHA1109026	3.0	3.0
PHA1110003A	1.0	1.0
PHA1110003B	0.0	1.0
PHA1111001A	2.0	1.0
PHA1111003A	1.0	1.0
PHA1111009A	1.0	1.0
TI071122B	1.0	1.0
VAR0910010	3.0	2.0
1325_1001011	3.0	3.0
1325_1001013	3.0	3.0
1325_1001014	3.0	3.0
1325_1001022	3.0	3.0
1325_1001043	3.0	3.0
1325_1001044	3.0	2.0
1325_1001050	3.0	3.0
1325_1001056	3.0	3.0
1325_1001057	2.0	3.0
1325_1001059	3.0	3.0
1325_1001062	3.0	3.0
1325_1001080	2.0	3.0
1325_1001083	3.0	2.0
1325_1001099	3.0	3.0
1325_1001108	3.0	3.0
1325_1001113	3.0	3.0
1325_1001121	2.0	2.0
1325_1001123	3.0	3.0
1325_1001131	3.0	3.0
1325_1001143	3.0	3.0
1325_1001158	3.0	3.0
1325_1001162	3.0	3.0
1325_1001170	3.0	3.0
1325_9000095	3.0	3.0
1325_9000139	3.0	3.0
1325_9000188	3.0	3.0
1325_9000215	3.0	3.0
1325_9000240	3.0	3.0
1325_9000504	3.0	3.0
1325_9000505	3.0	3.0
1325_9000554	3.0	3.0
1325_9000612	2.0	3.0
1325_9000678	3.0	3.0
1365_0100002	2.0	2.0
1365_0100013	3.0	3.0
1365_0100016	2.0	2.0
1365_0100023	2.0	2.0
1365_0100030	2.0	2.0
1365_0100056	2.0	2.0
1365_0100058	2.0	2.0
1365_0100093	2.0	2.0
1365_0100117	2.0	2.0
1365_0100118	2.0	2.0
1365_0100125	3.0	2.0
1365_0100135	2.0	2.0
1365_0100145	2.0	2.0
1365_0100146	2.0	2.0
1365_0100151	2.0	2.0
1365_0100166	2.0	2.0
1365_0100177	2.0	2.0
1365_0100182	2.0	2.0
1365_0100186	2.0	2.0
1365_0100187	2.0	2.0
1365_0100192	3.0	3.0
1365_0100194	2.0	2.0
1365_0100195	2.0	2.0
1365_0100201	2.0	2.0
1365_0100202	2.0	2.0
1365_0100205	2.0	2.0
1365_0100212	3.0	3.0
1365_0100219	2.0	3.0
1365_0100220	3.0	2.0
1365_0100232	2.0	2.0
1365_0100256	2.0	2.0
1365_0100257	2.0	2.0
1365_0100258	2.0	2.0
1365_0100261	2.0	2.0
1365_0100266	2.0	2.0
1365_0100274	2.0	3.0
1365_0100279	2.0	2.0
1365_0100280	1.0	2.0
1365_0100281	2.0	2.0
1365_0100451	2.0	3.0
1365_0100472	2.0	2.0
1365_0100477	2.0	3.0
1365_0100480	2.0	2.0
1385_0000012	1.0	1.0
1385_0000017	1.0	0.0
1385_0000020	1.0	1.0
1385_0000043	1.0	1.0
1385_0000054	2.0	1.0
1385_0000097	2.0	1.0
1385_0000101	1.0	1.0
1385_0000122	1.0	1.0
1385_0000127	2.0	1.0
1385_0000129	1.0	1.0
1385_0000130	1.0	1.0
1385_0001108	1.0	1.0
1385_0001109	1.0	1.0
1385_0001111	2.0	1.0
1385_0001112	2.0	1.0
1385_0001118	2.0	1.0
1385_0001125	1.0	1.0
1385_0001127	2.0	2.0
1385_0001129	1.0	1.0
1385_0001131	1.0	1.0
1385_0001149	2.0	1.0
1385_0001155	1.0	1.0
1385_0001158	1.0	1.0
1385_0001160	1.0	2.0
1385_0001162	1.0	1.0
1385_0001165	1.0	1.0
1385_0001188	1.0	1.0
1385_0001191	1.0	1.0
1385_0001194	1.0	1.0
1385_0001198	1.0	1.0
1385_0001199	1.0	1.0
1385_0001528	1.0	1.0
1385_0001716	1.0	1.0
1385_0001720	0.0	1.0
1385_0001724	1.0	2.0
1385_0001748	1.0	2.0
1385_0001749	1.0	1.0
1385_0001759	1.0	1.0
1385_0001765	0.0	0.0
1385_0001785	1.0	1.0
1385_0001794	1.0	1.0
1395_0000354	1.0	1.0
1395_0000364	2.0	2.0
1395_0000379	2.0	1.0
1395_0000399	2.0	2.0
1395_0000414	2.0	2.0
1395_0000446	2.0	2.0
1395_0000465	1.0	1.0
1395_0000469	2.0	1.0
1395_0000470	2.0	1.0
1395_0000515	2.0	2.0
1395_0000525	2.0	2.0
1395_0000528	2.0	2.0
1395_0000531	2.0	2.0
1395_0000535	1.0	1.0
1395_0000548	2.0	2.0
1395_0000565	1.0	1.0
1395_0000585	1.0	2.0
1395_0000604	0.0	1.0
1395_0000607	0.0	1.0
1395_0000611	1.0	1.0
1395_0000636	1.0	2.0
1395_0001019	1.0	1.0
1395_0001045	2.0	2.0
1395_0001066	1.0	2.0
1395_0001069	2.0	2.0
1395_0001090	1.0	2.0
1395_0001103	1.0	2.0
1395_0001121	0.0	1.0
1395_0001132	2.0	2.0
1395_0001133	1.0	1.0
1395_0001169	2.0	1.0
Averaged weighted F1-scores 0.695955999422634
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
144.90552995391704 65.35717405024758
LANGUAGE: IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
1 Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 11
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 1.12
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.01
              precision    recall  f1-score   support

         0.0       1.00      0.07      0.12        30
         1.0       0.52      0.46      0.49       119
         2.0       0.51      0.59      0.55       165
         3.0       0.60      0.70      0.64       115
         4.0       0.47      0.40      0.43        20
         5.0       0.00      0.00      0.00         3

    accuracy                           0.54       452
   macro avg       0.52      0.37      0.37       452
weighted avg       0.56      0.54      0.52       452

[[ 2 20  8  0  0  0]
 [ 0 55 63  1  0  0]
 [ 0 28 98 39  0  0]
 [ 0  2 24 80  9  0]
 [ 0  0  1 11  8  0]
 [ 0  0  0  3  0  0]]
0.5195028682247256
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.92
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.02
              precision    recall  f1-score   support

         0.0       0.56      0.17      0.26        30
         1.0       0.54      0.81      0.65       119
         2.0       0.56      0.49      0.52       165
         3.0       0.62      0.64      0.63       115
         4.0       0.00      0.00      0.00        20
         5.0       0.00      0.00      0.00         3

    accuracy                           0.57       452
   macro avg       0.38      0.35      0.34       452
weighted avg       0.54      0.57      0.54       452

[[ 5 22  3  0  0  0]
 [ 3 96 19  1  0  0]
 [ 1 57 81 26  0  0]
 [ 0  3 38 74  0  0]
 [ 0  0  2 18  0  0]
 [ 0  0  2  1  0  0]]
0.5382144667878477
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.78
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.07
              precision    recall  f1-score   support

         0.0       0.55      0.20      0.29        30
         1.0       0.54      0.62      0.58       119
         2.0       0.53      0.48      0.50       165
         3.0       0.57      0.70      0.63       115
         4.0       0.45      0.25      0.32        20
         5.0       0.00      0.00      0.00         3

    accuracy                           0.54       452
   macro avg       0.44      0.38      0.39       452
weighted avg       0.53      0.54      0.53       452

[[ 6 17  7  0  0  0]
 [ 5 74 39  1  0  0]
 [ 0 43 79 43  0  0]
 [ 0  3 25 81  6  0]
 [ 0  0  0 15  5  0]
 [ 0  0  0  3  0  0]]
0.5287614663619248
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.64
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.12
              precision    recall  f1-score   support

         0.0       0.41      0.30      0.35        30
         1.0       0.55      0.64      0.59       119
         2.0       0.55      0.47      0.50       165
         3.0       0.58      0.69      0.63       115
         4.0       0.56      0.45      0.50        20
         5.0       0.00      0.00      0.00         3

    accuracy                           0.55       452
   macro avg       0.44      0.42      0.43       452
weighted avg       0.55      0.55      0.55       452

[[ 9 15  6  0  0  0]
 [10 76 32  1  0  0]
 [ 3 43 77 42  0  0]
 [ 0  3 26 79  7  0]
 [ 0  0  0 11  9  0]
 [ 0  0  0  3  0  0]]
0.5452887595528803
452 452 452
Filename	True Label	Prediction
1023_0101683	3.0	3.0
1023_0101689	2.0	2.0
1023_0101693	3.0	3.0
1023_0101751	3.0	3.0
1023_0101847	3.0	3.0
1023_0101851	3.0	3.0
1023_0101852	5.0	3.0
1023_0101854	2.0	2.0
1023_0101855	2.0	3.0
1023_0101894	2.0	3.0
1023_0101899	2.0	3.0
1023_0103844	5.0	3.0
1023_0107727	5.0	3.0
1023_0107773	3.0	2.0
1023_0107781	3.0	3.0
1023_0107784	2.0	2.0
1023_0108649	3.0	3.0
1023_0108753	2.0	2.0
1023_0108766	2.0	3.0
1023_0108810	3.0	3.0
1023_0108812	3.0	3.0
1023_0108815	2.0	2.0
1023_0108887	2.0	2.0
1023_0108934	3.0	3.0
1023_0108935	2.0	2.0
1023_0109500	2.0	3.0
1023_0109520	2.0	3.0
1023_0109915	2.0	2.0
1031_0002005	4.0	3.0
1031_0002011	4.0	3.0
1031_0002083	3.0	3.0
1031_0002087	4.0	3.0
1031_0002187	3.0	3.0
1031_0002196	4.0	3.0
1031_0002199	4.0	4.0
1031_0003023	4.0	4.0
1031_0003029	4.0	3.0
1031_0003035	3.0	3.0
1031_0003054	4.0	4.0
1031_0003071	3.0	3.0
1031_0003077	3.0	3.0
1031_0003085	3.0	4.0
1031_0003099	3.0	3.0
1031_0003128	4.0	4.0
1031_0003136	3.0	3.0
1031_0003140	3.0	4.0
1031_0003154	4.0	4.0
1031_0003163	3.0	3.0
1031_0003174	3.0	4.0
1031_0003180	4.0	4.0
1031_0003182	4.0	4.0
1031_0003183	3.0	4.0
1031_0003185	3.0	3.0
1031_0003189	3.0	4.0
1031_0003205	3.0	4.0
1031_0003207	4.0	4.0
1031_0003218	3.0	4.0
1031_0003220	3.0	3.0
1031_0003221	2.0	3.0
1031_0003225	3.0	3.0
1031_0003226	3.0	3.0
1031_0003235	3.0	3.0
1031_0003244	3.0	3.0
1031_0003245	3.0	3.0
1031_0003249	3.0	3.0
1031_0003260	4.0	3.0
1031_0003272	3.0	2.0
1031_0003330	4.0	3.0
1031_0003336	3.0	3.0
1031_0003354	3.0	3.0
1031_0003356	3.0	3.0
1031_0003358	4.0	4.0
1031_0003366	3.0	3.0
1031_0003367	4.0	3.0
1031_0003389	3.0	3.0
1031_0003391	2.0	3.0
1031_0003392	4.0	3.0
1061_0120272	2.0	1.0
1061_0120279	1.0	1.0
1061_0120280	1.0	1.0
1061_0120282	0.0	1.0
1061_0120284	0.0	1.0
1061_0120286	1.0	1.0
1061_0120295	0.0	2.0
1061_0120312	1.0	1.0
1061_0120323	1.0	2.0
1061_0120331	1.0	1.0
1061_0120333	2.0	3.0
1061_0120338	2.0	2.0
1061_0120341	2.0	1.0
1061_0120346	2.0	2.0
1061_0120350	2.0	2.0
1061_0120360	3.0	2.0
1061_0120366	3.0	2.0
1061_0120369	2.0	2.0
1061_0120373	2.0	2.0
1061_0120384	1.0	1.0
1061_0120409	3.0	2.0
1061_0120411	3.0	2.0
1061_0120443	0.0	1.0
1061_0120455	2.0	2.0
1061_0120456	2.0	2.0
1061_0120460	2.0	1.0
1061_0120481	3.0	3.0
1061_0120493	2.0	2.0
1061_0120497	2.0	2.0
1061_0120499	2.0	2.0
1061_0120855	2.0	2.0
1061_0120858	2.0	2.0
1061_0120874	1.0	2.0
1061_0120877	2.0	2.0
1061_0120886	2.0	2.0
1061_1029119	1.0	2.0
1061_1202917	2.0	2.0
1071_0020001	1.0	1.0
1071_0024687	1.0	1.0
1071_0024689	1.0	1.0
1071_0024690	1.0	2.0
1071_0024701	2.0	1.0
1071_0024705	1.0	2.0
1071_0024708	1.0	1.0
1071_0024715	1.0	1.0
1071_0024766	1.0	0.0
1071_0024769	0.0	1.0
1071_0024772	0.0	0.0
1071_0024781	0.0	0.0
1071_0024797	0.0	1.0
1071_0024817	1.0	1.0
1071_0024827	1.0	1.0
1071_0024838	0.0	0.0
1071_0024845	0.0	1.0
1071_0024848	1.0	1.0
1071_0024851	2.0	1.0
1071_0024857	0.0	1.0
1071_0024861	0.0	1.0
1071_0024878	2.0	1.0
1071_0242072	0.0	0.0
1071_0242091	1.0	1.0
1071_0243502	1.0	0.0
1071_0243581	1.0	0.0
1071_0243623	1.0	1.0
1071_0248319	0.0	0.0
1071_0248327	0.0	1.0
1071_0248334	2.0	2.0
1071_0248336	1.0	1.0
1071_0248338	2.0	1.0
1071_0248339	2.0	1.0
1071_0248340	0.0	0.0
1071_0248344	1.0	0.0
1071_0248345	1.0	1.0
1071_0248350	2.0	0.0
1091_0000001	1.0	2.0
1091_0000008	2.0	2.0
1091_0000010	3.0	1.0
1091_0000012	1.0	1.0
1091_0000013	1.0	0.0
1091_0000014	0.0	1.0
1091_0000015	1.0	2.0
1091_0000025	1.0	1.0
1091_0000027	0.0	1.0
1091_0000029	2.0	2.0
1091_0000031	1.0	1.0
1091_0000033	1.0	2.0
1091_0000034	2.0	1.0
1091_0000035	1.0	1.0
1091_0000043	1.0	1.0
1091_0000053	0.0	2.0
1091_0000056	1.0	1.0
1091_0000067	1.0	1.0
1091_0000078	3.0	1.0
1091_0000125	2.0	2.0
1091_0000127	2.0	1.0
1091_0000152	1.0	1.0
1091_0000154	1.0	3.0
1091_0000155	3.0	2.0
1091_0000161	2.0	2.0
1091_0000164	2.0	1.0
1091_0000169	3.0	2.0
1091_0000203	2.0	1.0
1091_0000210	2.0	1.0
1091_0000214	2.0	1.0
1091_0000219	1.0	2.0
1091_0000224	1.0	1.0
1091_0000225	2.0	1.0
1091_0000230	1.0	2.0
1091_0000231	2.0	2.0
1091_0000236	2.0	2.0
1091_0000239	2.0	2.0
1091_0000243	1.0	1.0
1091_0000245	1.0	2.0
1091_0000247	1.0	1.0
1091_0000258	2.0	2.0
1091_0000260	2.0	2.0
1091_0000265	2.0	2.0
1091_0000270	2.0	2.0
1091_0000273	1.0	2.0
0603	2.0	2.0
0618	1.0	2.0
0619	1.0	1.0
0628	2.0	2.0
0631	2.0	2.0
0635	2.0	1.0
0715	2.0	2.0
0722	2.0	1.0
0805	2.0	2.0
0809	2.0	1.0
0812	1.0	1.0
0813	1.0	1.0
0819	3.0	2.0
0829	1.0	2.0
0901	2.0	2.0
0904	1.0	1.0
0911	1.0	2.0
0914	1.0	2.0
0915	3.0	2.0
0924	1.0	1.0
0925	2.0	2.0
0927	2.0	1.0
0928	1.0	2.0
0930	2.0	1.0
1015	1.0	2.0
1020	2.0	2.0
1022	2.0	1.0
1112	1.0	2.0
1113	2.0	2.0
1114	3.0	2.0
BER0611005	3.0	2.0
KYJ0611005A	0.0	1.0
KYJ0611006A	1.0	1.0
LIB0611003A	1.0	1.0
LON0610002B	2.0	1.0
LON0611004B	1.0	1.0
MOS0509004	2.0	2.0
PAR1011015	2.0	3.0
PHA0111002A	2.0	1.0
PHA0111004B	1.0	1.0
PHA0111010	4.0	3.0
PHA0111015	4.0	3.0
PHA0112006A	3.0	2.0
PHA0112007A	2.0	1.0
PHA0209039	3.0	3.0
PHA0210008	2.0	1.0
PHA0411008A	2.0	1.0
PHA0411031	3.0	3.0
PHA0411034	2.0	2.0
PHA0411041	3.0	3.0
PHA0509007	1.0	2.0
PHA0509015	3.0	2.0
PHA0509018	3.0	3.0
PHA0509019	2.0	3.0
PHA0509028	3.0	3.0
PHA0509034	2.0	3.0
PHA0509037	3.0	3.0
PHA0509040	2.0	3.0
PHA0509042	3.0	3.0
PHA0509044	2.0	3.0
PHA0510002A	2.0	2.0
PHA0510002B	1.0	1.0
PHA0510027	2.0	3.0
PHA0510029	3.0	3.0
PHA0510030	2.0	2.0
PHA0510035	2.0	3.0
PHA0510038	3.0	3.0
PHA0510040	2.0	3.0
PHA0510046	2.0	3.0
PHA0510048	3.0	2.0
PHA0610005B	1.0	1.0
PHA0610019A	2.0	1.0
PHA0710010	2.0	3.0
PHA0710011	3.0	3.0
PHA0809010	3.0	2.0
PHA0810002	2.0	3.0
PHA0810010	3.0	3.0
PHA0811013	3.0	3.0
PHA0811016	3.0	3.0
PHA1109007	3.0	2.0
PHA1109008	1.0	1.0
PHA1109025	1.0	1.0
PHA1109026	3.0	3.0
PHA1109028	2.0	3.0
PHA1110022	3.0	3.0
PHA1111003A	1.0	1.0
PHA1111003B	1.0	1.0
PHA1111006A	1.0	1.0
PHA1111008A	2.0	1.0
PHA1111009A	1.0	1.0
VAR0209036	3.0	3.0
VAR0909003	3.0	2.0
VAR0910011	2.0	2.0
1325_1001010	3.0	3.0
1325_1001013	3.0	3.0
1325_1001014	3.0	3.0
1325_1001016	2.0	3.0
1325_1001018	2.0	3.0
1325_1001022	3.0	3.0
1325_1001035	3.0	3.0
1325_1001042	3.0	3.0
1325_1001050	3.0	3.0
1325_1001051	3.0	3.0
1325_1001056	2.0	2.0
1325_1001058	3.0	3.0
1325_1001076	2.0	3.0
1325_1001079	3.0	3.0
1325_1001087	3.0	2.0
1325_1001094	2.0	3.0
1325_1001096	2.0	3.0
1325_1001125	3.0	3.0
1325_1001131	3.0	3.0
1325_1001139	3.0	3.0
1325_1001141	2.0	3.0
1325_1001153	3.0	3.0
1325_1001154	3.0	3.0
1325_1001162	2.0	3.0
1325_1001169	3.0	3.0
1325_9000059	2.0	3.0
1325_9000104	3.0	3.0
1325_9000209	3.0	3.0
1325_9000210	2.0	3.0
1325_9000237	3.0	3.0
1325_9000240	2.0	3.0
1325_9000278	3.0	3.0
1325_9000279	3.0	3.0
1325_9000304	3.0	3.0
1325_9000317	3.0	3.0
1325_9000320	3.0	2.0
1325_9000322	3.0	3.0
1325_9000533	3.0	3.0
1325_9000611	2.0	3.0
1325_9000674	3.0	3.0
1325_9000678	3.0	3.0
1365_0100004	2.0	2.0
1365_0100008	2.0	2.0
1365_0100011	2.0	2.0
1365_0100013	2.0	3.0
1365_0100014	2.0	2.0
1365_0100015	1.0	2.0
1365_0100020	2.0	2.0
1365_0100057	2.0	3.0
1365_0100058	3.0	3.0
1365_0100061	3.0	2.0
1365_0100066	2.0	2.0
1365_0100071	3.0	2.0
1365_0100080	2.0	3.0
1365_0100094	2.0	2.0
1365_0100096	2.0	2.0
1365_0100116	3.0	2.0
1365_0100121	3.0	2.0
1365_0100163	3.0	3.0
1365_0100166	2.0	2.0
1365_0100171	2.0	2.0
1365_0100174	2.0	2.0
1365_0100182	2.0	3.0
1365_0100190	3.0	2.0
1365_0100199	2.0	3.0
1365_0100200	3.0	3.0
1365_0100215	2.0	2.0
1365_0100228	2.0	2.0
1365_0100229	2.0	3.0
1365_0100255	2.0	3.0
1365_0100257	2.0	2.0
1365_0100260	2.0	2.0
1365_0100263	3.0	2.0
1365_0100265	3.0	3.0
1365_0100270	2.0	2.0
1365_0100282	2.0	3.0
1365_0100471	2.0	3.0
1365_0100478	2.0	2.0
1385_0000012	1.0	1.0
1385_0000016	1.0	0.0
1385_0000020	1.0	0.0
1385_0000023	1.0	1.0
1385_0000034	1.0	1.0
1385_0000035	1.0	2.0
1385_0000037	1.0	2.0
1385_0000038	1.0	1.0
1385_0000044	2.0	2.0
1385_0000045	2.0	2.0
1385_0000050	1.0	1.0
1385_0000098	2.0	1.0
1385_0000101	1.0	1.0
1385_0000102	2.0	2.0
1385_0000103	2.0	1.0
1385_0000122	1.0	1.0
1385_0000124	1.0	2.0
1385_0001103	2.0	0.0
1385_0001104	1.0	0.0
1385_0001110	2.0	1.0
1385_0001111	2.0	0.0
1385_0001132	1.0	1.0
1385_0001133	2.0	1.0
1385_0001137	2.0	1.0
1385_0001151	2.0	2.0
1385_0001163	1.0	1.0
1385_0001169	1.0	0.0
1385_0001190	0.0	1.0
1385_0001191	1.0	1.0
1385_0001193	1.0	2.0
1385_0001527	2.0	1.0
1385_0001730	2.0	1.0
1385_0001739	1.0	2.0
1385_0001750	0.0	0.0
1385_0001753	1.0	1.0
1385_0001754	1.0	1.0
1385_0001757	1.0	1.0
1385_0001760	1.0	1.0
1385_0001765	0.0	0.0
1385_0001772	1.0	1.0
1385_0001786	1.0	1.0
1385_0001787	1.0	1.0
1385_0001789	1.0	1.0
1385_0001795	0.0	0.0
1385_0001796	1.0	1.0
1395_0000340	2.0	2.0
1395_0000353	1.0	1.0
1395_0000360	3.0	1.0
1395_0000364	2.0	1.0
1395_0000379	2.0	1.0
1395_0000380	2.0	2.0
1395_0000390	1.0	1.0
1395_0000398	2.0	2.0
1395_0000415	1.0	2.0
1395_0000458	2.0	1.0
1395_0000469	1.0	1.0
1395_0000504	2.0	1.0
1395_0000528	2.0	2.0
1395_0000550	2.0	2.0
1395_0000559	2.0	1.0
1395_0000563	2.0	1.0
1395_0000575	1.0	1.0
1395_0000579	1.0	0.0
1395_0000593	0.0	2.0
1395_0000597	1.0	1.0
1395_0000602	1.0	2.0
1395_0000612	0.0	2.0
1395_0000642	1.0	1.0
1395_0000649	1.0	1.0
1395_0001013	1.0	2.0
1395_0001021	1.0	1.0
1395_0001024	1.0	2.0
1395_0001058	1.0	1.0
1395_0001061	2.0	3.0
1395_0001065	1.0	2.0
1395_0001070	2.0	2.0
1395_0001075	0.0	2.0
1395_0001076	0.0	2.0
1395_0001104	0.0	1.0
1395_0001116	2.0	1.0
1395_0001124	0.0	1.0
1395_0001141	2.0	1.0
1395_0001145	1.0	2.0
1395_0001150	1.0	1.0
1395_0001171	1.0	1.0
2 Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 1.16
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.99
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        30
         1.0       0.55      0.68      0.61       119
         2.0       0.60      0.55      0.57       166
         3.0       0.58      0.66      0.62       114
         4.0       0.32      0.38      0.35        21
         5.0       0.00      0.00      0.00         2

    accuracy                           0.56       452
   macro avg       0.34      0.38      0.36       452
weighted avg       0.53      0.56      0.54       452

[[ 0 26  4  0  0  0]
 [ 0 81 36  2  0  0]
 [ 0 36 91 38  1  0]
 [ 0  3 21 75 15  0]
 [ 0  0  0 13  8  0]
 [ 0  0  0  1  1  0]]
0.5429826931768899
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.95
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.01
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        30
         1.0       0.55      0.60      0.57       119
         2.0       0.53      0.58      0.55       166
         3.0       0.59      0.72      0.65       114
         4.0       0.00      0.00      0.00        21
         5.0       0.00      0.00      0.00         2

    accuracy                           0.55       452
   macro avg       0.28      0.32      0.30       452
weighted avg       0.49      0.55      0.52       452

[[ 0 21  9  0  0  0]
 [ 0 71 48  0  0  0]
 [ 0 33 97 36  0  0]
 [ 0  3 29 82  0  0]
 [ 0  0  1 20  0  0]
 [ 0  0  0  2  0  0]]
0.5177669983398541
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.82
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.97
              precision    recall  f1-score   support

         0.0       0.47      0.23      0.31        30
         1.0       0.54      0.75      0.62       119
         2.0       0.66      0.45      0.54       166
         3.0       0.61      0.83      0.71       114
         4.0       0.67      0.10      0.17        21
         5.0       0.00      0.00      0.00         2

    accuracy                           0.59       452
   macro avg       0.49      0.39      0.39       452
weighted avg       0.60      0.59      0.57       452

[[ 7 21  2  0  0  0]
 [ 8 89 22  0  0  0]
 [ 0 51 75 39  1  0]
 [ 0  5 14 95  0  0]
 [ 0  0  0 19  2  0]
 [ 0  0  0  2  0  0]]
0.5684157041493108
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.68
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.04
              precision    recall  f1-score   support

         0.0       0.46      0.20      0.28        30
         1.0       0.56      0.62      0.59       119
         2.0       0.59      0.51      0.54       166
         3.0       0.59      0.80      0.68       114
         4.0       0.50      0.24      0.32        21
         5.0       0.00      0.00      0.00         2

    accuracy                           0.58       452
   macro avg       0.45      0.39      0.40       452
weighted avg       0.57      0.58      0.56       452

[[ 6 18  6  0  0  0]
 [ 7 74 38  0  0  0]
 [ 0 37 84 44  1  0]
 [ 0  4 15 91  4  0]
 [ 0  0  0 16  5  0]
 [ 0  0  0  2  0  0]]
0.5597244863131353
452 452 452
Filename	True Label	Prediction
1023_0101848	2.0	2.0
1023_0101849	3.0	3.0
1023_0101893	3.0	3.0
1023_0101895	3.0	3.0
1023_0101898	4.0	3.0
1023_0101907	4.0	3.0
1023_0101909	4.0	3.0
1023_0103821	3.0	3.0
1023_0103824	3.0	3.0
1023_0103825	3.0	3.0
1023_0103829	2.0	3.0
1023_0103836	4.0	3.0
1023_0103838	3.0	3.0
1023_0107074	4.0	3.0
1023_0107244	3.0	3.0
1023_0107780	3.0	3.0
1023_0107788	3.0	3.0
1023_0108307	3.0	3.0
1023_0108423	3.0	2.0
1023_0108648	2.0	3.0
1023_0108650	3.0	3.0
1023_0108752	3.0	3.0
1023_0108811	3.0	3.0
1023_0108813	3.0	3.0
1023_0108933	3.0	3.0
1023_0108955	3.0	3.0
1023_0108992	3.0	3.0
1023_0109022	3.0	3.0
1023_0109026	2.0	3.0
1023_0109027	3.0	2.0
1023_0109029	1.0	2.0
1023_0109030	2.0	3.0
1023_0109096	3.0	3.0
1023_0109248	2.0	3.0
1023_0109250	2.0	3.0
1023_0109267	2.0	3.0
1023_0109399	2.0	3.0
1023_0109402	3.0	3.0
1023_0109516	3.0	3.0
1023_0109588	3.0	3.0
1023_0109590	2.0	3.0
1023_0109591	4.0	3.0
1023_0109671	2.0	3.0
1023_0109717	3.0	3.0
1023_0109721	2.0	3.0
1023_0109945	5.0	3.0
1023_0109946	3.0	2.0
1023_0109947	2.0	3.0
1023_0109951	3.0	3.0
1031_0002003	3.0	3.0
1031_0002040	4.0	4.0
1031_0002043	4.0	4.0
1031_0002061	3.0	3.0
1031_0002088	4.0	3.0
1031_0002089	4.0	4.0
1031_0002092	4.0	3.0
1031_0002198	3.0	3.0
1031_0003012	3.0	3.0
1031_0003052	4.0	3.0
1031_0003065	3.0	3.0
1031_0003072	3.0	4.0
1031_0003074	3.0	3.0
1031_0003106	3.0	3.0
1031_0003127	4.0	3.0
1031_0003144	3.0	3.0
1031_0003146	4.0	4.0
1031_0003165	2.0	3.0
1031_0003186	4.0	3.0
1031_0003214	2.0	4.0
1031_0003217	4.0	4.0
1031_0003219	3.0	3.0
1031_0003230	3.0	4.0
1031_0003237	3.0	4.0
1031_0003238	3.0	3.0
1031_0003274	3.0	3.0
1031_0003309	3.0	3.0
1031_0003310	3.0	4.0
1031_0003339	3.0	3.0
1031_0003353	3.0	3.0
1031_0003408	3.0	3.0
1031_0003409	5.0	3.0
1031_0003419	3.0	3.0
1061_0120273	1.0	2.0
1061_0120274	1.0	2.0
1061_0120289	1.0	1.0
1061_0120291	1.0	1.0
1061_0120304	1.0	2.0
1061_0120306	3.0	2.0
1061_0120311	3.0	2.0
1061_0120313	2.0	1.0
1061_0120314	2.0	2.0
1061_0120325	2.0	2.0
1061_0120336	1.0	2.0
1061_0120345	2.0	2.0
1061_0120349	1.0	1.0
1061_0120356	2.0	2.0
1061_0120358	1.0	2.0
1061_0120372	1.0	2.0
1061_0120375	2.0	1.0
1061_0120386	1.0	2.0
1061_0120389	2.0	2.0
1061_0120391	1.0	1.0
1061_0120403	3.0	2.0
1061_0120404	2.0	1.0
1061_0120406	2.0	2.0
1061_0120413	1.0	1.0
1061_0120426	2.0	2.0
1061_0120430	2.0	2.0
1061_0120432	2.0	2.0
1061_0120441	2.0	2.0
1061_0120450	2.0	2.0
1061_0120458	3.0	3.0
1061_0120480	2.0	2.0
1061_0120484	2.0	2.0
1061_0120496	2.0	2.0
1061_0120498	2.0	2.0
1061_0120856	2.0	2.0
1061_0120857	2.0	2.0
1061_0120881	2.0	3.0
1061_0120885	2.0	2.0
1061_0120887	1.0	1.0
1061_1029112	3.0	3.0
1061_1029120	2.0	2.0
1061_1202915	1.0	2.0
1061_1202918	1.0	2.0
1071_0024699	1.0	1.0
1071_0024710	0.0	1.0
1071_0024713	1.0	1.0
1071_0024716	1.0	0.0
1071_0024756	1.0	1.0
1071_0024758	2.0	2.0
1071_0024761	2.0	1.0
1071_0024763	1.0	1.0
1071_0024799	2.0	2.0
1071_0024802	1.0	1.0
1071_0024804	0.0	1.0
1071_0024811	1.0	1.0
1071_0024815	1.0	1.0
1071_0024820	0.0	1.0
1071_0024837	0.0	0.0
1071_0024840	1.0	1.0
1071_0024843	1.0	1.0
1071_0024844	1.0	1.0
1071_0024846	0.0	0.0
1071_0024855	1.0	1.0
1071_0024859	1.0	1.0
1071_0024873	0.0	1.0
1071_0024875	1.0	1.0
1071_0024879	1.0	1.0
1071_0242013	1.0	1.0
1071_0242021	1.0	1.0
1071_0242041	1.0	1.0
1071_0242043	0.0	1.0
1071_0243592	1.0	1.0
1071_0248301	2.0	1.0
1071_0248303	1.0	1.0
1071_0248304	1.0	1.0
1071_0248307	2.0	1.0
1071_0248311	2.0	1.0
1071_0248312	1.0	1.0
1071_0248316	1.0	0.0
1071_0248325	0.0	1.0
1071_0248326	1.0	1.0
1071_0248346	1.0	1.0
1071_0248348	1.0	1.0
1091_0000005	2.0	2.0
1091_0000016	1.0	1.0
1091_0000020	1.0	2.0
1091_0000030	0.0	1.0
1091_0000032	1.0	2.0
1091_0000036	1.0	2.0
1091_0000039	1.0	0.0
1091_0000044	0.0	2.0
1091_0000048	1.0	1.0
1091_0000050	1.0	1.0
1091_0000059	1.0	2.0
1091_0000062	2.0	1.0
1091_0000065	1.0	1.0
1091_0000072	0.0	2.0
1091_0000075	2.0	2.0
1091_0000101	2.0	1.0
1091_0000145	1.0	1.0
1091_0000153	1.0	2.0
1091_0000160	2.0	3.0
1091_0000168	2.0	2.0
1091_0000171	2.0	2.0
1091_0000191	1.0	1.0
1091_0000195	1.0	1.0
1091_0000197	1.0	2.0
1091_0000199	2.0	2.0
1091_0000217	3.0	1.0
1091_0000221	2.0	1.0
1091_0000226	1.0	2.0
1091_0000228	2.0	1.0
1091_0000229	2.0	2.0
1091_0000232	2.0	2.0
1091_0000235	1.0	2.0
1091_0000238	1.0	2.0
1091_0000240	1.0	1.0
1091_0000241	2.0	2.0
1091_0000244	2.0	2.0
1091_0000250	1.0	1.0
1091_0000251	1.0	2.0
1091_0000252	1.0	2.0
1091_0000254	3.0	1.0
1091_0000262	2.0	1.0
1091_0000266	1.0	2.0
1091_0000275	2.0	2.0
0602	1.0	2.0
0604	2.0	1.0
0605	2.0	2.0
0611	2.0	1.0
0613	1.0	1.0
0616	1.0	1.0
0625	1.0	1.0
0626	2.0	2.0
0630	1.0	1.0
0638	2.0	2.0
0640	2.0	2.0
0643	2.0	2.0
0644	1.0	2.0
0714	2.0	1.0
0801	2.0	2.0
0807	2.0	2.0
0814	2.0	1.0
0822	1.0	2.0
0824	1.0	2.0
0826	2.0	1.0
0902	2.0	2.0
0905	2.0	2.0
0907	2.0	2.0
0916	2.0	1.0
0917	1.0	2.0
0921	1.0	1.0
1002	2.0	2.0
1003	1.0	1.0
1006	2.0	2.0
1007	2.0	2.0
1009	2.0	1.0
1019	2.0	1.0
1021	2.0	2.0
1115	2.0	2.0
LIB0611002A	1.0	2.0
LIB0611002B	1.0	1.0
LIB0611004A	1.0	1.0
LON0610002A	1.0	1.0
LON0611002B	0.0	1.0
MOS0611012	2.0	3.0
PAR1011009A	2.0	1.0
PAR1011016	2.0	3.0
PHA0111001B	1.0	1.0
PHA0111002B	3.0	1.0
PHA0111004A	1.0	1.0
PHA0111012	2.0	2.0
PHA0112002A	2.0	1.0
PHA0112009B	1.0	1.0
PHA0209026	3.0	3.0
PHA0209031	4.0	3.0
PHA0209038	4.0	3.0
PHA0411009B	1.0	1.0
PHA0411029	3.0	2.0
PHA0411030	3.0	3.0
PHA0411036	3.0	3.0
PHA0411039	3.0	3.0
PHA0411047	3.0	3.0
PHA0411053	3.0	3.0
PHA0411056	4.0	3.0
PHA0411059	2.0	3.0
PHA0509002	1.0	1.0
PHA0509017	3.0	3.0
PHA0509021	3.0	2.0
PHA0509031	2.0	3.0
PHA0509036	2.0	3.0
PHA0509038	2.0	3.0
PHA0510034	3.0	3.0
PHA0510036	3.0	3.0
PHA0510039	2.0	3.0
PHA0510049	2.0	2.0
PHA0610007A	3.0	1.0
PHA0710009	2.0	2.0
PHA0710014	3.0	3.0
PHA0710015	3.0	3.0
PHA0710018	3.0	3.0
PHA0809009	2.0	3.0
PHA0811020	2.0	2.0
PHA1109023	2.0	1.0
PHA1110002B	2.0	1.0
PHA1110004A	2.0	2.0
PHA1110014	3.0	3.0
PHA1110016	2.0	2.0
PHA1111001B	1.0	1.0
VAR0909004	3.0	2.0
VAR0909010	3.0	3.0
1325_1001012	3.0	3.0
1325_1001028	3.0	3.0
1325_1001033	3.0	3.0
1325_1001040	3.0	3.0
1325_1001043	3.0	3.0
1325_1001047	2.0	2.0
1325_1001054	3.0	3.0
1325_1001063	2.0	3.0
1325_1001082	2.0	2.0
1325_1001084	3.0	2.0
1325_1001090	3.0	2.0
1325_1001093	2.0	3.0
1325_1001097	1.0	2.0
1325_1001098	2.0	3.0
1325_1001100	3.0	3.0
1325_1001107	3.0	3.0
1325_1001109	2.0	3.0
1325_1001111	3.0	3.0
1325_1001121	3.0	3.0
1325_1001123	3.0	3.0
1325_1001129	2.0	3.0
1325_1001136	2.0	3.0
1325_1001138	2.0	3.0
1325_1001142	2.0	3.0
1325_1001155	3.0	3.0
1325_1001156	2.0	3.0
1325_1001157	2.0	3.0
1325_1001158	3.0	3.0
1325_1001159	3.0	3.0
1325_1001166	3.0	3.0
1325_1001170	3.0	3.0
1325_9000090	3.0	3.0
1325_9000107	3.0	3.0
1325_9000139	3.0	3.0
1325_9000143	3.0	3.0
1325_9000187	3.0	3.0
1325_9000213	3.0	3.0
1325_9000296	2.0	3.0
1325_9000321	3.0	3.0
1325_9000505	3.0	3.0
1325_9000601	3.0	3.0
1325_9000602	4.0	3.0
1325_9000612	2.0	3.0
1325_9000675	3.0	3.0
1325_9000685	4.0	3.0
1325_9000686	2.0	3.0
1365_0100028	2.0	2.0
1365_0100029	1.0	2.0
1365_0100098	2.0	2.0
1365_0100100	3.0	3.0
1365_0100101	2.0	3.0
1365_0100118	2.0	3.0
1365_0100134	2.0	2.0
1365_0100135	2.0	2.0
1365_0100136	2.0	2.0
1365_0100137	2.0	2.0
1365_0100145	3.0	3.0
1365_0100147	3.0	2.0
1365_0100165	3.0	3.0
1365_0100178	2.0	2.0
1365_0100184	2.0	2.0
1365_0100188	2.0	2.0
1365_0100196	2.0	3.0
1365_0100198	2.0	2.0
1365_0100202	2.0	2.0
1365_0100213	2.0	2.0
1365_0100219	2.0	3.0
1365_0100224	3.0	3.0
1365_0100232	2.0	3.0
1365_0100252	3.0	3.0
1365_0100266	2.0	2.0
1365_0100276	3.0	3.0
1365_0100277	3.0	3.0
1365_0100290	2.0	2.0
1365_0100447	3.0	3.0
1365_0100458	2.0	3.0
1365_0100472	2.0	3.0
1365_0100477	2.0	3.0
1365_0100479	2.0	2.0
1385_0000017	1.0	0.0
1385_0000054	2.0	2.0
1385_0000095	1.0	0.0
1385_0000104	2.0	1.0
1385_0001122	2.0	1.0
1385_0001123	2.0	1.0
1385_0001128	1.0	1.0
1385_0001130	1.0	0.0
1385_0001134	1.0	0.0
1385_0001136	1.0	2.0
1385_0001138	1.0	1.0
1385_0001147	1.0	1.0
1385_0001148	2.0	1.0
1385_0001156	1.0	1.0
1385_0001157	1.0	1.0
1385_0001160	1.0	2.0
1385_0001161	1.0	1.0
1385_0001162	1.0	1.0
1385_0001164	1.0	1.0
1385_0001172	0.0	1.0
1385_0001173	0.0	0.0
1385_0001175	0.0	1.0
1385_0001198	1.0	2.0
1385_0001522	1.0	1.0
1385_0001714	1.0	1.0
1385_0001720	0.0	2.0
1385_0001727	0.0	1.0
1385_0001728	1.0	2.0
1385_0001729	1.0	2.0
1385_0001737	2.0	1.0
1385_0001744	0.0	1.0
1385_0001748	1.0	2.0
1385_0001758	0.0	1.0
1385_0001762	2.0	1.0
1385_0001764	1.0	1.0
1385_0001766	2.0	2.0
1385_0001767	0.0	1.0
1385_0001773	0.0	1.0
1385_0001774	0.0	0.0
1385_0001775	0.0	1.0
1395_0000333	1.0	1.0
1395_0000338	2.0	1.0
1395_0000355	2.0	2.0
1395_0000365	3.0	2.0
1395_0000376	2.0	2.0
1395_0000387	3.0	2.0
1395_0000388	2.0	2.0
1395_0000392	2.0	2.0
1395_0000414	2.0	1.0
1395_0000432	2.0	1.0
1395_0000438	2.0	2.0
1395_0000446	2.0	2.0
1395_0000447	2.0	2.0
1395_0000448	2.0	1.0
1395_0000499	1.0	2.0
1395_0000500	2.0	1.0
1395_0000512	2.0	2.0
1395_0000533	3.0	2.0
1395_0000548	2.0	2.0
1395_0000557	2.0	2.0
1395_0000560	2.0	1.0
1395_0000581	2.0	2.0
1395_0000584	0.0	0.0
1395_0000585	0.0	1.0
1395_0000599	1.0	1.0
1395_0000631	0.0	2.0
1395_0000636	0.0	2.0
1395_0001020	1.0	2.0
1395_0001034	1.0	1.0
1395_0001040	0.0	0.0
1395_0001064	2.0	2.0
1395_0001078	1.0	1.0
1395_0001093	1.0	1.0
1395_0001109	0.0	2.0
1395_0001120	1.0	1.0
1395_0001121	0.0	1.0
1395_0001123	1.0	1.0
1395_0001126	1.0	1.0
1395_0001149	1.0	1.0
1395_0001169	2.0	1.0
3 Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 1.16
  Training epoch took: 65
Running Validation...
  Average evaluation loss: 1.09
              precision    recall  f1-score   support

         0.0       1.00      0.03      0.06        30
         1.0       0.33      0.08      0.13       119
         2.0       0.44      0.82      0.57       166
         3.0       0.58      0.56      0.57       115
         4.0       0.00      0.00      0.00        20
         5.0       0.00      0.00      0.00         2

    accuracy                           0.47       452
   macro avg       0.39      0.25      0.22       452
weighted avg       0.46      0.47      0.39       452

[[  1  15  14   0   0   0]
 [  0  10 109   0   0   0]
 [  0   5 136  25   0   0]
 [  0   0  51  64   0   0]
 [  0   0   0  20   0   0]
 [  0   0   0   2   0   0]]
0.3935808063127402
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.95
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.95
              precision    recall  f1-score   support

         0.0       1.00      0.03      0.06        30
         1.0       0.58      0.58      0.58       119
         2.0       0.54      0.54      0.54       166
         3.0       0.55      0.80      0.65       115
         4.0       0.00      0.00      0.00        20
         5.0       0.00      0.00      0.00         2

    accuracy                           0.56       452
   macro avg       0.45      0.33      0.31       452
weighted avg       0.56      0.56      0.52       452

[[ 1 25  4  0  0  0]
 [ 0 69 50  0  0  0]
 [ 0 24 90 52  0  0]
 [ 0  0 23 92  0  0]
 [ 0  0  0 20  0  0]
 [ 0  0  0  2  0  0]]
0.5226965573991297
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.82
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.01
              precision    recall  f1-score   support

         0.0       0.76      0.43      0.55        30
         1.0       0.62      0.55      0.58       119
         2.0       0.55      0.52      0.54       166
         3.0       0.56      0.84      0.67       115
         4.0       0.00      0.00      0.00        20
         5.0       0.00      0.00      0.00         2

    accuracy                           0.58       452
   macro avg       0.42      0.39      0.39       452
weighted avg       0.56      0.58      0.56       452

[[13 15  2  0  0  0]
 [ 4 65 50  0  0  0]
 [ 0 24 87 55  0  0]
 [ 0  0 18 97  0  0]
 [ 0  0  0 20  0  0]
 [ 0  0  0  2  0  0]]
0.5588259345818347
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.68
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.03
              precision    recall  f1-score   support

         0.0       0.85      0.37      0.51        30
         1.0       0.63      0.55      0.59       119
         2.0       0.54      0.58      0.56       166
         3.0       0.56      0.76      0.64       115
         4.0       0.00      0.00      0.00        20
         5.0       0.00      0.00      0.00         2

    accuracy                           0.58       452
   macro avg       0.43      0.38      0.38       452
weighted avg       0.56      0.58      0.56       452

[[11 15  4  0  0  0]
 [ 2 66 51  0  0  0]
 [ 0 22 97 47  0  0]
 [ 0  2 26 87  0  0]
 [ 0  0  0 20  0  0]
 [ 0  0  0  2  0  0]]
0.559574678441269
452 452 452
Filename	True Label	Prediction
1023_0001419	3.0	3.0
1023_0001423	2.0	3.0
1023_0101675	3.0	3.0
1023_0101694	2.0	3.0
1023_0101700	2.0	3.0
1023_0101753	3.0	3.0
1023_0101841	3.0	3.0
1023_0101843	3.0	3.0
1023_0101844	2.0	3.0
1023_0101846	4.0	3.0
1023_0101853	2.0	3.0
1023_0101897	3.0	3.0
1023_0101900	3.0	3.0
1023_0103827	3.0	3.0
1023_0103843	3.0	2.0
1023_0103955	4.0	3.0
1023_0104206	3.0	2.0
1023_0106816	3.0	2.0
1023_0107682	2.0	3.0
1023_0107729	3.0	3.0
1023_0108306	3.0	3.0
1023_0108422	3.0	3.0
1023_0108510	3.0	3.0
1023_0108518	3.0	3.0
1023_0108641	3.0	3.0
1023_0108814	3.0	3.0
1023_0108885	2.0	3.0
1023_0108886	3.0	3.0
1023_0108889	3.0	3.0
1023_0108931	3.0	3.0
1023_0108958	2.0	3.0
1023_0109039	3.0	3.0
1023_0109192	3.0	3.0
1023_0109247	3.0	3.0
1023_0109391	2.0	3.0
1023_0109392	3.0	2.0
1023_0109395	2.0	2.0
1023_0109496	3.0	3.0
1023_0109518	2.0	2.0
1023_0109524	2.0	3.0
1023_0109651	3.0	3.0
1023_0109674	3.0	3.0
1023_0109880	3.0	3.0
1023_0111896	2.0	2.0
1031_0001950	4.0	3.0
1031_0001951	2.0	3.0
1031_0001998	4.0	3.0
1031_0002004	4.0	3.0
1031_0002006	4.0	3.0
1031_0002032	3.0	3.0
1031_0002036	4.0	3.0
1031_0002079	5.0	3.0
1031_0002085	3.0	3.0
1031_0002185	4.0	3.0
1031_0002195	3.0	3.0
1031_0002200	3.0	3.0
1031_0003013	4.0	3.0
1031_0003043	5.0	3.0
1031_0003073	3.0	3.0
1031_0003126	3.0	3.0
1031_0003129	4.0	3.0
1031_0003156	3.0	3.0
1031_0003157	4.0	3.0
1031_0003161	3.0	3.0
1031_0003164	4.0	3.0
1031_0003166	3.0	3.0
1031_0003170	3.0	3.0
1031_0003172	3.0	3.0
1031_0003181	4.0	3.0
1031_0003184	4.0	3.0
1031_0003203	2.0	3.0
1031_0003211	2.0	3.0
1031_0003212	3.0	3.0
1031_0003231	4.0	3.0
1031_0003234	3.0	3.0
1031_0003239	4.0	3.0
1031_0003242	3.0	3.0
1031_0003327	3.0	3.0
1031_0003338	3.0	3.0
1031_0003365	3.0	3.0
1031_0003368	3.0	3.0
1031_0003369	3.0	3.0
1031_0003383	3.0	3.0
1031_0003386	3.0	3.0
1031_0003387	3.0	3.0
1031_0003388	3.0	3.0
1031_0003390	3.0	3.0
1061_0120276	2.0	2.0
1061_0120277	1.0	2.0
1061_0120278	2.0	2.0
1061_0120290	1.0	2.0
1061_0120297	1.0	2.0
1061_0120328	1.0	2.0
1061_0120329	2.0	3.0
1061_0120332	2.0	2.0
1061_0120337	2.0	2.0
1061_0120347	2.0	2.0
1061_0120351	2.0	3.0
1061_0120353	1.0	1.0
1061_0120354	1.0	1.0
1061_0120361	3.0	2.0
1061_0120367	3.0	2.0
1061_0120371	3.0	3.0
1061_0120374	2.0	3.0
1061_0120383	2.0	3.0
1061_0120394	2.0	2.0
1061_0120405	2.0	2.0
1061_0120407	3.0	2.0
1061_0120408	2.0	2.0
1061_0120424	2.0	3.0
1061_0120425	2.0	2.0
1061_0120428	2.0	2.0
1061_0120429	3.0	2.0
1061_0120433	1.0	1.0
1061_0120439	1.0	2.0
1061_0120448	3.0	2.0
1061_0120459	2.0	3.0
1061_0120479	2.0	2.0
1061_0120482	2.0	2.0
1061_0120485	3.0	2.0
1061_0120489	2.0	2.0
1061_0120491	2.0	2.0
1061_0120492	3.0	2.0
1061_0120494	2.0	2.0
1061_0120495	2.0	2.0
1061_0120876	2.0	2.0
1061_0120878	2.0	2.0
1061_0120894	2.0	3.0
1061_1029111	2.0	2.0
1061_1029118	1.0	2.0
1061_1202913	2.0	2.0
1061_1202919	2.0	2.0
1071_0024678	2.0	1.0
1071_0024680	2.0	1.0
1071_0024683	0.0	1.0
1071_0024686	3.0	2.0
1071_0024688	1.0	2.0
1071_0024702	1.0	1.0
1071_0024706	1.0	1.0
1071_0024711	1.0	1.0
1071_0024759	0.0	1.0
1071_0024765	0.0	0.0
1071_0024773	1.0	1.0
1071_0024782	0.0	0.0
1071_0024783	0.0	1.0
1071_0024803	1.0	1.0
1071_0024814	1.0	1.0
1071_0024826	1.0	1.0
1071_0024831	0.0	1.0
1071_0024850	0.0	1.0
1071_0024863	1.0	1.0
1071_0024865	2.0	2.0
1071_0024871	1.0	1.0
1071_0024872	1.0	1.0
1071_0024874	0.0	1.0
1071_0024876	1.0	1.0
1071_0024881	1.0	2.0
1071_0241832	1.0	1.0
1071_0241833	1.0	1.0
1071_0242012	1.0	2.0
1071_0242023	1.0	1.0
1071_0242071	0.0	1.0
1071_0242093	0.0	0.0
1071_0248305	0.0	1.0
1071_0248309	2.0	1.0
1071_0248313	1.0	1.0
1071_0248315	0.0	0.0
1071_0248318	0.0	0.0
1071_0248323	0.0	1.0
1071_0248330	2.0	1.0
1071_0248333	2.0	1.0
1071_0248335	1.0	1.0
1071_0248337	1.0	2.0
1071_0248342	0.0	1.0
1091_0000011	2.0	1.0
1091_0000018	2.0	2.0
1091_0000021	1.0	2.0
1091_0000022	2.0	2.0
1091_0000038	2.0	1.0
1091_0000041	1.0	1.0
1091_0000045	1.0	2.0
1091_0000046	1.0	1.0
1091_0000047	2.0	1.0
1091_0000055	1.0	2.0
1091_0000071	1.0	2.0
1091_0000074	2.0	2.0
1091_0000086	1.0	2.0
1091_0000092	1.0	2.0
1091_0000095	1.0	1.0
1091_0000113	1.0	2.0
1091_0000146	1.0	0.0
1091_0000151	0.0	1.0
1091_0000156	3.0	2.0
1091_0000157	2.0	2.0
1091_0000165	1.0	1.0
1091_0000172	2.0	1.0
1091_0000174	2.0	1.0
1091_0000185	2.0	1.0
1091_0000192	1.0	1.0
1091_0000193	2.0	1.0
1091_0000194	1.0	2.0
1091_0000200	2.0	2.0
1091_0000202	1.0	1.0
1091_0000204	3.0	1.0
1091_0000207	2.0	2.0
1091_0000209	3.0	1.0
1091_0000216	1.0	2.0
1091_0000222	2.0	2.0
1091_0000234	2.0	2.0
1091_0000249	2.0	2.0
1091_0000259	2.0	2.0
1091_0000272	1.0	1.0
1091_0000274	1.0	2.0
1091_0000276	2.0	2.0
0601	1.0	2.0
0607	3.0	2.0
0610	1.0	2.0
0614	2.0	2.0
0617	1.0	2.0
0621	1.0	2.0
0633	2.0	2.0
0639	1.0	2.0
0717	1.0	2.0
0719	2.0	2.0
0723	2.0	2.0
0810	1.0	2.0
0811	2.0	2.0
0815	2.0	2.0
0821	1.0	2.0
0827	2.0	2.0
0906	2.0	2.0
0919	2.0	1.0
0920	2.0	2.0
0923	2.0	2.0
1001	1.0	2.0
1010	2.0	2.0
1111	1.0	2.0
KYJ0611005B	1.0	2.0
KYJ0611009A	1.0	1.0
LIB0611004B	1.0	1.0
LIB0611011	2.0	2.0
LON0611004A	0.0	1.0
MOS0611015	2.0	3.0
PAR1011009B	1.0	1.0
PHA0111005A	1.0	1.0
PHA0112002B	1.0	2.0
PHA0112003B	1.0	1.0
PHA0112009A	2.0	2.0
PHA0112012A	2.0	2.0
PHA0209013	1.0	1.0
PHA0210001	1.0	1.0
PHA0210004	2.0	2.0
PHA0210007	2.0	2.0
PHA0411009A	1.0	2.0
PHA0411010B	1.0	1.0
PHA0411027	2.0	3.0
PHA0411033	2.0	2.0
PHA0411035	3.0	2.0
PHA0411045	2.0	3.0
PHA0411054	3.0	2.0
PHA0411061	2.0	3.0
PHA0509025	4.0	3.0
PHA0509026	4.0	3.0
PHA0509032	2.0	3.0
PHA0509033	2.0	2.0
PHA0509043	3.0	3.0
PHA0509045	2.0	2.0
PHA0510003B	1.0	1.0
PHA0510004B	1.0	2.0
PHA0510013A	1.0	1.0
PHA0510013B	1.0	1.0
PHA0510032	2.0	3.0
PHA0510047	2.0	2.0
PHA0610005A	1.0	1.0
PHA0610015	2.0	3.0
PHA0610019B	2.0	2.0
PHA0709008	3.0	3.0
PHA0710012	3.0	3.0
PHA0810003	2.0	3.0
PHA0810008	2.0	3.0
PHA0810011	2.0	3.0
PHA0810015	2.0	3.0
PHA0811012	4.0	3.0
PHA0811019	3.0	3.0
PHA1109001	2.0	2.0
PHA1109002	3.0	3.0
PHA1109004	3.0	3.0
PHA1109005	3.0	2.0
PHA1110002A	2.0	2.0
PHA1110003B	1.0	1.0
PHA1111002B	1.0	1.0
PHA1111004A	2.0	1.0
PHA1111008B	1.0	1.0
ST071122B	1.0	1.0
VAR0909006	2.0	3.0
VAR0910004	3.0	3.0
VAR0910005	3.0	2.0
VAR0910007	2.0	3.0
1325_1001020	2.0	2.0
1325_1001023	3.0	2.0
1325_1001025	2.0	3.0
1325_1001029	3.0	3.0
1325_1001036	3.0	3.0
1325_1001080	3.0	3.0
1325_1001081	2.0	3.0
1325_1001085	3.0	3.0
1325_1001086	3.0	3.0
1325_1001088	2.0	2.0
1325_1001110	3.0	3.0
1325_1001113	3.0	3.0
1325_1001119	3.0	3.0
1325_1001120	3.0	3.0
1325_1001122	2.0	3.0
1325_1001152	3.0	3.0
1325_1001164	3.0	3.0
1325_1001165	2.0	3.0
1325_1001168	2.0	3.0
1325_9000089	3.0	3.0
1325_9000105	2.0	2.0
1325_9000106	3.0	3.0
1325_9000138	4.0	3.0
1325_9000140	3.0	3.0
1325_9000152	3.0	3.0
1325_9000188	3.0	3.0
1325_9000215	3.0	3.0
1325_9000302	2.0	3.0
1325_9000314	2.0	3.0
1325_9000319	2.0	3.0
1325_9000323	2.0	3.0
1325_9000503	3.0	3.0
1325_9000536	3.0	3.0
1325_9000554	2.0	3.0
1325_9000676	3.0	3.0
1365_0100005	1.0	2.0
1365_0100012	2.0	2.0
1365_0100017	2.0	3.0
1365_0100018	2.0	2.0
1365_0100021	2.0	2.0
1365_0100023	2.0	2.0
1365_0100024	2.0	2.0
1365_0100026	2.0	1.0
1365_0100027	3.0	2.0
1365_0100030	2.0	2.0
1365_0100063	3.0	3.0
1365_0100065	1.0	2.0
1365_0100072	2.0	2.0
1365_0100092	2.0	2.0
1365_0100102	2.0	2.0
1365_0100103	3.0	3.0
1365_0100104	2.0	3.0
1365_0100105	3.0	3.0
1365_0100107	3.0	3.0
1365_0100138	2.0	2.0
1365_0100148	3.0	3.0
1365_0100151	2.0	2.0
1365_0100167	2.0	2.0
1365_0100168	3.0	2.0
1365_0100173	2.0	2.0
1365_0100180	2.0	2.0
1365_0100183	2.0	2.0
1365_0100204	2.0	2.0
1365_0100211	3.0	3.0
1365_0100217	3.0	3.0
1365_0100220	3.0	2.0
1365_0100225	3.0	2.0
1365_0100226	3.0	2.0
1365_0100231	2.0	2.0
1365_0100233	3.0	3.0
1365_0100251	3.0	3.0
1365_0100261	2.0	2.0
1365_0100267	2.0	3.0
1365_0100269	2.0	2.0
1365_0100278	3.0	2.0
1365_0100285	2.0	2.0
1365_0100299	3.0	2.0
1365_0100459	3.0	3.0
1365_0100480	2.0	3.0
1385_0000011	1.0	1.0
1385_0000021	1.0	1.0
1385_0000033	2.0	2.0
1385_0000036	1.0	2.0
1385_0000042	1.0	1.0
1385_0000047	1.0	2.0
1385_0000049	1.0	1.0
1385_0000119	1.0	1.0
1385_0000129	2.0	2.0
1385_0001108	2.0	2.0
1385_0001118	2.0	1.0
1385_0001120	2.0	1.0
1385_0001121	2.0	2.0
1385_0001126	0.0	0.0
1385_0001135	1.0	2.0
1385_0001149	2.0	1.0
1385_0001154	1.0	1.0
1385_0001159	1.0	1.0
1385_0001170	0.0	0.0
1385_0001174	0.0	0.0
1385_0001192	1.0	1.0
1385_0001194	1.0	1.0
1385_0001199	1.0	1.0
1385_0001717	1.0	2.0
1385_0001718	0.0	0.0
1385_0001723	0.0	0.0
1385_0001733	1.0	2.0
1385_0001742	0.0	1.0
1385_0001747	1.0	1.0
1385_0001749	1.0	1.0
1385_0001751	0.0	2.0
1385_0001752	1.0	2.0
1385_0001761	0.0	1.0
1385_0001771	1.0	1.0
1385_0001800	1.0	1.0
1395_0000337	1.0	0.0
1395_0000354	1.0	1.0
1395_0000356	1.0	1.0
1395_0000366	2.0	2.0
1395_0000378	2.0	2.0
1395_0000396	1.0	2.0
1395_0000402	1.0	1.0
1395_0000404	2.0	2.0
1395_0000450	1.0	2.0
1395_0000452	1.0	1.0
1395_0000454	2.0	2.0
1395_0000471	2.0	1.0
1395_0000513	2.0	2.0
1395_0000525	2.0	1.0
1395_0000526	1.0	1.0
1395_0000527	1.0	1.0
1395_0000531	2.0	2.0
1395_0000534	2.0	2.0
1395_0000535	1.0	1.0
1395_0000554	2.0	2.0
1395_0000583	1.0	2.0
1395_0000607	0.0	0.0
1395_0000609	1.0	1.0
1395_0000610	2.0	1.0
1395_0000628	0.0	2.0
1395_0000630	0.0	2.0
1395_0000639	0.0	2.0
1395_0000644	1.0	2.0
1395_0001015	1.0	2.0
1395_0001019	1.0	1.0
1395_0001060	2.0	2.0
1395_0001066	1.0	2.0
1395_0001071	2.0	1.0
1395_0001074	1.0	1.0
1395_0001080	2.0	2.0
1395_0001131	0.0	1.0
1395_0001132	2.0	2.0
1395_0001133	1.0	2.0
1395_0001147	1.0	2.0
1395_0001170	1.0	2.0
4 Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 1.13
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       1.00      0.03      0.06        31
         1.0       0.56      0.64      0.60       118
         2.0       0.60      0.55      0.58       166
         3.0       0.61      0.86      0.71       115
         4.0       0.00      0.00      0.00        20
         5.0       0.00      0.00      0.00         2

    accuracy                           0.59       452
   macro avg       0.46      0.35      0.33       452
weighted avg       0.59      0.59      0.55       452

[[ 1 27  3  0  0  0]
 [ 0 76 42  0  0  0]
 [ 0 32 92 42  0  0]
 [ 0  1 15 99  0  0]
 [ 0  0  1 19  0  0]
 [ 0  0  0  2  0  0]]
0.5542102999873205
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.93
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.92
              precision    recall  f1-score   support

         0.0       0.50      0.23      0.31        31
         1.0       0.53      0.84      0.65       118
         2.0       0.60      0.47      0.53       166
         3.0       0.63      0.67      0.65       115
         4.0       0.00      0.00      0.00        20
         5.0       0.00      0.00      0.00         2

    accuracy                           0.58       452
   macro avg       0.38      0.37      0.36       452
weighted avg       0.56      0.58      0.55       452

[[ 7 23  1  0  0  0]
 [ 6 99 13  0  0  0]
 [ 1 63 78 24  0  0]
 [ 0  2 36 77  0  0]
 [ 0  0  1 19  0  0]
 [ 0  0  0  2  0  0]]
0.5503462663209095
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.78
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.92
              precision    recall  f1-score   support

         0.0       0.44      0.23      0.30        31
         1.0       0.59      0.66      0.62       118
         2.0       0.62      0.59      0.61       166
         3.0       0.61      0.72      0.66       115
         4.0       0.36      0.20      0.26        20
         5.0       0.00      0.00      0.00         2

    accuracy                           0.60       452
   macro avg       0.44      0.40      0.41       452
weighted avg       0.58      0.60      0.59       452

[[ 7 23  1  0  0  0]
 [ 8 78 32  0  0  0]
 [ 1 32 98 35  0  0]
 [ 0  0 25 83  7  0]
 [ 0  0  1 15  4  0]
 [ 0  0  0  2  0  0]]
0.5858951956834018
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.62
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.99
              precision    recall  f1-score   support

         0.0       0.47      0.29      0.36        31
         1.0       0.57      0.66      0.61       118
         2.0       0.63      0.55      0.59       166
         3.0       0.59      0.68      0.63       115
         4.0       0.19      0.20      0.20        20
         5.0       0.00      0.00      0.00         2

    accuracy                           0.58       452
   macro avg       0.41      0.40      0.40       452
weighted avg       0.57      0.58      0.57       452

[[ 9 20  1  1  0  0]
 [ 9 78 30  1  0  0]
 [ 1 37 91 36  1  0]
 [ 0  1 21 78 15  0]
 [ 0  0  1 15  4  0]
 [ 0  0  0  1  1  0]]
0.5699657598070124
452 452 452
Filename	True Label	Prediction
1023_0001416	5.0	4.0
1023_0001420	4.0	3.0
1023_0001422	3.0	3.0
1023_0101684	2.0	2.0
1023_0101691	4.0	3.0
1023_0101695	2.0	3.0
1023_0101701	2.0	3.0
1023_0101749	4.0	3.0
1023_0101845	3.0	3.0
1023_0101856	3.0	3.0
1023_0101904	2.0	3.0
1023_0101906	2.0	3.0
1023_0102117	3.0	3.0
1023_0102118	3.0	2.0
1023_0103822	2.0	2.0
1023_0103823	4.0	3.0
1023_0103830	3.0	3.0
1023_0103832	3.0	3.0
1023_0103834	3.0	3.0
1023_0103839	3.0	3.0
1023_0103840	3.0	3.0
1023_0103880	3.0	3.0
1023_0104207	2.0	3.0
1023_0104209	3.0	3.0
1023_0107075	2.0	3.0
1023_0107672	2.0	3.0
1023_0107740	3.0	3.0
1023_0107783	3.0	2.0
1023_0108426	2.0	3.0
1023_0108751	3.0	3.0
1023_0108888	2.0	3.0
1023_0108890	3.0	3.0
1023_0108908	2.0	3.0
1023_0108932	2.0	3.0
1023_0108993	3.0	3.0
1023_0109038	3.0	3.0
1023_0109396	2.0	3.0
1023_0109400	3.0	2.0
1023_0109505	3.0	3.0
1023_0109528	3.0	3.0
1023_0109614	2.0	2.0
1023_0109891	3.0	3.0
1023_0109917	5.0	3.0
1023_0109954	3.0	3.0
1031_0001949	3.0	4.0
1031_0001997	3.0	3.0
1031_0002010	3.0	4.0
1031_0002042	4.0	3.0
1031_0002086	3.0	4.0
1031_0002091	4.0	4.0
1031_0002184	3.0	4.0
1031_0002197	4.0	3.0
1031_0003042	3.0	3.0
1031_0003048	4.0	3.0
1031_0003053	3.0	4.0
1031_0003076	4.0	3.0
1031_0003078	3.0	3.0
1031_0003097	3.0	3.0
1031_0003131	3.0	4.0
1031_0003132	4.0	4.0
1031_0003145	4.0	3.0
1031_0003149	4.0	4.0
1031_0003150	3.0	4.0
1031_0003155	3.0	4.0
1031_0003160	3.0	4.0
1031_0003190	3.0	4.0
1031_0003206	3.0	3.0
1031_0003216	3.0	4.0
1031_0003224	3.0	4.0
1031_0003232	2.0	4.0
1031_0003236	3.0	3.0
1031_0003243	3.0	3.0
1031_0003246	4.0	3.0
1031_0003313	3.0	4.0
1031_0003314	4.0	4.0
1031_0003315	3.0	3.0
1031_0003331	3.0	4.0
1031_0003337	3.0	4.0
1031_0003355	3.0	2.0
1031_0003359	3.0	3.0
1031_0003407	3.0	3.0
1061_0012029	4.0	2.0
1061_0120271	2.0	2.0
1061_0120287	1.0	2.0
1061_0120298	2.0	2.0
1061_0120299	2.0	2.0
1061_0120307	2.0	2.0
1061_0120309	1.0	1.0
1061_0120310	2.0	2.0
1061_0120317	3.0	2.0
1061_0120319	2.0	2.0
1061_0120320	3.0	3.0
1061_0120321	2.0	2.0
1061_0120326	2.0	2.0
1061_0120334	2.0	2.0
1061_0120355	1.0	1.0
1061_0120359	2.0	2.0
1061_0120370	2.0	2.0
1061_0120376	2.0	2.0
1061_0120382	1.0	2.0
1061_0120387	1.0	2.0
1061_0120390	2.0	2.0
1061_0120414	2.0	2.0
1061_0120431	2.0	2.0
1061_0120438	3.0	2.0
1061_0120440	1.0	1.0
1061_0120442	3.0	3.0
1061_0120449	2.0	2.0
1061_0120483	2.0	1.0
1061_0120490	2.0	2.0
1061_0120859	2.0	2.0
1061_0120882	3.0	2.0
1061_1029115	1.0	1.0
1061_1029116	1.0	3.0
1061_1202911	0.0	3.0
1061_1202914	1.0	2.0
1061_1202916	2.0	2.0
1071_0024681	2.0	2.0
1071_0024692	2.0	2.0
1071_0024704	1.0	1.0
1071_0024712	1.0	1.0
1071_0024762	0.0	0.0
1071_0024768	1.0	1.0
1071_0024777	1.0	2.0
1071_0024778	0.0	1.0
1071_0024779	1.0	1.0
1071_0024784	1.0	0.0
1071_0024798	0.0	0.0
1071_0024800	0.0	1.0
1071_0024810	1.0	1.0
1071_0024816	1.0	1.0
1071_0024822	0.0	2.0
1071_0024824	1.0	1.0
1071_0024833	1.0	1.0
1071_0024834	2.0	2.0
1071_0024835	1.0	1.0
1071_0024853	0.0	0.0
1071_0024856	1.0	1.0
1071_0024860	1.0	1.0
1071_0024862	2.0	1.0
1071_0024864	0.0	0.0
1071_0024867	1.0	2.0
1071_0242011	1.0	1.0
1071_0242022	1.0	0.0
1071_0242042	0.0	0.0
1071_0243501	1.0	1.0
1071_0243582	1.0	1.0
1071_0243593	1.0	1.0
1071_0243621	2.0	1.0
1071_0243622	1.0	0.0
1071_0248310	1.0	1.0
1071_0248314	1.0	1.0
1071_0248317	0.0	0.0
1071_0248320	0.0	0.0
1071_0248322	1.0	2.0
1091_0000002	2.0	2.0
1091_0000004	1.0	1.0
1091_0000017	2.0	1.0
1091_0000019	1.0	1.0
1091_0000026	1.0	1.0
1091_0000028	1.0	1.0
1091_0000037	1.0	0.0
1091_0000049	1.0	2.0
1091_0000051	1.0	2.0
1091_0000052	1.0	1.0
1091_0000054	0.0	1.0
1091_0000060	2.0	2.0
1091_0000063	2.0	1.0
1091_0000064	2.0	2.0
1091_0000066	1.0	0.0
1091_0000068	1.0	1.0
1091_0000069	2.0	1.0
1091_0000073	3.0	2.0
1091_0000076	2.0	2.0
1091_0000079	1.0	2.0
1091_0000087	2.0	2.0
1091_0000102	2.0	1.0
1091_0000114	2.0	2.0
1091_0000116	2.0	2.0
1091_0000123	2.0	1.0
1091_0000140	2.0	1.0
1091_0000163	2.0	1.0
1091_0000166	2.0	1.0
1091_0000167	1.0	2.0
1091_0000198	2.0	2.0
1091_0000201	2.0	1.0
1091_0000205	1.0	2.0
1091_0000206	1.0	1.0
1091_0000211	1.0	2.0
1091_0000213	1.0	2.0
1091_0000220	1.0	2.0
1091_0000223	2.0	2.0
1091_0000237	2.0	2.0
1091_0000248	2.0	2.0
1091_0000253	1.0	1.0
1091_0000257	1.0	2.0
1091_0000261	2.0	1.0
1091_0000263	3.0	2.0
1091_0000264	2.0	1.0
1091_0000271	1.0	2.0
0606	2.0	1.0
0612	1.0	1.0
0620	1.0	2.0
0622	1.0	1.0
0627	2.0	2.0
0642	1.0	2.0
0645	2.0	2.0
0716	2.0	2.0
0718	2.0	2.0
0720	1.0	2.0
0721	2.0	2.0
0725	1.0	2.0
0802	2.0	1.0
0803	2.0	2.0
0808	2.0	2.0
0825	2.0	2.0
0828	2.0	1.0
0910	1.0	1.0
0918	1.0	2.0
0922	1.0	2.0
0926	2.0	2.0
0929	1.0	2.0
1004	1.0	1.0
1005	1.0	1.0
1014	2.0	2.0
1016	2.0	2.0
1018	2.0	2.0
1023	2.0	2.0
1116	2.0	2.0
1117	1.0	2.0
9999	1.0	1.0
BER0609003	3.0	3.0
BER0611003	2.0	3.0
BER0611007	3.0	3.0
KYJ0611003A	1.0	1.0
KYJ0611004A	1.0	1.0
KYJ0611006B	0.0	1.0
LIB0611001B	1.0	1.0
LON0611002A	1.0	1.0
LON0611003	3.0	3.0
MOS0611014	2.0	2.0
PAR1011008A	1.0	1.0
PAR1011013	2.0	3.0
PAR1011014	3.0	3.0
PHA0111003A	1.0	1.0
PHA0111016	4.0	3.0
PHA0111018	2.0	3.0
PHA0112003A	2.0	1.0
PHA0112007B	1.0	1.0
PHA0112012B	1.0	2.0
PHA0209008	2.0	1.0
PHA0209024	3.0	2.0
PHA0209028	2.0	3.0
PHA0209034	2.0	3.0
PHA0411008B	1.0	1.0
PHA0411011A	1.0	1.0
PHA0411011B	1.0	1.0
PHA0411028	2.0	2.0
PHA0411037	2.0	3.0
PHA0411042	2.0	3.0
PHA0411051	3.0	3.0
PHA0411055	3.0	3.0
PHA0411058	3.0	3.0
PHA0411060	3.0	3.0
PHA0509022	4.0	3.0
PHA0509035	3.0	3.0
PHA0509041	3.0	3.0
PHA0510010A	2.0	1.0
PHA0510023	4.0	3.0
PHA0510031	3.0	2.0
PHA0510037	2.0	2.0
PHA0510050	2.0	3.0
PHA0610006B	1.0	1.0
PHA0610016	3.0	3.0
PHA0610017	3.0	3.0
PHA0610025	3.0	3.0
PHA0710013	4.0	3.0
PHA0710016	3.0	2.0
PHA0710017	3.0	3.0
PHA0710019	4.0	3.0
PHA0810009	3.0	3.0
PHA0811017	3.0	3.0
PHA1109003	2.0	2.0
PHA1109006	2.0	2.0
PHA1110001A	2.0	2.0
PHA1110013	2.0	3.0
PHA1110015	3.0	3.0
PHA1110017	2.0	2.0
PHA1111006B	1.0	1.0
VAR0909007	3.0	3.0
VAR0909009	3.0	3.0
1325_1001009	3.0	3.0
1325_1001021	3.0	3.0
1325_1001032	2.0	3.0
1325_1001039	3.0	3.0
1325_1001044	3.0	3.0
1325_1001052	2.0	2.0
1325_1001055	3.0	3.0
1325_1001057	2.0	3.0
1325_1001062	3.0	3.0
1325_1001092	2.0	3.0
1325_1001095	2.0	3.0
1325_1001101	3.0	3.0
1325_1001108	3.0	3.0
1325_1001124	3.0	2.0
1325_1001127	3.0	3.0
1325_1001132	3.0	3.0
1325_1001133	3.0	3.0
1325_1001134	3.0	3.0
1325_1001135	3.0	3.0
1325_1001144	3.0	3.0
1325_1001167	3.0	3.0
1325_9000088	2.0	3.0
1325_9000095	2.0	3.0
1325_9000185	3.0	3.0
1325_9000211	2.0	3.0
1325_9000239	3.0	3.0
1325_9000303	2.0	3.0
1325_9000315	2.0	2.0
1325_9000534	3.0	3.0
1325_9000677	3.0	3.0
1325_9000700	3.0	3.0
1325_9000750	3.0	3.0
1365_0100002	3.0	2.0
1365_0100007	2.0	2.0
1365_0100009	2.0	2.0
1365_0100016	2.0	2.0
1365_0100019	2.0	2.0
1365_0100064	3.0	2.0
1365_0100069	3.0	2.0
1365_0100070	2.0	2.0
1365_0100073	3.0	2.0
1365_0100074	2.0	3.0
1365_0100095	2.0	2.0
1365_0100097	2.0	2.0
1365_0100117	3.0	2.0
1365_0100119	3.0	3.0
1365_0100123	2.0	3.0
1365_0100146	3.0	1.0
1365_0100162	3.0	3.0
1365_0100164	3.0	2.0
1365_0100169	2.0	2.0
1365_0100179	2.0	2.0
1365_0100181	2.0	2.0
1365_0100185	2.0	2.0
1365_0100186	2.0	2.0
1365_0100191	2.0	3.0
1365_0100192	3.0	3.0
1365_0100201	2.0	3.0
1365_0100205	2.0	2.0
1365_0100222	2.0	3.0
1365_0100253	2.0	2.0
1365_0100256	2.0	2.0
1365_0100258	2.0	2.0
1365_0100259	2.0	3.0
1365_0100262	3.0	3.0
1365_0100279	2.0	2.0
1365_0100286	2.0	2.0
1365_0100287	2.0	2.0
1365_0100288	2.0	2.0
1365_0100289	2.0	2.0
1365_0100448	2.0	2.0
1365_0100451	3.0	3.0
1365_0100457	3.0	3.0
1365_0100461	3.0	2.0
1365_0100469	2.0	2.0
1365_0100470	2.0	3.0
1385_0000013	1.0	0.0
1385_0000022	1.0	2.0
1385_0000039	1.0	1.0
1385_0000040	1.0	0.0
1385_0000041	1.0	1.0
1385_0000043	1.0	1.0
1385_0000052	1.0	1.0
1385_0000097	2.0	1.0
1385_0000114	2.0	1.0
1385_0000120	0.0	1.0
1385_0000123	1.0	1.0
1385_0000126	1.0	1.0
1385_0000127	2.0	1.0
1385_0000128	1.0	1.0
1385_0001105	1.0	1.0
1385_0001107	2.0	1.0
1385_0001109	2.0	1.0
1385_0001113	1.0	1.0
1385_0001119	2.0	1.0
1385_0001124	2.0	0.0
1385_0001127	2.0	1.0
1385_0001129	1.0	1.0
1385_0001131	1.0	1.0
1385_0001150	1.0	1.0
1385_0001152	2.0	1.0
1385_0001155	1.0	1.0
1385_0001158	1.0	1.0
1385_0001167	1.0	1.0
1385_0001188	0.0	1.0
1385_0001197	1.0	1.0
1385_0001501	1.0	0.0
1385_0001526	0.0	0.0
1385_0001716	1.0	1.0
1385_0001719	1.0	2.0
1385_0001734	1.0	1.0
1385_0001738	0.0	1.0
1385_0001756	2.0	1.0
1385_0001759	0.0	1.0
1385_0001768	2.0	1.0
1385_0001788	1.0	1.0
1385_0001790	1.0	1.0
1385_0001791	0.0	1.0
1385_0001792	1.0	1.0
1385_0001793	0.0	1.0
1385_0001794	1.0	1.0
1385_0001798	1.0	1.0
1385_0001799	1.0	2.0
1395_0000341	2.0	1.0
1395_0000359	2.0	2.0
1395_0000368	1.0	0.0
1395_0000369	2.0	2.0
1395_0000383	2.0	1.0
1395_0000391	3.0	2.0
1395_0000403	2.0	2.0
1395_0000409	2.0	1.0
1395_0000413	2.0	1.0
1395_0000443	2.0	1.0
1395_0000449	2.0	1.0
1395_0000455	2.0	2.0
1395_0000462	2.0	2.0
1395_0000518	2.0	2.0
1395_0000529	2.0	1.0
1395_0000537	2.0	2.0
1395_0000555	1.0	1.0
1395_0000556	1.0	2.0
1395_0000572	1.0	1.0
1395_0000582	0.0	1.0
1395_0000595	0.0	1.0
1395_0000598	0.0	1.0
1395_0000604	0.0	1.0
1395_0000606	0.0	1.0
1395_0000611	0.0	1.0
1395_0000635	0.0	1.0
1395_0001016	1.0	1.0
1395_0001022	1.0	1.0
1395_0001023	1.0	1.0
1395_0001067	0.0	1.0
1395_0001068	0.0	1.0
1395_0001084	1.0	1.0
1395_0001114	1.0	1.0
1395_0001117	1.0	1.0
1395_0001118	0.0	1.0
1395_0001119	2.0	2.0
1395_0001146	0.0	0.0
1395_0001164	2.0	2.0
5 Fold, Dimension = Vocabularycontrol

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 1.13
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        31
         1.0       0.52      0.57      0.54       119
         2.0       0.53      0.72      0.61       165
         3.0       0.65      0.52      0.58       115
         4.0       0.40      0.10      0.16        20
         5.0       0.00      0.00      0.00         2

    accuracy                           0.55       452
   macro avg       0.35      0.32      0.31       452
weighted avg       0.51      0.55      0.52       452

[[  0  30   1   0   0   0]
 [  0  68  51   0   0   0]
 [  0  31 118  15   1   0]
 [  0   3  50  60   2   0]
 [  0   0   2  16   2   0]
 [  0   0   0   2   0   0]]
0.5191247710936193
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.94
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.99
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        31
         1.0       0.50      0.87      0.63       119
         2.0       0.60      0.36      0.45       165
         3.0       0.57      0.58      0.58       115
         4.0       0.32      0.45      0.38        20
         5.0       0.00      0.00      0.00         2

    accuracy                           0.53       452
   macro avg       0.33      0.38      0.34       452
weighted avg       0.51      0.53      0.49       452

[[  0  31   0   0   0   0]
 [  0 104  15   0   0   0]
 [  0  66  59  37   3   0]
 [  0   8  24  67  16   0]
 [  0   0   0  11   9   0]
 [  0   0   0   2   0   0]]
0.49428373158893885
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.79
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.01
              precision    recall  f1-score   support

         0.0       0.73      0.35      0.48        31
         1.0       0.60      0.46      0.52       119
         2.0       0.54      0.69      0.60       165
         3.0       0.60      0.69      0.64       115
         4.0       0.00      0.00      0.00        20
         5.0       0.00      0.00      0.00         2

    accuracy                           0.57       452
   macro avg       0.41      0.37      0.37       452
weighted avg       0.56      0.57      0.55       452

[[ 11  17   3   0   0   0]
 [  2  55  62   0   0   0]
 [  2  18 114  31   0   0]
 [  0   2  34  79   0   0]
 [  0   0   0  20   0   0]
 [  0   0   0   2   0   0]]
0.5529879739130789
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.66
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 1.00
              precision    recall  f1-score   support

         0.0       0.85      0.35      0.50        31
         1.0       0.57      0.67      0.62       119
         2.0       0.58      0.56      0.57       165
         3.0       0.60      0.69      0.64       115
         4.0       0.56      0.25      0.34        20
         5.0       0.00      0.00      0.00         2

    accuracy                           0.59       452
   macro avg       0.53      0.42      0.45       452
weighted avg       0.60      0.59      0.58       452

[[11 20  0  0  0  0]
 [ 1 80 38  0  0  0]
 [ 1 36 92 35  1  0]
 [ 0  4 29 79  3  0]
 [ 0  0  0 15  5  0]
 [ 0  0  0  2  0  0]]
0.5829105255735428
452 452 452
Filename	True Label	Prediction
1023_0001418	4.0	3.0
1023_0001575	3.0	3.0
1023_0101688	4.0	3.0
1023_0101690	2.0	3.0
1023_0101752	3.0	3.0
1023_0101896	3.0	2.0
1023_0101901	3.0	3.0
1023_0103826	3.0	3.0
1023_0103828	1.0	2.0
1023_0103831	3.0	3.0
1023_0103833	5.0	3.0
1023_0103837	3.0	3.0
1023_0103841	5.0	3.0
1023_0103883	3.0	3.0
1023_0104203	2.0	3.0
1023_0107042	4.0	3.0
1023_0107725	3.0	3.0
1023_0107726	3.0	3.0
1023_0107787	2.0	2.0
1023_0108304	3.0	3.0
1023_0108305	3.0	3.0
1023_0108520	3.0	3.0
1023_0109033	4.0	3.0
1023_0109151	3.0	3.0
1023_0109249	2.0	3.0
1023_0109401	3.0	3.0
1023_0109422	4.0	3.0
1023_0109495	3.0	3.0
1023_0109515	4.0	3.0
1023_0109519	2.0	2.0
1023_0109522	3.0	3.0
1023_0109527	4.0	3.0
1023_0109606	3.0	3.0
1023_0109609	3.0	2.0
1023_0109649	3.0	3.0
1023_0109716	3.0	3.0
1023_0109878	2.0	3.0
1023_0109890	3.0	3.0
1023_0109914	2.0	2.0
1031_0001703	4.0	3.0
1031_0002002	2.0	3.0
1031_0002084	4.0	3.0
1031_0002131	3.0	3.0
1031_0003063	4.0	3.0
1031_0003088	4.0	4.0
1031_0003090	4.0	4.0
1031_0003091	2.0	3.0
1031_0003092	3.0	3.0
1031_0003095	3.0	3.0
1031_0003098	4.0	4.0
1031_0003121	3.0	3.0
1031_0003130	4.0	4.0
1031_0003133	4.0	3.0
1031_0003135	3.0	3.0
1031_0003141	3.0	4.0
1031_0003162	3.0	3.0
1031_0003167	3.0	3.0
1031_0003169	3.0	4.0
1031_0003173	3.0	4.0
1031_0003179	3.0	3.0
1031_0003187	4.0	3.0
1031_0003191	3.0	3.0
1031_0003233	3.0	3.0
1031_0003240	2.0	4.0
1031_0003261	3.0	3.0
1031_0003262	3.0	3.0
1031_0003273	3.0	3.0
1031_0003352	2.0	3.0
1031_0003357	3.0	3.0
1031_0003384	2.0	3.0
1031_0003393	3.0	3.0
1031_0003410	3.0	3.0
1031_0003414	3.0	3.0
1031_0003415	4.0	4.0
1061_0120275	2.0	2.0
1061_0120281	1.0	1.0
1061_0120283	1.0	1.0
1061_0120285	2.0	2.0
1061_0120288	2.0	2.0
1061_0120296	2.0	2.0
1061_0120300	3.0	1.0
1061_0120301	2.0	1.0
1061_0120302	1.0	2.0
1061_0120303	1.0	1.0
1061_0120308	2.0	2.0
1061_0120315	2.0	1.0
1061_0120316	2.0	2.0
1061_0120318	1.0	2.0
1061_0120324	2.0	2.0
1061_0120327	2.0	2.0
1061_0120330	2.0	2.0
1061_0120335	3.0	3.0
1061_0120343	2.0	2.0
1061_0120348	1.0	1.0
1061_0120352	1.0	1.0
1061_0120357	3.0	3.0
1061_0120368	2.0	2.0
1061_0120388	2.0	2.0
1061_0120410	2.0	1.0
1061_0120415	2.0	1.0
1061_0120421	2.0	2.0
1061_0120423	2.0	3.0
1061_0120427	2.0	2.0
1061_0120453	2.0	2.0
1061_0120457	3.0	2.0
1061_0120478	2.0	2.0
1061_0120486	2.0	2.0
1061_0120487	2.0	2.0
1061_0120488	3.0	2.0
1061_0120500	2.0	2.0
1061_0120853	2.0	2.0
1061_0120875	3.0	2.0
1061_0120880	3.0	2.0
1061_0120883	2.0	2.0
1061_0120884	2.0	2.0
1061_0120888	1.0	2.0
1061_0120889	1.0	1.0
1061_0120890	1.0	1.0
1061_1029113	2.0	2.0
1061_1029114	1.0	2.0
1061_1029117	2.0	2.0
1061_1202910	3.0	2.0
1061_1202912	2.0	2.0
1071_0024682	2.0	1.0
1071_0024685	2.0	2.0
1071_0024691	1.0	2.0
1071_0024693	1.0	2.0
1071_0024694	1.0	2.0
1071_0024703	1.0	1.0
1071_0024709	2.0	2.0
1071_0024714	2.0	1.0
1071_0024757	2.0	2.0
1071_0024767	2.0	2.0
1071_0024770	1.0	1.0
1071_0024774	0.0	0.0
1071_0024775	0.0	1.0
1071_0024776	0.0	0.0
1071_0024801	1.0	1.0
1071_0024806	1.0	1.0
1071_0024807	1.0	1.0
1071_0024808	1.0	1.0
1071_0024809	0.0	1.0
1071_0024812	0.0	1.0
1071_0024813	0.0	1.0
1071_0024818	2.0	1.0
1071_0024819	1.0	1.0
1071_0024821	0.0	0.0
1071_0024823	1.0	1.0
1071_0024825	1.0	1.0
1071_0024836	2.0	1.0
1071_0024841	0.0	1.0
1071_0024847	1.0	2.0
1071_0024849	0.0	0.0
1071_0024852	0.0	0.0
1071_0024854	0.0	1.0
1071_0024866	2.0	1.0
1071_0024877	1.0	1.0
1071_0241831	1.0	1.0
1071_0242073	1.0	1.0
1071_0242092	0.0	0.0
1071_0243591	1.0	1.0
1071_0248302	1.0	1.0
1071_0248308	1.0	1.0
1071_0248321	1.0	1.0
1071_0248324	0.0	0.0
1071_0248328	0.0	0.0
1071_0248329	1.0	1.0
1071_0248331	1.0	1.0
1071_0248332	2.0	2.0
1071_0248341	1.0	1.0
1071_0248343	1.0	1.0
1071_0248347	1.0	1.0
1071_0248349	0.0	1.0
1091_0000003	2.0	2.0
1091_0000006	0.0	1.0
1091_0000007	3.0	2.0
1091_0000009	0.0	1.0
1091_0000023	2.0	1.0
1091_0000024	3.0	1.0
1091_0000042	0.0	1.0
1091_0000057	2.0	1.0
1091_0000058	2.0	2.0
1091_0000061	2.0	0.0
1091_0000070	2.0	2.0
1091_0000077	2.0	1.0
1091_0000126	2.0	2.0
1091_0000144	1.0	1.0
1091_0000148	1.0	1.0
1091_0000158	2.0	2.0
1091_0000159	2.0	1.0
1091_0000162	1.0	2.0
1091_0000170	3.0	1.0
1091_0000173	2.0	1.0
1091_0000190	1.0	2.0
1091_0000196	2.0	1.0
1091_0000208	1.0	1.0
1091_0000212	1.0	2.0
1091_0000215	1.0	2.0
1091_0000218	2.0	1.0
1091_0000227	1.0	1.0
1091_0000233	2.0	2.0
1091_0000242	1.0	2.0
1091_0000246	2.0	2.0
1091_0000255	0.0	1.0
1091_0000256	1.0	2.0
1091_0000267	1.0	1.0
1091_0000268	2.0	1.0
1091_0000269	1.0	2.0
0608	1.0	1.0
0609	2.0	2.0
0615	1.0	1.0
0623	1.0	2.0
0624	2.0	2.0
0629	2.0	1.0
0632	1.0	1.0
0634	3.0	2.0
0636	2.0	2.0
0637	2.0	2.0
0641	1.0	1.0
0724	3.0	2.0
0804	1.0	2.0
0806	2.0	2.0
0816	3.0	1.0
0817	1.0	1.0
0818	1.0	1.0
0820	2.0	1.0
0823	1.0	2.0
0903	1.0	2.0
0912	1.0	2.0
0913	2.0	2.0
1008	1.0	2.0
1017	2.0	2.0
BER0611006	3.0	3.0
KYJ0611009B	0.0	1.0
LIB0611001A	1.0	1.0
MOS0509001	2.0	2.0
MOS0611013	2.0	3.0
PAR1011017	3.0	3.0
PAR1011018	3.0	2.0
PHA0111001A	1.0	1.0
PHA0111003B	1.0	1.0
PHA0111005B	1.0	1.0
PHA0111011	3.0	3.0
PHA0111014	1.0	2.0
PHA0112006B	3.0	2.0
PHA0209001	2.0	2.0
PHA0411010A	1.0	1.0
PHA0411012A	1.0	1.0
PHA0411012B	1.0	1.0
PHA0411032	2.0	3.0
PHA0411038	3.0	3.0
PHA0411043	3.0	2.0
PHA0411044	3.0	3.0
PHA0411062	2.0	3.0
PHA0509013	1.0	1.0
PHA0509020	3.0	3.0
PHA0509024	3.0	2.0
PHA0509027	2.0	2.0
PHA0509030	3.0	3.0
PHA0509039	2.0	3.0
PHA0510003A	2.0	2.0
PHA0510004A	1.0	2.0
PHA0510010B	1.0	1.0
PHA0610006A	2.0	1.0
PHA0610007B	1.0	1.0
PHA0610018	3.0	3.0
PHA0610026	3.0	3.0
PHA0710021	4.0	3.0
PHA0810001	2.0	3.0
PHA0810004	3.0	2.0
PHA0810006	2.0	2.0
PHA0810012	2.0	2.0
PHA0811010	2.0	2.0
PHA0811014	2.0	2.0
PHA1109024	4.0	3.0
PHA1109027	4.0	3.0
PHA1110001B	1.0	1.0
PHA1110003A	1.0	2.0
PHA1110019	2.0	2.0
PHA1110021	3.0	3.0
PHA1111001A	2.0	1.0
PHA1111002A	1.0	1.0
PHA1111004B	1.0	1.0
TI071122B	2.0	1.0
VAR0909005	2.0	2.0
VAR0909008	3.0	2.0
VAR0910006	2.0	3.0
VAR0910009	3.0	3.0
VAR0910010	3.0	2.0
1325_1001008	3.0	3.0
1325_1001011	3.0	3.0
1325_1001015	2.0	3.0
1325_1001017	3.0	2.0
1325_1001019	3.0	3.0
1325_1001024	2.0	3.0
1325_1001027	3.0	3.0
1325_1001037	3.0	2.0
1325_1001041	3.0	3.0
1325_1001045	3.0	2.0
1325_1001046	2.0	3.0
1325_1001048	2.0	3.0
1325_1001053	2.0	2.0
1325_1001059	2.0	2.0
1325_1001075	2.0	3.0
1325_1001077	3.0	2.0
1325_1001078	3.0	3.0
1325_1001083	2.0	2.0
1325_1001089	2.0	2.0
1325_1001091	2.0	3.0
1325_1001099	3.0	3.0
1325_1001126	2.0	2.0
1325_1001128	3.0	3.0
1325_1001130	3.0	2.0
1325_1001143	3.0	3.0
1325_1001160	3.0	3.0
1325_1001161	2.0	3.0
1325_1001163	2.0	3.0
1325_9000087	2.0	2.0
1325_9000099	3.0	3.0
1325_9000102	3.0	2.0
1325_9000136	3.0	3.0
1325_9000137	3.0	3.0
1325_9000144	3.0	3.0
1325_9000186	3.0	3.0
1325_9000214	3.0	3.0
1325_9000241	3.0	3.0
1325_9000316	3.0	3.0
1325_9000318	3.0	3.0
1325_9000504	3.0	3.0
1325_9000684	3.0	3.0
1365_0100003	2.0	2.0
1365_0100006	2.0	2.0
1365_0100010	2.0	2.0
1365_0100022	2.0	2.0
1365_0100031	2.0	2.0
1365_0100051	2.0	2.0
1365_0100056	2.0	2.0
1365_0100067	2.0	2.0
1365_0100079	2.0	2.0
1365_0100093	2.0	2.0
1365_0100099	2.0	3.0
1365_0100106	2.0	3.0
1365_0100120	3.0	3.0
1365_0100125	3.0	3.0
1365_0100133	2.0	2.0
1365_0100139	2.0	2.0
1365_0100170	2.0	3.0
1365_0100172	2.0	2.0
1365_0100175	2.0	2.0
1365_0100176	2.0	2.0
1365_0100177	3.0	2.0
1365_0100187	3.0	2.0
1365_0100194	3.0	3.0
1365_0100195	2.0	2.0
1365_0100203	2.0	2.0
1365_0100212	3.0	3.0
1365_0100218	3.0	2.0
1365_0100221	2.0	2.0
1365_0100223	2.0	3.0
1365_0100227	3.0	3.0
1365_0100230	2.0	3.0
1365_0100268	2.0	3.0
1365_0100274	3.0	3.0
1365_0100275	3.0	2.0
1365_0100280	1.0	2.0
1365_0100281	2.0	2.0
1365_0100455	3.0	3.0
1365_0100456	3.0	3.0
1365_0100473	2.0	3.0
1365_0100474	2.0	3.0
1365_0100475	2.0	3.0
1365_0100476	2.0	3.0
1365_0100481	2.0	3.0
1365_0100482	2.0	3.0
1385_0000048	1.0	1.0
1385_0000051	2.0	2.0
1385_0000053	2.0	1.0
1385_0000057	1.0	1.0
1385_0000058	1.0	1.0
1385_0000059	2.0	2.0
1385_0000099	1.0	1.0
1385_0000100	2.0	1.0
1385_0000125	2.0	1.0
1385_0000130	2.0	1.0
1385_0001112	2.0	2.0
1385_0001125	2.0	1.0
1385_0001153	2.0	1.0
1385_0001165	1.0	1.0
1385_0001166	1.0	1.0
1385_0001171	0.0	0.0
1385_0001178	0.0	1.0
1385_0001189	0.0	1.0
1385_0001195	2.0	2.0
1385_0001196	1.0	1.0
1385_0001503	1.0	1.0
1385_0001523	1.0	1.0
1385_0001524	1.0	1.0
1385_0001525	1.0	2.0
1385_0001528	1.0	2.0
1385_0001712	1.0	1.0
1385_0001715	0.0	1.0
1385_0001724	1.0	2.0
1385_0001725	1.0	1.0
1385_0001726	1.0	2.0
1385_0001732	0.0	1.0
1385_0001736	1.0	1.0
1385_0001740	1.0	1.0
1385_0001741	0.0	1.0
1385_0001746	1.0	1.0
1385_0001785	1.0	1.0
1395_0000357	3.0	2.0
1395_0000361	2.0	2.0
1395_0000389	1.0	0.0
1395_0000399	2.0	1.0
1395_0000451	1.0	2.0
1395_0000460	1.0	1.0
1395_0000465	1.0	1.0
1395_0000470	2.0	1.0
1395_0000514	3.0	2.0
1395_0000515	2.0	2.0
1395_0000516	1.0	1.0
1395_0000547	2.0	1.0
1395_0000549	2.0	2.0
1395_0000551	2.0	2.0
1395_0000552	2.0	2.0
1395_0000553	2.0	1.0
1395_0000564	1.0	1.0
1395_0000565	2.0	1.0
1395_0000587	0.0	0.0
1395_0000591	0.0	0.0
1395_0000596	2.0	1.0
1395_0000608	0.0	1.0
1395_0000626	1.0	1.0
1395_0000627	1.0	1.0
1395_0000646	1.0	1.0
1395_0001010	1.0	1.0
1395_0001017	1.0	2.0
1395_0001028	1.0	2.0
1395_0001033	1.0	2.0
1395_0001045	2.0	1.0
1395_0001069	2.0	2.0
1395_0001073	2.0	1.0
1395_0001090	1.0	2.0
1395_0001101	1.0	1.0
1395_0001103	1.0	1.0
1395_0001108	0.0	1.0
1395_0001115	1.0	2.0
1395_0001122	0.0	1.0
1395_0001158	1.0	1.0
1395_0001160	1.0	2.0
1395_0001161	1.0	2.0
1395_0001167	1.0	1.0
Averaged weighted F1-scores 0.563492841937568
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
144.90552995391704 65.35717405024758
LANGUAGE: IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
1 Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 1.05
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.82
              precision    recall  f1-score   support

         0.0       1.00      0.08      0.15        38
         1.0       0.63      0.63      0.63       133
         2.0       0.66      0.85      0.74       165
         3.0       0.72      0.77      0.75        97
         4.0       0.00      0.00      0.00        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.67       452
   macro avg       0.50      0.39      0.38       452
weighted avg       0.67      0.67      0.63       452

[[  3  33   2   0   0   0]
 [  0  84  48   1   0   0]
 [  0  16 140   9   0   0]
 [  0   0  22  75   0   0]
 [  0   0   0  18   0   0]
 [  0   0   0   1   0   0]]
0.6294147221446723
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.77
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.87
              precision    recall  f1-score   support

         0.0       0.57      0.21      0.31        38
         1.0       0.56      0.82      0.66       133
         2.0       0.72      0.57      0.64       165
         3.0       0.68      0.78      0.73        97
         4.0       0.00      0.00      0.00        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.63       452
   macro avg       0.42      0.40      0.39       452
weighted avg       0.62      0.63      0.61       452

[[  8  30   0   0   0   0]
 [  6 109  17   1   0   0]
 [  0  55  94  16   0   0]
 [  0   1  20  76   0   0]
 [  0   0   0  18   0   0]
 [  0   0   0   1   0   0]]
0.6093615066337924
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.67
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.82
              precision    recall  f1-score   support

         0.0       0.60      0.24      0.34        38
         1.0       0.64      0.69      0.67       133
         2.0       0.71      0.74      0.72       165
         3.0       0.67      0.85      0.75        97
         4.0       0.00      0.00      0.00        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.67       452
   macro avg       0.44      0.42      0.41       452
weighted avg       0.64      0.67      0.65       452

[[  9  28   1   0   0   0]
 [  6  92  34   1   0   0]
 [  0  23 122  20   0   0]
 [  0   0  15  82   0   0]
 [  0   0   0  18   0   0]
 [  0   0   0   1   0   0]]
0.6497289199587117
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.54
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.89
              precision    recall  f1-score   support

         0.0       0.63      0.32      0.42        38
         1.0       0.65      0.65      0.65       133
         2.0       0.69      0.70      0.69       165
         3.0       0.65      0.84      0.73        97
         4.0       0.43      0.17      0.24        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.66       452
   macro avg       0.51      0.44      0.46       452
weighted avg       0.65      0.66      0.65       452

[[ 12  24   2   0   0   0]
 [  7  86  39   1   0   0]
 [  0  23 116  26   0   0]
 [  0   0  12  81   4   0]
 [  0   0   0  15   3   0]
 [  0   0   0   1   0   0]]
0.6460948027462473
452 452 452
Filename	True Label	Prediction
1023_0101694	3.0	3.0
1023_0101751	3.0	3.0
1023_0101752	3.0	3.0
1023_0101846	4.0	3.0
1023_0101851	2.0	3.0
1023_0101852	2.0	3.0
1023_0101853	2.0	3.0
1023_0101855	2.0	2.0
1023_0101893	3.0	3.0
1023_0101904	2.0	2.0
1023_0102117	2.0	3.0
1023_0103823	3.0	3.0
1023_0103824	3.0	3.0
1023_0103825	2.0	3.0
1023_0103827	3.0	3.0
1023_0103828	1.0	1.0
1023_0103841	3.0	3.0
1023_0103843	2.0	2.0
1023_0103955	4.0	3.0
1023_0107074	3.0	3.0
1023_0107672	3.0	3.0
1023_0108518	3.0	3.0
1023_0108648	3.0	3.0
1023_0108649	3.0	3.0
1023_0108885	3.0	3.0
1023_0108886	3.0	3.0
1023_0108888	3.0	3.0
1023_0108889	3.0	3.0
1023_0108931	2.0	3.0
1023_0108934	2.0	3.0
1023_0108958	3.0	3.0
1023_0108992	2.0	3.0
1023_0109096	3.0	3.0
1023_0109402	2.0	3.0
1023_0109519	2.0	2.0
1023_0109520	3.0	3.0
1023_0109524	3.0	3.0
1023_0109527	3.0	3.0
1023_0109590	3.0	3.0
1023_0109591	2.0	3.0
1023_0109721	2.0	3.0
1023_0109878	2.0	3.0
1023_0109914	2.0	2.0
1023_0109945	3.0	3.0
1031_0002002	2.0	3.0
1031_0002010	3.0	4.0
1031_0002032	3.0	4.0
1031_0002086	3.0	3.0
1031_0002087	4.0	3.0
1031_0002196	3.0	3.0
1031_0003043	4.0	3.0
1031_0003052	4.0	3.0
1031_0003074	4.0	3.0
1031_0003078	3.0	3.0
1031_0003091	3.0	3.0
1031_0003098	4.0	4.0
1031_0003121	3.0	3.0
1031_0003129	3.0	3.0
1031_0003130	4.0	3.0
1031_0003132	4.0	3.0
1031_0003174	4.0	3.0
1031_0003182	4.0	3.0
1031_0003203	3.0	3.0
1031_0003206	3.0	3.0
1031_0003214	3.0	4.0
1031_0003221	3.0	3.0
1031_0003238	3.0	3.0
1031_0003245	3.0	3.0
1031_0003330	3.0	4.0
1031_0003331	3.0	3.0
1031_0003337	4.0	3.0
1031_0003352	3.0	3.0
1031_0003356	3.0	3.0
1031_0003358	4.0	4.0
1031_0003368	3.0	3.0
1031_0003369	4.0	3.0
1031_0003387	4.0	3.0
1031_0003388	3.0	3.0
1031_0003392	4.0	3.0
1031_0003393	4.0	4.0
1031_0003409	5.0	3.0
1031_0003414	4.0	3.0
1061_0120283	0.0	1.0
1061_0120284	0.0	1.0
1061_0120287	1.0	2.0
1061_0120296	2.0	2.0
1061_0120299	2.0	2.0
1061_0120301	2.0	1.0
1061_0120308	3.0	2.0
1061_0120320	3.0	3.0
1061_0120321	2.0	2.0
1061_0120324	2.0	2.0
1061_0120325	2.0	2.0
1061_0120329	2.0	2.0
1061_0120335	2.0	3.0
1061_0120347	1.0	2.0
1061_0120354	1.0	2.0
1061_0120376	2.0	2.0
1061_0120384	2.0	2.0
1061_0120389	2.0	2.0
1061_0120390	2.0	2.0
1061_0120413	2.0	1.0
1061_0120428	2.0	2.0
1061_0120429	3.0	2.0
1061_0120440	1.0	1.0
1061_0120441	2.0	2.0
1061_0120459	2.0	2.0
1061_0120481	3.0	2.0
1061_0120486	1.0	2.0
1061_0120492	2.0	2.0
1061_0120875	3.0	3.0
1061_0120877	3.0	2.0
1061_0120886	2.0	2.0
1061_1029117	1.0	2.0
1061_1202914	1.0	2.0
1071_0024680	2.0	2.0
1071_0024683	1.0	1.0
1071_0024772	0.0	0.0
1071_0024774	0.0	0.0
1071_0024803	0.0	1.0
1071_0024821	0.0	0.0
1071_0024825	0.0	1.0
1071_0024827	1.0	1.0
1071_0024852	0.0	0.0
1071_0024854	0.0	1.0
1071_0024861	0.0	1.0
1071_0024863	1.0	1.0
1071_0024867	2.0	1.0
1071_0024871	1.0	1.0
1071_0024873	1.0	1.0
1071_0241833	1.0	1.0
1071_0242093	0.0	0.0
1071_0248302	1.0	1.0
1071_0248307	2.0	1.0
1071_0248308	1.0	1.0
1071_0248311	1.0	1.0
1071_0248313	1.0	1.0
1071_0248316	1.0	0.0
1071_0248325	0.0	1.0
1071_0248329	1.0	1.0
1071_0248333	2.0	1.0
1071_0248335	1.0	1.0
1071_0248338	2.0	1.0
1071_0248339	1.0	1.0
1071_0248340	0.0	0.0
1071_0248341	1.0	1.0
1071_0248346	1.0	1.0
1091_0000002	2.0	2.0
1091_0000006	0.0	1.0
1091_0000014	0.0	1.0
1091_0000019	2.0	1.0
1091_0000025	1.0	1.0
1091_0000027	1.0	1.0
1091_0000043	1.0	2.0
1091_0000047	2.0	1.0
1091_0000049	1.0	2.0
1091_0000053	0.0	1.0
1091_0000054	0.0	1.0
1091_0000060	2.0	2.0
1091_0000066	2.0	1.0
1091_0000067	2.0	1.0
1091_0000127	1.0	2.0
1091_0000154	1.0	3.0
1091_0000160	2.0	3.0
1091_0000162	2.0	2.0
1091_0000164	1.0	1.0
1091_0000166	1.0	2.0
1091_0000167	1.0	2.0
1091_0000171	1.0	2.0
1091_0000173	2.0	2.0
1091_0000191	1.0	2.0
1091_0000207	2.0	2.0
1091_0000214	2.0	1.0
1091_0000218	2.0	2.0
1091_0000219	1.0	2.0
1091_0000223	2.0	2.0
1091_0000224	2.0	1.0
1091_0000232	2.0	2.0
1091_0000236	2.0	2.0
1091_0000237	2.0	2.0
1091_0000241	1.0	1.0
1091_0000243	0.0	2.0
1091_0000246	2.0	2.0
1091_0000250	2.0	2.0
1091_0000253	2.0	1.0
1091_0000257	2.0	2.0
1091_0000260	3.0	2.0
1091_0000269	1.0	2.0
1091_0000273	1.0	2.0
1091_0000274	1.0	2.0
1091_0000276	2.0	2.0
0602	2.0	2.0
0604	2.0	2.0
0611	2.0	2.0
0612	2.0	2.0
0615	2.0	2.0
0619	2.0	2.0
0621	2.0	2.0
0622	2.0	1.0
0634	2.0	2.0
0642	2.0	2.0
0643	2.0	2.0
0714	2.0	2.0
0723	1.0	2.0
0725	2.0	2.0
0804	2.0	2.0
0806	2.0	2.0
0808	2.0	2.0
0823	2.0	2.0
0825	2.0	2.0
0901	3.0	2.0
0919	2.0	2.0
0927	2.0	2.0
0930	2.0	2.0
1002	2.0	2.0
1005	2.0	2.0
1015	2.0	2.0
1018	2.0	2.0
1019	2.0	2.0
1020	2.0	2.0
1021	2.0	2.0
1116	2.0	2.0
1117	2.0	2.0
BER0611003	3.0	3.0
BER0611005	3.0	2.0
KYJ0611003A	1.0	1.0
KYJ0611005B	1.0	2.0
KYJ0611009B	1.0	2.0
LIB0611003A	2.0	2.0
LON0611002B	1.0	1.0
MOS0509001	2.0	2.0
MOS0611015	3.0	3.0
PAR1011009B	1.0	1.0
PAR1011018	3.0	3.0
PHA0111001B	1.0	1.0
PHA0111004B	1.0	2.0
PHA0111005B	2.0	2.0
PHA0112002A	1.0	1.0
PHA0112002B	2.0	2.0
PHA0112006A	3.0	2.0
PHA0112009A	2.0	1.0
PHA0112012A	1.0	2.0
PHA0112012B	1.0	2.0
PHA0209024	3.0	3.0
PHA0209038	3.0	3.0
PHA0411009B	2.0	1.0
PHA0411011A	1.0	1.0
PHA0411012A	2.0	1.0
PHA0411012B	1.0	1.0
PHA0411027	3.0	3.0
PHA0411035	3.0	3.0
PHA0411036	3.0	3.0
PHA0411037	2.0	3.0
PHA0411051	3.0	3.0
PHA0411060	3.0	3.0
PHA0509007	1.0	1.0
PHA0509017	3.0	3.0
PHA0509022	3.0	3.0
PHA0509024	3.0	3.0
PHA0509027	2.0	3.0
PHA0509037	3.0	3.0
PHA0509041	3.0	3.0
PHA0509044	3.0	3.0
PHA0510010A	1.0	1.0
PHA0510023	3.0	3.0
PHA0510032	3.0	3.0
PHA0510046	2.0	3.0
PHA0510048	3.0	3.0
PHA0510049	3.0	3.0
PHA0610025	3.0	3.0
PHA0810002	3.0	3.0
PHA0810012	3.0	3.0
PHA0810015	3.0	3.0
PHA0811012	3.0	3.0
PHA0811013	3.0	3.0
PHA1109002	3.0	3.0
PHA1109007	2.0	3.0
PHA1110015	3.0	3.0
PHA1110016	2.0	2.0
PHA1110017	3.0	3.0
PHA1111001B	1.0	1.0
PHA1111002A	1.0	1.0
PHA1111006A	1.0	1.0
PHA1111008A	2.0	1.0
ST071122B	1.0	1.0
VAR0909003	3.0	3.0
VAR0909007	3.0	3.0
VAR0909008	3.0	2.0
VAR0909009	3.0	3.0
VAR0909010	3.0	3.0
VAR0910004	3.0	3.0
VAR0910011	3.0	3.0
1325_1001017	2.0	2.0
1325_1001018	2.0	3.0
1325_1001035	3.0	3.0
1325_1001045	2.0	2.0
1325_1001046	2.0	2.0
1325_1001051	2.0	2.0
1325_1001078	2.0	2.0
1325_1001083	2.0	2.0
1325_1001088	2.0	2.0
1325_1001097	0.0	2.0
1325_1001107	2.0	3.0
1325_1001108	3.0	3.0
1325_1001110	2.0	3.0
1325_1001122	2.0	2.0
1325_1001124	2.0	2.0
1325_1001129	1.0	2.0
1325_1001130	2.0	2.0
1325_1001132	2.0	2.0
1325_1001158	2.0	2.0
1325_1001160	2.0	2.0
1325_1001162	2.0	2.0
1325_1001169	2.0	2.0
1325_9000059	2.0	2.0
1325_9000087	2.0	2.0
1325_9000099	3.0	2.0
1325_9000152	2.0	3.0
1325_9000187	2.0	3.0
1325_9000210	1.0	2.0
1325_9000241	3.0	3.0
1325_9000296	1.0	2.0
1325_9000303	2.0	2.0
1325_9000315	2.0	2.0
1325_9000317	3.0	3.0
1325_9000323	2.0	2.0
1325_9000504	2.0	3.0
1325_9000534	2.0	2.0
1325_9000612	2.0	2.0
1325_9000675	2.0	2.0
1325_9000700	2.0	2.0
1365_0100006	2.0	2.0
1365_0100019	1.0	1.0
1365_0100020	2.0	2.0
1365_0100021	2.0	2.0
1365_0100022	2.0	2.0
1365_0100057	2.0	2.0
1365_0100065	1.0	1.0
1365_0100067	1.0	2.0
1365_0100070	2.0	2.0
1365_0100079	2.0	2.0
1365_0100093	1.0	2.0
1365_0100099	1.0	2.0
1365_0100101	2.0	2.0
1365_0100104	2.0	2.0
1365_0100105	3.0	2.0
1365_0100117	2.0	2.0
1365_0100134	2.0	1.0
1365_0100137	1.0	2.0
1365_0100139	1.0	2.0
1365_0100165	2.0	2.0
1365_0100168	2.0	2.0
1365_0100174	1.0	1.0
1365_0100181	1.0	1.0
1365_0100185	1.0	1.0
1365_0100186	1.0	2.0
1365_0100188	2.0	2.0
1365_0100190	2.0	3.0
1365_0100191	2.0	2.0
1365_0100196	1.0	2.0
1365_0100199	2.0	2.0
1365_0100223	2.0	2.0
1365_0100228	1.0	2.0
1365_0100230	2.0	2.0
1365_0100255	1.0	2.0
1365_0100256	2.0	2.0
1365_0100260	2.0	2.0
1365_0100276	3.0	2.0
1365_0100280	1.0	1.0
1365_0100455	2.0	2.0
1365_0100469	2.0	2.0
1365_0100473	2.0	2.0
1365_0100480	1.0	2.0
1385_0000016	1.0	1.0
1385_0000017	1.0	0.0
1385_0000021	1.0	1.0
1385_0000053	1.0	1.0
1385_0000098	1.0	1.0
1385_0000099	1.0	1.0
1385_0000100	1.0	0.0
1385_0000103	1.0	0.0
1385_0000104	1.0	1.0
1385_0000122	1.0	1.0
1385_0000126	1.0	1.0
1385_0000130	1.0	1.0
1385_0001107	1.0	1.0
1385_0001118	1.0	1.0
1385_0001122	1.0	1.0
1385_0001123	1.0	0.0
1385_0001129	0.0	1.0
1385_0001133	1.0	1.0
1385_0001134	1.0	1.0
1385_0001148	1.0	1.0
1385_0001156	1.0	1.0
1385_0001159	1.0	1.0
1385_0001160	1.0	1.0
1385_0001165	1.0	1.0
1385_0001166	1.0	1.0
1385_0001172	0.0	0.0
1385_0001193	1.0	1.0
1385_0001195	1.0	1.0
1385_0001524	0.0	1.0
1385_0001527	1.0	1.0
1385_0001528	1.0	1.0
1385_0001712	1.0	1.0
1385_0001714	1.0	1.0
1385_0001718	0.0	0.0
1385_0001733	0.0	1.0
1385_0001736	1.0	2.0
1385_0001740	1.0	1.0
1385_0001744	0.0	1.0
1385_0001750	0.0	0.0
1385_0001790	0.0	1.0
1385_0001792	1.0	1.0
1385_0001798	1.0	1.0
1395_0000333	1.0	1.0
1395_0000353	1.0	1.0
1395_0000355	1.0	2.0
1395_0000357	2.0	1.0
1395_0000390	1.0	0.0
1395_0000399	1.0	1.0
1395_0000413	1.0	1.0
1395_0000451	1.0	1.0
1395_0000469	1.0	1.0
1395_0000504	1.0	1.0
1395_0000515	2.0	1.0
1395_0000516	1.0	0.0
1395_0000533	2.0	2.0
1395_0000547	1.0	1.0
1395_0000551	2.0	1.0
1395_0000572	1.0	1.0
1395_0000581	1.0	1.0
1395_0000582	0.0	0.0
1395_0000587	0.0	1.0
1395_0000593	0.0	1.0
1395_0000595	0.0	0.0
1395_0000608	0.0	1.0
1395_0000610	1.0	1.0
1395_0000628	0.0	1.0
1395_0000635	0.0	1.0
1395_0001010	1.0	1.0
1395_0001017	0.0	1.0
1395_0001022	1.0	1.0
1395_0001045	1.0	1.0
1395_0001069	2.0	1.0
1395_0001084	0.0	1.0
1395_0001090	1.0	1.0
1395_0001126	1.0	1.0
1395_0001131	0.0	0.0
1395_0001149	0.0	1.0
1395_0001164	1.0	2.0
1395_0001169	1.0	1.0
2 Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 1.01
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.82
              precision    recall  f1-score   support

         0.0       1.00      0.03      0.05        39
         1.0       0.62      0.76      0.68       132
         2.0       0.71      0.75      0.73       165
         3.0       0.67      0.80      0.73        97
         4.0       0.00      0.00      0.00        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.67       452
   macro avg       0.50      0.39      0.37       452
weighted avg       0.67      0.67      0.63       452

[[  1  38   0   0   0   0]
 [  0 100  32   0   0   0]
 [  0  22 124  19   0   0]
 [  0   1  18  78   0   0]
 [  0   0   0  18   0   0]
 [  0   0   0   1   0   0]]
0.6278818743393695
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.79
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.84
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        39
         1.0       0.62      0.70      0.65       132
         2.0       0.69      0.80      0.74       165
         3.0       0.69      0.72      0.71        97
         4.0       0.33      0.22      0.27        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.66       452
   macro avg       0.39      0.41      0.40       452
weighted avg       0.60      0.66      0.63       452

[[  0  38   1   0   0   0]
 [  0  92  40   0   0   0]
 [  0  17 132  16   0   0]
 [  0   2  17  70   8   0]
 [  0   0   0  14   4   0]
 [  0   0   0   1   0   0]]
0.6250536452191126
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.66
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.82
              precision    recall  f1-score   support

         0.0       0.86      0.15      0.26        39
         1.0       0.61      0.67      0.64       132
         2.0       0.68      0.81      0.74       165
         3.0       0.73      0.71      0.72        97
         4.0       0.44      0.22      0.30        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.67       452
   macro avg       0.55      0.43      0.44       452
weighted avg       0.67      0.67      0.65       452

[[  6  33   0   0   0   0]
 [  1  88  43   0   0   0]
 [  0  21 134  10   0   0]
 [  0   2  21  69   5   0]
 [  0   0   0  14   4   0]
 [  0   0   0   1   0   0]]
0.645095378035353
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.54
  Training epoch took: 65
Running Validation...
  Average evaluation loss: 0.85
              precision    recall  f1-score   support

         0.0       0.83      0.38      0.53        39
         1.0       0.66      0.67      0.67       132
         2.0       0.68      0.80      0.74       165
         3.0       0.72      0.71      0.72        97
         4.0       0.44      0.22      0.30        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.68       452
   macro avg       0.56      0.47      0.49       452
weighted avg       0.68      0.68      0.67       452

[[ 15  24   0   0   0   0]
 [  2  89  41   0   0   0]
 [  1  20 132  12   0   0]
 [  0   2  21  69   5   0]
 [  0   0   0  14   4   0]
 [  0   0   0   1   0   0]]
0.673792498738795
452 452 452
Filename	True Label	Prediction
1023_0001423	2.0	2.0
1023_0101688	3.0	3.0
1023_0101689	2.0	2.0
1023_0101691	4.0	3.0
1023_0101701	2.0	2.0
1023_0101753	3.0	3.0
1023_0101841	2.0	3.0
1023_0101845	2.0	2.0
1023_0101847	3.0	2.0
1023_0101849	2.0	2.0
1023_0101898	3.0	3.0
1023_0103822	2.0	2.0
1023_0103826	3.0	3.0
1023_0103834	3.0	3.0
1023_0103836	3.0	3.0
1023_0103839	3.0	3.0
1023_0103840	3.0	2.0
1023_0103880	3.0	2.0
1023_0103883	3.0	2.0
1023_0104207	2.0	3.0
1023_0107075	2.0	3.0
1023_0107244	3.0	3.0
1023_0108641	4.0	3.0
1023_0108887	2.0	2.0
1023_0108993	3.0	3.0
1023_0109249	3.0	3.0
1023_0109250	2.0	2.0
1023_0109392	3.0	3.0
1023_0109515	3.0	3.0
1023_0109516	3.0	3.0
1023_0109528	3.0	3.0
1023_0109606	2.0	2.0
1023_0109609	2.0	2.0
1023_0109649	3.0	3.0
1023_0109671	3.0	3.0
1023_0109946	2.0	3.0
1023_0111896	2.0	3.0
1031_0002006	5.0	3.0
1031_0002088	3.0	3.0
1031_0002195	3.0	3.0
1031_0002197	3.0	3.0
1031_0002200	3.0	3.0
1031_0003012	3.0	3.0
1031_0003029	3.0	3.0
1031_0003048	4.0	3.0
1031_0003054	3.0	4.0
1031_0003071	3.0	3.0
1031_0003072	3.0	3.0
1031_0003085	3.0	3.0
1031_0003088	4.0	3.0
1031_0003126	4.0	3.0
1031_0003127	4.0	3.0
1031_0003157	4.0	4.0
1031_0003160	3.0	3.0
1031_0003161	4.0	3.0
1031_0003166	2.0	3.0
1031_0003167	3.0	4.0
1031_0003181	4.0	4.0
1031_0003217	4.0	4.0
1031_0003219	3.0	3.0
1031_0003220	3.0	3.0
1031_0003226	3.0	3.0
1031_0003230	3.0	3.0
1031_0003231	3.0	3.0
1031_0003232	3.0	3.0
1031_0003233	3.0	3.0
1031_0003236	3.0	3.0
1031_0003237	3.0	4.0
1031_0003242	3.0	4.0
1031_0003273	3.0	3.0
1031_0003274	4.0	4.0
1031_0003313	4.0	3.0
1031_0003327	3.0	3.0
1031_0003355	4.0	3.0
1031_0003357	3.0	3.0
1031_0003359	3.0	4.0
1031_0003386	3.0	3.0
1031_0003389	3.0	3.0
1061_0120276	2.0	1.0
1061_0120279	2.0	1.0
1061_0120290	1.0	2.0
1061_0120291	1.0	1.0
1061_0120298	2.0	2.0
1061_0120307	2.0	2.0
1061_0120311	3.0	2.0
1061_0120312	1.0	1.0
1061_0120313	2.0	1.0
1061_0120315	2.0	1.0
1061_0120328	1.0	2.0
1061_0120332	2.0	2.0
1061_0120346	2.0	2.0
1061_0120351	2.0	2.0
1061_0120356	2.0	2.0
1061_0120366	3.0	2.0
1061_0120369	2.0	2.0
1061_0120373	2.0	2.0
1061_0120382	2.0	2.0
1061_0120388	2.0	2.0
1061_0120407	3.0	2.0
1061_0120408	2.0	2.0
1061_0120409	2.0	2.0
1061_0120414	3.0	2.0
1061_0120421	2.0	2.0
1061_0120425	2.0	2.0
1061_0120430	2.0	2.0
1061_0120431	3.0	2.0
1061_0120432	2.0	2.0
1061_0120478	2.0	2.0
1061_0120490	2.0	2.0
1061_0120494	2.0	2.0
1061_0120498	2.0	2.0
1061_0120500	2.0	2.0
1061_0120858	2.0	2.0
1061_0120874	1.0	2.0
1061_0120883	2.0	2.0
1061_0120885	2.0	2.0
1061_1029112	3.0	3.0
1061_1029116	1.0	2.0
1061_1029119	2.0	2.0
1061_1202913	2.0	2.0
1071_0024682	2.0	2.0
1071_0024693	1.0	2.0
1071_0024702	1.0	2.0
1071_0024703	1.0	1.0
1071_0024708	1.0	1.0
1071_0024713	1.0	1.0
1071_0024714	2.0	1.0
1071_0024757	2.0	2.0
1071_0024765	0.0	0.0
1071_0024775	0.0	1.0
1071_0024802	1.0	1.0
1071_0024804	0.0	1.0
1071_0024806	1.0	1.0
1071_0024815	0.0	1.0
1071_0024817	1.0	1.0
1071_0024833	1.0	1.0
1071_0024836	1.0	2.0
1071_0024848	1.0	1.0
1071_0024850	0.0	0.0
1071_0024851	1.0	1.0
1071_0024862	2.0	1.0
1071_0024865	1.0	1.0
1071_0024866	2.0	2.0
1071_0024874	1.0	1.0
1071_0024875	1.0	1.0
1071_0024876	1.0	1.0
1071_0024877	1.0	1.0
1071_0242021	1.0	1.0
1071_0242022	0.0	0.0
1071_0242023	0.0	1.0
1071_0242041	1.0	1.0
1071_0242043	0.0	1.0
1071_0242072	0.0	0.0
1071_0243582	1.0	1.0
1071_0243591	1.0	1.0
1071_0248301	1.0	1.0
1071_0248310	1.0	1.0
1071_0248315	0.0	0.0
1071_0248317	0.0	0.0
1071_0248319	0.0	0.0
1071_0248321	1.0	1.0
1071_0248334	1.0	2.0
1071_0248337	2.0	1.0
1071_0248347	1.0	1.0
1071_0248348	1.0	1.0
1091_0000011	2.0	2.0
1091_0000015	2.0	2.0
1091_0000016	1.0	1.0
1091_0000020	2.0	2.0
1091_0000022	2.0	2.0
1091_0000023	2.0	1.0
1091_0000030	0.0	1.0
1091_0000036	1.0	2.0
1091_0000038	1.0	2.0
1091_0000039	1.0	0.0
1091_0000048	1.0	1.0
1091_0000055	2.0	2.0
1091_0000056	1.0	2.0
1091_0000064	1.0	2.0
1091_0000068	1.0	1.0
1091_0000072	1.0	2.0
1091_0000073	2.0	1.0
1091_0000076	2.0	2.0
1091_0000086	1.0	1.0
1091_0000092	1.0	2.0
1091_0000125	3.0	2.0
1091_0000144	2.0	0.0
1091_0000146	0.0	1.0
1091_0000152	1.0	1.0
1091_0000163	2.0	1.0
1091_0000168	2.0	2.0
1091_0000170	3.0	1.0
1091_0000192	2.0	1.0
1091_0000197	1.0	2.0
1091_0000200	2.0	2.0
1091_0000204	2.0	2.0
1091_0000208	1.0	1.0
1091_0000226	1.0	2.0
1091_0000234	2.0	2.0
1091_0000238	2.0	2.0
1091_0000245	1.0	1.0
1091_0000259	2.0	2.0
1091_0000267	2.0	2.0
1091_0000268	2.0	2.0
0603	2.0	2.0
0610	2.0	2.0
0617	2.0	2.0
0625	1.0	2.0
0628	2.0	2.0
0636	2.0	2.0
0638	2.0	2.0
0640	2.0	2.0
0724	3.0	2.0
0801	1.0	2.0
0802	1.0	2.0
0803	1.0	2.0
0807	2.0	2.0
0810	2.0	2.0
0816	2.0	2.0
0826	2.0	2.0
0827	2.0	2.0
0828	2.0	2.0
0903	2.0	2.0
0904	1.0	2.0
0910	1.0	2.0
0914	2.0	2.0
0918	2.0	2.0
0920	2.0	2.0
0921	2.0	2.0
0922	1.0	2.0
0925	2.0	2.0
0929	1.0	2.0
1001	2.0	2.0
1014	2.0	2.0
1111	2.0	2.0
9999	1.0	2.0
LON0610002A	2.0	1.0
LON0611002A	1.0	1.0
PHA0111003B	1.0	1.0
PHA0111005A	2.0	1.0
PHA0111010	3.0	3.0
PHA0111014	2.0	3.0
PHA0111015	4.0	3.0
PHA0111016	3.0	3.0
PHA0111018	2.0	3.0
PHA0112006B	2.0	2.0
PHA0209001	2.0	1.0
PHA0209028	3.0	3.0
PHA0411008B	1.0	1.0
PHA0411038	3.0	3.0
PHA0411042	3.0	3.0
PHA0411043	3.0	3.0
PHA0411044	4.0	3.0
PHA0411054	3.0	3.0
PHA0411056	3.0	3.0
PHA0509015	3.0	2.0
PHA0509021	2.0	3.0
PHA0509028	3.0	3.0
PHA0509034	2.0	2.0
PHA0509039	3.0	3.0
PHA0509040	3.0	3.0
PHA0510010B	1.0	1.0
PHA0510029	3.0	3.0
PHA0510036	3.0	3.0
PHA0510038	3.0	3.0
PHA0510039	3.0	3.0
PHA0610006A	1.0	1.0
PHA0610007B	1.0	1.0
PHA0610018	3.0	3.0
PHA0610019A	1.0	1.0
PHA0610026	3.0	3.0
PHA0710010	3.0	3.0
PHA0710017	3.0	3.0
PHA0710019	3.0	3.0
PHA0710021	4.0	3.0
PHA0810001	3.0	3.0
PHA0810004	3.0	3.0
PHA0810011	3.0	3.0
PHA0811014	3.0	3.0
PHA0811017	4.0	3.0
PHA0811020	2.0	3.0
PHA1109026	3.0	3.0
PHA1110001A	2.0	1.0
PHA1110004A	1.0	1.0
PHA1110014	3.0	3.0
PHA1110022	4.0	3.0
VAR0910006	3.0	3.0
VAR0910007	3.0	3.0
1325_1001009	3.0	2.0
1325_1001024	2.0	2.0
1325_1001029	2.0	2.0
1325_1001032	2.0	2.0
1325_1001036	2.0	2.0
1325_1001043	2.0	2.0
1325_1001048	2.0	2.0
1325_1001053	1.0	2.0
1325_1001056	2.0	2.0
1325_1001058	2.0	2.0
1325_1001076	2.0	2.0
1325_1001082	2.0	2.0
1325_1001084	2.0	2.0
1325_1001093	2.0	2.0
1325_1001100	2.0	2.0
1325_1001109	2.0	2.0
1325_1001111	3.0	2.0
1325_1001119	2.0	2.0
1325_1001123	2.0	2.0
1325_1001134	2.0	2.0
1325_1001138	2.0	2.0
1325_1001142	2.0	2.0
1325_1001155	2.0	2.0
1325_1001161	2.0	2.0
1325_1001163	2.0	2.0
1325_1001166	2.0	2.0
1325_9000088	2.0	2.0
1325_9000089	2.0	2.0
1325_9000090	2.0	2.0
1325_9000136	2.0	3.0
1325_9000144	3.0	3.0
1325_9000185	3.0	2.0
1325_9000188	2.0	2.0
1325_9000209	2.0	2.0
1325_9000211	2.0	2.0
1325_9000237	2.0	3.0
1325_9000239	2.0	2.0
1325_9000533	2.0	2.0
1325_9000554	2.0	2.0
1325_9000601	2.0	2.0
1325_9000602	3.0	2.0
1325_9000686	2.0	2.0
1325_9000750	3.0	1.0
1365_0100008	2.0	2.0
1365_0100010	1.0	1.0
1365_0100011	2.0	1.0
1365_0100015	2.0	1.0
1365_0100016	2.0	2.0
1365_0100023	1.0	2.0
1365_0100028	1.0	2.0
1365_0100061	3.0	2.0
1365_0100066	1.0	2.0
1365_0100071	2.0	2.0
1365_0100092	2.0	2.0
1365_0100119	3.0	2.0
1365_0100162	2.0	2.0
1365_0100179	1.0	2.0
1365_0100194	2.0	2.0
1365_0100201	1.0	2.0
1365_0100204	1.0	2.0
1365_0100211	3.0	2.0
1365_0100212	3.0	2.0
1365_0100217	3.0	2.0
1365_0100218	2.0	2.0
1365_0100233	2.0	2.0
1365_0100251	2.0	2.0
1365_0100259	2.0	2.0
1365_0100261	2.0	2.0
1365_0100262	2.0	2.0
1365_0100274	2.0	2.0
1365_0100287	1.0	1.0
1365_0100290	1.0	2.0
1365_0100299	2.0	2.0
1365_0100448	1.0	2.0
1365_0100451	2.0	2.0
1365_0100458	2.0	2.0
1365_0100470	1.0	2.0
1365_0100471	1.0	2.0
1365_0100475	2.0	2.0
1365_0100476	2.0	2.0
1365_0100478	1.0	2.0
1365_0100479	2.0	2.0
1365_0100481	1.0	2.0
1365_0100482	2.0	2.0
1385_0000020	1.0	1.0
1385_0000022	1.0	1.0
1385_0000033	1.0	1.0
1385_0000034	1.0	1.0
1385_0000036	1.0	1.0
1385_0000042	1.0	1.0
1385_0000049	1.0	1.0
1385_0000058	1.0	1.0
1385_0000101	1.0	1.0
1385_0000120	0.0	0.0
1385_0000128	1.0	1.0
1385_0000129	1.0	1.0
1385_0001104	0.0	1.0
1385_0001105	1.0	1.0
1385_0001120	1.0	1.0
1385_0001131	1.0	1.0
1385_0001136	1.0	1.0
1385_0001149	1.0	1.0
1385_0001150	1.0	1.0
1385_0001154	1.0	1.0
1385_0001164	1.0	1.0
1385_0001170	0.0	0.0
1385_0001175	0.0	0.0
1385_0001189	0.0	1.0
1385_0001190	0.0	1.0
1385_0001194	1.0	1.0
1385_0001199	1.0	1.0
1385_0001501	0.0	1.0
1385_0001727	0.0	1.0
1385_0001728	0.0	1.0
1385_0001729	1.0	1.0
1385_0001730	1.0	1.0
1385_0001747	0.0	1.0
1385_0001748	2.0	1.0
1385_0001751	0.0	1.0
1385_0001752	1.0	1.0
1385_0001754	0.0	1.0
1385_0001760	1.0	1.0
1385_0001761	0.0	1.0
1385_0001766	1.0	1.0
1385_0001771	0.0	1.0
1385_0001791	1.0	1.0
1385_0001794	1.0	1.0
1385_0001795	0.0	0.0
1385_0001800	1.0	1.0
1395_0000337	0.0	0.0
1395_0000341	1.0	0.0
1395_0000380	1.0	1.0
1395_0000383	1.0	1.0
1395_0000388	2.0	1.0
1395_0000389	0.0	0.0
1395_0000404	1.0	1.0
1395_0000409	2.0	1.0
1395_0000414	1.0	1.0
1395_0000443	2.0	2.0
1395_0000454	1.0	1.0
1395_0000518	1.0	1.0
1395_0000528	1.0	1.0
1395_0000548	1.0	2.0
1395_0000554	1.0	1.0
1395_0000575	1.0	1.0
1395_0000583	1.0	1.0
1395_0000606	0.0	1.0
1395_0000626	1.0	1.0
1395_0000636	0.0	1.0
1395_0000642	0.0	1.0
1395_0001023	1.0	1.0
1395_0001024	1.0	1.0
1395_0001040	0.0	0.0
1395_0001068	0.0	1.0
1395_0001070	1.0	2.0
1395_0001071	1.0	1.0
1395_0001078	0.0	1.0
1395_0001104	0.0	0.0
1395_0001116	1.0	1.0
1395_0001119	1.0	2.0
1395_0001121	0.0	1.0
1395_0001124	1.0	1.0
1395_0001161	1.0	1.0
1395_0001171	1.0	1.0
3 Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 1.01
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.85
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        39
         1.0       0.58      0.75      0.65       132
         2.0       0.66      0.72      0.69       166
         3.0       0.67      0.69      0.68        96
         4.0       0.00      0.00      0.00        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.63       452
   macro avg       0.32      0.36      0.34       452
weighted avg       0.55      0.63      0.59       452

[[  0  38   1   0   0   0]
 [  0  99  33   0   0   0]
 [  0  32 120  14   0   0]
 [  0   3  27  66   0   0]
 [  0   0   0  18   0   0]
 [  0   0   0   1   0   0]]
0.587988893164636
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.75
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.92
              precision    recall  f1-score   support

         0.0       0.89      0.21      0.33        39
         1.0       0.61      0.48      0.54       132
         2.0       0.59      0.80      0.68       166
         3.0       0.70      0.64      0.67        96
         4.0       0.38      0.56      0.45        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.61       452
   macro avg       0.53      0.45      0.45       452
weighted avg       0.64      0.61      0.60       452

[[  8  26   5   0   0   0]
 [  1  64  67   0   0   0]
 [  0  13 133  18   2   0]
 [  0   2  20  61  13   0]
 [  0   0   0   8  10   0]
 [  0   0   0   0   1   0]]
0.5960263356501873
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.63
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.94
              precision    recall  f1-score   support

         0.0       0.56      0.38      0.45        39
         1.0       0.61      0.52      0.56       132
         2.0       0.64      0.71      0.68       166
         3.0       0.64      0.83      0.72        96
         4.0       0.50      0.11      0.18        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.63       452
   macro avg       0.49      0.43      0.43       452
weighted avg       0.62      0.63      0.61       452

[[ 15  22   2   0   0   0]
 [ 12  69  51   0   0   0]
 [  0  19 118  29   0   0]
 [  0   3  12  80   1   0]
 [  0   0   0  16   2   0]
 [  0   0   0   0   1   0]]
0.6130651147095948
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.50
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.92
              precision    recall  f1-score   support

         0.0       0.54      0.36      0.43        39
         1.0       0.60      0.61      0.60       132
         2.0       0.65      0.73      0.69       166
         3.0       0.70      0.67      0.68        96
         4.0       0.42      0.28      0.33        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.63       452
   macro avg       0.48      0.44      0.46       452
weighted avg       0.62      0.63      0.62       452

[[ 14  22   3   0   0   0]
 [ 12  81  39   0   0   0]
 [  0  30 121  15   0   0]
 [  0   3  23  64   6   0]
 [  0   0   0  13   5   0]
 [  0   0   0   0   1   0]]
0.6240658117340242
452 452 452
Filename	True Label	Prediction
1023_0001416	3.0	3.0
1023_0001420	3.0	3.0
1023_0001575	3.0	2.0
1023_0101693	4.0	3.0
1023_0101700	3.0	2.0
1023_0101843	2.0	2.0
1023_0101854	2.0	2.0
1023_0101896	3.0	2.0
1023_0101907	3.0	3.0
1023_0102118	2.0	2.0
1023_0103821	3.0	3.0
1023_0103829	2.0	3.0
1023_0103831	3.0	3.0
1023_0103833	4.0	3.0
1023_0104206	3.0	2.0
1023_0107042	3.0	2.0
1023_0107682	3.0	2.0
1023_0107725	2.0	2.0
1023_0107727	3.0	3.0
1023_0107729	3.0	3.0
1023_0107784	1.0	2.0
1023_0108307	3.0	3.0
1023_0108422	3.0	3.0
1023_0108751	3.0	3.0
1023_0108811	3.0	3.0
1023_0108812	3.0	3.0
1023_0108933	3.0	3.0
1023_0109027	3.0	2.0
1023_0109033	4.0	3.0
1023_0109151	4.0	3.0
1023_0109247	4.0	3.0
1023_0109248	2.0	3.0
1023_0109399	2.0	2.0
1023_0109496	3.0	3.0
1023_0109505	3.0	3.0
1023_0109518	2.0	2.0
1023_0109588	3.0	3.0
1023_0109651	3.0	3.0
1023_0109716	3.0	2.0
1023_0109890	4.0	3.0
1023_0109917	2.0	3.0
1023_0109951	2.0	3.0
1031_0001951	3.0	3.0
1031_0001997	4.0	3.0
1031_0002011	4.0	3.0
1031_0002040	5.0	4.0
1031_0002091	3.0	4.0
1031_0002092	4.0	4.0
1031_0002184	3.0	3.0
1031_0002187	3.0	4.0
1031_0002199	3.0	3.0
1031_0003092	2.0	3.0
1031_0003095	3.0	3.0
1031_0003128	3.0	4.0
1031_0003131	3.0	4.0
1031_0003135	3.0	4.0
1031_0003149	3.0	3.0
1031_0003154	3.0	3.0
1031_0003172	3.0	4.0
1031_0003173	3.0	3.0
1031_0003180	4.0	3.0
1031_0003183	4.0	3.0
1031_0003184	4.0	3.0
1031_0003185	3.0	2.0
1031_0003187	3.0	3.0
1031_0003191	3.0	3.0
1031_0003218	4.0	4.0
1031_0003239	4.0	4.0
1031_0003243	3.0	3.0
1031_0003244	4.0	4.0
1031_0003249	4.0	3.0
1031_0003310	3.0	3.0
1031_0003314	4.0	4.0
1031_0003315	4.0	3.0
1031_0003353	2.0	3.0
1031_0003365	3.0	3.0
1031_0003407	2.0	3.0
1031_0003408	2.0	3.0
1031_0003410	3.0	3.0
1061_0120271	2.0	2.0
1061_0120272	1.0	1.0
1061_0120274	2.0	2.0
1061_0120275	2.0	2.0
1061_0120278	2.0	2.0
1061_0120280	1.0	1.0
1061_0120288	2.0	2.0
1061_0120295	0.0	2.0
1061_0120300	2.0	1.0
1061_0120302	1.0	2.0
1061_0120304	2.0	2.0
1061_0120309	1.0	1.0
1061_0120319	3.0	2.0
1061_0120334	3.0	2.0
1061_0120337	2.0	2.0
1061_0120345	2.0	2.0
1061_0120349	1.0	1.0
1061_0120352	1.0	1.0
1061_0120355	1.0	1.0
1061_0120357	2.0	3.0
1061_0120367	3.0	2.0
1061_0120368	2.0	2.0
1061_0120371	3.0	3.0
1061_0120375	2.0	2.0
1061_0120386	0.0	2.0
1061_0120403	2.0	2.0
1061_0120406	2.0	2.0
1061_0120415	2.0	1.0
1061_0120423	3.0	2.0
1061_0120426	2.0	2.0
1061_0120460	2.0	1.0
1061_0120479	2.0	2.0
1061_0120480	2.0	2.0
1061_0120484	2.0	2.0
1061_0120485	2.0	2.0
1061_0120487	2.0	2.0
1061_0120493	2.0	2.0
1061_0120881	2.0	2.0
1061_0120889	1.0	1.0
1061_0120894	2.0	3.0
1061_1029111	2.0	2.0
1061_1029115	2.0	2.0
1061_1029120	2.0	2.0
1061_1202910	2.0	2.0
1061_1202915	1.0	1.0
1061_1202919	1.0	1.0
1071_0020001	2.0	1.0
1071_0024678	2.0	1.0
1071_0024689	1.0	1.0
1071_0024694	2.0	2.0
1071_0024706	1.0	1.0
1071_0024715	2.0	1.0
1071_0024758	1.0	1.0
1071_0024768	0.0	1.0
1071_0024778	0.0	1.0
1071_0024779	1.0	1.0
1071_0024784	1.0	0.0
1071_0024798	0.0	0.0
1071_0024799	2.0	2.0
1071_0024800	0.0	0.0
1071_0024801	1.0	1.0
1071_0024808	1.0	2.0
1071_0024812	0.0	1.0
1071_0024814	1.0	1.0
1071_0024816	1.0	1.0
1071_0024820	1.0	1.0
1071_0024835	1.0	1.0
1071_0024841	0.0	1.0
1071_0024844	1.0	1.0
1071_0024845	0.0	1.0
1071_0024846	1.0	1.0
1071_0024847	1.0	1.0
1071_0024849	0.0	0.0
1071_0024859	1.0	2.0
1071_0024872	1.0	2.0
1071_0024881	2.0	1.0
1071_0241831	1.0	1.0
1071_0242013	1.0	1.0
1071_0242042	1.0	1.0
1071_0242092	0.0	0.0
1071_0243502	0.0	0.0
1071_0243622	0.0	0.0
1071_0248312	1.0	1.0
1071_0248320	0.0	0.0
1071_0248328	0.0	1.0
1071_0248349	1.0	1.0
1071_0248350	2.0	1.0
1091_0000003	2.0	2.0
1091_0000005	2.0	2.0
1091_0000018	2.0	2.0
1091_0000021	2.0	2.0
1091_0000024	3.0	1.0
1091_0000035	2.0	1.0
1091_0000037	1.0	0.0
1091_0000046	2.0	1.0
1091_0000051	1.0	1.0
1091_0000057	2.0	1.0
1091_0000061	2.0	1.0
1091_0000069	2.0	1.0
1091_0000070	2.0	1.0
1091_0000071	2.0	2.0
1091_0000074	2.0	1.0
1091_0000078	3.0	1.0
1091_0000101	2.0	1.0
1091_0000102	2.0	1.0
1091_0000116	2.0	2.0
1091_0000123	2.0	1.0
1091_0000126	2.0	2.0
1091_0000140	2.0	1.0
1091_0000145	1.0	0.0
1091_0000148	1.0	1.0
1091_0000151	0.0	1.0
1091_0000155	2.0	2.0
1091_0000156	2.0	2.0
1091_0000157	2.0	2.0
1091_0000172	2.0	1.0
1091_0000185	2.0	1.0
1091_0000193	2.0	1.0
1091_0000196	2.0	1.0
1091_0000198	2.0	2.0
1091_0000201	2.0	2.0
1091_0000202	1.0	2.0
1091_0000206	1.0	1.0
1091_0000209	2.0	2.0
1091_0000212	1.0	2.0
1091_0000215	2.0	2.0
1091_0000216	1.0	2.0
1091_0000228	1.0	2.0
1091_0000231	2.0	2.0
1091_0000233	2.0	2.0
1091_0000235	2.0	1.0
1091_0000239	2.0	1.0
1091_0000240	2.0	2.0
1091_0000242	1.0	2.0
1091_0000248	2.0	2.0
1091_0000251	2.0	2.0
1091_0000252	2.0	2.0
1091_0000254	2.0	2.0
1091_0000258	1.0	2.0
1091_0000263	2.0	2.0
1091_0000264	1.0	2.0
0605	2.0	2.0
0606	2.0	2.0
0609	2.0	2.0
0614	2.0	2.0
0618	2.0	2.0
0620	2.0	2.0
0624	2.0	2.0
0641	2.0	2.0
0644	2.0	2.0
0716	2.0	2.0
0717	2.0	2.0
0719	2.0	2.0
0813	2.0	2.0
0814	1.0	2.0
0819	3.0	2.0
0824	2.0	2.0
0829	2.0	2.0
0907	2.0	2.0
0912	2.0	2.0
0913	2.0	2.0
0916	1.0	2.0
1003	2.0	2.0
1006	2.0	2.0
1007	2.0	2.0
1008	2.0	2.0
1009	2.0	2.0
1010	1.0	2.0
1115	2.0	3.0
BER0611007	3.0	3.0
KYJ0611004A	1.0	2.0
KYJ0611005A	1.0	1.0
KYJ0611006A	1.0	2.0
LIB0611001A	1.0	1.0
LIB0611001B	1.0	1.0
LIB0611004B	2.0	2.0
MOS0611013	3.0	3.0
PAR1011013	3.0	3.0
PHA0111001A	1.0	1.0
PHA0111004A	1.0	1.0
PHA0112007A	1.0	2.0
PHA0209008	1.0	1.0
PHA0209013	1.0	1.0
PHA0209031	3.0	3.0
PHA0210004	1.0	2.0
PHA0411010A	0.0	1.0
PHA0411011B	1.0	2.0
PHA0411031	3.0	3.0
PHA0411032	3.0	3.0
PHA0411045	3.0	3.0
PHA0411047	3.0	3.0
PHA0509018	3.0	3.0
PHA0509019	3.0	3.0
PHA0509025	3.0	3.0
PHA0509032	3.0	3.0
PHA0509033	2.0	2.0
PHA0509036	3.0	3.0
PHA0509038	2.0	2.0
PHA0510002A	1.0	2.0
PHA0510027	3.0	3.0
PHA0510035	3.0	3.0
PHA0610005A	1.0	1.0
PHA0610006B	1.0	1.0
PHA0610007A	1.0	0.0
PHA0610015	3.0	3.0
PHA0610016	3.0	3.0
PHA0610017	3.0	3.0
PHA0710009	3.0	3.0
PHA0710011	3.0	3.0
PHA0710012	3.0	3.0
PHA0710018	3.0	3.0
PHA0810008	3.0	3.0
PHA0811010	3.0	3.0
PHA0811019	3.0	3.0
PHA1109004	3.0	3.0
PHA1109023	1.0	1.0
PHA1109025	1.0	1.0
PHA1110003A	1.0	2.0
PHA1111003B	1.0	1.0
PHA1111004A	1.0	2.0
PHA1111004B	1.0	2.0
PHA1111008B	1.0	1.0
VAR0910005	3.0	3.0
VAR0910009	3.0	3.0
1325_1001008	2.0	2.0
1325_1001010	2.0	2.0
1325_1001011	2.0	2.0
1325_1001014	3.0	2.0
1325_1001020	2.0	2.0
1325_1001040	2.0	2.0
1325_1001044	2.0	2.0
1325_1001047	1.0	2.0
1325_1001054	2.0	2.0
1325_1001081	2.0	2.0
1325_1001085	2.0	2.0
1325_1001090	2.0	2.0
1325_1001098	2.0	2.0
1325_1001101	3.0	2.0
1325_1001120	2.0	3.0
1325_1001121	2.0	2.0
1325_1001127	3.0	2.0
1325_1001128	2.0	2.0
1325_1001135	2.0	2.0
1325_1001141	2.0	2.0
1325_1001143	2.0	3.0
1325_1001154	3.0	3.0
1325_1001159	2.0	2.0
1325_1001170	2.0	2.0
1325_9000102	2.0	2.0
1325_9000105	2.0	2.0
1325_9000107	2.0	2.0
1325_9000137	2.0	2.0
1325_9000139	2.0	2.0
1325_9000186	3.0	3.0
1325_9000213	3.0	2.0
1325_9000278	3.0	2.0
1325_9000279	3.0	2.0
1325_9000302	2.0	2.0
1325_9000316	2.0	2.0
1325_9000320	3.0	2.0
1325_9000611	2.0	3.0
1325_9000677	2.0	2.0
1325_9000678	3.0	3.0
1325_9000685	3.0	3.0
1365_0100004	2.0	2.0
1365_0100012	1.0	1.0
1365_0100024	1.0	2.0
1365_0100030	1.0	2.0
1365_0100064	2.0	2.0
1365_0100069	1.0	2.0
1365_0100095	2.0	2.0
1365_0100100	2.0	2.0
1365_0100116	2.0	2.0
1365_0100120	3.0	3.0
1365_0100121	2.0	2.0
1365_0100123	2.0	2.0
1365_0100136	1.0	2.0
1365_0100145	2.0	2.0
1365_0100151	1.0	1.0
1365_0100163	2.0	2.0
1365_0100166	1.0	2.0
1365_0100167	1.0	1.0
1365_0100169	1.0	2.0
1365_0100171	1.0	1.0
1365_0100184	1.0	2.0
1365_0100192	3.0	2.0
1365_0100195	1.0	2.0
1365_0100198	1.0	2.0
1365_0100200	2.0	2.0
1365_0100220	2.0	2.0
1365_0100224	2.0	3.0
1365_0100225	2.0	1.0
1365_0100253	1.0	2.0
1365_0100263	3.0	2.0
1365_0100275	2.0	2.0
1365_0100278	2.0	2.0
1365_0100281	2.0	2.0
1365_0100456	2.0	2.0
1385_0000011	0.0	1.0
1385_0000043	1.0	1.0
1385_0000044	1.0	1.0
1385_0000051	1.0	1.0
1385_0000095	1.0	0.0
1385_0000102	1.0	1.0
1385_0000123	1.0	1.0
1385_0000127	1.0	1.0
1385_0001111	1.0	1.0
1385_0001112	1.0	1.0
1385_0001113	1.0	1.0
1385_0001124	1.0	0.0
1385_0001125	1.0	1.0
1385_0001130	1.0	0.0
1385_0001132	1.0	1.0
1385_0001137	1.0	1.0
1385_0001151	1.0	2.0
1385_0001155	1.0	1.0
1385_0001171	0.0	0.0
1385_0001196	0.0	1.0
1385_0001197	0.0	1.0
1385_0001198	2.0	1.0
1385_0001503	1.0	2.0
1385_0001526	0.0	0.0
1385_0001715	1.0	1.0
1385_0001720	0.0	1.0
1385_0001732	0.0	1.0
1385_0001741	0.0	0.0
1385_0001757	1.0	1.0
1385_0001759	1.0	1.0
1385_0001765	0.0	0.0
1385_0001775	1.0	1.0
1385_0001785	0.0	1.0
1385_0001796	1.0	1.0
1385_0001799	1.0	1.0
1395_0000354	1.0	0.0
1395_0000361	1.0	1.0
1395_0000368	0.0	0.0
1395_0000387	3.0	1.0
1395_0000396	1.0	1.0
1395_0000438	2.0	2.0
1395_0000449	2.0	1.0
1395_0000452	1.0	0.0
1395_0000455	1.0	1.0
1395_0000458	1.0	1.0
1395_0000462	1.0	1.0
1395_0000470	1.0	0.0
1395_0000499	1.0	1.0
1395_0000513	2.0	1.0
1395_0000526	1.0	1.0
1395_0000550	1.0	1.0
1395_0000552	1.0	1.0
1395_0000555	1.0	1.0
1395_0000564	1.0	1.0
1395_0000584	0.0	1.0
1395_0000585	0.0	1.0
1395_0000598	1.0	1.0
1395_0000602	1.0	0.0
1395_0000609	1.0	0.0
1395_0000627	1.0	1.0
1395_0000639	0.0	2.0
1395_0001019	0.0	1.0
1395_0001058	1.0	1.0
1395_0001061	1.0	2.0
1395_0001065	0.0	1.0
1395_0001075	0.0	1.0
1395_0001076	1.0	1.0
1395_0001093	0.0	1.0
1395_0001114	0.0	1.0
1395_0001120	0.0	0.0
1395_0001122	0.0	1.0
1395_0001146	0.0	0.0
1395_0001158	2.0	1.0
1395_0001160	1.0	1.0
1395_0001170	1.0	2.0
4 Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 1.03
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.82
              precision    recall  f1-score   support

         0.0       1.00      0.03      0.05        39
         1.0       0.61      0.86      0.71       132
         2.0       0.72      0.77      0.75       166
         3.0       0.72      0.67      0.69        96
         4.0       0.00      0.00      0.00        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.68       452
   macro avg       0.51      0.39      0.37       452
weighted avg       0.68      0.68      0.63       452

[[  1  38   0   0   0   0]
 [  0 113  18   1   0   0]
 [  0  32 128   6   0   0]
 [  0   2  30  64   0   0]
 [  0   0   1  17   0   0]
 [  0   0   0   1   0   0]]
0.6335706214024442
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.81
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.71
              precision    recall  f1-score   support

         0.0       0.62      0.33      0.43        39
         1.0       0.69      0.78      0.73       132
         2.0       0.72      0.83      0.77       166
         3.0       0.70      0.66      0.68        96
         4.0       0.00      0.00      0.00        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.70       452
   macro avg       0.45      0.43      0.44       452
weighted avg       0.67      0.70      0.68       452

[[ 13  26   0   0   0   0]
 [  7 103  21   1   0   0]
 [  1  19 138   8   0   0]
 [  0   1  32  63   0   0]
 [  0   0   1  17   0   0]
 [  0   0   0   1   0   0]]
0.6784924667473946
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.68
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.71
              precision    recall  f1-score   support

         0.0       0.69      0.23      0.35        39
         1.0       0.68      0.77      0.72       132
         2.0       0.74      0.81      0.77       166
         3.0       0.67      0.77      0.72        96
         4.0       0.00      0.00      0.00        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.70       452
   macro avg       0.46      0.43      0.43       452
weighted avg       0.67      0.70      0.68       452

[[  9  30   0   0   0   0]
 [  4 101  26   1   0   0]
 [  0  16 134  16   0   0]
 [  0   1  21  74   0   0]
 [  0   0   0  18   0   0]
 [  0   0   0   1   0   0]]
0.6767852610848856
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.55
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.70
              precision    recall  f1-score   support

         0.0       0.70      0.36      0.47        39
         1.0       0.72      0.78      0.75       132
         2.0       0.76      0.84      0.79       166
         3.0       0.69      0.73      0.71        96
         4.0       0.00      0.00      0.00        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.72       452
   macro avg       0.48      0.45      0.45       452
weighted avg       0.69      0.72      0.70       452

[[ 14  24   1   0   0   0]
 [  6 103  22   1   0   0]
 [  0  16 139  11   0   0]
 [  0   1  22  70   3   0]
 [  0   0   0  18   0   0]
 [  0   0   0   1   0   0]]
0.7015597179910213
452 452 452
Filename	True Label	Prediction
1023_0001419	3.0	3.0
1023_0001422	3.0	2.0
1023_0101675	3.0	2.0
1023_0101690	2.0	2.0
1023_0101749	3.0	3.0
1023_0101844	2.0	2.0
1023_0101848	2.0	2.0
1023_0101856	2.0	3.0
1023_0101894	3.0	3.0
1023_0101897	2.0	3.0
1023_0101899	2.0	2.0
1023_0101900	3.0	3.0
1023_0104209	3.0	3.0
1023_0106816	3.0	3.0
1023_0107726	3.0	3.0
1023_0107773	2.0	2.0
1023_0107780	3.0	3.0
1023_0107781	3.0	3.0
1023_0107787	2.0	2.0
1023_0107788	3.0	2.0
1023_0108304	3.0	3.0
1023_0108305	3.0	3.0
1023_0108510	3.0	3.0
1023_0108650	3.0	3.0
1023_0108753	3.0	2.0
1023_0108815	3.0	3.0
1023_0108935	2.0	3.0
1023_0108955	4.0	3.0
1023_0109022	3.0	3.0
1023_0109026	2.0	3.0
1023_0109039	3.0	3.0
1023_0109267	2.0	3.0
1023_0109391	2.0	2.0
1023_0109422	3.0	3.0
1023_0109500	2.0	3.0
1023_0109614	2.0	2.0
1023_0109880	3.0	3.0
1023_0109915	2.0	2.0
1023_0109954	3.0	3.0
1031_0001703	4.0	3.0
1031_0001950	4.0	3.0
1031_0001998	4.0	3.0
1031_0002003	3.0	3.0
1031_0002004	4.0	3.0
1031_0002005	4.0	3.0
1031_0002042	4.0	3.0
1031_0002084	3.0	3.0
1031_0002085	3.0	3.0
1031_0003013	4.0	3.0
1031_0003023	3.0	4.0
1031_0003053	4.0	3.0
1031_0003063	5.0	3.0
1031_0003065	3.0	3.0
1031_0003073	4.0	3.0
1031_0003076	4.0	3.0
1031_0003097	4.0	3.0
1031_0003136	4.0	3.0
1031_0003145	4.0	3.0
1031_0003146	4.0	3.0
1031_0003155	3.0	3.0
1031_0003169	3.0	4.0
1031_0003170	3.0	3.0
1031_0003179	4.0	3.0
1031_0003207	4.0	3.0
1031_0003216	3.0	3.0
1031_0003234	3.0	3.0
1031_0003246	3.0	3.0
1031_0003336	3.0	3.0
1031_0003339	3.0	4.0
1031_0003366	3.0	3.0
1031_0003367	3.0	3.0
1031_0003384	3.0	3.0
1061_0120273	2.0	1.0
1061_0120282	0.0	1.0
1061_0120285	1.0	2.0
1061_0120297	2.0	2.0
1061_0120306	2.0	2.0
1061_0120316	2.0	2.0
1061_0120317	3.0	2.0
1061_0120318	2.0	2.0
1061_0120323	1.0	2.0
1061_0120326	2.0	2.0
1061_0120330	3.0	2.0
1061_0120341	1.0	1.0
1061_0120359	2.0	2.0
1061_0120360	3.0	3.0
1061_0120361	2.0	2.0
1061_0120370	2.0	2.0
1061_0120374	3.0	3.0
1061_0120387	2.0	2.0
1061_0120391	1.0	1.0
1061_0120394	2.0	2.0
1061_0120404	2.0	2.0
1061_0120410	2.0	2.0
1061_0120411	4.0	3.0
1061_0120424	2.0	2.0
1061_0120427	2.0	2.0
1061_0120442	2.0	2.0
1061_0120448	2.0	2.0
1061_0120453	2.0	2.0
1061_0120456	2.0	2.0
1061_0120458	3.0	2.0
1061_0120482	2.0	2.0
1061_0120488	2.0	2.0
1061_0120489	2.0	2.0
1061_0120495	2.0	2.0
1061_0120497	3.0	3.0
1061_0120853	2.0	2.0
1061_0120855	2.0	2.0
1061_0120856	2.0	2.0
1061_0120857	2.0	2.0
1061_0120859	2.0	2.0
1061_0120880	3.0	2.0
1061_0120882	3.0	2.0
1061_0120884	2.0	2.0
1061_0120888	1.0	2.0
1061_0120890	2.0	2.0
1061_1029113	1.0	1.0
1061_1202918	2.0	2.0
1071_0024681	2.0	2.0
1071_0024688	1.0	1.0
1071_0024692	2.0	2.0
1071_0024699	1.0	2.0
1071_0024710	0.0	0.0
1071_0024711	1.0	1.0
1071_0024716	1.0	0.0
1071_0024759	0.0	2.0
1071_0024761	1.0	1.0
1071_0024762	0.0	1.0
1071_0024763	1.0	1.0
1071_0024766	0.0	0.0
1071_0024767	2.0	1.0
1071_0024769	1.0	1.0
1071_0024773	1.0	1.0
1071_0024776	0.0	0.0
1071_0024777	1.0	1.0
1071_0024781	1.0	1.0
1071_0024783	0.0	0.0
1071_0024809	1.0	1.0
1071_0024810	1.0	1.0
1071_0024813	1.0	1.0
1071_0024818	1.0	1.0
1071_0024819	1.0	1.0
1071_0024822	0.0	1.0
1071_0024824	1.0	1.0
1071_0024834	2.0	2.0
1071_0024838	0.0	0.0
1071_0024856	1.0	1.0
1071_0024860	1.0	0.0
1071_0024879	1.0	0.0
1071_0241832	0.0	1.0
1071_0242012	1.0	1.0
1071_0242071	0.0	0.0
1071_0242073	1.0	1.0
1071_0242091	0.0	1.0
1071_0243501	1.0	1.0
1071_0243581	0.0	0.0
1071_0243592	1.0	1.0
1071_0248309	2.0	1.0
1071_0248318	0.0	0.0
1071_0248322	0.0	1.0
1071_0248324	0.0	1.0
1071_0248326	1.0	1.0
1071_0248332	2.0	2.0
1071_0248336	0.0	0.0
1071_0248343	1.0	1.0
1071_0248345	2.0	2.0
1091_0000007	2.0	2.0
1091_0000008	2.0	1.0
1091_0000012	1.0	1.0
1091_0000013	1.0	1.0
1091_0000031	1.0	1.0
1091_0000032	1.0	2.0
1091_0000033	1.0	2.0
1091_0000042	1.0	1.0
1091_0000044	0.0	1.0
1091_0000045	2.0	1.0
1091_0000058	2.0	2.0
1091_0000062	3.0	1.0
1091_0000063	1.0	1.0
1091_0000095	2.0	2.0
1091_0000114	2.0	2.0
1091_0000153	1.0	2.0
1091_0000161	2.0	2.0
1091_0000169	2.0	2.0
1091_0000190	1.0	2.0
1091_0000199	2.0	2.0
1091_0000203	1.0	2.0
1091_0000213	2.0	1.0
1091_0000221	2.0	1.0
1091_0000225	2.0	2.0
1091_0000227	1.0	2.0
1091_0000230	2.0	2.0
1091_0000247	2.0	2.0
1091_0000265	2.0	2.0
1091_0000266	2.0	2.0
1091_0000270	2.0	2.0
1091_0000271	2.0	2.0
0601	2.0	2.0
0607	2.0	2.0
0608	1.0	2.0
0613	2.0	2.0
0616	2.0	2.0
0629	2.0	2.0
0630	1.0	1.0
0632	1.0	2.0
0637	2.0	2.0
0639	2.0	2.0
0715	2.0	2.0
0720	2.0	2.0
0721	2.0	2.0
0722	2.0	2.0
0811	2.0	2.0
0812	1.0	2.0
0820	1.0	2.0
0905	2.0	2.0
0911	2.0	2.0
0924	1.0	2.0
0926	2.0	2.0
1016	2.0	2.0
1022	2.0	2.0
1023	2.0	2.0
1114	2.0	2.0
BER0609003	3.0	3.0
KYJ0611006B	1.0	1.0
KYJ0611009A	2.0	1.0
LIB0611002A	1.0	2.0
LIB0611011	2.0	3.0
LON0611003	3.0	3.0
LON0611004A	1.0	1.0
MOS0611012	3.0	3.0
MOS0611014	1.0	3.0
PAR1011014	3.0	3.0
PAR1011016	3.0	3.0
PHA0111003A	1.0	1.0
PHA0111011	3.0	3.0
PHA0111012	3.0	3.0
PHA0112003A	1.0	1.0
PHA0112003B	1.0	2.0
PHA0112007B	1.0	1.0
PHA0209026	3.0	3.0
PHA0209034	3.0	3.0
PHA0210001	1.0	1.0
PHA0210007	1.0	2.0
PHA0210008	1.0	1.0
PHA0411008A	2.0	2.0
PHA0411009A	1.0	2.0
PHA0411028	2.0	2.0
PHA0411029	3.0	3.0
PHA0411034	3.0	3.0
PHA0411041	3.0	3.0
PHA0411055	3.0	3.0
PHA0411058	3.0	3.0
PHA0411059	3.0	3.0
PHA0509002	1.0	1.0
PHA0509020	3.0	3.0
PHA0509030	3.0	3.0
PHA0509035	3.0	3.0
PHA0509042	3.0	3.0
PHA0509043	3.0	3.0
PHA0510002B	2.0	2.0
PHA0510003B	1.0	1.0
PHA0510004A	1.0	1.0
PHA0510040	3.0	3.0
PHA0610005B	1.0	1.0
PHA0709008	3.0	3.0
PHA0710013	3.0	3.0
PHA0710014	3.0	3.0
PHA0710015	3.0	3.0
PHA0710016	3.0	3.0
PHA0809009	3.0	3.0
PHA0810006	3.0	3.0
PHA0810009	3.0	3.0
PHA0810010	3.0	3.0
PHA1109006	2.0	2.0
PHA1109028	3.0	3.0
PHA1110002B	2.0	2.0
PHA1110003B	1.0	2.0
PHA1111001A	2.0	2.0
PHA1111002B	1.0	1.0
PHA1111009A	1.0	1.0
VAR0909004	3.0	3.0
VAR0909006	3.0	3.0
1325_1001015	2.0	2.0
1325_1001016	2.0	2.0
1325_1001021	2.0	2.0
1325_1001022	2.0	2.0
1325_1001027	3.0	2.0
1325_1001028	2.0	3.0
1325_1001042	2.0	2.0
1325_1001052	2.0	2.0
1325_1001055	2.0	2.0
1325_1001062	2.0	2.0
1325_1001075	1.0	2.0
1325_1001079	2.0	2.0
1325_1001080	2.0	2.0
1325_1001086	2.0	2.0
1325_1001089	2.0	2.0
1325_1001092	2.0	2.0
1325_1001094	2.0	2.0
1325_1001095	2.0	2.0
1325_1001099	3.0	2.0
1325_1001113	3.0	2.0
1325_1001131	2.0	2.0
1325_1001133	2.0	2.0
1325_1001136	2.0	2.0
1325_1001144	2.0	2.0
1325_1001152	2.0	3.0
1325_1001153	2.0	2.0
1325_1001156	2.0	2.0
1325_1001157	2.0	2.0
1325_1001164	2.0	2.0
1325_1001165	2.0	2.0
1325_1001167	2.0	2.0
1325_1001168	2.0	2.0
1325_9000095	2.0	2.0
1325_9000104	2.0	2.0
1325_9000140	3.0	2.0
1325_9000215	3.0	2.0
1325_9000240	3.0	2.0
1325_9000321	3.0	2.0
1325_9000503	3.0	3.0
1325_9000505	3.0	2.0
1325_9000536	3.0	3.0
1325_9000676	3.0	2.0
1325_9000684	3.0	2.0
1365_0100003	1.0	1.0
1365_0100018	2.0	2.0
1365_0100027	2.0	2.0
1365_0100051	0.0	1.0
1365_0100056	2.0	2.0
1365_0100063	2.0	2.0
1365_0100072	2.0	2.0
1365_0100073	2.0	2.0
1365_0100080	2.0	2.0
1365_0100096	2.0	2.0
1365_0100097	2.0	2.0
1365_0100098	1.0	1.0
1365_0100102	2.0	2.0
1365_0100103	2.0	2.0
1365_0100106	2.0	2.0
1365_0100107	2.0	3.0
1365_0100118	2.0	2.0
1365_0100133	2.0	2.0
1365_0100135	2.0	1.0
1365_0100146	2.0	1.0
1365_0100148	2.0	2.0
1365_0100173	1.0	1.0
1365_0100175	1.0	1.0
1365_0100178	1.0	1.0
1365_0100180	1.0	1.0
1365_0100202	1.0	1.0
1365_0100221	3.0	2.0
1365_0100222	2.0	3.0
1365_0100229	2.0	2.0
1365_0100231	2.0	2.0
1365_0100232	1.0	2.0
1365_0100252	2.0	2.0
1365_0100257	2.0	2.0
1365_0100258	2.0	2.0
1365_0100269	2.0	2.0
1365_0100270	2.0	2.0
1365_0100277	3.0	2.0
1365_0100279	2.0	2.0
1365_0100282	2.0	2.0
1365_0100288	1.0	1.0
1365_0100447	2.0	2.0
1365_0100457	2.0	2.0
1365_0100459	2.0	2.0
1385_0000012	1.0	1.0
1385_0000013	1.0	1.0
1385_0000023	1.0	1.0
1385_0000035	1.0	1.0
1385_0000038	1.0	1.0
1385_0000048	1.0	1.0
1385_0000052	1.0	1.0
1385_0000097	1.0	1.0
1385_0000119	1.0	1.0
1385_0000125	1.0	1.0
1385_0001103	1.0	0.0
1385_0001109	1.0	1.0
1385_0001110	1.0	1.0
1385_0001121	1.0	1.0
1385_0001126	0.0	1.0
1385_0001138	1.0	1.0
1385_0001152	1.0	1.0
1385_0001153	2.0	1.0
1385_0001157	1.0	1.0
1385_0001163	1.0	1.0
1385_0001167	0.0	1.0
1385_0001173	0.0	0.0
1385_0001174	0.0	0.0
1385_0001178	0.0	0.0
1385_0001191	1.0	1.0
1385_0001192	1.0	1.0
1385_0001525	1.0	1.0
1385_0001723	0.0	0.0
1385_0001724	2.0	1.0
1385_0001725	0.0	1.0
1385_0001726	1.0	1.0
1385_0001737	1.0	1.0
1385_0001738	0.0	1.0
1385_0001739	1.0	1.0
1385_0001742	0.0	1.0
1385_0001746	1.0	1.0
1385_0001758	0.0	1.0
1385_0001762	1.0	1.0
1385_0001773	0.0	1.0
1385_0001786	1.0	1.0
1385_0001787	0.0	1.0
1385_0001789	1.0	1.0
1395_0000340	1.0	1.0
1395_0000356	1.0	0.0
1395_0000364	1.0	1.0
1395_0000365	2.0	1.0
1395_0000366	2.0	1.0
1395_0000391	3.0	2.0
1395_0000392	1.0	1.0
1395_0000402	1.0	1.0
1395_0000403	1.0	1.0
1395_0000415	1.0	1.0
1395_0000446	2.0	1.0
1395_0000460	1.0	0.0
1395_0000500	1.0	1.0
1395_0000514	2.0	2.0
1395_0000525	1.0	1.0
1395_0000531	1.0	1.0
1395_0000537	1.0	1.0
1395_0000556	1.0	1.0
1395_0000565	1.0	1.0
1395_0000591	0.0	0.0
1395_0000599	1.0	1.0
1395_0000611	0.0	1.0
1395_0000631	0.0	1.0
1395_0000644	1.0	1.0
1395_0001013	1.0	1.0
1395_0001015	0.0	1.0
1395_0001021	1.0	1.0
1395_0001028	1.0	1.0
1395_0001033	1.0	1.0
1395_0001060	1.0	1.0
1395_0001064	1.0	1.0
1395_0001073	1.0	1.0
1395_0001074	1.0	1.0
1395_0001101	1.0	1.0
1395_0001108	0.0	1.0
1395_0001117	0.0	1.0
1395_0001123	0.0	1.0
1395_0001141	1.0	1.0
1395_0001145	1.0	1.0
1395_0001150	0.0	1.0
1395_0001167	2.0	1.0
5 Fold, Dimension = CoherenceCohesion

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 1.06
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.80
              precision    recall  f1-score   support

         0.0       0.00      0.00      0.00        39
         1.0       0.60      0.79      0.68       133
         2.0       0.74      0.72      0.73       165
         3.0       0.72      0.89      0.79        96
         4.0       0.00      0.00      0.00        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.68       452
   macro avg       0.34      0.40      0.37       452
weighted avg       0.60      0.68      0.64       452

[[  0  37   2   0   0   0]
 [  0 105  28   0   0   0]
 [  0  33 118  14   0   0]
 [  0   0  11  85   0   0]
 [  0   0   0  18   0   0]
 [  0   0   0   1   0   0]]
0.6352404548893673
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.80
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.80
              precision    recall  f1-score   support

         0.0       0.67      0.10      0.18        39
         1.0       0.60      0.71      0.65       133
         2.0       0.67      0.78      0.72       165
         3.0       0.74      0.76      0.75        96
         4.0       0.00      0.00      0.00        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.66       452
   macro avg       0.45      0.39      0.38       452
weighted avg       0.64      0.66      0.63       452

[[  4  32   3   0   0   0]
 [  2  94  37   0   0   0]
 [  0  30 128   7   0   0]
 [  0   0  23  73   0   0]
 [  0   0   0  18   0   0]
 [  0   0   0   1   0   0]]
0.6282764888172198
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.67
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       0.73      0.28      0.41        39
         1.0       0.61      0.76      0.68       133
         2.0       0.74      0.67      0.70       165
         3.0       0.71      0.92      0.80        96
         4.0       0.00      0.00      0.00        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.69       452
   macro avg       0.47      0.44      0.43       452
weighted avg       0.67      0.69      0.66       452

[[ 11  27   1   0   0   0]
 [  3 101  29   0   0   0]
 [  1  37 110  17   0   0]
 [  0   0   8  88   0   0]
 [  0   0   0  18   0   0]
 [  0   0   0   1   0   0]]
0.6611010889370905
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.56
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.82
              precision    recall  f1-score   support

         0.0       0.69      0.23      0.35        39
         1.0       0.61      0.77      0.68       133
         2.0       0.75      0.70      0.72       165
         3.0       0.75      0.80      0.78        96
         4.0       0.41      0.39      0.40        18
         5.0       0.00      0.00      0.00         1

    accuracy                           0.69       452
   macro avg       0.54      0.48      0.49       452
weighted avg       0.69      0.69      0.67       452

[[  9  29   1   0   0   0]
 [  3 102  28   0   0   0]
 [  1  35 115  14   0   0]
 [  0   0  10  77   9   0]
 [  0   0   0  11   7   0]
 [  0   0   0   0   1   0]]
0.6749439354522875
452 452 452
Filename	True Label	Prediction
1023_0001418	3.0	3.0
1023_0101683	2.0	2.0
1023_0101684	2.0	2.0
1023_0101695	2.0	2.0
1023_0101895	4.0	3.0
1023_0101901	3.0	3.0
1023_0101906	2.0	2.0
1023_0101909	4.0	3.0
1023_0103830	3.0	3.0
1023_0103832	2.0	2.0
1023_0103837	3.0	3.0
1023_0103838	3.0	3.0
1023_0103844	4.0	3.0
1023_0104203	3.0	3.0
1023_0107740	3.0	3.0
1023_0107783	3.0	2.0
1023_0108306	4.0	3.0
1023_0108423	3.0	2.0
1023_0108426	2.0	3.0
1023_0108520	3.0	3.0
1023_0108752	3.0	3.0
1023_0108766	2.0	3.0
1023_0108810	3.0	3.0
1023_0108813	3.0	3.0
1023_0108814	3.0	3.0
1023_0108890	3.0	3.0
1023_0108908	3.0	3.0
1023_0108932	3.0	3.0
1023_0109029	2.0	1.0
1023_0109030	3.0	3.0
1023_0109038	3.0	3.0
1023_0109192	3.0	3.0
1023_0109395	2.0	2.0
1023_0109396	2.0	3.0
1023_0109400	3.0	3.0
1023_0109401	3.0	3.0
1023_0109495	3.0	3.0
1023_0109522	3.0	3.0
1023_0109674	3.0	3.0
1023_0109717	3.0	3.0
1023_0109891	2.0	3.0
1023_0109947	3.0	3.0
1031_0001949	4.0	3.0
1031_0002036	4.0	4.0
1031_0002043	4.0	4.0
1031_0002061	3.0	4.0
1031_0002079	4.0	4.0
1031_0002083	3.0	3.0
1031_0002089	4.0	3.0
1031_0002131	3.0	3.0
1031_0002185	4.0	4.0
1031_0002198	4.0	4.0
1031_0003035	4.0	3.0
1031_0003042	3.0	3.0
1031_0003077	3.0	3.0
1031_0003090	3.0	3.0
1031_0003099	4.0	3.0
1031_0003106	3.0	3.0
1031_0003133	4.0	4.0
1031_0003140	3.0	3.0
1031_0003141	3.0	4.0
1031_0003144	3.0	3.0
1031_0003150	3.0	3.0
1031_0003156	3.0	3.0
1031_0003162	3.0	3.0
1031_0003163	3.0	3.0
1031_0003164	3.0	3.0
1031_0003165	3.0	3.0
1031_0003186	3.0	3.0
1031_0003189	4.0	4.0
1031_0003190	3.0	4.0
1031_0003205	3.0	4.0
1031_0003211	3.0	4.0
1031_0003212	3.0	3.0
1031_0003224	3.0	3.0
1031_0003225	3.0	4.0
1031_0003235	4.0	3.0
1031_0003240	3.0	4.0
1031_0003260	3.0	3.0
1031_0003261	3.0	3.0
1031_0003262	3.0	4.0
1031_0003272	3.0	3.0
1031_0003309	3.0	4.0
1031_0003338	4.0	3.0
1031_0003354	3.0	3.0
1031_0003383	4.0	3.0
1031_0003390	3.0	3.0
1031_0003391	2.0	3.0
1031_0003415	5.0	4.0
1031_0003419	3.0	3.0
1061_0012029	2.0	2.0
1061_0120277	1.0	2.0
1061_0120281	1.0	2.0
1061_0120286	1.0	1.0
1061_0120289	2.0	2.0
1061_0120303	0.0	1.0
1061_0120310	3.0	2.0
1061_0120314	2.0	2.0
1061_0120327	2.0	2.0
1061_0120331	1.0	1.0
1061_0120333	2.0	3.0
1061_0120336	1.0	2.0
1061_0120338	1.0	2.0
1061_0120343	2.0	2.0
1061_0120348	1.0	1.0
1061_0120350	2.0	2.0
1061_0120353	1.0	1.0
1061_0120358	1.0	2.0
1061_0120372	2.0	2.0
1061_0120383	3.0	3.0
1061_0120405	2.0	2.0
1061_0120433	1.0	2.0
1061_0120438	2.0	2.0
1061_0120439	2.0	1.0
1061_0120443	1.0	1.0
1061_0120449	2.0	2.0
1061_0120450	2.0	2.0
1061_0120455	2.0	2.0
1061_0120457	2.0	2.0
1061_0120483	2.0	2.0
1061_0120491	2.0	2.0
1061_0120496	2.0	2.0
1061_0120499	2.0	2.0
1061_0120876	2.0	2.0
1061_0120878	1.0	2.0
1061_0120887	2.0	2.0
1061_1029114	2.0	2.0
1061_1029118	1.0	2.0
1061_1202911	1.0	2.0
1061_1202912	2.0	2.0
1061_1202916	1.0	2.0
1061_1202917	1.0	2.0
1071_0024685	2.0	2.0
1071_0024686	2.0	2.0
1071_0024687	0.0	1.0
1071_0024690	2.0	2.0
1071_0024691	1.0	2.0
1071_0024701	2.0	1.0
1071_0024704	1.0	1.0
1071_0024705	2.0	2.0
1071_0024709	2.0	2.0
1071_0024712	1.0	1.0
1071_0024756	1.0	1.0
1071_0024770	1.0	1.0
1071_0024782	0.0	0.0
1071_0024797	0.0	1.0
1071_0024807	0.0	1.0
1071_0024811	1.0	1.0
1071_0024823	1.0	1.0
1071_0024826	2.0	1.0
1071_0024831	0.0	0.0
1071_0024837	0.0	0.0
1071_0024840	1.0	1.0
1071_0024843	0.0	0.0
1071_0024853	0.0	0.0
1071_0024855	1.0	1.0
1071_0024857	1.0	1.0
1071_0024864	0.0	0.0
1071_0024878	2.0	1.0
1071_0242011	1.0	1.0
1071_0243593	0.0	1.0
1071_0243621	1.0	1.0
1071_0243623	1.0	1.0
1071_0248303	0.0	1.0
1071_0248304	0.0	1.0
1071_0248305	0.0	1.0
1071_0248314	1.0	1.0
1071_0248323	1.0	1.0
1071_0248327	0.0	1.0
1071_0248330	2.0	1.0
1071_0248331	1.0	1.0
1071_0248342	1.0	1.0
1071_0248344	2.0	1.0
1091_0000001	1.0	2.0
1091_0000004	1.0	1.0
1091_0000009	1.0	1.0
1091_0000010	3.0	2.0
1091_0000017	2.0	2.0
1091_0000026	1.0	1.0
1091_0000028	1.0	1.0
1091_0000029	2.0	1.0
1091_0000034	2.0	1.0
1091_0000041	1.0	1.0
1091_0000050	0.0	1.0
1091_0000052	0.0	1.0
1091_0000059	1.0	2.0
1091_0000065	1.0	2.0
1091_0000075	1.0	1.0
1091_0000077	2.0	0.0
1091_0000079	1.0	2.0
1091_0000087	2.0	2.0
1091_0000113	1.0	2.0
1091_0000158	2.0	2.0
1091_0000159	2.0	2.0
1091_0000165	1.0	1.0
1091_0000174	2.0	1.0
1091_0000194	1.0	2.0
1091_0000195	1.0	1.0
1091_0000205	1.0	2.0
1091_0000210	2.0	1.0
1091_0000211	1.0	2.0
1091_0000217	2.0	1.0
1091_0000220	1.0	2.0
1091_0000222	2.0	2.0
1091_0000229	2.0	2.0
1091_0000244	2.0	2.0
1091_0000249	2.0	2.0
1091_0000255	0.0	2.0
1091_0000256	1.0	2.0
1091_0000261	2.0	2.0
1091_0000262	2.0	2.0
1091_0000272	1.0	1.0
1091_0000275	2.0	2.0
0623	2.0	2.0
0626	2.0	2.0
0627	2.0	2.0
0631	2.0	2.0
0633	2.0	2.0
0635	2.0	2.0
0645	2.0	2.0
0718	2.0	2.0
0805	2.0	2.0
0809	2.0	2.0
0815	2.0	2.0
0817	2.0	2.0
0818	1.0	2.0
0821	2.0	2.0
0822	2.0	2.0
0902	2.0	2.0
0906	2.0	2.0
0915	2.0	2.0
0917	2.0	2.0
0923	2.0	2.0
0928	2.0	2.0
1004	2.0	2.0
1017	2.0	2.0
1112	2.0	2.0
1113	2.0	2.0
BER0611006	3.0	3.0
LIB0611002B	1.0	1.0
LIB0611004A	1.0	1.0
LON0610002B	1.0	1.0
LON0611004B	1.0	1.0
MOS0509004	2.0	2.0
PAR1011008A	2.0	1.0
PAR1011009A	2.0	2.0
PAR1011015	2.0	3.0
PAR1011017	3.0	3.0
PHA0111002A	1.0	1.0
PHA0111002B	2.0	2.0
PHA0112009B	2.0	2.0
PHA0209039	3.0	3.0
PHA0411010B	1.0	1.0
PHA0411030	3.0	3.0
PHA0411033	3.0	3.0
PHA0411039	3.0	3.0
PHA0411053	3.0	3.0
PHA0411061	3.0	3.0
PHA0411062	3.0	3.0
PHA0509013	1.0	1.0
PHA0509026	3.0	3.0
PHA0509031	2.0	3.0
PHA0509045	3.0	2.0
PHA0510003A	1.0	1.0
PHA0510004B	1.0	2.0
PHA0510013A	2.0	2.0
PHA0510013B	1.0	1.0
PHA0510030	3.0	3.0
PHA0510031	3.0	3.0
PHA0510034	3.0	3.0
PHA0510037	2.0	3.0
PHA0510047	2.0	3.0
PHA0510050	3.0	3.0
PHA0610019B	2.0	1.0
PHA0809010	2.0	3.0
PHA0810003	3.0	3.0
PHA0811016	3.0	3.0
PHA1109001	2.0	1.0
PHA1109003	2.0	2.0
PHA1109005	3.0	2.0
PHA1109008	1.0	1.0
PHA1109024	3.0	3.0
PHA1109027	3.0	3.0
PHA1110001B	2.0	1.0
PHA1110002A	2.0	2.0
PHA1110013	3.0	3.0
PHA1110019	3.0	3.0
PHA1110021	3.0	3.0
PHA1111003A	1.0	1.0
PHA1111006B	2.0	1.0
TI071122B	1.0	1.0
VAR0209036	2.0	3.0
VAR0909005	3.0	3.0
VAR0910010	3.0	3.0
1325_1001012	2.0	3.0
1325_1001013	2.0	2.0
1325_1001019	2.0	2.0
1325_1001023	2.0	2.0
1325_1001025	2.0	2.0
1325_1001033	2.0	2.0
1325_1001037	2.0	2.0
1325_1001039	3.0	3.0
1325_1001041	3.0	2.0
1325_1001050	2.0	2.0
1325_1001057	1.0	2.0
1325_1001059	2.0	2.0
1325_1001063	2.0	2.0
1325_1001077	2.0	2.0
1325_1001087	2.0	2.0
1325_1001091	2.0	2.0
1325_1001096	2.0	2.0
1325_1001125	3.0	2.0
1325_1001126	2.0	2.0
1325_1001139	2.0	2.0
1325_9000106	2.0	2.0
1325_9000138	3.0	3.0
1325_9000143	3.0	3.0
1325_9000214	3.0	2.0
1325_9000304	2.0	2.0
1325_9000314	2.0	2.0
1325_9000318	3.0	2.0
1325_9000319	2.0	2.0
1325_9000322	3.0	3.0
1325_9000674	3.0	3.0
1365_0100002	2.0	2.0
1365_0100005	1.0	1.0
1365_0100007	1.0	1.0
1365_0100009	1.0	1.0
1365_0100013	2.0	2.0
1365_0100014	2.0	1.0
1365_0100017	2.0	2.0
1365_0100026	1.0	1.0
1365_0100029	1.0	1.0
1365_0100031	2.0	1.0
1365_0100058	2.0	2.0
1365_0100074	2.0	2.0
1365_0100094	2.0	1.0
1365_0100125	2.0	2.0
1365_0100138	2.0	1.0
1365_0100147	2.0	2.0
1365_0100164	2.0	2.0
1365_0100170	2.0	2.0
1365_0100172	1.0	2.0
1365_0100176	2.0	2.0
1365_0100177	2.0	1.0
1365_0100182	2.0	2.0
1365_0100183	2.0	2.0
1365_0100187	2.0	2.0
1365_0100203	2.0	1.0
1365_0100205	2.0	1.0
1365_0100213	1.0	1.0
1365_0100215	2.0	2.0
1365_0100219	2.0	3.0
1365_0100226	2.0	2.0
1365_0100227	2.0	2.0
1365_0100265	2.0	2.0
1365_0100266	2.0	2.0
1365_0100267	2.0	2.0
1365_0100268	1.0	2.0
1365_0100285	2.0	1.0
1365_0100286	1.0	1.0
1365_0100289	2.0	1.0
1365_0100461	2.0	2.0
1365_0100472	2.0	2.0
1365_0100474	2.0	2.0
1365_0100477	1.0	2.0
1385_0000037	1.0	1.0
1385_0000039	1.0	1.0
1385_0000040	1.0	0.0
1385_0000041	1.0	1.0
1385_0000045	1.0	1.0
1385_0000047	1.0	1.0
1385_0000050	1.0	1.0
1385_0000054	1.0	1.0
1385_0000057	1.0	1.0
1385_0000059	1.0	1.0
1385_0000114	1.0	1.0
1385_0000124	1.0	1.0
1385_0001108	1.0	1.0
1385_0001119	1.0	1.0
1385_0001127	1.0	1.0
1385_0001128	0.0	1.0
1385_0001135	1.0	1.0
1385_0001147	1.0	1.0
1385_0001158	1.0	1.0
1385_0001161	1.0	1.0
1385_0001162	1.0	1.0
1385_0001169	1.0	0.0
1385_0001188	1.0	1.0
1385_0001522	0.0	1.0
1385_0001523	1.0	1.0
1385_0001716	1.0	1.0
1385_0001717	1.0	1.0
1385_0001719	1.0	1.0
1385_0001734	0.0	1.0
1385_0001749	0.0	1.0
1385_0001753	1.0	1.0
1385_0001756	1.0	1.0
1385_0001764	0.0	1.0
1385_0001767	0.0	1.0
1385_0001768	1.0	1.0
1385_0001772	1.0	1.0
1385_0001774	0.0	0.0
1385_0001788	0.0	1.0
1385_0001793	0.0	1.0
1395_0000338	1.0	1.0
1395_0000359	1.0	1.0
1395_0000360	2.0	1.0
1395_0000369	2.0	1.0
1395_0000376	2.0	1.0
1395_0000378	1.0	1.0
1395_0000379	1.0	1.0
1395_0000398	2.0	1.0
1395_0000432	1.0	1.0
1395_0000447	1.0	1.0
1395_0000448	1.0	1.0
1395_0000450	1.0	1.0
1395_0000465	1.0	1.0
1395_0000471	1.0	1.0
1395_0000512	1.0	1.0
1395_0000527	0.0	1.0
1395_0000529	1.0	1.0
1395_0000534	1.0	1.0
1395_0000535	1.0	1.0
1395_0000549	1.0	1.0
1395_0000553	1.0	1.0
1395_0000557	2.0	1.0
1395_0000559	1.0	1.0
1395_0000560	2.0	1.0
1395_0000563	1.0	1.0
1395_0000579	1.0	0.0
1395_0000596	2.0	1.0
1395_0000597	1.0	1.0
1395_0000604	0.0	0.0
1395_0000607	0.0	1.0
1395_0000612	1.0	1.0
1395_0000630	0.0	1.0
1395_0000646	0.0	1.0
1395_0000649	2.0	1.0
1395_0001016	1.0	1.0
1395_0001020	1.0	1.0
1395_0001034	0.0	1.0
1395_0001066	0.0	1.0
1395_0001067	0.0	1.0
1395_0001080	1.0	1.0
1395_0001103	0.0	1.0
1395_0001109	0.0	1.0
1395_0001115	1.0	1.0
1395_0001118	0.0	0.0
1395_0001132	2.0	1.0
1395_0001133	1.0	1.0
1395_0001147	0.0	1.0
Averaged weighted F1-scores 0.664091353332475
LANGUAGE: DE
130.68810916179336 82.83744971317162
LANGUAGE: CZ
144.90552995391704 65.35717405024758
LANGUAGE: IT
148.5775 138.9822884174455
LABEL SET ['A1', 'A2', 'B1', 'B2', 'C1']
1 Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 1.05
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.80
              precision    recall  f1-score   support

         0.0       1.00      0.64      0.78        74
         1.0       0.60      0.77      0.68       140
         2.0       0.70      0.61      0.65       146
         3.0       0.66      0.83      0.74        78
         4.0       0.00      0.00      0.00        14

    accuracy                           0.68       452
   macro avg       0.59      0.57      0.57       452
weighted avg       0.69      0.68      0.67       452

[[ 47  26   1   0   0]
 [  0 108  30   2   0]
 [  0  40  89  17   0]
 [  0   5   8  65   0]
 [  0   0   0  14   0]]
0.6742128169079706
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 47
Elapsed time 59

  Average training loss: 0.82
  Training epoch took: 67
Running Validation...
  Average evaluation loss: 0.78
              precision    recall  f1-score   support

         0.0       0.98      0.64      0.77        74
         1.0       0.64      0.74      0.68       140
         2.0       0.69      0.69      0.69       146
         3.0       0.68      0.85      0.75        78
         4.0       0.00      0.00      0.00        14

    accuracy                           0.70       452
   macro avg       0.60      0.58      0.58       452
weighted avg       0.70      0.70      0.69       452

[[ 47  24   3   0   0]
 [  1 103  34   2   0]
 [  0  30 101  15   0]
 [  0   4   8  66   0]
 [  0   0   0  14   0]]
0.6917359129320372
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.72
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.78
              precision    recall  f1-score   support

         0.0       1.00      0.64      0.78        74
         1.0       0.68      0.74      0.71       140
         2.0       0.69      0.71      0.70       146
         3.0       0.65      0.86      0.74        78
         4.0       0.00      0.00      0.00        14

    accuracy                           0.71       452
   macro avg       0.60      0.59      0.59       452
weighted avg       0.71      0.71      0.70       452

[[ 47  23   4   0   0]
 [  0 103  35   2   0]
 [  0  22 104  20   0]
 [  0   4   7  67   0]
 [  0   0   0  14   0]]
0.700431808720447
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.64
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.77
              precision    recall  f1-score   support

         0.0       0.96      0.64      0.76        74
         1.0       0.69      0.75      0.72       140
         2.0       0.72      0.73      0.72       146
         3.0       0.65      0.86      0.74        78
         4.0       0.00      0.00      0.00        14

    accuracy                           0.72       452
   macro avg       0.60      0.59      0.59       452
weighted avg       0.71      0.72      0.71       452

[[ 47  24   3   0   0]
 [  1 105  32   2   0]
 [  1  19 106  20   0]
 [  0   4   7  67   0]
 [  0   0   0  14   0]]
0.7085454534369946
452 452 452
Filename	True Label	Prediction
1023_0001420	3.0	3.0
1023_0001423	2.0	3.0
1023_0001575	3.0	3.0
1023_0101675	3.0	3.0
1023_0101691	3.0	3.0
1023_0101695	2.0	3.0
1023_0101749	3.0	3.0
1023_0101847	3.0	3.0
1023_0101848	2.0	2.0
1023_0101854	2.0	2.0
1023_0101895	3.0	3.0
1023_0101897	2.0	3.0
1023_0101904	2.0	3.0
1023_0103821	3.0	3.0
1023_0103823	3.0	3.0
1023_0103837	3.0	3.0
1023_0103880	3.0	3.0
1023_0104207	3.0	3.0
1023_0107787	3.0	3.0
1023_0108304	4.0	3.0
1023_0108423	3.0	3.0
1023_0108641	4.0	3.0
1023_0108650	3.0	3.0
1023_0108752	4.0	3.0
1023_0108753	2.0	3.0
1023_0108815	3.0	3.0
1023_0108888	3.0	3.0
1023_0108890	3.0	3.0
1023_0108993	4.0	3.0
1023_0109030	3.0	3.0
1023_0109039	3.0	3.0
1023_0109396	2.0	3.0
1023_0109651	3.0	3.0
1023_0109671	3.0	3.0
1023_0109717	3.0	3.0
1023_0109890	4.0	3.0
1023_0109891	3.0	3.0
1023_0109951	3.0	3.0
1023_0109954	3.0	3.0
1031_0001949	3.0	3.0
1031_0001951	2.0	3.0
1031_0002003	3.0	3.0
1031_0002006	4.0	3.0
1031_0002040	4.0	3.0
1031_0002083	3.0	3.0
1031_0002088	3.0	3.0
1031_0002131	3.0	3.0
1031_0002195	3.0	3.0
1031_0003023	3.0	3.0
1031_0003048	4.0	3.0
1031_0003071	3.0	3.0
1031_0003074	3.0	3.0
1031_0003077	3.0	3.0
1031_0003088	4.0	3.0
1031_0003098	4.0	3.0
1031_0003128	3.0	3.0
1031_0003144	3.0	3.0
1031_0003149	3.0	3.0
1031_0003156	3.0	3.0
1031_0003165	2.0	3.0
1031_0003170	3.0	3.0
1031_0003173	3.0	3.0
1031_0003179	3.0	3.0
1031_0003189	4.0	3.0
1031_0003207	4.0	3.0
1031_0003211	2.0	3.0
1031_0003218	3.0	3.0
1031_0003262	3.0	3.0
1031_0003309	3.0	3.0
1031_0003315	4.0	3.0
1031_0003330	3.0	3.0
1031_0003331	3.0	3.0
1031_0003354	3.0	3.0
1031_0003357	3.0	3.0
1031_0003358	4.0	3.0
1031_0003367	3.0	3.0
1031_0003386	2.0	3.0
1031_0003389	3.0	3.0
1031_0003393	3.0	3.0
1061_0120274	2.0	2.0
1061_0120289	2.0	2.0
1061_0120295	0.0	2.0
1061_0120302	1.0	2.0
1061_0120303	1.0	1.0
1061_0120304	2.0	2.0
1061_0120306	3.0	2.0
1061_0120310	2.0	2.0
1061_0120320	3.0	3.0
1061_0120321	2.0	2.0
1061_0120325	3.0	3.0
1061_0120326	3.0	3.0
1061_0120331	1.0	1.0
1061_0120335	3.0	3.0
1061_0120336	1.0	2.0
1061_0120348	1.0	1.0
1061_0120350	3.0	3.0
1061_0120356	2.0	3.0
1061_0120358	1.0	2.0
1061_0120368	2.0	2.0
1061_0120374	3.0	3.0
1061_0120383	2.0	3.0
1061_0120409	3.0	3.0
1061_0120411	3.0	3.0
1061_0120426	2.0	3.0
1061_0120427	2.0	3.0
1061_0120442	3.0	3.0
1061_0120455	2.0	2.0
1061_0120489	3.0	3.0
1061_0120856	2.0	2.0
1061_0120857	2.0	3.0
1061_0120875	3.0	3.0
1061_0120880	3.0	3.0
1061_0120881	2.0	3.0
1061_0120882	3.0	3.0
1061_1029112	3.0	3.0
1061_1029115	2.0	2.0
1061_1029119	1.0	2.0
1061_1202911	1.0	2.0
1061_1202918	2.0	3.0
1071_0024680	2.0	2.0
1071_0024686	2.0	2.0
1071_0024687	0.0	1.0
1071_0024690	2.0	3.0
1071_0024699	1.0	2.0
1071_0024704	1.0	1.0
1071_0024706	1.0	1.0
1071_0024712	1.0	1.0
1071_0024713	2.0	1.0
1071_0024757	2.0	2.0
1071_0024758	2.0	2.0
1071_0024778	0.0	1.0
1071_0024784	1.0	1.0
1071_0024808	1.0	1.0
1071_0024812	1.0	1.0
1071_0024813	0.0	1.0
1071_0024818	2.0	1.0
1071_0024819	1.0	1.0
1071_0024821	1.0	1.0
1071_0024824	1.0	1.0
1071_0024826	1.0	1.0
1071_0024837	1.0	1.0
1071_0024843	1.0	1.0
1071_0024844	1.0	1.0
1071_0024847	2.0	2.0
1071_0024849	0.0	1.0
1071_0024864	0.0	1.0
1071_0024873	1.0	1.0
1071_0024879	1.0	1.0
1071_0242012	2.0	2.0
1071_0242013	1.0	1.0
1071_0242022	1.0	1.0
1071_0242023	1.0	1.0
1071_0242071	0.0	1.0
1071_0242093	0.0	1.0
1071_0243621	2.0	2.0
1071_0243623	1.0	1.0
1071_0248301	1.0	1.0
1071_0248303	1.0	1.0
1071_0248309	2.0	1.0
1071_0248315	0.0	1.0
1071_0248317	0.0	1.0
1071_0248329	1.0	1.0
1071_0248334	2.0	2.0
1071_0248347	1.0	1.0
1091_0000010	3.0	2.0
1091_0000013	1.0	1.0
1091_0000016	1.0	1.0
1091_0000021	1.0	2.0
1091_0000031	1.0	1.0
1091_0000039	1.0	1.0
1091_0000045	2.0	2.0
1091_0000054	0.0	1.0
1091_0000062	3.0	1.0
1091_0000066	2.0	0.0
1091_0000067	2.0	1.0
1091_0000068	1.0	2.0
1091_0000069	1.0	1.0
1091_0000072	1.0	2.0
1091_0000073	3.0	1.0
1091_0000074	2.0	1.0
1091_0000076	2.0	2.0
1091_0000078	3.0	1.0
1091_0000079	1.0	2.0
1091_0000140	1.0	1.0
1091_0000144	1.0	1.0
1091_0000152	1.0	1.0
1091_0000154	1.0	3.0
1091_0000155	2.0	3.0
1091_0000159	2.0	3.0
1091_0000166	1.0	1.0
1091_0000167	1.0	3.0
1091_0000169	2.0	2.0
1091_0000185	2.0	1.0
1091_0000193	2.0	1.0
1091_0000196	2.0	1.0
1091_0000197	1.0	2.0
1091_0000198	2.0	2.0
1091_0000210	2.0	2.0
1091_0000215	1.0	2.0
1091_0000227	0.0	2.0
1091_0000235	1.0	1.0
1091_0000236	2.0	2.0
1091_0000238	1.0	2.0
1091_0000241	2.0	1.0
1091_0000244	2.0	2.0
1091_0000245	1.0	2.0
1091_0000257	1.0	2.0
0621	2.0	2.0
0622	1.0	1.0
0627	2.0	2.0
0631	2.0	2.0
0635	2.0	2.0
0645	2.0	2.0
0714	2.0	2.0
0715	2.0	1.0
0721	2.0	2.0
0805	2.0	2.0
0811	2.0	2.0
0814	1.0	1.0
0815	2.0	2.0
0816	3.0	1.0
0823	1.0	1.0
0824	1.0	2.0
0829	1.0	2.0
0904	1.0	1.0
0905	1.0	2.0
0911	2.0	2.0
0919	2.0	2.0
0930	1.0	1.0
1006	1.0	1.0
1020	1.0	1.0
1114	1.0	1.0
BER0609003	0.0	0.0
BER0611005	0.0	0.0
KYJ0611004A	1.0	1.0
KYJ0611009A	1.0	1.0
LIB0611001A	0.0	1.0
LIB0611001B	0.0	0.0
LIB0611002A	0.0	1.0
LIB0611004A	1.0	1.0
LIB0611011	0.0	0.0
LON0611002B	0.0	0.0
MOS0611013	0.0	0.0
PHA0111001A	1.0	1.0
PHA0111003A	1.0	1.0
PHA0111003B	0.0	0.0
PHA0111004A	1.0	1.0
PHA0111005A	1.0	1.0
PHA0112007B	0.0	0.0
PHA0112012A	1.0	0.0
PHA0112012B	0.0	0.0
PHA0209024	0.0	0.0
PHA0210007	0.0	0.0
PHA0411008B	0.0	0.0
PHA0411012A	1.0	1.0
PHA0411028	0.0	0.0
PHA0411031	0.0	0.0
PHA0411035	0.0	0.0
PHA0411038	0.0	0.0
PHA0411039	0.0	0.0
PHA0411041	0.0	0.0
PHA0411043	0.0	0.0
PHA0411056	0.0	0.0
PHA0411061	0.0	0.0
PHA0509022	0.0	0.0
PHA0509037	0.0	0.0
PHA0509040	0.0	0.0
PHA0510002B	0.0	0.0
PHA0510010A	1.0	1.0
PHA0510032	0.0	0.0
PHA0510038	0.0	0.0
PHA0510040	0.0	0.0
PHA0510046	0.0	0.0
PHA0510049	0.0	0.0
PHA0610005A	1.0	1.0
PHA0610005B	0.0	0.0
PHA0610006A	0.0	1.0
PHA0610006B	0.0	0.0
PHA0610019A	0.0	1.0
PHA0610019B	0.0	0.0
PHA0810004	0.0	0.0
PHA0810015	0.0	0.0
PHA0811012	0.0	0.0
PHA0811019	0.0	0.0
PHA1109003	0.0	0.0
PHA1109005	0.0	0.0
PHA1109007	0.0	0.0
PHA1109024	0.0	0.0
PHA1110003B	0.0	0.0
PHA1110004A	1.0	1.0
PHA1111002B	0.0	0.0
PHA1111004B	0.0	0.0
PHA1111006A	0.0	1.0
VAR0909009	0.0	0.0
VAR0910005	0.0	0.0
VAR0910011	0.0	0.0
1325_1001008	2.0	2.0
1325_1001012	2.0	2.0
1325_1001013	3.0	2.0
1325_1001020	2.0	2.0
1325_1001021	2.0	2.0
1325_1001027	2.0	2.0
1325_1001047	2.0	2.0
1325_1001062	2.0	2.0
1325_1001077	2.0	2.0
1325_1001088	2.0	2.0
1325_1001089	2.0	2.0
1325_1001093	2.0	2.0
1325_1001095	2.0	2.0
1325_1001096	2.0	2.0
1325_1001123	2.0	2.0
1325_1001126	2.0	2.0
1325_1001127	2.0	2.0
1325_1001144	2.0	2.0
1325_1001152	2.0	2.0
1325_1001163	1.0	2.0
1325_9000088	2.0	2.0
1325_9000107	3.0	2.0
1325_9000136	2.0	2.0
1325_9000139	2.0	2.0
1325_9000144	3.0	2.0
1325_9000215	2.0	2.0
1325_9000240	2.0	2.0
1325_9000296	2.0	2.0
1325_9000315	1.0	2.0
1325_9000503	3.0	2.0
1325_9000533	2.0	2.0
1325_9000601	2.0	2.0
1325_9000676	2.0	2.0
1325_9000686	2.0	2.0
1365_0100009	2.0	2.0
1365_0100012	2.0	2.0
1365_0100018	2.0	2.0
1365_0100021	2.0	2.0
1365_0100026	2.0	1.0
1365_0100073	2.0	2.0
1365_0100080	2.0	2.0
1365_0100101	2.0	2.0
1365_0100107	2.0	2.0
1365_0100118	2.0	2.0
1365_0100125	2.0	2.0
1365_0100133	2.0	2.0
1365_0100137	2.0	2.0
1365_0100145	2.0	2.0
1365_0100164	1.0	2.0
1365_0100166	2.0	2.0
1365_0100168	2.0	2.0
1365_0100169	2.0	2.0
1365_0100171	1.0	2.0
1365_0100175	2.0	2.0
1365_0100177	2.0	2.0
1365_0100190	2.0	2.0
1365_0100192	3.0	2.0
1365_0100195	1.0	2.0
1365_0100200	2.0	2.0
1365_0100203	2.0	2.0
1365_0100215	2.0	2.0
1365_0100225	2.0	2.0
1365_0100227	2.0	2.0
1365_0100228	2.0	2.0
1365_0100229	2.0	2.0
1365_0100231	2.0	2.0
1365_0100255	1.0	2.0
1365_0100260	1.0	2.0
1365_0100261	2.0	2.0
1365_0100274	2.0	2.0
1365_0100286	1.0	2.0
1365_0100448	2.0	2.0
1365_0100457	2.0	2.0
1365_0100475	2.0	2.0
1385_0000012	1.0	1.0
1385_0000033	1.0	1.0
1385_0000042	2.0	2.0
1385_0000044	2.0	2.0
1385_0000045	2.0	2.0
1385_0000052	2.0	1.0
1385_0000095	1.0	1.0
1385_0000119	2.0	2.0
1385_0000127	1.0	1.0
1385_0001107	2.0	1.0
1385_0001108	1.0	1.0
1385_0001121	1.0	1.0
1385_0001123	1.0	1.0
1385_0001130	1.0	1.0
1385_0001131	1.0	1.0
1385_0001133	1.0	1.0
1385_0001156	2.0	2.0
1385_0001161	2.0	2.0
1385_0001171	1.0	1.0
1385_0001173	0.0	1.0
1385_0001193	1.0	1.0
1385_0001196	1.0	1.0
1385_0001198	1.0	1.0
1385_0001503	1.0	2.0
1385_0001527	1.0	1.0
1385_0001715	0.0	1.0
1385_0001718	1.0	1.0
1385_0001720	0.0	1.0
1385_0001724	1.0	1.0
1385_0001737	2.0	2.0
1385_0001738	0.0	1.0
1385_0001748	1.0	1.0
1385_0001760	0.0	1.0
1385_0001765	0.0	1.0
1385_0001767	1.0	1.0
1385_0001772	0.0	1.0
1385_0001775	1.0	1.0
1385_0001785	0.0	1.0
1385_0001794	1.0	1.0
1385_0001795	1.0	1.0
1395_0000355	1.0	1.0
1395_0000357	2.0	2.0
1395_0000383	1.0	1.0
1395_0000389	1.0	1.0
1395_0000398	1.0	1.0
1395_0000402	1.0	1.0
1395_0000415	1.0	1.0
1395_0000448	1.0	1.0
1395_0000450	1.0	1.0
1395_0000454	2.0	1.0
1395_0000470	1.0	1.0
1395_0000471	1.0	1.0
1395_0000513	1.0	1.0
1395_0000518	2.0	1.0
1395_0000529	2.0	1.0
1395_0000535	1.0	1.0
1395_0000555	2.0	1.0
1395_0000559	1.0	1.0
1395_0000565	1.0	1.0
1395_0000610	2.0	1.0
1395_0000612	1.0	1.0
1395_0000639	0.0	2.0
1395_0000642	1.0	1.0
1395_0001015	1.0	1.0
1395_0001016	1.0	1.0
1395_0001021	1.0	1.0
1395_0001045	2.0	1.0
1395_0001061	1.0	2.0
1395_0001067	1.0	1.0
1395_0001069	1.0	1.0
1395_0001070	1.0	2.0
1395_0001071	1.0	1.0
1395_0001076	1.0	1.0
1395_0001104	1.0	1.0
1395_0001114	1.0	1.0
1395_0001118	1.0	1.0
1395_0001119	1.0	2.0
1395_0001120	1.0	1.0
1395_0001124	0.0	1.0
1395_0001132	1.0	2.0
1395_0001145	1.0	2.0
1395_0001150	1.0	1.0
2 Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.98
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.85
              precision    recall  f1-score   support

         0.0       1.00      0.65      0.79        74
         1.0       0.62      0.76      0.68       140
         2.0       0.65      0.63      0.64       147
         3.0       0.61      0.73      0.66        77
         4.0       0.00      0.00      0.00        14

    accuracy                           0.67       452
   macro avg       0.58      0.55      0.55       452
weighted avg       0.67      0.67      0.66       452

[[ 48  25   1   0   0]
 [  0 106  33   1   0]
 [  0  34  92  21   0]
 [  0   5  16  56   0]
 [  0   0   0  14   0]]
0.6606033738520772
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.78
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.79
              precision    recall  f1-score   support

         0.0       0.91      0.69      0.78        74
         1.0       0.68      0.63      0.65       140
         2.0       0.60      0.76      0.67       147
         3.0       0.65      0.68      0.66        77
         4.0       0.00      0.00      0.00        14

    accuracy                           0.67       452
   macro avg       0.57      0.55      0.55       452
weighted avg       0.67      0.67      0.66       452

[[ 51  20   3   0   0]
 [  3  88  49   0   0]
 [  0  21 112  14   0]
 [  2   1  22  52   0]
 [  0   0   0  14   0]]
0.6619691372354694
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.65
  Training epoch took: 65
Running Validation...
  Average evaluation loss: 0.81
              precision    recall  f1-score   support

         0.0       0.91      0.68      0.78        74
         1.0       0.67      0.70      0.68       140
         2.0       0.66      0.70      0.68       147
         3.0       0.64      0.78      0.70        77
         4.0       0.00      0.00      0.00        14

    accuracy                           0.69       452
   macro avg       0.57      0.57      0.57       452
weighted avg       0.68      0.69      0.68       452

[[ 50  23   1   0   0]
 [  3  98  38   1   0]
 [  0  25 103  19   0]
 [  2   1  14  60   0]
 [  0   0   0  14   0]]
0.679092431330833
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.52
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.86
              precision    recall  f1-score   support

         0.0       0.91      0.68      0.78        74
         1.0       0.66      0.61      0.63       140
         2.0       0.60      0.69      0.64       147
         3.0       0.62      0.75      0.68        77
         4.0       0.67      0.14      0.24        14

    accuracy                           0.66       452
   macro avg       0.69      0.57      0.59       452
weighted avg       0.67      0.66      0.65       452

[[ 50  22   2   0   0]
 [  3  85  51   1   0]
 [  0  22 102  23   0]
 [  2   0  16  58   1]
 [  0   0   0  12   2]]
0.6541373895678424
452 452 452
Filename	True Label	Prediction
1023_0001416	3.0	3.0
1023_0101753	3.0	3.0
1023_0101843	2.0	3.0
1023_0101849	3.0	3.0
1023_0101893	3.0	3.0
1023_0101894	2.0	3.0
1023_0101896	2.0	2.0
1023_0101900	3.0	3.0
1023_0101901	3.0	3.0
1023_0101907	3.0	3.0
1023_0101909	4.0	3.0
1023_0102117	3.0	3.0
1023_0102118	3.0	3.0
1023_0103825	2.0	3.0
1023_0103826	3.0	3.0
1023_0103827	3.0	3.0
1023_0103828	1.0	2.0
1023_0103832	2.0	3.0
1023_0103838	3.0	3.0
1023_0104203	3.0	3.0
1023_0104209	3.0	3.0
1023_0107074	4.0	3.0
1023_0107682	2.0	2.0
1023_0107725	3.0	3.0
1023_0107727	3.0	3.0
1023_0108426	2.0	3.0
1023_0108510	3.0	3.0
1023_0108810	3.0	3.0
1023_0108932	3.0	3.0
1023_0108934	3.0	3.0
1023_0109029	2.0	3.0
1023_0109033	4.0	3.0
1023_0109248	3.0	3.0
1023_0109267	3.0	3.0
1023_0109399	2.0	3.0
1023_0109518	2.0	3.0
1023_0109520	3.0	3.0
1023_0109528	3.0	3.0
1023_0109588	2.0	3.0
1023_0109614	2.0	3.0
1023_0109721	2.0	3.0
1023_0109947	3.0	3.0
1031_0002004	4.0	3.0
1031_0002005	4.0	3.0
1031_0002042	3.0	3.0
1031_0002061	3.0	3.0
1031_0002085	3.0	3.0
1031_0002091	3.0	3.0
1031_0002197	3.0	3.0
1031_0003013	4.0	3.0
1031_0003042	3.0	3.0
1031_0003072	3.0	3.0
1031_0003073	4.0	4.0
1031_0003097	3.0	3.0
1031_0003126	4.0	3.0
1031_0003127	4.0	3.0
1031_0003131	3.0	4.0
1031_0003132	3.0	3.0
1031_0003133	4.0	3.0
1031_0003135	3.0	3.0
1031_0003141	3.0	3.0
1031_0003154	3.0	3.0
1031_0003157	4.0	3.0
1031_0003169	3.0	3.0
1031_0003184	4.0	3.0
1031_0003206	3.0	3.0
1031_0003224	2.0	3.0
1031_0003225	3.0	3.0
1031_0003234	2.0	3.0
1031_0003242	2.0	3.0
1031_0003243	3.0	3.0
1031_0003244	4.0	3.0
1031_0003336	3.0	3.0
1031_0003365	3.0	3.0
1031_0003366	3.0	3.0
1031_0003383	3.0	3.0
1031_0003415	4.0	4.0
1061_0120277	1.0	2.0
1061_0120278	2.0	2.0
1061_0120282	0.0	1.0
1061_0120286	1.0	1.0
1061_0120288	2.0	3.0
1061_0120290	1.0	2.0
1061_0120297	2.0	2.0
1061_0120309	1.0	1.0
1061_0120312	1.0	2.0
1061_0120316	2.0	2.0
1061_0120319	3.0	3.0
1061_0120333	3.0	3.0
1061_0120345	3.0	2.0
1061_0120346	2.0	2.0
1061_0120352	1.0	1.0
1061_0120359	1.0	2.0
1061_0120360	3.0	3.0
1061_0120367	3.0	3.0
1061_0120369	2.0	2.0
1061_0120376	2.0	2.0
1061_0120382	2.0	2.0
1061_0120388	2.0	2.0
1061_0120394	2.0	3.0
1061_0120404	2.0	2.0
1061_0120406	2.0	3.0
1061_0120407	3.0	3.0
1061_0120408	3.0	3.0
1061_0120413	1.0	2.0
1061_0120431	3.0	3.0
1061_0120440	1.0	2.0
1061_0120448	3.0	3.0
1061_0120457	3.0	2.0
1061_0120481	3.0	3.0
1061_0120482	2.0	3.0
1061_0120486	2.0	3.0
1061_0120487	3.0	2.0
1061_0120491	2.0	2.0
1061_0120492	3.0	3.0
1061_0120495	2.0	3.0
1061_0120498	3.0	3.0
1061_0120855	1.0	2.0
1061_0120859	3.0	3.0
1061_0120887	2.0	2.0
1061_0120894	2.0	3.0
1061_1029113	1.0	2.0
1061_1029118	2.0	2.0
1061_1029120	2.0	2.0
1061_1202914	1.0	2.0
1061_1202917	2.0	2.0
1071_0024691	2.0	3.0
1071_0024692	3.0	2.0
1071_0024703	1.0	2.0
1071_0024711	2.0	1.0
1071_0024714	2.0	2.0
1071_0024769	1.0	1.0
1071_0024772	0.0	0.0
1071_0024774	0.0	0.0
1071_0024775	0.0	2.0
1071_0024777	1.0	1.0
1071_0024798	1.0	1.0
1071_0024802	1.0	2.0
1071_0024814	0.0	1.0
1071_0024815	1.0	2.0
1071_0024816	1.0	1.0
1071_0024827	1.0	1.0
1071_0024833	2.0	2.0
1071_0024846	1.0	1.0
1071_0024853	0.0	1.0
1071_0024854	1.0	1.0
1071_0024855	1.0	1.0
1071_0024861	0.0	1.0
1071_0024862	1.0	2.0
1071_0024876	1.0	1.0
1071_0241833	1.0	1.0
1071_0242092	0.0	0.0
1071_0243582	1.0	2.0
1071_0243593	1.0	2.0
1071_0248304	1.0	1.0
1071_0248310	1.0	1.0
1071_0248312	1.0	1.0
1071_0248318	0.0	0.0
1071_0248319	0.0	1.0
1071_0248323	1.0	1.0
1071_0248324	0.0	1.0
1071_0248325	1.0	2.0
1071_0248327	0.0	1.0
1071_0248330	1.0	2.0
1071_0248332	2.0	2.0
1071_0248342	1.0	1.0
1071_0248343	1.0	1.0
1071_0248346	0.0	1.0
1091_0000007	2.0	2.0
1091_0000008	3.0	2.0
1091_0000009	0.0	1.0
1091_0000012	1.0	1.0
1091_0000014	1.0	1.0
1091_0000023	3.0	0.0
1091_0000033	1.0	2.0
1091_0000037	1.0	1.0
1091_0000038	2.0	1.0
1091_0000042	1.0	0.0
1091_0000044	0.0	1.0
1091_0000046	2.0	2.0
1091_0000050	1.0	1.0
1091_0000052	1.0	1.0
1091_0000059	1.0	2.0
1091_0000092	1.0	2.0
1091_0000125	3.0	2.0
1091_0000145	1.0	1.0
1091_0000156	3.0	2.0
1091_0000160	1.0	3.0
1091_0000161	2.0	2.0
1091_0000163	2.0	1.0
1091_0000194	2.0	2.0
1091_0000204	2.0	2.0
1091_0000206	1.0	2.0
1091_0000211	1.0	2.0
1091_0000213	2.0	2.0
1091_0000224	2.0	1.0
1091_0000225	3.0	0.0
1091_0000226	1.0	2.0
1091_0000239	3.0	2.0
1091_0000248	3.0	2.0
1091_0000259	2.0	2.0
1091_0000260	2.0	3.0
1091_0000263	3.0	3.0
1091_0000264	1.0	1.0
1091_0000266	1.0	2.0
1091_0000274	2.0	2.0
0601	1.0	2.0
0602	1.0	2.0
0605	2.0	2.0
0606	2.0	2.0
0609	1.0	2.0
0611	1.0	2.0
0612	2.0	2.0
0613	0.0	1.0
0618	2.0	2.0
0625	1.0	1.0
0626	2.0	2.0
0630	0.0	1.0
0633	2.0	2.0
0801	1.0	1.0
0804	2.0	2.0
0806	2.0	1.0
0807	2.0	1.0
0809	1.0	2.0
0813	2.0	1.0
0818	1.0	1.0
0819	3.0	2.0
0825	1.0	2.0
0826	2.0	1.0
0902	2.0	2.0
0914	1.0	2.0
0916	1.0	2.0
0923	2.0	2.0
0924	1.0	1.0
0928	2.0	1.0
1004	1.0	1.0
1010	1.0	1.0
1014	1.0	1.0
1017	1.0	1.0
1021	1.0	1.0
1022	1.0	1.0
1111	1.0	1.0
KYJ0611003A	1.0	1.0
LIB0611003A	1.0	1.0
LON0610002A	1.0	1.0
LON0611004B	0.0	0.0
MOS0611014	0.0	0.0
PHA0111002A	0.0	1.0
PHA0111004B	0.0	0.0
PHA0111012	0.0	0.0
PHA0111016	0.0	0.0
PHA0112009A	2.0	1.0
PHA0209034	0.0	0.0
PHA0210008	0.0	0.0
PHA0411011B	0.0	0.0
PHA0411030	0.0	0.0
PHA0411037	0.0	0.0
PHA0411042	0.0	0.0
PHA0411047	0.0	0.0
PHA0411054	0.0	0.0
PHA0411062	0.0	0.0
PHA0509007	0.0	0.0
PHA0509027	0.0	0.0
PHA0509030	0.0	0.0
PHA0509032	0.0	0.0
PHA0509035	0.0	0.0
PHA0509038	0.0	0.0
PHA0509039	0.0	0.0
PHA0510013A	1.0	1.0
PHA0510023	0.0	0.0
PHA0510027	0.0	0.0
PHA0510031	0.0	0.0
PHA0510034	0.0	0.0
PHA0510037	0.0	0.0
PHA0510047	0.0	0.0
PHA0510048	0.0	0.0
PHA0709008	0.0	0.0
PHA0710012	0.0	0.0
PHA0710014	0.0	0.0
PHA0710018	0.0	0.0
PHA0810001	0.0	0.0
PHA0810006	0.0	0.0
PHA0810008	0.0	0.0
PHA0810011	0.0	0.0
PHA0810012	0.0	0.0
PHA0811010	0.0	0.0
PHA0811013	0.0	0.0
PHA0811014	0.0	0.0
PHA1110002A	1.0	2.0
PHA1111009A	0.0	1.0
ST071122B	0.0	0.0
TI071122B	0.0	0.0
VAR0209036	0.0	0.0
VAR0910006	0.0	0.0
VAR0910009	0.0	0.0
1325_1001009	2.0	2.0
1325_1001014	2.0	2.0
1325_1001039	2.0	2.0
1325_1001044	2.0	2.0
1325_1001045	2.0	2.0
1325_1001059	2.0	2.0
1325_1001078	2.0	2.0
1325_1001086	2.0	2.0
1325_1001091	2.0	2.0
1325_1001098	2.0	2.0
1325_1001099	3.0	2.0
1325_1001100	2.0	2.0
1325_1001101	3.0	2.0
1325_1001122	2.0	2.0
1325_1001131	2.0	2.0
1325_1001135	2.0	2.0
1325_1001141	1.0	2.0
1325_1001157	2.0	2.0
1325_1001164	2.0	2.0
1325_1001166	2.0	2.0
1325_1001169	2.0	2.0
1325_9000089	2.0	2.0
1325_9000099	2.0	2.0
1325_9000105	2.0	2.0
1325_9000138	2.0	2.0
1325_9000214	2.0	2.0
1325_9000239	3.0	2.0
1325_9000304	2.0	2.0
1325_9000323	1.0	2.0
1325_9000536	3.0	2.0
1325_9000611	2.0	2.0
1325_9000700	2.0	2.0
1325_9000750	3.0	2.0
1365_0100004	1.0	2.0
1365_0100006	2.0	2.0
1365_0100008	1.0	2.0
1365_0100011	2.0	1.0
1365_0100013	2.0	2.0
1365_0100015	2.0	2.0
1365_0100024	1.0	1.0
1365_0100027	2.0	2.0
1365_0100028	2.0	2.0
1365_0100029	2.0	1.0
1365_0100051	1.0	2.0
1365_0100057	1.0	2.0
1365_0100061	3.0	2.0
1365_0100067	1.0	2.0
1365_0100070	2.0	2.0
1365_0100092	2.0	2.0
1365_0100093	2.0	2.0
1365_0100094	2.0	2.0
1365_0100096	2.0	2.0
1365_0100098	2.0	2.0
1365_0100116	2.0	2.0
1365_0100147	2.0	1.0
1365_0100162	2.0	2.0
1365_0100163	2.0	2.0
1365_0100174	1.0	2.0
1365_0100181	1.0	1.0
1365_0100182	2.0	2.0
1365_0100184	2.0	2.0
1365_0100188	1.0	2.0
1365_0100191	1.0	2.0
1365_0100230	2.0	2.0
1365_0100233	2.0	2.0
1365_0100256	2.0	2.0
1365_0100267	2.0	2.0
1365_0100270	2.0	2.0
1365_0100278	2.0	2.0
1365_0100289	2.0	2.0
1365_0100458	2.0	2.0
1365_0100459	2.0	2.0
1365_0100469	2.0	2.0
1365_0100470	2.0	2.0
1365_0100474	2.0	2.0
1365_0100479	2.0	2.0
1365_0100480	2.0	2.0
1365_0100482	2.0	2.0
1385_0000011	1.0	1.0
1385_0000016	1.0	1.0
1385_0000023	1.0	0.0
1385_0000037	0.0	1.0
1385_0000058	2.0	2.0
1385_0000059	2.0	2.0
1385_0000099	1.0	1.0
1385_0000100	1.0	0.0
1385_0000120	0.0	0.0
1385_0000123	1.0	2.0
1385_0000129	2.0	1.0
1385_0000130	2.0	1.0
1385_0001109	1.0	1.0
1385_0001113	1.0	1.0
1385_0001124	1.0	1.0
1385_0001127	1.0	1.0
1385_0001136	1.0	1.0
1385_0001138	1.0	1.0
1385_0001152	2.0	2.0
1385_0001153	2.0	1.0
1385_0001155	2.0	2.0
1385_0001157	2.0	1.0
1385_0001158	2.0	2.0
1385_0001159	2.0	2.0
1385_0001164	1.0	1.0
1385_0001174	1.0	1.0
1385_0001189	0.0	1.0
1385_0001191	1.0	1.0
1385_0001501	0.0	2.0
1385_0001522	1.0	1.0
1385_0001741	0.0	1.0
1385_0001744	0.0	1.0
1385_0001758	1.0	1.0
1385_0001759	1.0	1.0
1385_0001773	1.0	1.0
1385_0001793	1.0	1.0
1395_0000340	1.0	1.0
1395_0000360	2.0	1.0
1395_0000364	1.0	1.0
1395_0000369	2.0	1.0
1395_0000376	2.0	2.0
1395_0000380	1.0	2.0
1395_0000390	1.0	1.0
1395_0000403	1.0	1.0
1395_0000446	2.0	1.0
1395_0000452	1.0	1.0
1395_0000455	1.0	2.0
1395_0000460	1.0	1.0
1395_0000462	2.0	1.0
1395_0000514	2.0	2.0
1395_0000525	2.0	1.0
1395_0000526	1.0	1.0
1395_0000533	2.0	2.0
1395_0000549	1.0	2.0
1395_0000553	1.0	1.0
1395_0000556	1.0	2.0
1395_0000564	1.0	1.0
1395_0000598	1.0	1.0
1395_0000599	1.0	1.0
1395_0000602	1.0	1.0
1395_0000626	0.0	1.0
1395_0000644	1.0	1.0
1395_0000646	1.0	1.0
1395_0001022	1.0	1.0
1395_0001023	0.0	1.0
1395_0001024	0.0	1.0
1395_0001058	1.0	1.0
1395_0001064	1.0	1.0
1395_0001066	1.0	2.0
1395_0001068	0.0	1.0
1395_0001074	1.0	2.0
1395_0001090	1.0	1.0
1395_0001101	1.0	1.0
1395_0001103	1.0	1.0
1395_0001115	1.0	1.0
1395_0001133	1.0	1.0
1395_0001141	1.0	1.0
1395_0001160	1.0	1.0
1395_0001167	1.0	1.0
3 Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.98
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.81
              precision    recall  f1-score   support

         0.0       0.96      0.68      0.79        74
         1.0       0.60      0.69      0.64       140
         2.0       0.56      0.63      0.60       147
         3.0       0.63      0.61      0.62        77
         4.0       0.00      0.00      0.00        14

    accuracy                           0.63       452
   macro avg       0.55      0.52      0.53       452
weighted avg       0.63      0.63      0.63       452

[[50 23  1  0  0]
 [ 2 96 42  0  0]
 [ 0 40 93 14  0]
 [ 0  1 29 47  0]
 [ 0  0  0 14  0]]
0.627396449485411
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.78
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.83
              precision    recall  f1-score   support

         0.0       0.96      0.69      0.80        74
         1.0       0.65      0.64      0.64       140
         2.0       0.60      0.65      0.63       147
         3.0       0.63      0.86      0.73        77
         4.0       0.00      0.00      0.00        14

    accuracy                           0.67       452
   macro avg       0.57      0.57      0.56       452
weighted avg       0.66      0.67      0.66       452

[[51 20  3  0  0]
 [ 2 89 49  0  0]
 [ 0 27 96 24  0]
 [ 0  0 11 66  0]
 [ 0  0  0 14  0]]
0.6595418499753302
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.66
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.89
              precision    recall  f1-score   support

         0.0       0.87      0.73      0.79        74
         1.0       0.68      0.55      0.61       140
         2.0       0.58      0.69      0.63       147
         3.0       0.62      0.81      0.70        77
         4.0       0.00      0.00      0.00        14

    accuracy                           0.65       452
   macro avg       0.55      0.56      0.55       452
weighted avg       0.65      0.65      0.64       452

[[ 54  16   4   0   0]
 [  7  77  55   1   0]
 [  1  21 102  23   0]
 [  0   0  15  62   0]
 [  0   0   0  14   0]]
0.6425491233056397
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.54
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.91
              precision    recall  f1-score   support

         0.0       0.87      0.74      0.80        74
         1.0       0.68      0.62      0.65       140
         2.0       0.60      0.65      0.63       147
         3.0       0.60      0.79      0.69        77
         4.0       0.00      0.00      0.00        14

    accuracy                           0.66       452
   macro avg       0.55      0.56      0.55       452
weighted avg       0.65      0.66      0.65       452

[[55 16  3  0  0]
 [ 7 87 45  1  0]
 [ 1 25 96 25  0]
 [ 0  0 16 61  0]
 [ 0  0  0 14  0]]
0.6527029095161331
452 452 452
Filename	True Label	Prediction
1023_0001422	2.0	3.0
1023_0101683	3.0	2.0
1023_0101684	2.0	2.0
1023_0101688	3.0	3.0
1023_0101689	2.0	2.0
1023_0101700	3.0	3.0
1023_0101701	2.0	3.0
1023_0101751	3.0	3.0
1023_0101752	3.0	3.0
1023_0101851	3.0	3.0
1023_0101898	3.0	3.0
1023_0103822	2.0	2.0
1023_0103829	2.0	3.0
1023_0103831	3.0	3.0
1023_0103833	3.0	3.0
1023_0107075	3.0	3.0
1023_0107244	3.0	3.0
1023_0107740	3.0	3.0
1023_0107780	3.0	3.0
1023_0107783	3.0	3.0
1023_0107788	3.0	3.0
1023_0108305	3.0	3.0
1023_0108307	2.0	3.0
1023_0108648	3.0	3.0
1023_0108649	4.0	3.0
1023_0108751	3.0	3.0
1023_0108813	2.0	3.0
1023_0108886	3.0	3.0
1023_0108889	3.0	3.0
1023_0108908	3.0	3.0
1023_0108933	3.0	3.0
1023_0108955	4.0	3.0
1023_0108958	3.0	3.0
1023_0109026	2.0	3.0
1023_0109192	3.0	3.0
1023_0109249	3.0	3.0
1023_0109250	2.0	3.0
1023_0109400	3.0	2.0
1023_0109402	3.0	3.0
1023_0109515	3.0	3.0
1023_0109524	3.0	3.0
1023_0109527	3.0	3.0
1023_0109590	2.0	3.0
1023_0109649	3.0	3.0
1023_0109674	3.0	3.0
1023_0109878	3.0	3.0
1023_0109914	3.0	2.0
1023_0109946	2.0	3.0
1023_0111896	3.0	2.0
1031_0001703	3.0	3.0
1031_0002002	2.0	3.0
1031_0002010	3.0	3.0
1031_0002011	3.0	3.0
1031_0002036	4.0	3.0
1031_0002087	4.0	3.0
1031_0002184	3.0	3.0
1031_0002198	3.0	3.0
1031_0002200	2.0	3.0
1031_0003012	4.0	3.0
1031_0003043	4.0	3.0
1031_0003065	3.0	3.0
1031_0003090	4.0	3.0
1031_0003092	2.0	3.0
1031_0003099	3.0	3.0
1031_0003129	3.0	3.0
1031_0003160	3.0	3.0
1031_0003161	3.0	3.0
1031_0003163	3.0	3.0
1031_0003164	4.0	3.0
1031_0003166	2.0	3.0
1031_0003174	4.0	3.0
1031_0003180	4.0	3.0
1031_0003186	4.0	3.0
1031_0003212	2.0	3.0
1031_0003230	3.0	3.0
1031_0003231	3.0	3.0
1031_0003232	2.0	3.0
1031_0003233	3.0	3.0
1031_0003240	3.0	3.0
1031_0003245	3.0	3.0
1031_0003246	3.0	3.0
1031_0003260	3.0	3.0
1031_0003310	3.0	3.0
1031_0003314	4.0	3.0
1031_0003353	2.0	3.0
1031_0003369	4.0	3.0
1031_0003384	3.0	3.0
1031_0003391	2.0	3.0
1031_0003409	4.0	3.0
1031_0003410	3.0	3.0
1031_0003414	3.0	3.0
1061_0012029	2.0	3.0
1061_0120273	2.0	2.0
1061_0120275	3.0	2.0
1061_0120284	0.0	0.0
1061_0120285	2.0	2.0
1061_0120298	2.0	2.0
1061_0120300	2.0	2.0
1061_0120314	2.0	2.0
1061_0120315	2.0	1.0
1061_0120318	2.0	3.0
1061_0120327	3.0	3.0
1061_0120328	1.0	2.0
1061_0120334	3.0	3.0
1061_0120337	3.0	2.0
1061_0120338	1.0	3.0
1061_0120341	1.0	2.0
1061_0120343	3.0	3.0
1061_0120347	2.0	2.0
1061_0120361	2.0	3.0
1061_0120373	2.0	3.0
1061_0120384	2.0	1.0
1061_0120387	2.0	2.0
1061_0120405	3.0	3.0
1061_0120415	1.0	2.0
1061_0120423	3.0	3.0
1061_0120424	2.0	3.0
1061_0120458	3.0	3.0
1061_0120479	2.0	2.0
1061_0120488	3.0	3.0
1061_0120493	2.0	2.0
1061_0120494	2.0	3.0
1061_0120878	2.0	2.0
1061_0120883	2.0	2.0
1061_0120884	2.0	2.0
1061_0120885	3.0	3.0
1061_0120890	1.0	1.0
1061_1029114	1.0	2.0
1061_1029117	2.0	3.0
1061_1202913	1.0	2.0
1061_1202916	2.0	2.0
1071_0024678	2.0	1.0
1071_0024688	2.0	2.0
1071_0024693	1.0	1.0
1071_0024705	1.0	2.0
1071_0024708	2.0	1.0
1071_0024756	1.0	1.0
1071_0024761	1.0	1.0
1071_0024773	1.0	1.0
1071_0024776	0.0	0.0
1071_0024779	1.0	1.0
1071_0024783	0.0	1.0
1071_0024797	0.0	1.0
1071_0024800	1.0	1.0
1071_0024803	1.0	1.0
1071_0024806	1.0	1.0
1071_0024807	1.0	1.0
1071_0024809	1.0	1.0
1071_0024810	1.0	1.0
1071_0024817	0.0	1.0
1071_0024831	0.0	0.0
1071_0024836	2.0	1.0
1071_0024840	1.0	1.0
1071_0024845	1.0	1.0
1071_0024851	2.0	1.0
1071_0024857	1.0	1.0
1071_0024860	0.0	1.0
1071_0024872	1.0	2.0
1071_0024874	1.0	1.0
1071_0024881	2.0	2.0
1071_0242011	2.0	1.0
1071_0242042	1.0	1.0
1071_0242043	0.0	1.0
1071_0243501	1.0	1.0
1071_0248302	1.0	0.0
1071_0248311	2.0	1.0
1071_0248320	0.0	0.0
1071_0248337	1.0	2.0
1071_0248338	2.0	1.0
1071_0248341	1.0	0.0
1071_0248344	1.0	1.0
1071_0248345	2.0	2.0
1071_0248348	1.0	1.0
1071_0248349	1.0	0.0
1091_0000001	1.0	1.0
1091_0000005	3.0	2.0
1091_0000011	1.0	1.0
1091_0000019	1.0	1.0
1091_0000026	1.0	1.0
1091_0000056	1.0	2.0
1091_0000061	2.0	0.0
1091_0000070	2.0	2.0
1091_0000077	2.0	1.0
1091_0000095	2.0	1.0
1091_0000101	1.0	2.0
1091_0000114	1.0	2.0
1091_0000123	2.0	2.0
1091_0000126	2.0	2.0
1091_0000127	2.0	1.0
1091_0000158	1.0	2.0
1091_0000164	1.0	1.0
1091_0000192	1.0	2.0
1091_0000202	2.0	2.0
1091_0000212	2.0	2.0
1091_0000214	2.0	1.0
1091_0000218	3.0	2.0
1091_0000220	1.0	2.0
1091_0000222	2.0	2.0
1091_0000223	1.0	2.0
1091_0000229	1.0	2.0
1091_0000232	2.0	2.0
1091_0000234	3.0	2.0
1091_0000240	1.0	1.0
1091_0000251	2.0	2.0
1091_0000252	2.0	2.0
1091_0000255	0.0	2.0
1091_0000256	0.0	1.0
1091_0000258	2.0	3.0
1091_0000261	2.0	2.0
1091_0000268	2.0	2.0
1091_0000275	2.0	2.0
0614	2.0	2.0
0615	1.0	1.0
0617	0.0	1.0
0624	2.0	1.0
0628	2.0	2.0
0643	1.0	2.0
0718	1.0	1.0
0725	1.0	1.0
0802	1.0	1.0
0803	1.0	2.0
0810	1.0	2.0
0812	1.0	1.0
0907	2.0	2.0
0910	2.0	2.0
0915	2.0	2.0
0918	1.0	2.0
0921	2.0	1.0
0922	1.0	1.0
0925	2.0	1.0
0926	2.0	2.0
1001	1.0	2.0
1005	1.0	1.0
1008	1.0	1.0
1009	2.0	1.0
1019	1.0	1.0
1023	1.0	2.0
1112	1.0	1.0
1117	1.0	1.0
9999	0.0	2.0
BER0611003	0.0	0.0
BER0611007	0.0	0.0
KYJ0611005B	0.0	0.0
KYJ0611006A	1.0	1.0
KYJ0611009B	0.0	0.0
LIB0611002B	0.0	0.0
LON0610002B	0.0	0.0
LON0611002A	1.0	1.0
LON0611003	0.0	0.0
MOS0509001	0.0	0.0
MOS0509004	0.0	0.0
MOS0611015	0.0	0.0
PAR1011008A	2.0	1.0
PHA0111005B	0.0	0.0
PHA0111010	0.0	0.0
PHA0111014	0.0	0.0
PHA0112002B	0.0	0.0
PHA0112006B	0.0	0.0
PHA0112007A	1.0	0.0
PHA0209031	0.0	0.0
PHA0209039	0.0	0.0
PHA0210004	0.0	0.0
PHA0411010B	0.0	0.0
PHA0411029	0.0	0.0
PHA0411034	0.0	0.0
PHA0411044	0.0	0.0
PHA0411060	0.0	0.0
PHA0509013	0.0	0.0
PHA0509015	0.0	0.0
PHA0509025	0.0	0.0
PHA0509034	0.0	0.0
PHA0509041	0.0	0.0
PHA0509042	0.0	0.0
PHA0510002A	0.0	1.0
PHA0510004A	0.0	0.0
PHA0510010B	0.0	0.0
PHA0510013B	0.0	0.0
PHA0510036	0.0	0.0
PHA0510039	0.0	0.0
PHA0610007B	0.0	0.0
PHA0610016	0.0	0.0
PHA0610017	0.0	0.0
PHA0710009	0.0	0.0
PHA0710015	0.0	0.0
PHA0710017	0.0	0.0
PHA0710021	0.0	0.0
PHA0809009	0.0	0.0
PHA0810009	0.0	0.0
PHA1110002B	0.0	0.0
PHA1110016	0.0	0.0
PHA1111001A	1.0	1.0
PHA1111001B	0.0	0.0
PHA1111002A	1.0	0.0
PHA1111003A	1.0	1.0
PHA1111003B	0.0	0.0
PHA1111004A	1.0	0.0
PHA1111008A	1.0	1.0
PHA1111008B	0.0	0.0
VAR0909003	0.0	0.0
VAR0909010	0.0	0.0
1325_1001017	1.0	2.0
1325_1001019	2.0	2.0
1325_1001024	2.0	2.0
1325_1001025	2.0	2.0
1325_1001033	2.0	2.0
1325_1001050	2.0	2.0
1325_1001063	1.0	2.0
1325_1001076	2.0	2.0
1325_1001085	2.0	2.0
1325_1001087	2.0	2.0
1325_1001097	1.0	2.0
1325_1001111	3.0	2.0
1325_1001113	3.0	2.0
1325_1001121	2.0	2.0
1325_1001124	1.0	2.0
1325_1001125	2.0	2.0
1325_1001129	1.0	2.0
1325_1001153	2.0	2.0
1325_1001160	2.0	2.0
1325_9000087	2.0	2.0
1325_9000106	2.0	2.0
1325_9000137	2.0	2.0
1325_9000152	2.0	2.0
1325_9000187	2.0	2.0
1325_9000209	2.0	2.0
1325_9000211	2.0	2.0
1325_9000279	2.0	2.0
1325_9000302	2.0	2.0
1325_9000316	2.0	2.0
1325_9000318	2.0	2.0
1325_9000319	2.0	2.0
1325_9000320	3.0	2.0
1325_9000321	3.0	2.0
1325_9000505	2.0	2.0
1325_9000554	2.0	2.0
1325_9000685	3.0	2.0
1365_0100002	1.0	2.0
1365_0100003	2.0	1.0
1365_0100005	2.0	2.0
1365_0100010	1.0	2.0
1365_0100016	1.0	2.0
1365_0100020	2.0	2.0
1365_0100022	1.0	2.0
1365_0100031	1.0	2.0
1365_0100065	1.0	2.0
1365_0100071	2.0	2.0
1365_0100079	1.0	2.0
1365_0100095	2.0	2.0
1365_0100097	2.0	2.0
1365_0100102	2.0	2.0
1365_0100117	2.0	2.0
1365_0100119	3.0	2.0
1365_0100121	2.0	2.0
1365_0100134	2.0	2.0
1365_0100167	1.0	2.0
1365_0100170	1.0	2.0
1365_0100176	2.0	2.0
1365_0100178	2.0	2.0
1365_0100180	1.0	2.0
1365_0100185	2.0	2.0
1365_0100198	1.0	2.0
1365_0100202	2.0	2.0
1365_0100211	2.0	2.0
1365_0100213	2.0	2.0
1365_0100218	2.0	2.0
1365_0100222	2.0	2.0
1365_0100252	2.0	2.0
1365_0100253	1.0	2.0
1365_0100257	2.0	2.0
1365_0100258	2.0	2.0
1365_0100263	3.0	2.0
1365_0100265	2.0	2.0
1365_0100281	2.0	2.0
1365_0100285	2.0	2.0
1365_0100299	1.0	2.0
1365_0100451	2.0	2.0
1365_0100456	2.0	2.0
1365_0100461	2.0	2.0
1365_0100472	2.0	2.0
1365_0100481	2.0	2.0
1385_0000022	0.0	1.0
1385_0000036	1.0	1.0
1385_0000038	2.0	2.0
1385_0000047	2.0	2.0
1385_0000057	2.0	2.0
1385_0000097	1.0	1.0
1385_0000101	1.0	1.0
1385_0000102	1.0	1.0
1385_0000114	2.0	2.0
1385_0000125	1.0	1.0
1385_0000128	1.0	1.0
1385_0001110	1.0	1.0
1385_0001111	2.0	1.0
1385_0001118	1.0	1.0
1385_0001134	1.0	1.0
1385_0001163	1.0	1.0
1385_0001166	1.0	1.0
1385_0001190	0.0	1.0
1385_0001195	2.0	1.0
1385_0001199	0.0	1.0
1385_0001524	1.0	1.0
1385_0001528	1.0	1.0
1385_0001712	1.0	2.0
1385_0001716	1.0	1.0
1385_0001717	1.0	1.0
1385_0001725	1.0	1.0
1385_0001726	0.0	1.0
1385_0001730	1.0	1.0
1385_0001750	0.0	0.0
1385_0001751	1.0	1.0
1385_0001754	0.0	1.0
1385_0001761	1.0	1.0
1385_0001764	1.0	1.0
1385_0001774	0.0	1.0
1385_0001788	0.0	2.0
1385_0001792	1.0	2.0
1385_0001798	1.0	2.0
1385_0001800	1.0	1.0
1395_0000333	1.0	1.0
1395_0000337	1.0	0.0
1395_0000361	1.0	1.0
1395_0000365	2.0	1.0
1395_0000388	1.0	1.0
1395_0000399	1.0	1.0
1395_0000404	1.0	2.0
1395_0000413	1.0	1.0
1395_0000443	1.0	1.0
1395_0000449	1.0	1.0
1395_0000499	1.0	1.0
1395_0000500	1.0	1.0
1395_0000504	1.0	1.0
1395_0000550	1.0	1.0
1395_0000551	2.0	1.0
1395_0000552	2.0	1.0
1395_0000557	2.0	2.0
1395_0000581	1.0	1.0
1395_0000582	1.0	1.0
1395_0000585	1.0	1.0
1395_0000596	2.0	1.0
1395_0000604	0.0	1.0
1395_0000611	1.0	1.0
1395_0000628	1.0	1.0
1395_0000630	1.0	1.0
1395_0000631	1.0	2.0
1395_0001017	1.0	1.0
1395_0001019	1.0	1.0
1395_0001078	1.0	1.0
1395_0001108	1.0	1.0
1395_0001109	1.0	1.0
1395_0001147	0.0	1.0
1395_0001158	1.0	1.0
1395_0001170	1.0	1.0
4 Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 1.02
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.95
              precision    recall  f1-score   support

         0.0       1.00      0.66      0.80        74
         1.0       0.54      0.92      0.68       140
         2.0       0.61      0.39      0.48       147
         3.0       0.66      0.61      0.64        77
         4.0       0.00      0.00      0.00        14

    accuracy                           0.63       452
   macro avg       0.56      0.52      0.52       452
weighted avg       0.64      0.63      0.61       452

[[ 49  25   0   0   0]
 [  0 129  11   0   0]
 [  0  79  58  10   0]
 [  0   4  26  47   0]
 [  0   0   0  14   0]]
0.606496866114814
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 0.81
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.80
              precision    recall  f1-score   support

         0.0       1.00      0.66      0.80        74
         1.0       0.65      0.78      0.71       140
         2.0       0.66      0.63      0.65       147
         3.0       0.61      0.77      0.68        77
         4.0       0.00      0.00      0.00        14

    accuracy                           0.69       452
   macro avg       0.59      0.57      0.57       452
weighted avg       0.69      0.69      0.68       452

[[ 49  22   3   0   0]
 [  0 109  29   2   0]
 [  0  33  93  21   0]
 [  0   3  15  59   0]
 [  0   0   0  14   0]]
0.6773485713983508
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.72
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.81
              precision    recall  f1-score   support

         0.0       0.84      0.72      0.77        74
         1.0       0.64      0.71      0.67       140
         2.0       0.62      0.65      0.64       147
         3.0       0.65      0.66      0.65        77
         4.0       0.00      0.00      0.00        14

    accuracy                           0.66       452
   macro avg       0.55      0.55      0.55       452
weighted avg       0.65      0.66      0.65       452

[[53 18  3  0  0]
 [ 8 99 33  0  0]
 [ 1 36 96 14  0]
 [ 1  2 23 51  0]
 [ 0  0  0 14  0]]
0.652709477070814
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.63
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.81
              precision    recall  f1-score   support

         0.0       0.96      0.69      0.80        74
         1.0       0.66      0.71      0.69       140
         2.0       0.63      0.69      0.66       147
         3.0       0.63      0.71      0.67        77
         4.0       0.00      0.00      0.00        14

    accuracy                           0.68       452
   macro avg       0.58      0.56      0.56       452
weighted avg       0.67      0.68      0.67       452

[[ 51  19   4   0   0]
 [  2 100  37   1   0]
 [  0  29 101  17   0]
 [  0   3  19  55   0]
 [  0   0   0  14   0]]
0.6719213152878871
452 452 452
Filename	True Label	Prediction
1023_0001419	3.0	3.0
1023_0101693	4.0	3.0
1023_0101844	2.0	3.0
1023_0101853	2.0	3.0
1023_0103824	3.0	3.0
1023_0103830	3.0	3.0
1023_0103834	3.0	3.0
1023_0103836	2.0	3.0
1023_0103839	3.0	3.0
1023_0103844	4.0	3.0
1023_0103955	3.0	3.0
1023_0104206	2.0	3.0
1023_0107729	3.0	3.0
1023_0107773	3.0	3.0
1023_0108306	4.0	3.0
1023_0108518	3.0	3.0
1023_0108812	2.0	3.0
1023_0108885	3.0	3.0
1023_0109038	4.0	3.0
1023_0109391	3.0	3.0
1023_0109422	3.0	3.0
1023_0109495	3.0	3.0
1023_0109500	2.0	3.0
1023_0109505	3.0	3.0
1023_0109519	2.0	3.0
1023_0109522	3.0	3.0
1023_0109591	2.0	3.0
1023_0109606	3.0	3.0
1023_0109609	2.0	3.0
1023_0109880	3.0	3.0
1023_0109915	2.0	2.0
1023_0109917	3.0	3.0
1031_0001997	3.0	3.0
1031_0001998	4.0	3.0
1031_0002043	4.0	3.0
1031_0002086	3.0	3.0
1031_0002089	3.0	3.0
1031_0002185	3.0	3.0
1031_0002187	3.0	3.0
1031_0002199	4.0	3.0
1031_0003053	3.0	3.0
1031_0003076	3.0	3.0
1031_0003085	3.0	3.0
1031_0003095	3.0	3.0
1031_0003121	3.0	3.0
1031_0003136	3.0	3.0
1031_0003140	3.0	3.0
1031_0003145	3.0	3.0
1031_0003150	3.0	3.0
1031_0003155	3.0	3.0
1031_0003162	4.0	3.0
1031_0003181	4.0	3.0
1031_0003182	4.0	3.0
1031_0003183	4.0	3.0
1031_0003185	3.0	3.0
1031_0003187	3.0	3.0
1031_0003203	2.0	3.0
1031_0003205	3.0	3.0
1031_0003214	3.0	3.0
1031_0003216	2.0	3.0
1031_0003217	4.0	3.0
1031_0003219	3.0	3.0
1031_0003226	3.0	3.0
1031_0003236	3.0	3.0
1031_0003237	3.0	3.0
1031_0003261	3.0	3.0
1031_0003272	3.0	3.0
1031_0003313	3.0	3.0
1031_0003338	3.0	3.0
1031_0003388	4.0	3.0
1031_0003392	4.0	3.0
1031_0003407	3.0	3.0
1031_0003419	3.0	3.0
1061_0120271	2.0	2.0
1061_0120272	1.0	2.0
1061_0120276	3.0	2.0
1061_0120280	1.0	1.0
1061_0120287	1.0	2.0
1061_0120299	2.0	2.0
1061_0120301	2.0	2.0
1061_0120308	2.0	3.0
1061_0120311	2.0	3.0
1061_0120313	2.0	1.0
1061_0120324	3.0	2.0
1061_0120349	1.0	1.0
1061_0120366	3.0	3.0
1061_0120370	2.0	3.0
1061_0120372	2.0	2.0
1061_0120375	2.0	1.0
1061_0120386	1.0	2.0
1061_0120403	3.0	3.0
1061_0120410	2.0	2.0
1061_0120421	2.0	3.0
1061_0120425	3.0	2.0
1061_0120429	3.0	3.0
1061_0120430	2.0	2.0
1061_0120432	2.0	2.0
1061_0120433	1.0	2.0
1061_0120438	2.0	3.0
1061_0120439	1.0	1.0
1061_0120449	3.0	2.0
1061_0120459	3.0	3.0
1061_0120478	3.0	3.0
1061_0120480	2.0	2.0
1061_0120485	3.0	3.0
1061_0120497	3.0	3.0
1061_0120500	2.0	2.0
1061_0120853	2.0	3.0
1061_0120874	2.0	2.0
1061_0120876	3.0	3.0
1061_0120877	2.0	2.0
1061_0120888	2.0	2.0
1061_1029111	3.0	2.0
1061_1202910	3.0	2.0
1061_1202919	2.0	2.0
1071_0024681	1.0	2.0
1071_0024682	2.0	2.0
1071_0024683	0.0	1.0
1071_0024689	2.0	1.0
1071_0024710	1.0	1.0
1071_0024715	2.0	2.0
1071_0024763	1.0	1.0
1071_0024765	1.0	1.0
1071_0024766	1.0	1.0
1071_0024767	2.0	2.0
1071_0024770	1.0	1.0
1071_0024781	1.0	1.0
1071_0024782	0.0	0.0
1071_0024801	1.0	1.0
1071_0024804	1.0	1.0
1071_0024811	1.0	1.0
1071_0024822	0.0	1.0
1071_0024835	1.0	1.0
1071_0024848	1.0	1.0
1071_0024856	1.0	1.0
1071_0024863	2.0	1.0
1071_0024865	2.0	2.0
1071_0024867	2.0	2.0
1071_0024875	1.0	1.0
1071_0024877	1.0	1.0
1071_0241831	1.0	1.0
1071_0241832	1.0	1.0
1071_0242091	1.0	1.0
1071_0243592	1.0	1.0
1071_0248308	1.0	1.0
1071_0248313	1.0	1.0
1071_0248314	1.0	1.0
1071_0248316	1.0	0.0
1071_0248321	2.0	1.0
1071_0248322	1.0	1.0
1071_0248333	2.0	1.0
1071_0248336	1.0	1.0
1071_0248339	1.0	1.0
1071_0248340	0.0	0.0
1071_0248350	1.0	1.0
1091_0000002	2.0	2.0
1091_0000006	0.0	1.0
1091_0000025	1.0	1.0
1091_0000034	2.0	1.0
1091_0000047	2.0	1.0
1091_0000048	1.0	1.0
1091_0000051	1.0	1.0
1091_0000065	1.0	1.0
1091_0000113	1.0	2.0
1091_0000116	2.0	2.0
1091_0000151	1.0	1.0
1091_0000157	3.0	2.0
1091_0000162	2.0	2.0
1091_0000165	2.0	1.0
1091_0000168	1.0	3.0
1091_0000171	1.0	2.0
1091_0000172	2.0	1.0
1091_0000173	2.0	1.0
1091_0000174	1.0	0.0
1091_0000190	0.0	2.0
1091_0000200	2.0	2.0
1091_0000209	2.0	2.0
1091_0000216	1.0	2.0
1091_0000219	2.0	2.0
1091_0000230	2.0	2.0
1091_0000231	2.0	2.0
1091_0000233	2.0	2.0
1091_0000242	1.0	2.0
1091_0000243	1.0	2.0
1091_0000246	2.0	2.0
1091_0000249	2.0	2.0
1091_0000250	1.0	2.0
1091_0000253	2.0	1.0
1091_0000254	3.0	1.0
1091_0000267	2.0	2.0
1091_0000271	2.0	2.0
1091_0000273	1.0	2.0
0603	2.0	2.0
0607	2.0	1.0
0608	0.0	2.0
0616	2.0	1.0
0623	0.0	2.0
0629	2.0	1.0
0634	3.0	2.0
0636	2.0	2.0
0637	2.0	2.0
0639	1.0	2.0
0641	1.0	1.0
0642	2.0	2.0
0716	2.0	1.0
0717	1.0	1.0
0719	1.0	2.0
0720	2.0	1.0
0808	1.0	2.0
0820	1.0	1.0
0827	2.0	1.0
0901	2.0	2.0
0906	2.0	2.0
0913	2.0	2.0
0920	2.0	1.0
1003	1.0	1.0
1007	1.0	1.0
1018	1.0	1.0
1116	1.0	2.0
BER0611006	0.0	0.0
KYJ0611005A	1.0	1.0
LON0611004A	1.0	1.0
PAR1011009A	2.0	1.0
PAR1011013	0.0	0.0
PAR1011014	0.0	0.0
PAR1011015	0.0	0.0
PHA0112003A	1.0	1.0
PHA0112003B	0.0	0.0
PHA0112006A	3.0	1.0
PHA0209001	0.0	0.0
PHA0209008	0.0	0.0
PHA0209038	0.0	0.0
PHA0411009A	1.0	1.0
PHA0411009B	0.0	0.0
PHA0411010A	1.0	1.0
PHA0411012B	0.0	0.0
PHA0411027	0.0	0.0
PHA0411032	0.0	0.0
PHA0411033	0.0	0.0
PHA0411055	0.0	0.0
PHA0411059	0.0	0.0
PHA0509002	0.0	0.0
PHA0509017	0.0	0.0
PHA0509018	0.0	0.0
PHA0509020	0.0	0.0
PHA0509024	0.0	0.0
PHA0509026	0.0	0.0
PHA0509028	0.0	0.0
PHA0509031	0.0	0.0
PHA0509036	0.0	0.0
PHA0509045	0.0	0.0
PHA0510003B	0.0	0.0
PHA0510030	0.0	0.0
PHA0510035	0.0	0.0
PHA0710010	0.0	0.0
PHA0710011	0.0	0.0
PHA0810003	0.0	0.0
PHA0810010	0.0	0.0
PHA0811016	0.0	0.0
PHA0811020	0.0	0.0
PHA1109001	0.0	0.0
PHA1109002	0.0	0.0
PHA1109004	0.0	0.0
PHA1109006	0.0	0.0
PHA1109008	0.0	0.0
PHA1109028	0.0	0.0
PHA1110001A	1.0	1.0
PHA1110001B	0.0	0.0
PHA1110017	0.0	0.0
PHA1110019	0.0	0.0
PHA1110022	0.0	0.0
VAR0909005	0.0	0.0
VAR0909006	0.0	0.0
VAR0909008	0.0	0.0
VAR0910004	0.0	0.0
VAR0910007	0.0	0.0
1325_1001010	2.0	2.0
1325_1001011	3.0	2.0
1325_1001022	2.0	2.0
1325_1001028	1.0	2.0
1325_1001036	2.0	2.0
1325_1001042	2.0	2.0
1325_1001046	1.0	2.0
1325_1001052	2.0	2.0
1325_1001056	2.0	2.0
1325_1001075	2.0	2.0
1325_1001079	2.0	2.0
1325_1001082	2.0	2.0
1325_1001083	2.0	2.0
1325_1001084	2.0	2.0
1325_1001107	2.0	2.0
1325_1001108	2.0	2.0
1325_1001110	2.0	2.0
1325_1001120	2.0	2.0
1325_1001128	2.0	2.0
1325_1001133	2.0	2.0
1325_1001134	2.0	2.0
1325_1001136	2.0	2.0
1325_1001154	3.0	2.0
1325_1001159	2.0	2.0
1325_1001161	2.0	2.0
1325_1001162	2.0	2.0
1325_1001168	1.0	2.0
1325_9000059	3.0	2.0
1325_9000104	2.0	2.0
1325_9000140	3.0	2.0
1325_9000143	3.0	2.0
1325_9000185	3.0	2.0
1325_9000210	1.0	2.0
1325_9000241	3.0	2.0
1325_9000278	3.0	2.0
1325_9000303	2.0	2.0
1325_9000322	2.0	2.0
1325_9000504	2.0	2.0
1325_9000534	1.0	2.0
1325_9000602	2.0	2.0
1325_9000612	1.0	2.0
1325_9000674	3.0	2.0
1325_9000675	2.0	2.0
1325_9000677	2.0	2.0
1325_9000678	3.0	2.0
1365_0100014	2.0	2.0
1365_0100017	2.0	2.0
1365_0100023	1.0	2.0
1365_0100058	2.0	2.0
1365_0100063	2.0	2.0
1365_0100064	2.0	2.0
1365_0100066	1.0	2.0
1365_0100069	2.0	2.0
1365_0100072	2.0	2.0
1365_0100103	2.0	2.0
1365_0100104	1.0	2.0
1365_0100135	1.0	2.0
1365_0100173	1.0	2.0
1365_0100187	2.0	2.0
1365_0100194	2.0	2.0
1365_0100196	1.0	2.0
1365_0100199	2.0	2.0
1365_0100201	2.0	2.0
1365_0100205	2.0	1.0
1365_0100212	3.0	2.0
1365_0100219	2.0	2.0
1365_0100232	2.0	2.0
1365_0100262	2.0	2.0
1365_0100266	1.0	2.0
1365_0100269	2.0	2.0
1365_0100277	2.0	2.0
1365_0100279	2.0	2.0
1365_0100280	1.0	2.0
1365_0100282	2.0	2.0
1365_0100455	2.0	2.0
1365_0100473	2.0	2.0
1385_0000020	2.0	2.0
1385_0000034	1.0	1.0
1385_0000035	1.0	1.0
1385_0000039	1.0	1.0
1385_0000040	1.0	1.0
1385_0000041	2.0	2.0
1385_0000048	2.0	2.0
1385_0000049	2.0	2.0
1385_0000051	2.0	2.0
1385_0000053	1.0	2.0
1385_0000054	2.0	2.0
1385_0000098	1.0	1.0
1385_0000122	1.0	2.0
1385_0000124	2.0	2.0
1385_0001105	1.0	1.0
1385_0001112	1.0	1.0
1385_0001119	1.0	1.0
1385_0001125	1.0	1.0
1385_0001126	1.0	1.0
1385_0001129	1.0	1.0
1385_0001132	1.0	1.0
1385_0001135	1.0	1.0
1385_0001137	1.0	1.0
1385_0001148	1.0	1.0
1385_0001149	2.0	2.0
1385_0001154	2.0	2.0
1385_0001162	1.0	1.0
1385_0001165	1.0	1.0
1385_0001169	1.0	1.0
1385_0001170	0.0	1.0
1385_0001178	0.0	1.0
1385_0001188	1.0	1.0
1385_0001192	0.0	1.0
1385_0001523	1.0	1.0
1385_0001714	0.0	1.0
1385_0001719	1.0	1.0
1385_0001727	0.0	1.0
1385_0001728	1.0	1.0
1385_0001732	1.0	2.0
1385_0001740	1.0	1.0
1385_0001746	1.0	1.0
1385_0001747	0.0	1.0
1385_0001749	0.0	1.0
1385_0001752	0.0	1.0
1385_0001753	1.0	1.0
1385_0001757	1.0	1.0
1385_0001762	1.0	1.0
1385_0001766	2.0	1.0
1385_0001787	0.0	2.0
1385_0001790	1.0	2.0
1385_0001791	1.0	2.0
1395_0000341	1.0	1.0
1395_0000353	1.0	1.0
1395_0000354	0.0	1.0
1395_0000356	1.0	1.0
1395_0000366	2.0	2.0
1395_0000368	0.0	1.0
1395_0000378	1.0	1.0
1395_0000387	3.0	1.0
1395_0000391	2.0	2.0
1395_0000392	1.0	1.0
1395_0000396	1.0	1.0
1395_0000414	1.0	1.0
1395_0000438	2.0	2.0
1395_0000469	1.0	1.0
1395_0000512	2.0	1.0
1395_0000515	1.0	1.0
1395_0000527	1.0	1.0
1395_0000528	2.0	1.0
1395_0000531	2.0	1.0
1395_0000534	1.0	1.0
1395_0000537	1.0	1.0
1395_0000548	1.0	2.0
1395_0000563	2.0	1.0
1395_0000575	1.0	1.0
1395_0000579	1.0	1.0
1395_0000584	0.0	1.0
1395_0000591	0.0	1.0
1395_0000595	0.0	1.0
1395_0000606	1.0	1.0
1395_0000607	0.0	1.0
1395_0000608	1.0	1.0
1395_0000609	0.0	1.0
1395_0000627	1.0	1.0
1395_0000649	1.0	1.0
1395_0001010	2.0	1.0
1395_0001013	1.0	1.0
1395_0001028	1.0	2.0
1395_0001033	1.0	1.0
1395_0001040	1.0	1.0
1395_0001060	1.0	1.0
1395_0001065	1.0	1.0
1395_0001073	1.0	1.0
1395_0001116	2.0	1.0
1395_0001117	1.0	1.0
1395_0001126	1.0	1.0
1395_0001131	1.0	1.0
1395_0001146	0.0	1.0
1395_0001149	1.0	1.0
1395_0001164	2.0	1.0
5 Fold, Dimension = Sociolinguisticappropriateness

======== Epoch 1 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 46
Elapsed time 58

  Average training loss: 1.04
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.85
              precision    recall  f1-score   support

         0.0       0.95      0.69      0.80        75
         1.0       0.59      0.70      0.64       140
         2.0       0.63      0.63      0.63       147
         3.0       0.65      0.73      0.69        77
         4.0       0.00      0.00      0.00        13

    accuracy                           0.66       452
   macro avg       0.56      0.55      0.55       452
weighted avg       0.66      0.66      0.65       452

[[52 23  0  0  0]
 [ 2 98 39  1  0]
 [ 1 38 92 16  0]
 [ 0  7 14 56  0]
 [ 0  0  0 13  0]]
0.6531223784531043
452 452 452



======== Epoch 2 / 4 ========
Training...
Elapsed time 12
Elapsed time 23
Elapsed time 35
Elapsed time 47
Elapsed time 58

  Average training loss: 0.82
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.78
              precision    recall  f1-score   support

         0.0       0.93      0.69      0.79        75
         1.0       0.65      0.75      0.70       140
         2.0       0.64      0.64      0.64       147
         3.0       0.63      0.73      0.67        77
         4.0       0.00      0.00      0.00        13

    accuracy                           0.68       452
   macro avg       0.57      0.56      0.56       452
weighted avg       0.67      0.68      0.67       452

[[ 52  21   2   0   0]
 [  2 105  31   2   0]
 [  2  33  94  18   0]
 [  0   2  19  56   0]
 [  0   0   0  13   0]]
0.6714359063695117
452 452 452



======== Epoch 3 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 36
Elapsed time 48
Elapsed time 60

  Average training loss: 0.69
  Training epoch took: 68
Running Validation...
  Average evaluation loss: 0.81
              precision    recall  f1-score   support

         0.0       0.88      0.79      0.83        75
         1.0       0.72      0.58      0.64       140
         2.0       0.60      0.71      0.65       147
         3.0       0.60      0.77      0.67        77
         4.0       0.00      0.00      0.00        13

    accuracy                           0.67       452
   macro avg       0.56      0.57      0.56       452
weighted avg       0.67      0.67      0.66       452

[[ 59  11   5   0   0]
 [  7  81  48   4   0]
 [  1  20 104  22   0]
 [  0   1  17  59   0]
 [  0   0   0  13   0]]
0.6618153736399857
452 452 452



======== Epoch 4 / 4 ========
Training...
Elapsed time 12
Elapsed time 24
Elapsed time 35
Elapsed time 47
Elapsed time 59

  Average training loss: 0.58
  Training epoch took: 66
Running Validation...
  Average evaluation loss: 0.81
              precision    recall  f1-score   support

         0.0       0.92      0.76      0.83        75
         1.0       0.67      0.64      0.65       140
         2.0       0.60      0.67      0.63       147
         3.0       0.60      0.71      0.65        77
         4.0       0.00      0.00      0.00        13

    accuracy                           0.66       452
   macro avg       0.56      0.56      0.55       452
weighted avg       0.66      0.66      0.66       452

[[57 14  4  0  0]
 [ 4 89 43  4  0]
 [ 1 27 99 20  0]
 [ 0  2 20 55  0]
 [ 0  0  0 13  0]]
0.6573786829947801
452 452 452
Filename	True Label	Prediction
1023_0001418	3.0	3.0
1023_0101690	2.0	3.0
1023_0101694	3.0	2.0
1023_0101841	3.0	3.0
1023_0101845	2.0	3.0
1023_0101846	4.0	3.0
1023_0101852	3.0	3.0
1023_0101855	2.0	3.0
1023_0101856	2.0	3.0
1023_0101899	3.0	2.0
1023_0101906	2.0	3.0
1023_0103840	4.0	3.0
1023_0103841	3.0	3.0
1023_0103843	3.0	2.0
1023_0103883	3.0	3.0
1023_0106816	3.0	3.0
1023_0107042	3.0	3.0
1023_0107672	3.0	3.0
1023_0107726	2.0	3.0
1023_0107781	3.0	3.0
1023_0107784	2.0	2.0
1023_0108422	4.0	3.0
1023_0108520	3.0	3.0
1023_0108766	2.0	3.0
1023_0108811	4.0	3.0
1023_0108814	3.0	3.0
1023_0108887	2.0	3.0
1023_0108931	3.0	3.0
1023_0108935	3.0	3.0
1023_0108992	3.0	3.0
1023_0109022	3.0	3.0
1023_0109027	3.0	3.0
1023_0109096	3.0	3.0
1023_0109151	4.0	3.0
1023_0109247	4.0	3.0
1023_0109392	3.0	3.0
1023_0109395	2.0	2.0
1023_0109401	3.0	3.0
1023_0109496	3.0	3.0
1023_0109516	3.0	3.0
1023_0109716	3.0	3.0
1023_0109945	3.0	3.0
1031_0001950	3.0	3.0
1031_0002032	3.0	3.0
1031_0002079	4.0	3.0
1031_0002084	3.0	3.0
1031_0002092	3.0	3.0
1031_0002196	3.0	2.0
1031_0003029	3.0	3.0
1031_0003035	3.0	3.0
1031_0003052	3.0	3.0
1031_0003054	3.0	3.0
1031_0003063	4.0	3.0
1031_0003078	3.0	3.0
1031_0003091	2.0	3.0
1031_0003106	3.0	3.0
1031_0003130	4.0	3.0
1031_0003146	4.0	3.0
1031_0003167	3.0	3.0
1031_0003172	3.0	3.0
1031_0003190	3.0	3.0
1031_0003191	3.0	3.0
1031_0003220	3.0	3.0
1031_0003221	2.0	3.0
1031_0003235	3.0	3.0
1031_0003238	3.0	3.0
1031_0003239	4.0	3.0
1031_0003249	4.0	3.0
1031_0003273	3.0	3.0
1031_0003274	3.0	3.0
1031_0003327	2.0	3.0
1031_0003337	3.0	3.0
1031_0003339	3.0	3.0
1031_0003352	3.0	3.0
1031_0003355	3.0	3.0
1031_0003356	3.0	3.0
1031_0003359	2.0	3.0
1031_0003368	3.0	3.0
1031_0003387	4.0	3.0
1031_0003390	3.0	3.0
1031_0003408	3.0	3.0
1061_0120279	1.0	2.0
1061_0120281	1.0	2.0
1061_0120283	1.0	1.0
1061_0120291	1.0	1.0
1061_0120296	2.0	2.0
1061_0120307	3.0	2.0
1061_0120317	2.0	3.0
1061_0120323	1.0	2.0
1061_0120329	2.0	3.0
1061_0120330	3.0	3.0
1061_0120332	1.0	2.0
1061_0120351	2.0	3.0
1061_0120353	1.0	1.0
1061_0120354	2.0	2.0
1061_0120355	1.0	1.0
1061_0120357	3.0	3.0
1061_0120371	3.0	3.0
1061_0120389	2.0	2.0
1061_0120390	3.0	3.0
1061_0120391	1.0	1.0
1061_0120414	3.0	2.0
1061_0120428	2.0	3.0
1061_0120441	2.0	2.0
1061_0120443	0.0	1.0
1061_0120450	1.0	3.0
1061_0120453	2.0	3.0
1061_0120456	1.0	3.0
1061_0120460	2.0	1.0
1061_0120483	2.0	2.0
1061_0120484	2.0	3.0
1061_0120490	3.0	3.0
1061_0120496	2.0	2.0
1061_0120499	2.0	3.0
1061_0120858	2.0	2.0
1061_0120886	2.0	2.0
1061_0120889	1.0	2.0
1061_1029116	1.0	3.0
1061_1202912	3.0	2.0
1061_1202915	1.0	1.0
1071_0020001	2.0	1.0
1071_0024685	2.0	3.0
1071_0024694	2.0	2.0
1071_0024701	2.0	2.0
1071_0024702	2.0	1.0
1071_0024709	3.0	2.0
1071_0024716	1.0	1.0
1071_0024759	0.0	2.0
1071_0024762	1.0	1.0
1071_0024768	1.0	1.0
1071_0024799	3.0	2.0
1071_0024820	0.0	1.0
1071_0024823	1.0	1.0
1071_0024825	0.0	1.0
1071_0024834	2.0	1.0
1071_0024838	1.0	0.0
1071_0024841	1.0	1.0
1071_0024850	1.0	1.0
1071_0024852	0.0	0.0
1071_0024859	1.0	2.0
1071_0024866	2.0	2.0
1071_0024871	1.0	1.0
1071_0024878	2.0	2.0
1071_0242021	1.0	1.0
1071_0242041	1.0	2.0
1071_0242072	0.0	0.0
1071_0242073	1.0	1.0
1071_0243502	1.0	1.0
1071_0243581	1.0	1.0
1071_0243591	1.0	2.0
1071_0243622	1.0	1.0
1071_0248305	0.0	1.0
1071_0248307	2.0	1.0
1071_0248326	1.0	1.0
1071_0248328	0.0	1.0
1071_0248331	1.0	1.0
1071_0248335	1.0	1.0
1091_0000003	2.0	1.0
1091_0000004	1.0	1.0
1091_0000015	2.0	1.0
1091_0000017	3.0	1.0
1091_0000018	3.0	2.0
1091_0000020	1.0	2.0
1091_0000022	2.0	2.0
1091_0000024	3.0	2.0
1091_0000027	1.0	2.0
1091_0000028	1.0	1.0
1091_0000029	3.0	2.0
1091_0000030	1.0	2.0
1091_0000032	1.0	2.0
1091_0000035	2.0	2.0
1091_0000036	1.0	2.0
1091_0000041	0.0	2.0
1091_0000043	1.0	2.0
1091_0000049	1.0	1.0
1091_0000053	1.0	1.0
1091_0000055	2.0	2.0
1091_0000057	2.0	1.0
1091_0000058	2.0	1.0
1091_0000060	3.0	2.0
1091_0000063	1.0	1.0
1091_0000064	1.0	1.0
1091_0000071	2.0	2.0
1091_0000075	1.0	2.0
1091_0000086	1.0	1.0
1091_0000087	2.0	1.0
1091_0000102	2.0	1.0
1091_0000146	0.0	1.0
1091_0000148	1.0	1.0
1091_0000153	1.0	3.0
1091_0000170	2.0	1.0
1091_0000191	1.0	2.0
1091_0000195	1.0	1.0
1091_0000199	2.0	2.0
1091_0000201	2.0	2.0
1091_0000203	1.0	1.0
1091_0000205	1.0	2.0
1091_0000207	2.0	2.0
1091_0000208	1.0	1.0
1091_0000217	2.0	2.0
1091_0000221	2.0	2.0
1091_0000228	1.0	1.0
1091_0000237	1.0	2.0
1091_0000247	2.0	1.0
1091_0000262	2.0	2.0
1091_0000265	3.0	2.0
1091_0000269	1.0	2.0
1091_0000270	3.0	1.0
1091_0000272	1.0	1.0
1091_0000276	2.0	1.0
0604	1.0	2.0
0610	1.0	1.0
0619	2.0	1.0
0620	1.0	2.0
0632	1.0	2.0
0638	1.0	1.0
0640	2.0	2.0
0644	1.0	2.0
0722	2.0	2.0
0723	2.0	1.0
0724	2.0	1.0
0817	1.0	1.0
0821	2.0	1.0
0822	1.0	2.0
0828	1.0	2.0
0903	1.0	2.0
0912	2.0	2.0
0917	2.0	2.0
0927	2.0	1.0
0929	0.0	2.0
1002	1.0	1.0
1015	1.0	1.0
1016	1.0	1.0
1113	1.0	1.0
1115	1.0	1.0
KYJ0611006B	0.0	0.0
LIB0611004B	0.0	0.0
MOS0611012	0.0	0.0
PAR1011009B	0.0	0.0
PAR1011016	0.0	0.0
PAR1011017	0.0	0.0
PAR1011018	0.0	0.0
PHA0111001B	0.0	0.0
PHA0111002B	0.0	0.0
PHA0111011	0.0	0.0
PHA0111015	0.0	0.0
PHA0111018	0.0	0.0
PHA0112002A	1.0	1.0
PHA0112009B	0.0	0.0
PHA0209013	0.0	0.0
PHA0209026	0.0	0.0
PHA0209028	0.0	0.0
PHA0210001	0.0	0.0
PHA0411008A	0.0	1.0
PHA0411011A	1.0	1.0
PHA0411036	0.0	0.0
PHA0411045	0.0	0.0
PHA0411051	0.0	0.0
PHA0411053	0.0	0.0
PHA0411058	0.0	0.0
PHA0509019	0.0	0.0
PHA0509021	0.0	0.0
PHA0509033	0.0	0.0
PHA0509043	0.0	0.0
PHA0509044	0.0	0.0
PHA0510003A	0.0	1.0
PHA0510004B	0.0	0.0
PHA0510029	0.0	0.0
PHA0510050	0.0	0.0
PHA0610007A	0.0	0.0
PHA0610015	0.0	0.0
PHA0610018	0.0	0.0
PHA0610025	0.0	0.0
PHA0610026	0.0	0.0
PHA0710013	0.0	0.0
PHA0710016	0.0	0.0
PHA0710019	0.0	0.0
PHA0809010	0.0	0.0
PHA0810002	0.0	0.0
PHA0811017	0.0	0.0
PHA1109023	0.0	0.0
PHA1109025	0.0	0.0
PHA1109026	0.0	0.0
PHA1109027	0.0	0.0
PHA1110003A	1.0	1.0
PHA1110013	0.0	0.0
PHA1110014	0.0	0.0
PHA1110015	0.0	0.0
PHA1110021	0.0	0.0
PHA1111006B	0.0	0.0
VAR0909004	0.0	0.0
VAR0909007	0.0	0.0
VAR0910010	0.0	0.0
1325_1001015	2.0	2.0
1325_1001016	1.0	2.0
1325_1001018	2.0	2.0
1325_1001023	2.0	2.0
1325_1001029	2.0	2.0
1325_1001032	2.0	2.0
1325_1001035	3.0	2.0
1325_1001037	2.0	2.0
1325_1001040	2.0	2.0
1325_1001041	3.0	2.0
1325_1001043	2.0	1.0
1325_1001048	2.0	2.0
1325_1001051	1.0	2.0
1325_1001053	2.0	2.0
1325_1001054	3.0	2.0
1325_1001055	2.0	2.0
1325_1001057	2.0	2.0
1325_1001058	2.0	2.0
1325_1001080	2.0	2.0
1325_1001081	2.0	2.0
1325_1001090	2.0	2.0
1325_1001092	2.0	2.0
1325_1001094	2.0	0.0
1325_1001109	2.0	2.0
1325_1001119	2.0	2.0
1325_1001130	2.0	2.0
1325_1001132	2.0	2.0
1325_1001138	2.0	2.0
1325_1001139	2.0	2.0
1325_1001142	1.0	2.0
1325_1001143	2.0	2.0
1325_1001155	2.0	2.0
1325_1001156	2.0	2.0
1325_1001158	2.0	2.0
1325_1001165	1.0	2.0
1325_1001167	2.0	2.0
1325_1001170	2.0	2.0
1325_9000090	2.0	2.0
1325_9000095	2.0	2.0
1325_9000102	2.0	2.0
1325_9000186	3.0	2.0
1325_9000188	2.0	2.0
1325_9000213	3.0	2.0
1325_9000237	2.0	2.0
1325_9000314	2.0	2.0
1325_9000317	2.0	2.0
1325_9000684	2.0	2.0
1365_0100007	1.0	1.0
1365_0100019	1.0	2.0
1365_0100030	1.0	2.0
1365_0100056	1.0	2.0
1365_0100074	1.0	2.0
1365_0100099	2.0	2.0
1365_0100100	2.0	2.0
1365_0100105	3.0	2.0
1365_0100106	1.0	2.0
1365_0100120	2.0	2.0
1365_0100123	2.0	2.0
1365_0100136	2.0	2.0
1365_0100138	2.0	2.0
1365_0100139	2.0	2.0
1365_0100146	2.0	1.0
1365_0100148	1.0	1.0
1365_0100151	2.0	2.0
1365_0100165	2.0	2.0
1365_0100172	2.0	2.0
1365_0100179	2.0	2.0
1365_0100183	2.0	2.0
1365_0100186	2.0	2.0
1365_0100204	1.0	1.0
1365_0100217	2.0	2.0
1365_0100220	2.0	2.0
1365_0100221	2.0	2.0
1365_0100223	2.0	2.0
1365_0100224	2.0	2.0
1365_0100226	2.0	2.0
1365_0100251	2.0	2.0
1365_0100259	2.0	2.0
1365_0100268	2.0	2.0
1365_0100275	2.0	2.0
1365_0100276	2.0	2.0
1365_0100287	2.0	2.0
1365_0100288	2.0	2.0
1365_0100290	2.0	2.0
1365_0100447	2.0	2.0
1365_0100471	2.0	2.0
1365_0100476	2.0	2.0
1365_0100477	2.0	2.0
1365_0100478	2.0	2.0
1385_0000013	0.0	0.0
1385_0000017	1.0	1.0
1385_0000021	2.0	1.0
1385_0000043	2.0	2.0
1385_0000050	2.0	2.0
1385_0000103	1.0	0.0
1385_0000104	1.0	1.0
1385_0000126	1.0	1.0
1385_0001103	1.0	0.0
1385_0001104	1.0	1.0
1385_0001120	1.0	1.0
1385_0001122	1.0	1.0
1385_0001128	1.0	0.0
1385_0001147	1.0	1.0
1385_0001150	2.0	2.0
1385_0001151	2.0	1.0
1385_0001160	1.0	1.0
1385_0001167	1.0	1.0
1385_0001172	1.0	1.0
1385_0001175	1.0	1.0
1385_0001194	1.0	1.0
1385_0001197	1.0	1.0
1385_0001525	1.0	2.0
1385_0001526	0.0	1.0
1385_0001723	0.0	1.0
1385_0001729	1.0	1.0
1385_0001733	1.0	2.0
1385_0001734	1.0	2.0
1385_0001736	1.0	2.0
1385_0001739	0.0	1.0
1385_0001742	0.0	1.0
1385_0001756	1.0	1.0
1385_0001768	1.0	1.0
1385_0001771	1.0	1.0
1385_0001786	1.0	2.0
1385_0001789	1.0	2.0
1385_0001796	0.0	2.0
1385_0001799	1.0	2.0
1395_0000338	1.0	1.0
1395_0000359	1.0	1.0
1395_0000379	1.0	1.0
1395_0000409	2.0	1.0
1395_0000432	1.0	1.0
1395_0000447	1.0	1.0
1395_0000451	2.0	1.0
1395_0000458	1.0	1.0
1395_0000465	1.0	1.0
1395_0000516	1.0	1.0
1395_0000547	1.0	1.0
1395_0000554	2.0	1.0
1395_0000560	1.0	1.0
1395_0000572	1.0	1.0
1395_0000583	1.0	2.0
1395_0000587	0.0	1.0
1395_0000593	1.0	1.0
1395_0000597	1.0	1.0
1395_0000635	1.0	1.0
1395_0000636	1.0	1.0
1395_0001020	1.0	1.0
1395_0001034	1.0	1.0
1395_0001075	1.0	1.0
1395_0001080	2.0	1.0
1395_0001084	1.0	1.0
1395_0001093	1.0	1.0
1395_0001121	0.0	1.0
1395_0001122	1.0	1.0
1395_0001123	0.0	0.0
1395_0001161	1.0	1.0
1395_0001169	1.0	1.0
1395_0001171	1.0	1.0
Averaged weighted F1-scores 0.6689371501607274
