Doing monolingual classification for  DE
************for dimension:  OverallCEFRrating  ***************
Extracted all features: 
Printing class statistics
Counter({'B1': 328, 'A2': 306, 'B2': 293, 'A1': 57, 'C1': 42})
With Word ngrams: 
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3414
3414
Fold 0
[[ 4  7  1  0  0]
 [ 1 54  7  0  0]
 [ 0 18 34 14  0]
 [ 0  2  2 55  0]
 [ 0  0  0  9  0]]
              precision    recall  f1-score   support

          A1       0.80      0.33      0.47        12
          A2       0.67      0.87      0.76        62
          B1       0.77      0.52      0.62        66
          B2       0.71      0.93      0.80        59
          C1       0.00      0.00      0.00         9

    accuracy                           0.71       208
   macro avg       0.59      0.53      0.53       208
weighted avg       0.69      0.71      0.68       208


Fold 1
[[ 1 11  0  0  0]
 [ 2 54  5  0  0]
 [ 0 20 26 20  0]
 [ 0  1  5 53  0]
 [ 0  0  0  9  0]]
              precision    recall  f1-score   support

          A1       0.33      0.08      0.13        12
          A2       0.63      0.89      0.73        61
          B1       0.72      0.39      0.51        66
          B2       0.65      0.90      0.75        59
          C1       0.00      0.00      0.00         9

    accuracy                           0.65       207
   macro avg       0.47      0.45      0.43       207
weighted avg       0.62      0.65      0.60       207


Fold 2
[[ 4  7  0  0  0]
 [ 0 46 15  0  0]
 [ 0 15 33 18  0]
 [ 0  0  2 57  0]
 [ 0  0  0  8  0]]
              precision    recall  f1-score   support

          A1       1.00      0.36      0.53        11
          A2       0.68      0.75      0.71        61
          B1       0.66      0.50      0.57        66
          B2       0.69      0.97      0.80        59
          C1       0.00      0.00      0.00         8

    accuracy                           0.68       205
   macro avg       0.60      0.52      0.52       205
weighted avg       0.67      0.68      0.66       205


Fold 3
[[ 3  8  0  0  0]
 [ 1 50 10  0  0]
 [ 0 16 36 13  0]
 [ 0  1  4 53  0]
 [ 0  0  0  8  0]]
              precision    recall  f1-score   support

          A1       0.75      0.27      0.40        11
          A2       0.67      0.82      0.74        61
          B1       0.72      0.55      0.63        65
          B2       0.72      0.91      0.80        58
          C1       0.00      0.00      0.00         8

    accuracy                           0.70       203
   macro avg       0.57      0.51      0.51       203
weighted avg       0.68      0.70      0.67       203


Fold 4
[[ 5  6  0  0  0]
 [ 2 53  6  0  0]
 [ 0 20 35 10  0]
 [ 0  0  7 51  0]
 [ 0  0  0  8  0]]
              precision    recall  f1-score   support

          A1       0.71      0.45      0.56        11
          A2       0.67      0.87      0.76        61
          B1       0.73      0.54      0.62        65
          B2       0.74      0.88      0.80        58
          C1       0.00      0.00      0.00         8

    accuracy                           0.71       203
   macro avg       0.57      0.55      0.55       203
weighted avg       0.68      0.71      0.69       203


K-fold scores
[0.67617546356936, 0.6010531173102399, 0.655065677837505, 0.6725337484046365, 0.6854432969844505]
SKF f1 score mean 0.6580542608212385

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3414
3414
Fold 0
[[ 3  8  1  0  0]
 [ 4 48 10  0  0]
 [ 0 16 44  5  1]
 [ 0  2 14 39  4]
 [ 0  0  1  8  0]]
              precision    recall  f1-score   support

          A1       0.43      0.25      0.32        12
          A2       0.65      0.77      0.71        62
          B1       0.63      0.67      0.65        66
          B2       0.75      0.66      0.70        59
          C1       0.00      0.00      0.00         9

    accuracy                           0.64       208
   macro avg       0.49      0.47      0.47       208
weighted avg       0.63      0.64      0.63       208


Fold 1
[[ 4  7  1  0  0]
 [ 4 50  7  0  0]
 [ 1 19 31 15  0]
 [ 0  0 12 44  3]
 [ 0  0  1  7  1]]
              precision    recall  f1-score   support

          A1       0.44      0.33      0.38        12
          A2       0.66      0.82      0.73        61
          B1       0.60      0.47      0.53        66
          B2       0.67      0.75      0.70        59
          C1       0.25      0.11      0.15         9

    accuracy                           0.63       207
   macro avg       0.52      0.50      0.50       207
weighted avg       0.61      0.63      0.61       207


Fold 2
[[ 7  3  1  0  0]
 [ 5 40 15  1  0]
 [ 1 15 37 13  0]
 [ 0  2 10 44  3]
 [ 0  0  1  6  1]]
              precision    recall  f1-score   support

          A1       0.54      0.64      0.58        11
          A2       0.67      0.66      0.66        61
          B1       0.58      0.56      0.57        66
          B2       0.69      0.75      0.72        59
          C1       0.25      0.12      0.17         8

    accuracy                           0.63       205
   macro avg       0.54      0.54      0.54       205
weighted avg       0.62      0.63      0.62       205


Fold 3
[[ 6  4  1  0  0]
 [ 6 41 14  0  0]
 [ 1 15 43  6  0]
 [ 0  0 14 39  5]
 [ 0  0  0  8  0]]
              precision    recall  f1-score   support

          A1       0.46      0.55      0.50        11
          A2       0.68      0.67      0.68        61
          B1       0.60      0.66      0.63        65
          B2       0.74      0.67      0.70        58
          C1       0.00      0.00      0.00         8

    accuracy                           0.64       203
   macro avg       0.50      0.51      0.50       203
weighted avg       0.63      0.64      0.63       203


Fold 4
[[ 6  5  0  0  0]
 [ 5 38 17  1  0]
 [ 2 12 40 10  1]
 [ 0  2 14 40  2]
 [ 0  0  0  5  3]]
              precision    recall  f1-score   support

          A1       0.46      0.55      0.50        11
          A2       0.67      0.62      0.64        61
          B1       0.56      0.62      0.59        65
          B2       0.71      0.69      0.70        58
          C1       0.50      0.38      0.43         8

    accuracy                           0.63       203
   macro avg       0.58      0.57      0.57       203
weighted avg       0.63      0.63      0.63       203


K-fold scores
[0.6332669297065581, 0.6120558333477988, 0.6237131287582858, 0.6325050218718845, 0.6263731799280972]
SKF f1 score mean 0.6255828187225247

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3414
3414
Fold 0
[[ 3  8  1  0  0]
 [ 4 47 11  0  0]
 [ 1 13 46  5  1]
 [ 0  3  9 42  5]
 [ 0  0  1  8  0]]
              precision    recall  f1-score   support

          A1       0.38      0.25      0.30        12
          A2       0.66      0.76      0.71        62
          B1       0.68      0.70      0.69        66
          B2       0.76      0.71      0.74        59
          C1       0.00      0.00      0.00         9

    accuracy                           0.66       208
   macro avg       0.50      0.48      0.49       208
weighted avg       0.65      0.66      0.65       208


Fold 1
[[ 4  8  0  0  0]
 [ 4 51  6  0  0]
 [ 1 19 31 15  0]
 [ 0  0 12 43  4]
 [ 0  0  0  8  1]]
              precision    recall  f1-score   support

          A1       0.44      0.33      0.38        12
          A2       0.65      0.84      0.73        61
          B1       0.63      0.47      0.54        66
          B2       0.65      0.73      0.69        59
          C1       0.20      0.11      0.14         9

    accuracy                           0.63       207
   macro avg       0.52      0.50      0.50       207
weighted avg       0.61      0.63      0.61       207


Fold 2
[[ 5  5  1  0  0]
 [ 3 42 16  0  0]
 [ 0 14 37 15  0]
 [ 0  0  5 51  3]
 [ 0  0  1  6  1]]
              precision    recall  f1-score   support

          A1       0.62      0.45      0.53        11
          A2       0.69      0.69      0.69        61
          B1       0.62      0.56      0.59        66
          B2       0.71      0.86      0.78        59
          C1       0.25      0.12      0.17         8

    accuracy                           0.66       205
   macro avg       0.58      0.54      0.55       205
weighted avg       0.65      0.66      0.65       205


Fold 3
[[ 6  4  1  0  0]
 [ 4 45 12  0  0]
 [ 1 11 45  8  0]
 [ 0  0 11 43  4]
 [ 0  0  0  8  0]]
              precision    recall  f1-score   support

          A1       0.55      0.55      0.55        11
          A2       0.75      0.74      0.74        61
          B1       0.65      0.69      0.67        65
          B2       0.73      0.74      0.74        58
          C1       0.00      0.00      0.00         8

    accuracy                           0.68       203
   macro avg       0.54      0.54      0.54       203
weighted avg       0.67      0.68      0.68       203


Fold 4
[[ 6  5  0  0  0]
 [ 5 39 16  1  0]
 [ 0 10 44 10  1]
 [ 0  2 15 39  2]
 [ 0  0  0  5  3]]
              precision    recall  f1-score   support

          A1       0.55      0.55      0.55        11
          A2       0.70      0.64      0.67        61
          B1       0.59      0.68      0.63        65
          B2       0.71      0.67      0.69        58
          C1       0.50      0.38      0.43         8

    accuracy                           0.65       203
   macro avg       0.61      0.58      0.59       203
weighted avg       0.65      0.65      0.65       203


K-fold scores
[0.6548397399929216, 0.612533050632138, 0.6527982589129685, 0.67813347722579, 0.6452599959727559]
SKF f1 score mean 0.6487129045473148

SAME LANG EVAL DONE FOR THIS LANG
With POS ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
6373
6373
Fold 0
[[ 3  9  0  0  0]
 [ 2 45 15  0  0]
 [ 0  8 45 13  0]
 [ 0  1  2 56  0]
 [ 0  0  0  9  0]]
              precision    recall  f1-score   support

          A1       0.60      0.25      0.35        12
          A2       0.71      0.73      0.72        62
          B1       0.73      0.68      0.70        66
          B2       0.72      0.95      0.82        59
          C1       0.00      0.00      0.00         9

    accuracy                           0.72       208
   macro avg       0.55      0.52      0.52       208
weighted avg       0.68      0.72      0.69       208


Fold 1
[[ 2 10  0  0  0]
 [ 0 53  8  0  0]
 [ 0 15 32 19  0]
 [ 0  1  9 49  0]
 [ 0  0  0  9  0]]
              precision    recall  f1-score   support

          A1       1.00      0.17      0.29        12
          A2       0.67      0.87      0.76        61
          B1       0.65      0.48      0.56        66
          B2       0.64      0.83      0.72        59
          C1       0.00      0.00      0.00         9

    accuracy                           0.66       207
   macro avg       0.59      0.47      0.46       207
weighted avg       0.65      0.66      0.62       207


Fold 2
[[ 3  8  0  0  0]
 [ 0 46 15  0  0]
 [ 0 11 42 13  0]
 [ 0  1  7 51  0]
 [ 0  0  0  8  0]]
              precision    recall  f1-score   support

          A1       1.00      0.27      0.43        11
          A2       0.70      0.75      0.72        61
          B1       0.66      0.64      0.65        66
          B2       0.71      0.86      0.78        59
          C1       0.00      0.00      0.00         8

    accuracy                           0.69       205
   macro avg       0.61      0.51      0.52       205
weighted avg       0.68      0.69      0.67       205


Fold 3
[[ 4  7  0  0  0]
 [ 0 52  9  0  0]
 [ 0 12 45  8  0]
 [ 0  0  7 51  0]
 [ 0  0  0  8  0]]
              precision    recall  f1-score   support

          A1       1.00      0.36      0.53        11
          A2       0.73      0.85      0.79        61
          B1       0.74      0.69      0.71        65
          B2       0.76      0.88      0.82        58
          C1       0.00      0.00      0.00         8

    accuracy                           0.75       203
   macro avg       0.65      0.56      0.57       203
weighted avg       0.73      0.75      0.73       203


Fold 4
[[ 7  4  0  0  0]
 [ 0 56  5  0  0]
 [ 0 18 37 10  0]
 [ 0  0  8 50  0]
 [ 0  0  0  8  0]]
              precision    recall  f1-score   support

          A1       1.00      0.64      0.78        11
          A2       0.72      0.92      0.81        61
          B1       0.74      0.57      0.64        65
          B2       0.74      0.86      0.79        58
          C1       0.00      0.00      0.00         8

    accuracy                           0.74       203
   macro avg       0.64      0.60      0.60       203
weighted avg       0.72      0.74      0.72       203


K-fold scores
[0.6899765421153021, 0.6225093061799389, 0.6706748645663524, 0.7275066214573603, 0.7170663432537635]
SKF f1 score mean 0.6855467355145435

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
6373
6373
Fold 0
[[ 5  6  1  0  0]
 [ 7 41 12  2  0]
 [ 1 19 35 11  0]
 [ 0  3 15 35  6]
 [ 0  0  0  9  0]]
              precision    recall  f1-score   support

          A1       0.38      0.42      0.40        12
          A2       0.59      0.66      0.63        62
          B1       0.56      0.53      0.54        66
          B2       0.61      0.59      0.60        59
          C1       0.00      0.00      0.00         9

    accuracy                           0.56       208
   macro avg       0.43      0.44      0.43       208
weighted avg       0.55      0.56      0.55       208


Fold 1
[[ 1  9  2  0  0]
 [ 5 49  6  0  1]
 [ 0 16 36 14  0]
 [ 0  4 14 38  3]
 [ 0  0  0  9  0]]
              precision    recall  f1-score   support

          A1       0.17      0.08      0.11        12
          A2       0.63      0.80      0.71        61
          B1       0.62      0.55      0.58        66
          B2       0.62      0.64      0.63        59
          C1       0.00      0.00      0.00         9

    accuracy                           0.60       207
   macro avg       0.41      0.42      0.41       207
weighted avg       0.57      0.60      0.58       207


Fold 2
[[ 1 10  0  0  0]
 [ 5 38 17  1  0]
 [ 0 14 40 11  1]
 [ 0  1 22 32  4]
 [ 0  1  0  6  1]]
              precision    recall  f1-score   support

          A1       0.17      0.09      0.12        11
          A2       0.59      0.62      0.61        61
          B1       0.51      0.61      0.55        66
          B2       0.64      0.54      0.59        59
          C1       0.17      0.12      0.14         8

    accuracy                           0.55       205
   macro avg       0.41      0.40      0.40       205
weighted avg       0.54      0.55      0.54       205


Fold 3
[[ 5  5  1  0  0]
 [ 7 41 12  1  0]
 [ 0 20 35 10  0]
 [ 0  2 11 45  0]
 [ 0  0  1  7  0]]
              precision    recall  f1-score   support

          A1       0.42      0.45      0.43        11
          A2       0.60      0.67      0.64        61
          B1       0.58      0.54      0.56        65
          B2       0.71      0.78      0.74        58
          C1       0.00      0.00      0.00         8

    accuracy                           0.62       203
   macro avg       0.46      0.49      0.47       203
weighted avg       0.59      0.62      0.61       203


Fold 4
[[ 4  7  0  0  0]
 [ 8 41 11  1  0]
 [ 3 19 37  6  0]
 [ 0  3 16 36  3]
 [ 0  0  0  7  1]]
              precision    recall  f1-score   support

          A1       0.27      0.36      0.31        11
          A2       0.59      0.67      0.63        61
          B1       0.58      0.57      0.57        65
          B2       0.72      0.62      0.67        58
          C1       0.25      0.12      0.17         8

    accuracy                           0.59       203
   macro avg       0.48      0.47      0.47       203
weighted avg       0.59      0.59      0.59       203


K-fold scores
[0.5530123176422003, 0.5798539849747132, 0.5394193645279348, 0.606395558432859, 0.5854908531867584]
SKF f1 score mean 0.5728344157528932

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
6373
6373
Fold 0
[[ 4  7  1  0  0]
 [ 7 42 12  1  0]
 [ 0 12 43 11  0]
 [ 0  2 12 39  6]
 [ 0  0  0  9  0]]
              precision    recall  f1-score   support

          A1       0.36      0.33      0.35        12
          A2       0.67      0.68      0.67        62
          B1       0.63      0.65      0.64        66
          B2       0.65      0.66      0.66        59
          C1       0.00      0.00      0.00         9

    accuracy                           0.62       208
   macro avg       0.46      0.46      0.46       208
weighted avg       0.60      0.62      0.61       208


Fold 1
[[ 1  9  2  0  0]
 [ 5 47  9  0  0]
 [ 0 16 35 15  0]
 [ 0  3 11 42  3]
 [ 0  0  0  9  0]]
              precision    recall  f1-score   support

          A1       0.17      0.08      0.11        12
          A2       0.63      0.77      0.69        61
          B1       0.61      0.53      0.57        66
          B2       0.64      0.71      0.67        59
          C1       0.00      0.00      0.00         9

    accuracy                           0.60       207
   macro avg       0.41      0.42      0.41       207
weighted avg       0.57      0.60      0.58       207


Fold 2
[[ 2  9  0  0  0]
 [ 4 40 16  1  0]
 [ 0 13 41 11  1]
 [ 0  1 18 36  4]
 [ 0  0  0  7  1]]
              precision    recall  f1-score   support

          A1       0.33      0.18      0.24        11
          A2       0.63      0.66      0.65        61
          B1       0.55      0.62      0.58        66
          B2       0.65      0.61      0.63        59
          C1       0.17      0.12      0.14         8

    accuracy                           0.59       205
   macro avg       0.47      0.44      0.45       205
weighted avg       0.58      0.59      0.58       205


Fold 3
[[ 3  5  3  0  0]
 [ 6 41 13  1  0]
 [ 0 23 32 10  0]
 [ 0  2 10 46  0]
 [ 0  0  1  7  0]]
              precision    recall  f1-score   support

          A1       0.33      0.27      0.30        11
          A2       0.58      0.67      0.62        61
          B1       0.54      0.49      0.52        65
          B2       0.72      0.79      0.75        58
          C1       0.00      0.00      0.00         8

    accuracy                           0.60       203
   macro avg       0.43      0.45      0.44       203
weighted avg       0.57      0.60      0.58       203


Fold 4
[[ 4  7  0  0  0]
 [ 7 42 10  2  0]
 [ 2 17 40  6  0]
 [ 0  2 17 36  3]
 [ 0  0  0  7  1]]
              precision    recall  f1-score   support

          A1       0.31      0.36      0.33        11
          A2       0.62      0.69      0.65        61
          B1       0.60      0.62      0.61        65
          B2       0.71      0.62      0.66        58
          C1       0.25      0.12      0.17         8

    accuracy                           0.61       203
   macro avg       0.50      0.48      0.48       203
weighted avg       0.60      0.61      0.60       203


K-fold scores
[0.6099441870493517, 0.5831114669032453, 0.5791808183648454, 0.5836454749199329, 0.6030876661674981]
SKF f1 score mean 0.5917939226809746

SAME LANG EVAL DONE FOR THIS LANG
Dep ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5787
5787
Fold 0
[[ 3  8  1  0  0]
 [ 1 44 17  0  0]
 [ 0 15 37 14  0]
 [ 0  1  1 57  0]
 [ 0  0  1  8  0]]
              precision    recall  f1-score   support

          A1       0.75      0.25      0.38        12
          A2       0.65      0.71      0.68        62
          B1       0.65      0.56      0.60        66
          B2       0.72      0.97      0.83        59
          C1       0.00      0.00      0.00         9

    accuracy                           0.68       208
   macro avg       0.55      0.50      0.50       208
weighted avg       0.65      0.68      0.65       208


Fold 1
[[ 2 10  0  0  0]
 [ 2 53  6  0  0]
 [ 0 12 34 20  0]
 [ 0  1 11 47  0]
 [ 0  0  0  9  0]]
              precision    recall  f1-score   support

          A1       0.50      0.17      0.25        12
          A2       0.70      0.87      0.77        61
          B1       0.67      0.52      0.58        66
          B2       0.62      0.80      0.70        59
          C1       0.00      0.00      0.00         9

    accuracy                           0.66       207
   macro avg       0.50      0.47      0.46       207
weighted avg       0.62      0.66      0.63       207


Fold 2
[[ 3  8  0  0  0]
 [ 0 47 14  0  0]
 [ 0 15 36 15  0]
 [ 0  1  6 52  0]
 [ 0  0  0  8  0]]
              precision    recall  f1-score   support

          A1       1.00      0.27      0.43        11
          A2       0.66      0.77      0.71        61
          B1       0.64      0.55      0.59        66
          B2       0.69      0.88      0.78        59
          C1       0.00      0.00      0.00         8

    accuracy                           0.67       205
   macro avg       0.60      0.49      0.50       205
weighted avg       0.66      0.67      0.65       205


Fold 3
[[ 4  7  0  0  0]
 [ 0 57  4  0  0]
 [ 0 15 43  7  0]
 [ 0  3  4 51  0]
 [ 0  0  0  8  0]]
              precision    recall  f1-score   support

          A1       1.00      0.36      0.53        11
          A2       0.70      0.93      0.80        61
          B1       0.84      0.66      0.74        65
          B2       0.77      0.88      0.82        58
          C1       0.00      0.00      0.00         8

    accuracy                           0.76       203
   macro avg       0.66      0.57      0.58       203
weighted avg       0.75      0.76      0.74       203


Fold 4
[[ 7  4  0  0  0]
 [ 0 56  4  1  0]
 [ 0 24 32  9  0]
 [ 0  0 11 47  0]
 [ 0  0  0  8  0]]
              precision    recall  f1-score   support

          A1       1.00      0.64      0.78        11
          A2       0.67      0.92      0.77        61
          B1       0.68      0.49      0.57        65
          B2       0.72      0.81      0.76        58
          C1       0.00      0.00      0.00         8

    accuracy                           0.70       203
   macro avg       0.61      0.57      0.58       203
weighted avg       0.68      0.70      0.68       203


K-fold scores
[0.6486330686403081, 0.6262682904948147, 0.648270946838684, 0.7408638910729357, 0.6755707257623427]
SKF f1 score mean 0.667921384561817

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5787
5787
Fold 0
[[ 3  8  1  0  0]
 [ 6 37 18  1  0]
 [ 1 14 37 14  0]
 [ 0  2 17 36  4]
 [ 0  0  3  5  1]]
              precision    recall  f1-score   support

          A1       0.30      0.25      0.27        12
          A2       0.61      0.60      0.60        62
          B1       0.49      0.56      0.52        66
          B2       0.64      0.61      0.63        59
          C1       0.20      0.11      0.14         9

    accuracy                           0.55       208
   macro avg       0.45      0.43      0.43       208
weighted avg       0.54      0.55      0.54       208


Fold 1
[[ 2  6  4  0  0]
 [ 4 45 11  1  0]
 [ 0 17 32 17  0]
 [ 0  5 16 36  2]
 [ 0  0  2  7  0]]
              precision    recall  f1-score   support

          A1       0.33      0.17      0.22        12
          A2       0.62      0.74      0.67        61
          B1       0.49      0.48      0.49        66
          B2       0.59      0.61      0.60        59
          C1       0.00      0.00      0.00         9

    accuracy                           0.56       207
   macro avg       0.41      0.40      0.40       207
weighted avg       0.53      0.56      0.54       207


Fold 2
[[ 1 10  0  0  0]
 [ 5 40 13  3  0]
 [ 0 18 33 15  0]
 [ 0  3 19 35  2]
 [ 0  0  2  6  0]]
              precision    recall  f1-score   support

          A1       0.17      0.09      0.12        11
          A2       0.56      0.66      0.61        61
          B1       0.49      0.50      0.50        66
          B2       0.59      0.59      0.59        59
          C1       0.00      0.00      0.00         8

    accuracy                           0.53       205
   macro avg       0.36      0.37      0.36       205
weighted avg       0.51      0.53      0.52       205


Fold 3
[[ 4  7  0  0  0]
 [ 3 46 11  1  0]
 [ 1 27 29  8  0]
 [ 0  4  5 49  0]
 [ 0  0  0  8  0]]
              precision    recall  f1-score   support

          A1       0.50      0.36      0.42        11
          A2       0.55      0.75      0.63        61
          B1       0.64      0.45      0.53        65
          B2       0.74      0.84      0.79        58
          C1       0.00      0.00      0.00         8

    accuracy                           0.63       203
   macro avg       0.49      0.48      0.47       203
weighted avg       0.61      0.63      0.61       203


Fold 4
[[ 5  6  0  0  0]
 [ 4 44 11  2  0]
 [ 1 22 34  7  1]
 [ 0  2 20 34  2]
 [ 0  0  0  8  0]]
              precision    recall  f1-score   support

          A1       0.50      0.45      0.48        11
          A2       0.59      0.72      0.65        61
          B1       0.52      0.52      0.52        65
          B2       0.67      0.59      0.62        58
          C1       0.00      0.00      0.00         8

    accuracy                           0.58       203
   macro avg       0.46      0.46      0.45       203
weighted avg       0.56      0.58      0.57       203


K-fold scores
[0.5441959192237372, 0.5375898102877213, 0.5171497283707508, 0.6081106609525967, 0.5674115489634991]
SKF f1 score mean 0.554891533559661

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5787
5787
Fold 0
[[ 2  9  1  0  0]
 [ 4 38 19  1  0]
 [ 0 14 38 14  0]
 [ 0  1 12 43  3]
 [ 0  0  2  7  0]]
              precision    recall  f1-score   support

          A1       0.33      0.17      0.22        12
          A2       0.61      0.61      0.61        62
          B1       0.53      0.58      0.55        66
          B2       0.66      0.73      0.69        59
          C1       0.00      0.00      0.00         9

    accuracy                           0.58       208
   macro avg       0.43      0.42      0.42       208
weighted avg       0.56      0.58      0.57       208


Fold 1
[[ 2  7  3  0  0]
 [ 4 45 11  1  0]
 [ 0 15 33 18  0]
 [ 0  2 11 44  2]
 [ 0  0  1  8  0]]
              precision    recall  f1-score   support

          A1       0.33      0.17      0.22        12
          A2       0.65      0.74      0.69        61
          B1       0.56      0.50      0.53        66
          B2       0.62      0.75      0.68        59
          C1       0.00      0.00      0.00         9

    accuracy                           0.60       207
   macro avg       0.43      0.43      0.42       207
weighted avg       0.57      0.60      0.58       207


Fold 2
[[ 1  9  1  0  0]
 [ 6 39 13  3  0]
 [ 0 17 33 16  0]
 [ 0  2 15 40  2]
 [ 0  0  2  6  0]]
              precision    recall  f1-score   support

          A1       0.14      0.09      0.11        11
          A2       0.58      0.64      0.61        61
          B1       0.52      0.50      0.51        66
          B2       0.62      0.68      0.65        59
          C1       0.00      0.00      0.00         8

    accuracy                           0.55       205
   macro avg       0.37      0.38      0.37       205
weighted avg       0.52      0.55      0.54       205


Fold 3
[[ 3  8  0  0  0]
 [ 2 45 14  0  0]
 [ 1 24 33  7  0]
 [ 0  1  7 50  0]
 [ 0  0  0  8  0]]
              precision    recall  f1-score   support

          A1       0.50      0.27      0.35        11
          A2       0.58      0.74      0.65        61
          B1       0.61      0.51      0.55        65
          B2       0.77      0.86      0.81        58
          C1       0.00      0.00      0.00         8

    accuracy                           0.65       203
   macro avg       0.49      0.48      0.47       203
weighted avg       0.62      0.65      0.62       203


Fold 4
[[ 4  7  0  0  0]
 [ 2 45 12  2  0]
 [ 1 20 36  8  0]
 [ 0  1 18 37  2]
 [ 0  0  0  8  0]]
              precision    recall  f1-score   support

          A1       0.57      0.36      0.44        11
          A2       0.62      0.74      0.67        61
          B1       0.55      0.55      0.55        65
          B2       0.67      0.64      0.65        58
          C1       0.00      0.00      0.00         8

    accuracy                           0.60       203
   macro avg       0.48      0.46      0.46       203
weighted avg       0.58      0.60      0.59       203


K-fold scores
[0.5669896518862156, 0.5781830794004708, 0.5364210032143746, 0.623564766165798, 0.5889976840752249]
SKF f1 score mean 0.5788312369484168

SAME LANG EVAL DONE FOR THIS LANG
************for dimension:  Grammaticalaccuracy  ***************
Extracted all features: 
Printing class statistics
Counter({'B1': 349, 'A2': 284, 'B2': 258, 'A1': 88, 'C1': 47})
With Word ngrams: 
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3414
3414
Fold 0
[[ 8  9  1  0  0]
 [ 5 38 14  0  0]
 [ 2 18 40 10  0]
 [ 0  0 12 40  0]
 [ 0  0  0 10  0]]
              precision    recall  f1-score   support

          A1       0.53      0.44      0.48        18
          A2       0.58      0.67      0.62        57
          B1       0.60      0.57      0.58        70
          B2       0.67      0.77      0.71        52
          C1       0.00      0.00      0.00        10

    accuracy                           0.61       207
   macro avg       0.48      0.49      0.48       207
weighted avg       0.58      0.61      0.59       207


Fold 1
[[ 4 13  1  0  0]
 [ 2 43 12  0  0]
 [ 3 19 32 16  0]
 [ 0  3  8 41  0]
 [ 0  0  0 10  0]]
              precision    recall  f1-score   support

          A1       0.44      0.22      0.30        18
          A2       0.55      0.75      0.64        57
          B1       0.60      0.46      0.52        70
          B2       0.61      0.79      0.69        52
          C1       0.00      0.00      0.00        10

    accuracy                           0.58       207
   macro avg       0.44      0.44      0.43       207
weighted avg       0.55      0.58      0.55       207


Fold 2
[[ 8 10  0  0  0]
 [ 1 42 14  0  0]
 [ 0 28 36  6  0]
 [ 0  1 13 38  0]
 [ 0  0  1  8  0]]
              precision    recall  f1-score   support

          A1       0.89      0.44      0.59        18
          A2       0.52      0.74      0.61        57
          B1       0.56      0.51      0.54        70
          B2       0.73      0.73      0.73        52
          C1       0.00      0.00      0.00         9

    accuracy                           0.60       206
   macro avg       0.54      0.49      0.49       206
weighted avg       0.60      0.60      0.59       206


Fold 3
[[ 8  8  1  0  0]
 [ 3 42 12  0  0]
 [ 0 27 25 18  0]
 [ 0  1  9 41  0]
 [ 0  0  0  9  0]]
              precision    recall  f1-score   support

          A1       0.73      0.47      0.57        17
          A2       0.54      0.74      0.62        57
          B1       0.53      0.36      0.43        70
          B2       0.60      0.80      0.69        51
          C1       0.00      0.00      0.00         9

    accuracy                           0.57       204
   macro avg       0.48      0.47      0.46       204
weighted avg       0.54      0.57      0.54       204


Fold 4
[[ 9  8  0  0  0]
 [ 5 41 10  0  0]
 [ 0 24 34 11  0]
 [ 0  1 12 38  0]
 [ 0  0  0  9  0]]
              precision    recall  f1-score   support

          A1       0.64      0.53      0.58        17
          A2       0.55      0.73      0.63        56
          B1       0.61      0.49      0.54        69
          B2       0.66      0.75      0.70        51
          C1       0.00      0.00      0.00         9

    accuracy                           0.60       202
   macro avg       0.49      0.50      0.49       202
weighted avg       0.58      0.60      0.59       202


K-fold scores
[0.5906001884067655, 0.5502373982859708, 0.5872536851411998, 0.5403840168546051, 0.5855924638244323]
SKF f1 score mean 0.5708135505025947

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3414
3414
Fold 0
[[ 7  6  5  0  0]
 [ 7 28 21  1  0]
 [ 3 15 42  9  1]
 [ 0  2 11 32  7]
 [ 0  0  1  8  1]]
              precision    recall  f1-score   support

          A1       0.41      0.39      0.40        18
          A2       0.55      0.49      0.52        57
          B1       0.53      0.60      0.56        70
          B2       0.64      0.62      0.63        52
          C1       0.11      0.10      0.11        10

    accuracy                           0.53       207
   macro avg       0.45      0.44      0.44       207
weighted avg       0.53      0.53      0.53       207


Fold 1
[[ 6 11  1  0  0]
 [ 8 32 17  0  0]
 [ 1 21 32 16  0]
 [ 0  4 11 31  6]
 [ 0  0  0 10  0]]
              precision    recall  f1-score   support

          A1       0.40      0.33      0.36        18
          A2       0.47      0.56      0.51        57
          B1       0.52      0.46      0.49        70
          B2       0.54      0.60      0.57        52
          C1       0.00      0.00      0.00        10

    accuracy                           0.49       207
   macro avg       0.39      0.39      0.39       207
weighted avg       0.48      0.49      0.48       207


Fold 2
[[ 8  9  1  0  0]
 [ 5 37 14  1  0]
 [ 4 20 37  8  1]
 [ 0  1 15 31  5]
 [ 0  0  1  7  1]]
              precision    recall  f1-score   support

          A1       0.47      0.44      0.46        18
          A2       0.55      0.65      0.60        57
          B1       0.54      0.53      0.54        70
          B2       0.66      0.60      0.63        52
          C1       0.14      0.11      0.12         9

    accuracy                           0.55       206
   macro avg       0.47      0.47      0.47       206
weighted avg       0.55      0.55      0.55       206


Fold 3
[[ 8  7  2  0  0]
 [ 4 34 19  0  0]
 [ 2 19 35 13  1]
 [ 0  2 12 33  4]
 [ 0  0  2  6  1]]
              precision    recall  f1-score   support

          A1       0.57      0.47      0.52        17
          A2       0.55      0.60      0.57        57
          B1       0.50      0.50      0.50        70
          B2       0.63      0.65      0.64        51
          C1       0.17      0.11      0.13         9

    accuracy                           0.54       204
   macro avg       0.48      0.47      0.47       204
weighted avg       0.54      0.54      0.54       204


Fold 4
[[10  4  3  0  0]
 [ 6 32 18  0  0]
 [ 4 21 30 14  0]
 [ 0  3 11 33  4]
 [ 0  0  0  7  2]]
              precision    recall  f1-score   support

          A1       0.50      0.59      0.54        17
          A2       0.53      0.57      0.55        56
          B1       0.48      0.43      0.46        69
          B2       0.61      0.65      0.63        51
          C1       0.33      0.22      0.27         9

    accuracy                           0.53       202
   macro avg       0.49      0.49      0.49       202
weighted avg       0.52      0.53      0.53       202


K-fold scores
[0.5296407638400729, 0.4807048767114659, 0.5508329558764273, 0.540319773383829, 0.5254749366613094]
SKF f1 score mean 0.5253946612946209

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3414
3414
Fold 0
[[ 7  7  4  0  0]
 [ 4 30 22  1  0]
 [ 2 16 41 10  1]
 [ 0  2 10 34  6]
 [ 0  0  0  9  1]]
              precision    recall  f1-score   support

          A1       0.54      0.39      0.45        18
          A2       0.55      0.53      0.54        57
          B1       0.53      0.59      0.56        70
          B2       0.63      0.65      0.64        52
          C1       0.12      0.10      0.11        10

    accuracy                           0.55       207
   macro avg       0.47      0.45      0.46       207
weighted avg       0.54      0.55      0.54       207


Fold 1
[[ 5 12  1  0  0]
 [ 7 36 14  0  0]
 [ 1 21 31 17  0]
 [ 0  2 12 34  4]
 [ 0  0  0 10  0]]
              precision    recall  f1-score   support

          A1       0.38      0.28      0.32        18
          A2       0.51      0.63      0.56        57
          B1       0.53      0.44      0.48        70
          B2       0.56      0.65      0.60        52
          C1       0.00      0.00      0.00        10

    accuracy                           0.51       207
   macro avg       0.40      0.40      0.39       207
weighted avg       0.49      0.51      0.50       207


Fold 2
[[ 9  8  1  0  0]
 [ 3 38 16  0  0]
 [ 3 19 41  7  0]
 [ 0  1 12 34  5]
 [ 0  0  1  7  1]]
              precision    recall  f1-score   support

          A1       0.60      0.50      0.55        18
          A2       0.58      0.67      0.62        57
          B1       0.58      0.59      0.58        70
          B2       0.71      0.65      0.68        52
          C1       0.17      0.11      0.13         9

    accuracy                           0.60       206
   macro avg       0.53      0.50      0.51       206
weighted avg       0.59      0.60      0.59       206


Fold 3
[[ 8  8  1  0  0]
 [ 5 34 18  0  0]
 [ 2 22 34 11  1]
 [ 0  2 12 32  5]
 [ 0  0  0  8  1]]
              precision    recall  f1-score   support

          A1       0.53      0.47      0.50        17
          A2       0.52      0.60      0.55        57
          B1       0.52      0.49      0.50        70
          B2       0.63      0.63      0.63        51
          C1       0.14      0.11      0.12         9

    accuracy                           0.53       204
   macro avg       0.47      0.46      0.46       204
weighted avg       0.53      0.53      0.53       204


Fold 4
[[11  4  2  0  0]
 [ 6 33 17  0  0]
 [ 4 17 34 14  0]
 [ 0  2 11 33  5]
 [ 0  0  0  7  2]]
              precision    recall  f1-score   support

          A1       0.52      0.65      0.58        17
          A2       0.59      0.59      0.59        56
          B1       0.53      0.49      0.51        69
          B2       0.61      0.65      0.63        51
          C1       0.29      0.22      0.25         9

    accuracy                           0.56       202
   macro avg       0.51      0.52      0.51       202
weighted avg       0.56      0.56      0.56       202


K-fold scores
[0.5419418708625445, 0.49790935754170684, 0.593722882871164, 0.5313551685353455, 0.5565715030149632]
SKF f1 score mean 0.5443001565651449

SAME LANG EVAL DONE FOR THIS LANG
With POS ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
6373
6373
Fold 0
[[ 5 13  0  0  0]
 [ 2 39 16  0  0]
 [ 3 18 34 15  0]
 [ 0  0  7 45  0]
 [ 0  0  0 10  0]]
              precision    recall  f1-score   support

          A1       0.50      0.28      0.36        18
          A2       0.56      0.68      0.61        57
          B1       0.60      0.49      0.54        70
          B2       0.64      0.87      0.74        52
          C1       0.00      0.00      0.00        10

    accuracy                           0.59       207
   macro avg       0.46      0.46      0.45       207
weighted avg       0.56      0.59      0.57       207


Fold 1
[[ 6 11  1  0  0]
 [ 2 37 18  0  0]
 [ 1 23 28 18  0]
 [ 0  1 10 41  0]
 [ 0  0  0 10  0]]
              precision    recall  f1-score   support

          A1       0.67      0.33      0.44        18
          A2       0.51      0.65      0.57        57
          B1       0.49      0.40      0.44        70
          B2       0.59      0.79      0.68        52
          C1       0.00      0.00      0.00        10

    accuracy                           0.54       207
   macro avg       0.45      0.43      0.43       207
weighted avg       0.51      0.54      0.52       207


Fold 2
[[ 4 14  0  0  0]
 [ 1 36 20  0  0]
 [ 1 23 37  9  0]
 [ 0  1 11 40  0]
 [ 0  0  0  9  0]]
              precision    recall  f1-score   support

          A1       0.67      0.22      0.33        18
          A2       0.49      0.63      0.55        57
          B1       0.54      0.53      0.54        70
          B2       0.69      0.77      0.73        52
          C1       0.00      0.00      0.00         9

    accuracy                           0.57       206
   macro avg       0.48      0.43      0.43       206
weighted avg       0.55      0.57      0.55       206


Fold 3
[[ 6 11  0  0  0]
 [ 2 37 18  0  0]
 [ 2 20 32 16  0]
 [ 0  0  7 44  0]
 [ 0  0  2  7  0]]
              precision    recall  f1-score   support

          A1       0.60      0.35      0.44        17
          A2       0.54      0.65      0.59        57
          B1       0.54      0.46      0.50        70
          B2       0.66      0.86      0.75        51
          C1       0.00      0.00      0.00         9

    accuracy                           0.58       204
   macro avg       0.47      0.46      0.46       204
weighted avg       0.55      0.58      0.56       204


Fold 4
[[11  6  0  0  0]
 [ 7 39  9  1  0]
 [ 1 20 35 13  0]
 [ 0  2  9 40  0]
 [ 0  0  0  9  0]]
              precision    recall  f1-score   support

          A1       0.58      0.65      0.61        17
          A2       0.58      0.70      0.63        56
          B1       0.66      0.51      0.57        69
          B2       0.63      0.78      0.70        51
          C1       0.00      0.00      0.00         9

    accuracy                           0.62       202
   macro avg       0.49      0.53      0.50       202
weighted avg       0.60      0.62      0.60       202


K-fold scores
[0.5665575659065425, 0.5159588674993996, 0.5470031940633444, 0.5591281177999162, 0.6003996120271131]
SKF f1 score mean 0.5578094714592632

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
6373
6373
Fold 0
[[ 5 10  3  0  0]
 [ 8 28 19  2  0]
 [ 4 21 35  9  1]
 [ 0  5 15 28  4]
 [ 0  1  1  8  0]]
              precision    recall  f1-score   support

          A1       0.29      0.28      0.29        18
          A2       0.43      0.49      0.46        57
          B1       0.48      0.50      0.49        70
          B2       0.60      0.54      0.57        52
          C1       0.00      0.00      0.00        10

    accuracy                           0.46       207
   macro avg       0.36      0.36      0.36       207
weighted avg       0.46      0.46      0.46       207


Fold 1
[[ 7  6  5  0  0]
 [ 9 25 20  3  0]
 [ 2 28 27 13  0]
 [ 1  2 17 30  2]
 [ 0  0  1  9  0]]
              precision    recall  f1-score   support

          A1       0.37      0.39      0.38        18
          A2       0.41      0.44      0.42        57
          B1       0.39      0.39      0.39        70
          B2       0.55      0.58      0.56        52
          C1       0.00      0.00      0.00        10

    accuracy                           0.43       207
   macro avg       0.34      0.36      0.35       207
weighted avg       0.41      0.43      0.42       207


Fold 2
[[ 6 11  0  1  0]
 [ 5 29 23  0  0]
 [ 4 14 43  9  0]
 [ 0  3 11 35  3]
 [ 0  0  0  9  0]]
              precision    recall  f1-score   support

          A1       0.40      0.33      0.36        18
          A2       0.51      0.51      0.51        57
          B1       0.56      0.61      0.59        70
          B2       0.65      0.67      0.66        52
          C1       0.00      0.00      0.00         9

    accuracy                           0.55       206
   macro avg       0.42      0.43      0.42       206
weighted avg       0.53      0.55      0.54       206


Fold 3
[[ 6  7  4  0  0]
 [ 9 20 21  7  0]
 [ 2 31 25 10  2]
 [ 0  3 10 36  2]
 [ 0  0  3  6  0]]
              precision    recall  f1-score   support

          A1       0.35      0.35      0.35        17
          A2       0.33      0.35      0.34        57
          B1       0.40      0.36      0.38        70
          B2       0.61      0.71      0.65        51
          C1       0.00      0.00      0.00         9

    accuracy                           0.43       204
   macro avg       0.34      0.35      0.34       204
weighted avg       0.41      0.43      0.42       204


Fold 4
[[11  5  1  0  0]
 [ 9 28 19  0  0]
 [ 3 20 36  8  2]
 [ 0  6 16 26  3]
 [ 0  0  1  8  0]]
              precision    recall  f1-score   support

          A1       0.48      0.65      0.55        17
          A2       0.47      0.50      0.49        56
          B1       0.49      0.52      0.51        69
          B2       0.62      0.51      0.56        51
          C1       0.00      0.00      0.00         9

    accuracy                           0.50       202
   macro avg       0.41      0.44      0.42       202
weighted avg       0.50      0.50      0.50       202


K-fold scores
[0.4588727886423301, 0.42088034632060417, 0.538045913297791, 0.41676294879317394, 0.49565153337924317]
SKF f1 score mean 0.46604270608662846

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
6373
6373
Fold 0
[[ 5 11  2  0  0]
 [ 6 29 20  2  0]
 [ 3 19 36 11  1]
 [ 0  1 15 32  4]
 [ 0  1  1  8  0]]
              precision    recall  f1-score   support

          A1       0.36      0.28      0.31        18
          A2       0.48      0.51      0.49        57
          B1       0.49      0.51      0.50        70
          B2       0.60      0.62      0.61        52
          C1       0.00      0.00      0.00        10

    accuracy                           0.49       207
   macro avg       0.38      0.38      0.38       207
weighted avg       0.48      0.49      0.48       207


Fold 1
[[ 4  8  6  0  0]
 [ 4 30 20  3  0]
 [ 1 27 29 13  0]
 [ 0  1 17 31  3]
 [ 0  0  1  9  0]]
              precision    recall  f1-score   support

          A1       0.44      0.22      0.30        18
          A2       0.45      0.53      0.49        57
          B1       0.40      0.41      0.41        70
          B2       0.55      0.60      0.57        52
          C1       0.00      0.00      0.00        10

    accuracy                           0.45       207
   macro avg       0.37      0.35      0.35       207
weighted avg       0.44      0.45      0.44       207


Fold 2
[[ 4 13  0  1  0]
 [ 4 28 25  0  0]
 [ 2 16 40 12  0]
 [ 1  2 12 35  2]
 [ 0  0  0  9  0]]
              precision    recall  f1-score   support

          A1       0.36      0.22      0.28        18
          A2       0.47      0.49      0.48        57
          B1       0.52      0.57      0.54        70
          B2       0.61      0.67      0.64        52
          C1       0.00      0.00      0.00         9

    accuracy                           0.52       206
   macro avg       0.39      0.39      0.39       206
weighted avg       0.49      0.52      0.50       206


Fold 3
[[ 7  7  3  0  0]
 [ 6 26 19  6  0]
 [ 1 26 28 14  1]
 [ 0  3  9 38  1]
 [ 0  0  2  7  0]]
              precision    recall  f1-score   support

          A1       0.50      0.41      0.45        17
          A2       0.42      0.46      0.44        57
          B1       0.46      0.40      0.43        70
          B2       0.58      0.75      0.66        51
          C1       0.00      0.00      0.00         9

    accuracy                           0.49       204
   macro avg       0.39      0.40      0.39       204
weighted avg       0.46      0.49      0.47       204


Fold 4
[[11  5  1  0  0]
 [ 9 30 17  0  0]
 [ 3 23 37  4  2]
 [ 0  4 17 26  4]
 [ 0  0  1  8  0]]
              precision    recall  f1-score   support

          A1       0.48      0.65      0.55        17
          A2       0.48      0.54      0.51        56
          B1       0.51      0.54      0.52        69
          B2       0.68      0.51      0.58        51
          C1       0.00      0.00      0.00         9

    accuracy                           0.51       202
   macro avg       0.43      0.45      0.43       202
weighted avg       0.52      0.51      0.51       202


K-fold scores
[0.4847207113419346, 0.4414573508481838, 0.504720835576562, 0.470208037282522, 0.5127726512824181]
SKF f1 score mean 0.48277591726632413

SAME LANG EVAL DONE FOR THIS LANG
Dep ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5787
5787
Fold 0
[[ 4 14  0  0  0]
 [ 3 39 15  0  0]
 [ 2 22 35 11  0]
 [ 0  0 13 39  0]
 [ 0  0  3  7  0]]
              precision    recall  f1-score   support

          A1       0.44      0.22      0.30        18
          A2       0.52      0.68      0.59        57
          B1       0.53      0.50      0.51        70
          B2       0.68      0.75      0.72        52
          C1       0.00      0.00      0.00        10

    accuracy                           0.57       207
   macro avg       0.44      0.43      0.42       207
weighted avg       0.53      0.57      0.54       207


Fold 1
[[ 4 12  2  0  0]
 [ 2 39 13  3  0]
 [ 1 21 28 20  0]
 [ 0  2 10 40  0]
 [ 0  0  0 10  0]]
              precision    recall  f1-score   support

          A1       0.57      0.22      0.32        18
          A2       0.53      0.68      0.60        57
          B1       0.53      0.40      0.46        70
          B2       0.55      0.77      0.64        52
          C1       0.00      0.00      0.00        10

    accuracy                           0.54       207
   macro avg       0.43      0.42      0.40       207
weighted avg       0.51      0.54      0.51       207


Fold 2
[[ 6 12  0  0  0]
 [ 2 40 15  0  0]
 [ 1 22 36 11  0]
 [ 0  2 13 37  0]
 [ 0  0  1  8  0]]
              precision    recall  f1-score   support

          A1       0.67      0.33      0.44        18
          A2       0.53      0.70      0.60        57
          B1       0.55      0.51      0.53        70
          B2       0.66      0.71      0.69        52
          C1       0.00      0.00      0.00         9

    accuracy                           0.58       206
   macro avg       0.48      0.45      0.45       206
weighted avg       0.56      0.58      0.56       206


Fold 3
[[ 6 11  0  0  0]
 [ 5 37 15  0  0]
 [ 0 22 34 14  0]
 [ 0  0 10 41  0]
 [ 0  1  1  7  0]]
              precision    recall  f1-score   support

          A1       0.55      0.35      0.43        17
          A2       0.52      0.65      0.58        57
          B1       0.57      0.49      0.52        70
          B2       0.66      0.80      0.73        51
          C1       0.00      0.00      0.00         9

    accuracy                           0.58       204
   macro avg       0.46      0.46      0.45       204
weighted avg       0.55      0.58      0.56       204


Fold 4
[[ 8  9  0  0  0]
 [ 4 46  5  1  0]
 [ 1 22 32 14  0]
 [ 0  2  9 40  0]
 [ 0  0  0  9  0]]
              precision    recall  f1-score   support

          A1       0.62      0.47      0.53        17
          A2       0.58      0.82      0.68        56
          B1       0.70      0.46      0.56        69
          B2       0.62      0.78      0.70        51
          C1       0.00      0.00      0.00         9

    accuracy                           0.62       202
   macro avg       0.50      0.51      0.49       202
weighted avg       0.61      0.62      0.60       202


K-fold scores
[0.5422974514694171, 0.5065161835598885, 0.5594595982945497, 0.5581523208755932, 0.5995440123722516]
SKF f1 score mean 0.55319391331434

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5787
5787
Fold 0
[[ 9  6  3  0  0]
 [ 7 32 16  2  0]
 [ 4 18 38  9  1]
 [ 0  5 12 31  4]
 [ 0  1  2  7  0]]
              precision    recall  f1-score   support

          A1       0.45      0.50      0.47        18
          A2       0.52      0.56      0.54        57
          B1       0.54      0.54      0.54        70
          B2       0.63      0.60      0.61        52
          C1       0.00      0.00      0.00        10

    accuracy                           0.53       207
   macro avg       0.43      0.44      0.43       207
weighted avg       0.52      0.53      0.53       207


Fold 1
[[ 8  4  6  0  0]
 [ 6 30 17  4  0]
 [ 3 20 31 16  0]
 [ 0  8 19 23  2]
 [ 0  0  5  5  0]]
              precision    recall  f1-score   support

          A1       0.47      0.44      0.46        18
          A2       0.48      0.53      0.50        57
          B1       0.40      0.44      0.42        70
          B2       0.48      0.44      0.46        52
          C1       0.00      0.00      0.00        10

    accuracy                           0.44       207
   macro avg       0.37      0.37      0.37       207
weighted avg       0.43      0.44      0.44       207


Fold 2
[[ 5 10  3  0  0]
 [ 7 25 22  3  0]
 [ 3 17 39 10  1]
 [ 1  1 24 25  1]
 [ 0  1  2  6  0]]
              precision    recall  f1-score   support

          A1       0.31      0.28      0.29        18
          A2       0.46      0.44      0.45        57
          B1       0.43      0.56      0.49        70
          B2       0.57      0.48      0.52        52
          C1       0.00      0.00      0.00         9

    accuracy                           0.46       206
   macro avg       0.36      0.35      0.35       206
weighted avg       0.45      0.46      0.45       206


Fold 3
[[ 7  8  2  0  0]
 [10 27 15  5  0]
 [ 3 23 27 17  0]
 [ 0  5 11 34  1]
 [ 0  0  2  7  0]]
              precision    recall  f1-score   support

          A1       0.35      0.41      0.38        17
          A2       0.43      0.47      0.45        57
          B1       0.47      0.39      0.43        70
          B2       0.54      0.67      0.60        51
          C1       0.00      0.00      0.00         9

    accuracy                           0.47       204
   macro avg       0.36      0.39      0.37       204
weighted avg       0.45      0.47      0.45       204


Fold 4
[[ 9  7  1  0  0]
 [ 7 25 21  3  0]
 [ 3 23 32 11  0]
 [ 0  2 23 22  4]
 [ 0  0  0  7  2]]
              precision    recall  f1-score   support

          A1       0.47      0.53      0.50        17
          A2       0.44      0.45      0.44        56
          B1       0.42      0.46      0.44        69
          B2       0.51      0.43      0.47        51
          C1       0.33      0.22      0.27         9

    accuracy                           0.45       202
   macro avg       0.43      0.42      0.42       202
weighted avg       0.45      0.45      0.44       202


K-fold scores
[0.5257636063162138, 0.4358086548367878, 0.44746663425275646, 0.4522905127037767, 0.44454295460368604]
SKF f1 score mean 0.46117447254264415

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5787
5787
Fold 0
[[10  5  3  0  0]
 [ 7 30 19  1  0]
 [ 4 17 39  9  1]
 [ 0  4 13 32  3]
 [ 0  1  1  8  0]]
              precision    recall  f1-score   support

          A1       0.48      0.56      0.51        18
          A2       0.53      0.53      0.53        57
          B1       0.52      0.56      0.54        70
          B2       0.64      0.62      0.63        52
          C1       0.00      0.00      0.00        10

    accuracy                           0.54       207
   macro avg       0.43      0.45      0.44       207
weighted avg       0.52      0.54      0.53       207


Fold 1
[[ 6  6  6  0  0]
 [ 6 29 18  4  0]
 [ 2 20 32 16  0]
 [ 0  4 18 25  5]
 [ 0  0  3  6  1]]
              precision    recall  f1-score   support

          A1       0.43      0.33      0.38        18
          A2       0.49      0.51      0.50        57
          B1       0.42      0.46      0.44        70
          B2       0.49      0.48      0.49        52
          C1       0.17      0.10      0.12        10

    accuracy                           0.45       207
   macro avg       0.40      0.38      0.38       207
weighted avg       0.44      0.45      0.45       207


Fold 2
[[ 5 10  3  0  0]
 [ 5 28 21  3  0]
 [ 3 18 40  8  1]
 [ 0  1 22 27  2]
 [ 0  1  1  7  0]]
              precision    recall  f1-score   support

          A1       0.38      0.28      0.32        18
          A2       0.48      0.49      0.49        57
          B1       0.46      0.57      0.51        70
          B2       0.60      0.52      0.56        52
          C1       0.00      0.00      0.00         9

    accuracy                           0.49       206
   macro avg       0.39      0.37      0.38       206
weighted avg       0.47      0.49      0.48       206


Fold 3
[[ 7  8  2  0  0]
 [ 8 30 14  5  0]
 [ 2 22 28 16  2]
 [ 0  3 10 37  1]
 [ 0  0  1  8  0]]
              precision    recall  f1-score   support

          A1       0.41      0.41      0.41        17
          A2       0.48      0.53      0.50        57
          B1       0.51      0.40      0.45        70
          B2       0.56      0.73      0.63        51
          C1       0.00      0.00      0.00         9

    accuracy                           0.50       204
   macro avg       0.39      0.41      0.40       204
weighted avg       0.48      0.50      0.49       204


Fold 4
[[ 9  6  2  0  0]
 [ 8 31 15  2  0]
 [ 4 18 33 14  0]
 [ 0  2 18 29  2]
 [ 0  0  0  8  1]]
              precision    recall  f1-score   support

          A1       0.43      0.53      0.47        17
          A2       0.54      0.55      0.55        56
          B1       0.49      0.48      0.48        69
          B2       0.55      0.57      0.56        51
          C1       0.33      0.11      0.17         9

    accuracy                           0.51       202
   macro avg       0.47      0.45      0.45       202
weighted avg       0.51      0.51      0.50       202


K-fold scores
[0.5290502059176545, 0.44550197547242715, 0.4766029940252343, 0.48586475615887376, 0.5047597965370197]
SKF f1 score mean 0.4883559456222419

SAME LANG EVAL DONE FOR THIS LANG
************for dimension:  Orthography  ***************
Extracted all features: 
Printing class statistics
Counter({'B2': 338, 'B1': 321, 'C1': 161, 'A2': 150, 'C2': 38, 'A1': 18})
With Word ngrams: 
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3414
3414
Fold 0
[[ 1  3  0  0  0  0]
 [ 0 15 15  0  0  0]
 [ 0  7 51  7  0  0]
 [ 0  3 13 48  4  0]
 [ 0  0  1 27  5  0]
 [ 0  0  0  7  1  0]]
              precision    recall  f1-score   support

          A1       1.00      0.25      0.40         4
          A2       0.54      0.50      0.52        30
          B1       0.64      0.78      0.70        65
          B2       0.54      0.71      0.61        68
          C1       0.50      0.15      0.23        33
          C2       0.00      0.00      0.00         8

    accuracy                           0.58       208
   macro avg       0.54      0.40      0.41       208
weighted avg       0.55      0.58      0.54       208


Fold 1
[[ 1  3  0  0  0  0]
 [ 0 18 11  1  0  0]
 [ 0 13 41 10  0  0]
 [ 0  2 17 45  4  0]
 [ 0  0  3 23  6  0]
 [ 0  0  0  4  4  0]]
              precision    recall  f1-score   support

          A1       1.00      0.25      0.40         4
          A2       0.50      0.60      0.55        30
          B1       0.57      0.64      0.60        64
          B2       0.54      0.66      0.60        68
          C1       0.43      0.19      0.26        32
          C2       0.00      0.00      0.00         8

    accuracy                           0.54       206
   macro avg       0.51      0.39      0.40       206
weighted avg       0.51      0.54      0.51       206


Fold 2
[[ 1  2  0  1  0  0]
 [ 1 19 10  0  0  0]
 [ 0  8 41 14  1  0]
 [ 0  2 18 41  7  0]
 [ 0  1  2 22  7  0]
 [ 0  0  0  7  1  0]]
              precision    recall  f1-score   support

          A1       0.50      0.25      0.33         4
          A2       0.59      0.63      0.61        30
          B1       0.58      0.64      0.61        64
          B2       0.48      0.60      0.54        68
          C1       0.44      0.22      0.29        32
          C2       0.00      0.00      0.00         8

    accuracy                           0.53       206
   macro avg       0.43      0.39      0.40       206
weighted avg       0.50      0.53      0.51       206


Fold 3
[[ 0  3  0  0  0  0]
 [ 0 19 11  0  0  0]
 [ 0  7 48  9  0  0]
 [ 0  4 29 33  1  0]
 [ 0  0  3 29  0  0]
 [ 0  0  0  5  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         3
          A2       0.58      0.63      0.60        30
          B1       0.53      0.75      0.62        64
          B2       0.43      0.49      0.46        67
          C1       0.00      0.00      0.00        32
          C2       0.00      0.00      0.00         7

    accuracy                           0.49       203
   macro avg       0.26      0.31      0.28       203
weighted avg       0.39      0.49      0.44       203


Fold 4
[[ 1  2  0  0  0  0]
 [ 0 18 11  1  0  0]
 [ 0 10 48  5  1  0]
 [ 0  0 27 38  2  0]
 [ 0  0  0 29  3  0]
 [ 0  0  1  4  2  0]]
              precision    recall  f1-score   support

          A1       1.00      0.33      0.50         3
          A2       0.60      0.60      0.60        30
          B1       0.55      0.75      0.64        64
          B2       0.49      0.57      0.53        67
          C1       0.38      0.09      0.15        32
          C2       0.00      0.00      0.00         7

    accuracy                           0.53       203
   macro avg       0.50      0.39      0.40       203
weighted avg       0.50      0.53      0.49       203


K-fold scores
[0.5389202680259177, 0.5117936848020089, 0.5066615629096055, 0.4367341118016472, 0.4943342504086966]
SKF f1 score mean 0.4976887755895752

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3414
3414
Fold 0
[[ 1  1  2  0  0  0]
 [ 0 11 18  1  0  0]
 [ 0 10 39 13  3  0]
 [ 0  5 12 33 14  4]
 [ 0  0  3 17 11  2]
 [ 0  0  1  4  3  0]]
              precision    recall  f1-score   support

          A1       1.00      0.25      0.40         4
          A2       0.41      0.37      0.39        30
          B1       0.52      0.60      0.56        65
          B2       0.49      0.49      0.49        68
          C1       0.35      0.33      0.34        33
          C2       0.00      0.00      0.00         8

    accuracy                           0.46       208
   macro avg       0.46      0.34      0.36       208
weighted avg       0.46      0.46      0.45       208


Fold 1
[[ 3  1  0  0  0  0]
 [ 0 12 14  4  0  0]
 [ 0 12 32 19  1  0]
 [ 0  3 19 33 11  2]
 [ 0  0  6 12 11  3]
 [ 0  0  1  6  1  0]]
              precision    recall  f1-score   support

          A1       1.00      0.75      0.86         4
          A2       0.43      0.40      0.41        30
          B1       0.44      0.50      0.47        64
          B2       0.45      0.49      0.46        68
          C1       0.46      0.34      0.39        32
          C2       0.00      0.00      0.00         8

    accuracy                           0.44       206
   macro avg       0.46      0.41      0.43       206
weighted avg       0.44      0.44      0.44       206


Fold 2
[[ 1  1  1  1  0  0]
 [ 1 14 14  0  1  0]
 [ 0 12 26 21  5  0]
 [ 0  3 20 29 12  4]
 [ 0  2  6 14 10  0]
 [ 0  0  0  3  4  1]]
              precision    recall  f1-score   support

          A1       0.50      0.25      0.33         4
          A2       0.44      0.47      0.45        30
          B1       0.39      0.41      0.40        64
          B2       0.43      0.43      0.43        68
          C1       0.31      0.31      0.31        32
          C2       0.20      0.12      0.15         8

    accuracy                           0.39       206
   macro avg       0.38      0.33      0.35       206
weighted avg       0.39      0.39      0.39       206


Fold 3
[[ 1  0  2  0  0  0]
 [ 1 15 14  0  0  0]
 [ 0 11 36 16  1  0]
 [ 0  6 28 25  7  1]
 [ 0  0  2 16 10  4]
 [ 0  0  0  3  3  1]]
              precision    recall  f1-score   support

          A1       0.50      0.33      0.40         3
          A2       0.47      0.50      0.48        30
          B1       0.44      0.56      0.49        64
          B2       0.42      0.37      0.39        67
          C1       0.48      0.31      0.38        32
          C2       0.17      0.14      0.15         7

    accuracy                           0.43       203
   macro avg       0.41      0.37      0.38       203
weighted avg       0.43      0.43      0.43       203


Fold 4
[[ 2  0  0  1  0  0]
 [ 1 14 10  4  1  0]
 [ 1  9 34 19  1  0]
 [ 0  1 22 30 13  1]
 [ 0  0  0 21 10  1]
 [ 0  1  0  4  2  0]]
              precision    recall  f1-score   support

          A1       0.50      0.67      0.57         3
          A2       0.56      0.47      0.51        30
          B1       0.52      0.53      0.52        64
          B2       0.38      0.45      0.41        67
          C1       0.37      0.31      0.34        32
          C2       0.00      0.00      0.00         7

    accuracy                           0.44       203
   macro avg       0.39      0.40      0.39       203
weighted avg       0.44      0.44      0.44       203


K-fold scores
[0.4506585725130133, 0.4375586114810131, 0.3908595622016918, 0.42762620886116404, 0.43766325254382843]
SKF f1 score mean 0.4288732415201421

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3414
3414
Fold 0
[[ 1  1  2  0  0  0]
 [ 0 11 18  1  0  0]
 [ 0 11 38 14  2  0]
 [ 0  6 12 32 14  4]
 [ 0  0  2 17 12  2]
 [ 0  0  0  4  3  1]]
              precision    recall  f1-score   support

          A1       1.00      0.25      0.40         4
          A2       0.38      0.37      0.37        30
          B1       0.53      0.58      0.55        65
          B2       0.47      0.47      0.47        68
          C1       0.39      0.36      0.38        33
          C2       0.14      0.12      0.13         8

    accuracy                           0.46       208
   macro avg       0.48      0.36      0.38       208
weighted avg       0.46      0.46      0.45       208


Fold 1
[[ 2  2  0  0  0  0]
 [ 0 14 12  4  0  0]
 [ 0 13 33 16  2  0]
 [ 0  3 20 32 11  2]
 [ 0  0  6 12 12  2]
 [ 0  0  1  6  1  0]]
              precision    recall  f1-score   support

          A1       1.00      0.50      0.67         4
          A2       0.44      0.47      0.45        30
          B1       0.46      0.52      0.49        64
          B2       0.46      0.47      0.46        68
          C1       0.46      0.38      0.41        32
          C2       0.00      0.00      0.00         8

    accuracy                           0.45       206
   macro avg       0.47      0.39      0.41       206
weighted avg       0.45      0.45      0.45       206


Fold 2
[[ 1  1  1  1  0  0]
 [ 1 17 12  0  0  0]
 [ 0 15 29 15  5  0]
 [ 0  3 19 31 12  3]
 [ 0  2  5 13 12  0]
 [ 0  0  0  3  5  0]]
              precision    recall  f1-score   support

          A1       0.50      0.25      0.33         4
          A2       0.45      0.57      0.50        30
          B1       0.44      0.45      0.45        64
          B2       0.49      0.46      0.47        68
          C1       0.35      0.38      0.36        32
          C2       0.00      0.00      0.00         8

    accuracy                           0.44       206
   macro avg       0.37      0.35      0.35       206
weighted avg       0.43      0.44      0.43       206


Fold 3
[[ 0  1  2  0  0  0]
 [ 1 15 14  0  0  0]
 [ 0 12 38 13  1  0]
 [ 0  7 29 25  6  0]
 [ 0  0  2 17 12  1]
 [ 0  0  0  2  5  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         3
          A2       0.43      0.50      0.46        30
          B1       0.45      0.59      0.51        64
          B2       0.44      0.37      0.40        67
          C1       0.50      0.38      0.43        32
          C2       0.00      0.00      0.00         7

    accuracy                           0.44       203
   macro avg       0.30      0.31      0.30       203
weighted avg       0.43      0.44      0.43       203


Fold 4
[[ 2  1  0  0  0  0]
 [ 1 12 12  5  0  0]
 [ 1 11 34 17  1  0]
 [ 0  2 24 27 13  1]
 [ 0  0  0 19 12  1]
 [ 0  1  0  5  1  0]]
              precision    recall  f1-score   support

          A1       0.50      0.67      0.57         3
          A2       0.44      0.40      0.42        30
          B1       0.49      0.53      0.51        64
          B2       0.37      0.40      0.39        67
          C1       0.44      0.38      0.41        32
          C2       0.00      0.00      0.00         7

    accuracy                           0.43       203
   macro avg       0.37      0.40      0.38       203
weighted avg       0.42      0.43      0.42       203


K-fold scores
[0.45330048800584954, 0.44685188586046326, 0.4306152875262783, 0.42965942805276375, 0.42208513742538]
SKF f1 score mean 0.43650244537414695

SAME LANG EVAL DONE FOR THIS LANG
With POS ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
6373
6373
Fold 0
[[ 0  4  0  0  0  0]
 [ 0 15 15  0  0  0]
 [ 0  9 42 13  1  0]
 [ 0  6 11 50  1  0]
 [ 0  0  1 30  2  0]
 [ 0  0  0  8  0  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.44      0.50      0.47        30
          B1       0.61      0.65      0.63        65
          B2       0.50      0.74      0.59        68
          C1       0.50      0.06      0.11        33
          C2       0.00      0.00      0.00         8

    accuracy                           0.52       208
   macro avg       0.34      0.32      0.30       208
weighted avg       0.50      0.52      0.47       208


Fold 1
[[ 1  3  0  0  0  0]
 [ 0 17 12  1  0  0]
 [ 0 12 42 10  0  0]
 [ 0  3 15 44  6  0]
 [ 0  0  2 29  1  0]
 [ 0  0  0  7  1  0]]
              precision    recall  f1-score   support

          A1       1.00      0.25      0.40         4
          A2       0.49      0.57      0.52        30
          B1       0.59      0.66      0.62        64
          B2       0.48      0.65      0.55        68
          C1       0.12      0.03      0.05        32
          C2       0.00      0.00      0.00         8

    accuracy                           0.51       206
   macro avg       0.45      0.36      0.36       206
weighted avg       0.45      0.51      0.47       206


Fold 2
[[ 1  2  1  0  0  0]
 [ 1 21  8  0  0  0]
 [ 0  7 40 17  0  0]
 [ 0  2 16 42  8  0]
 [ 0  1  1 25  5  0]
 [ 0  0  0  5  3  0]]
              precision    recall  f1-score   support

          A1       0.50      0.25      0.33         4
          A2       0.64      0.70      0.67        30
          B1       0.61      0.62      0.62        64
          B2       0.47      0.62      0.54        68
          C1       0.31      0.16      0.21        32
          C2       0.00      0.00      0.00         8

    accuracy                           0.53       206
   macro avg       0.42      0.39      0.39       206
weighted avg       0.49      0.53      0.50       206


Fold 3
[[ 0  3  0  0  0  0]
 [ 1 13 15  1  0  0]
 [ 0  9 47  8  0  0]
 [ 0  7 24 34  2  0]
 [ 0  0  3 28  1  0]
 [ 0  0  0  6  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         3
          A2       0.41      0.43      0.42        30
          B1       0.53      0.73      0.61        64
          B2       0.44      0.51      0.47        67
          C1       0.25      0.03      0.06        32
          C2       0.00      0.00      0.00         7

    accuracy                           0.47       203
   macro avg       0.27      0.28      0.26       203
weighted avg       0.41      0.47      0.42       203


Fold 4
[[ 1  2  0  0  0  0]
 [ 0 16 14  0  0  0]
 [ 0 11 44  9  0  0]
 [ 0  0 19 47  1  0]
 [ 0  0  0 31  1  0]
 [ 0  0  1  6  0  0]]
              precision    recall  f1-score   support

          A1       1.00      0.33      0.50         3
          A2       0.55      0.53      0.54        30
          B1       0.56      0.69      0.62        64
          B2       0.51      0.70      0.59        67
          C1       0.50      0.03      0.06        32
          C2       0.00      0.00      0.00         7

    accuracy                           0.54       203
   macro avg       0.52      0.38      0.38       203
weighted avg       0.52      0.54      0.49       203


K-fold scores
[0.474101070263541, 0.4677172332054178, 0.5037222378141307, 0.42028361214712057, 0.48609857740315443]
SKF f1 score mean 0.4703845461666729

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
6373
6373
Fold 0
[[ 1  3  0  0  0  0]
 [ 0 11 16  3  0  0]
 [ 0 13 36 15  1  0]
 [ 0  5 24 25 12  2]
 [ 0  0  4 18 10  1]
 [ 0  0  0  4  3  1]]
              precision    recall  f1-score   support

          A1       1.00      0.25      0.40         4
          A2       0.34      0.37      0.35        30
          B1       0.45      0.55      0.50        65
          B2       0.38      0.37      0.38        68
          C1       0.38      0.30      0.34        33
          C2       0.25      0.12      0.17         8

    accuracy                           0.40       208
   macro avg       0.47      0.33      0.36       208
weighted avg       0.41      0.40      0.40       208


Fold 1
[[ 1  0  3  0  0  0]
 [ 3 10  9  7  1  0]
 [ 0 18 30 13  3  0]
 [ 0  3 13 35 13  4]
 [ 0  1  5 12 12  2]
 [ 0  0  2  2  4  0]]
              precision    recall  f1-score   support

          A1       0.25      0.25      0.25         4
          A2       0.31      0.33      0.32        30
          B1       0.48      0.47      0.48        64
          B2       0.51      0.51      0.51        68
          C1       0.36      0.38      0.37        32
          C2       0.00      0.00      0.00         8

    accuracy                           0.43       206
   macro avg       0.32      0.32      0.32       206
weighted avg       0.42      0.43      0.43       206


Fold 2
[[ 2  1  0  1  0  0]
 [ 1  7 17  5  0  0]
 [ 1 17 21 20  5  0]
 [ 0  3 18 27 19  1]
 [ 0  1  3 17 10  1]
 [ 0  0  0  4  3  1]]
              precision    recall  f1-score   support

          A1       0.50      0.50      0.50         4
          A2       0.24      0.23      0.24        30
          B1       0.36      0.33      0.34        64
          B2       0.36      0.40      0.38        68
          C1       0.27      0.31      0.29        32
          C2       0.33      0.12      0.18         8

    accuracy                           0.33       206
   macro avg       0.34      0.32      0.32       206
weighted avg       0.33      0.33      0.33       206


Fold 3
[[ 2  1  0  0  0  0]
 [ 3 12 13  2  0  0]
 [ 1 14 36 13  0  0]
 [ 0  9 24 21 12  1]
 [ 0  1  3 13 11  4]
 [ 0  0  0  2  5  0]]
              precision    recall  f1-score   support

          A1       0.33      0.67      0.44         3
          A2       0.32      0.40      0.36        30
          B1       0.47      0.56      0.51        64
          B2       0.41      0.31      0.36        67
          C1       0.39      0.34      0.37        32
          C2       0.00      0.00      0.00         7

    accuracy                           0.40       203
   macro avg       0.32      0.38      0.34       203
weighted avg       0.40      0.40      0.40       203


Fold 4
[[ 1  2  0  0  0  0]
 [ 1 13 14  2  0  0]
 [ 0 15 32 13  3  1]
 [ 0  3 16 33 13  2]
 [ 0  1  3 12 15  1]
 [ 0  1  0  5  1  0]]
              precision    recall  f1-score   support

          A1       0.50      0.33      0.40         3
          A2       0.37      0.43      0.40        30
          B1       0.49      0.50      0.50        64
          B2       0.51      0.49      0.50        67
          C1       0.47      0.47      0.47        32
          C2       0.00      0.00      0.00         7

    accuracy                           0.46       203
   macro avg       0.39      0.37      0.38       203
weighted avg       0.46      0.46      0.46       203


K-fold scores
[0.39713801512030594, 0.42579378637795656, 0.32796779275410426, 0.3969195993339283, 0.4603543743078627]
SKF f1 score mean 0.4016347135788315

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
6373
6373
Fold 0
[[ 1  3  0  0  0  0]
 [ 0 12 16  2  0  0]
 [ 0 14 35 14  2  0]
 [ 0  3 20 31 13  1]
 [ 0  0  2 16 14  1]
 [ 0  0  0  5  3  0]]
              precision    recall  f1-score   support

          A1       1.00      0.25      0.40         4
          A2       0.38      0.40      0.39        30
          B1       0.48      0.54      0.51        65
          B2       0.46      0.46      0.46        68
          C1       0.44      0.42      0.43        33
          C2       0.00      0.00      0.00         8

    accuracy                           0.45       208
   macro avg       0.46      0.34      0.36       208
weighted avg       0.44      0.45      0.44       208


Fold 1
[[ 0  2  2  0  0  0]
 [ 2  9 11  8  0  0]
 [ 0 19 29 13  3  0]
 [ 0  2 11 37 14  4]
 [ 0  0  5 17  8  2]
 [ 0  0  1  3  4  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.28      0.30      0.29        30
          B1       0.49      0.45      0.47        64
          B2       0.47      0.54      0.51        68
          C1       0.28      0.25      0.26        32
          C2       0.00      0.00      0.00         8

    accuracy                           0.40       206
   macro avg       0.25      0.26      0.26       206
weighted avg       0.39      0.40      0.40       206


Fold 2
[[ 1  2  0  1  0  0]
 [ 1  6 20  3  0  0]
 [ 1 15 25 18  5  0]
 [ 0  3 19 28 17  1]
 [ 0  1  3 18  9  1]
 [ 0  0  0  5  2  1]]
              precision    recall  f1-score   support

          A1       0.33      0.25      0.29         4
          A2       0.22      0.20      0.21        30
          B1       0.37      0.39      0.38        64
          B2       0.38      0.41      0.40        68
          C1       0.27      0.28      0.28        32
          C2       0.33      0.12      0.18         8

    accuracy                           0.34       206
   macro avg       0.32      0.28      0.29       206
weighted avg       0.34      0.34      0.34       206


Fold 3
[[ 2  1  0  0  0  0]
 [ 3 13 12  2  0  0]
 [ 1 15 35 13  0  0]
 [ 0  9 23 21 13  1]
 [ 0  1  3 13 12  3]
 [ 0  0  0  2  5  0]]
              precision    recall  f1-score   support

          A1       0.33      0.67      0.44         3
          A2       0.33      0.43      0.38        30
          B1       0.48      0.55      0.51        64
          B2       0.41      0.31      0.36        67
          C1       0.40      0.38      0.39        32
          C2       0.00      0.00      0.00         7

    accuracy                           0.41       203
   macro avg       0.33      0.39      0.35       203
weighted avg       0.40      0.41      0.40       203


Fold 4
[[ 1  2  0  0  0  0]
 [ 0 15 14  1  0  0]
 [ 0 17 34 12  1  0]
 [ 0  2 17 35 11  2]
 [ 0  0  2 15 13  2]
 [ 0  0  1  5  1  0]]
              precision    recall  f1-score   support

          A1       1.00      0.33      0.50         3
          A2       0.42      0.50      0.45        30
          B1       0.50      0.53      0.52        64
          B2       0.51      0.52      0.52        67
          C1       0.50      0.41      0.45        32
          C2       0.00      0.00      0.00         7

    accuracy                           0.48       203
   macro avg       0.49      0.38      0.41       203
weighted avg       0.48      0.48      0.48       203


K-fold scores
[0.4394197227593495, 0.3968336662892285, 0.3359674934331911, 0.4018372684125112, 0.47877649720693716]
SKF f1 score mean 0.4105669296202435

SAME LANG EVAL DONE FOR THIS LANG
Dep ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5787
5787
Fold 0
[[ 1  2  1  0  0  0]
 [ 0 11 19  0  0  0]
 [ 0  9 42 14  0  0]
 [ 0  5 14 47  2  0]
 [ 0  0  2 30  1  0]
 [ 0  0  0  7  1  0]]
              precision    recall  f1-score   support

          A1       1.00      0.25      0.40         4
          A2       0.41      0.37      0.39        30
          B1       0.54      0.65      0.59        65
          B2       0.48      0.69      0.57        68
          C1       0.25      0.03      0.05        33
          C2       0.00      0.00      0.00         8

    accuracy                           0.49       208
   macro avg       0.45      0.33      0.33       208
weighted avg       0.44      0.49      0.44       208


Fold 1
[[ 0  4  0  0  0  0]
 [ 0 17 12  1  0  0]
 [ 0  9 39 16  0  0]
 [ 0  3 15 44  6  0]
 [ 0  0  2 29  1  0]
 [ 0  0  0  7  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.52      0.57      0.54        30
          B1       0.57      0.61      0.59        64
          B2       0.45      0.65      0.53        68
          C1       0.12      0.03      0.05        32
          C2       0.00      0.00      0.00         8

    accuracy                           0.49       206
   macro avg       0.28      0.31      0.29       206
weighted avg       0.42      0.49      0.45       206


Fold 2
[[ 1  2  1  0  0  0]
 [ 0 22  8  0  0  0]
 [ 0  9 40 15  0  0]
 [ 0  3 17 47  1  0]
 [ 0  1  2 27  2  0]
 [ 0  0  1  6  1  0]]
              precision    recall  f1-score   support

          A1       1.00      0.25      0.40         4
          A2       0.59      0.73      0.66        30
          B1       0.58      0.62      0.60        64
          B2       0.49      0.69      0.58        68
          C1       0.50      0.06      0.11        32
          C2       0.00      0.00      0.00         8

    accuracy                           0.54       206
   macro avg       0.53      0.39      0.39       206
weighted avg       0.53      0.54      0.50       206


Fold 3
[[ 1  2  0  0  0  0]
 [ 1 12 17  0  0  0]
 [ 0  8 47  9  0  0]
 [ 0  7 23 36  1  0]
 [ 0  0  4 28  0  0]
 [ 0  0  0  7  0  0]]
              precision    recall  f1-score   support

          A1       0.50      0.33      0.40         3
          A2       0.41      0.40      0.41        30
          B1       0.52      0.73      0.61        64
          B2       0.45      0.54      0.49        67
          C1       0.00      0.00      0.00        32
          C2       0.00      0.00      0.00         7

    accuracy                           0.47       203
   macro avg       0.31      0.33      0.32       203
weighted avg       0.38      0.47      0.42       203


Fold 4
[[ 0  3  0  0  0  0]
 [ 0 16 13  1  0  0]
 [ 0 12 42 10  0  0]
 [ 0  0 17 50  0  0]
 [ 0  0  0 30  2  0]
 [ 0  0  1  6  0  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         3
          A2       0.52      0.53      0.52        30
          B1       0.58      0.66      0.61        64
          B2       0.52      0.75      0.61        67
          C1       1.00      0.06      0.12        32
          C2       0.00      0.00      0.00         7

    accuracy                           0.54       203
   macro avg       0.43      0.33      0.31       203
weighted avg       0.59      0.54      0.49       203


K-fold scores
[0.4406277568769642, 0.445996721724877, 0.4979029738871511, 0.41887989944299103, 0.4906253460560584]
SKF f1 score mean 0.4588065395976083

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5787
5787
Fold 0
[[ 0  2  2  0  0  0]
 [ 0 12 14  4  0  0]
 [ 0  9 38 16  2  0]
 [ 0  5 15 35 13  0]
 [ 0  0  3 15 13  2]
 [ 0  0  1  1  6  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.43      0.40      0.41        30
          B1       0.52      0.58      0.55        65
          B2       0.49      0.51      0.50        68
          C1       0.38      0.39      0.39        33
          C2       0.00      0.00      0.00         8

    accuracy                           0.47       208
   macro avg       0.30      0.32      0.31       208
weighted avg       0.45      0.47      0.46       208


Fold 1
[[ 0  3  1  0  0  0]
 [ 2  9 16  3  0  0]
 [ 0 14 31 17  2  0]
 [ 0  6 17 32 11  2]
 [ 0  1  7 17  6  1]
 [ 0  0  1  5  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.27      0.30      0.29        30
          B1       0.42      0.48      0.45        64
          B2       0.43      0.47      0.45        68
          C1       0.29      0.19      0.23        32
          C2       0.00      0.00      0.00         8

    accuracy                           0.38       206
   macro avg       0.24      0.24      0.24       206
weighted avg       0.36      0.38      0.37       206


Fold 2
[[ 1  2  0  1  0  0]
 [ 2 16 10  2  0  0]
 [ 1 14 31 12  6  0]
 [ 0  7 18 30 13  0]
 [ 0  1 10 13  7  1]
 [ 0  1  1  2  4  0]]
              precision    recall  f1-score   support

          A1       0.25      0.25      0.25         4
          A2       0.39      0.53      0.45        30
          B1       0.44      0.48      0.46        64
          B2       0.50      0.44      0.47        68
          C1       0.23      0.22      0.23        32
          C2       0.00      0.00      0.00         8

    accuracy                           0.41       206
   macro avg       0.30      0.32      0.31       206
weighted avg       0.40      0.41      0.40       206


Fold 3
[[ 2  0  1  0  0  0]
 [ 2  7 17  3  0  1]
 [ 2 13 31 17  1  0]
 [ 0  8 23 28  8  0]
 [ 0  1  5  9 16  1]
 [ 0  0  0  4  2  1]]
              precision    recall  f1-score   support

          A1       0.33      0.67      0.44         3
          A2       0.24      0.23      0.24        30
          B1       0.40      0.48      0.44        64
          B2       0.46      0.42      0.44        67
          C1       0.59      0.50      0.54        32
          C2       0.33      0.14      0.20         7

    accuracy                           0.42       203
   macro avg       0.39      0.41      0.38       203
weighted avg       0.42      0.42      0.42       203


Fold 4
[[ 0  1  2  0  0  0]
 [ 0 12 15  3  0  0]
 [ 0 14 32 16  2  0]
 [ 0  7 21 28 11  0]
 [ 0  0  6 14  9  3]
 [ 0  1  0  4  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         3
          A2       0.34      0.40      0.37        30
          B1       0.42      0.50      0.46        64
          B2       0.43      0.42      0.42        67
          C1       0.38      0.28      0.32        32
          C2       0.00      0.00      0.00         7

    accuracy                           0.40       203
   macro avg       0.26      0.27      0.26       203
weighted avg       0.38      0.40      0.39       203


K-fold scores
[0.4579878318198275, 0.36615583768871457, 0.40404792966351327, 0.4170554363357192, 0.3893794218917372]
SKF f1 score mean 0.4069252914799023

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5787
5787
Fold 0
[[ 0  2  2  0  0  0]
 [ 0 10 15  5  0  0]
 [ 0  9 37 17  2  0]
 [ 0  5 14 35 14  0]
 [ 0  0  3 15 13  2]
 [ 0  0  0  2  6  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.38      0.33      0.36        30
          B1       0.52      0.57      0.54        65
          B2       0.47      0.51      0.49        68
          C1       0.37      0.39      0.38        33
          C2       0.00      0.00      0.00         8

    accuracy                           0.46       208
   macro avg       0.29      0.30      0.30       208
weighted avg       0.43      0.46      0.44       208


Fold 1
[[ 0  2  2  0  0  0]
 [ 3  8 16  3  0  0]
 [ 0 13 34 16  1  0]
 [ 0  6 14 31 15  2]
 [ 0  0  8 16  8  0]
 [ 0  0  1  4  3  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.28      0.27      0.27        30
          B1       0.45      0.53      0.49        64
          B2       0.44      0.46      0.45        68
          C1       0.30      0.25      0.27        32
          C2       0.00      0.00      0.00         8

    accuracy                           0.39       206
   macro avg       0.24      0.25      0.25       206
weighted avg       0.37      0.39      0.38       206


Fold 2
[[ 1  2  0  1  0  0]
 [ 2 15 10  3  0  0]
 [ 1 10 32 14  7  0]
 [ 0  6 13 36 13  0]
 [ 0  1  7 14 10  0]
 [ 0  0  1  2  5  0]]
              precision    recall  f1-score   support

          A1       0.25      0.25      0.25         4
          A2       0.44      0.50      0.47        30
          B1       0.51      0.50      0.50        64
          B2       0.51      0.53      0.52        68
          C1       0.29      0.31      0.30        32
          C2       0.00      0.00      0.00         8

    accuracy                           0.46       206
   macro avg       0.33      0.35      0.34       206
weighted avg       0.44      0.46      0.45       206


Fold 3
[[ 2  0  1  0  0  0]
 [ 2  9 14  4  0  1]
 [ 0 13 34 16  1  0]
 [ 0  8 25 26  8  0]
 [ 0  1  5  9 16  1]
 [ 0  0  0  4  3  0]]
              precision    recall  f1-score   support

          A1       0.50      0.67      0.57         3
          A2       0.29      0.30      0.30        30
          B1       0.43      0.53      0.48        64
          B2       0.44      0.39      0.41        67
          C1       0.57      0.50      0.53        32
          C2       0.00      0.00      0.00         7

    accuracy                           0.43       203
   macro avg       0.37      0.40      0.38       203
weighted avg       0.42      0.43      0.42       203


Fold 4
[[ 0  1  2  0  0  0]
 [ 0 10 15  5  0  0]
 [ 0 13 33 17  1  0]
 [ 0  5 22 29 11  0]
 [ 0  0  5 15  9  3]
 [ 0  1  0  3  3  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         3
          A2       0.33      0.33      0.33        30
          B1       0.43      0.52      0.47        64
          B2       0.42      0.43      0.43        67
          C1       0.38      0.28      0.32        32
          C2       0.00      0.00      0.00         7

    accuracy                           0.40       203
   macro avg       0.26      0.26      0.26       203
weighted avg       0.38      0.40      0.39       203


K-fold scores
[0.44336878169469307, 0.38191085668786734, 0.4482765445605455, 0.42225503192958574, 0.38825955914280613]
SKF f1 score mean 0.4168141548030995

SAME LANG EVAL DONE FOR THIS LANG
************for dimension:  Vocabularyrange  ***************
Extracted all features: 
Printing class statistics
Counter({'B1': 349, 'B2': 271, 'A2': 213, 'C1': 138, 'A1': 50, 'C2': 5})
With Word ngrams: 
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3414
3414
Fold 0
[[ 4  2  4  0  0  0]
 [ 1 23 19  0  0  0]
 [ 1 12 50  7  0  0]
 [ 0  0  6 39 10  0]
 [ 0  0  0  7 21  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.67      0.40      0.50        10
          A2       0.62      0.53      0.57        43
          B1       0.63      0.71      0.67        70
          B2       0.74      0.71      0.72        55
          C1       0.66      0.75      0.70        28
          C2       0.00      0.00      0.00         1

    accuracy                           0.66       207
   macro avg       0.55      0.52      0.53       207
weighted avg       0.66      0.66      0.66       207


Fold 1
[[ 3  6  1  0  0  0]
 [ 0 23 20  0  0  0]
 [ 0 12 56  2  0  0]
 [ 0  0  8 34 12  0]
 [ 0  0  1  4 23  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       1.00      0.30      0.46        10
          A2       0.56      0.53      0.55        43
          B1       0.65      0.80      0.72        70
          B2       0.85      0.63      0.72        54
          C1       0.64      0.82      0.72        28
          C2       0.00      0.00      0.00         1

    accuracy                           0.67       206
   macro avg       0.62      0.51      0.53       206
weighted avg       0.70      0.67      0.67       206


Fold 2
[[ 4  6  0  0  0  0]
 [ 1 25 17  0  0  0]
 [ 1 11 55  3  0  0]
 [ 0  0  6 37 11  0]
 [ 0  0  0  5 23  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.67      0.40      0.50        10
          A2       0.60      0.58      0.59        43
          B1       0.71      0.79      0.74        70
          B2       0.82      0.69      0.75        54
          C1       0.66      0.82      0.73        28
          C2       0.00      0.00      0.00         1

    accuracy                           0.70       206
   macro avg       0.57      0.55      0.55       206
weighted avg       0.70      0.70      0.69       206


Fold 3
[[ 3  7  0  0  0  0]
 [ 1 30 11  0  0  0]
 [ 0  9 56  5  0  0]
 [ 0  1  8 28 17  0]
 [ 0  0  0  5 22  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.75      0.30      0.43        10
          A2       0.64      0.71      0.67        42
          B1       0.75      0.80      0.77        70
          B2       0.74      0.52      0.61        54
          C1       0.55      0.81      0.66        27
          C2       0.00      0.00      0.00         1

    accuracy                           0.68       204
   macro avg       0.57      0.52      0.52       204
weighted avg       0.69      0.68      0.67       204


Fold 4
[[ 2  8  0  0  0  0]
 [ 0 24 18  0  0  0]
 [ 0  5 55  9  0  0]
 [ 0  0 13 26 15  0]
 [ 0  0  0  6 21  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       1.00      0.20      0.33        10
          A2       0.65      0.57      0.61        42
          B1       0.64      0.80      0.71        69
          B2       0.63      0.48      0.55        54
          C1       0.57      0.78      0.66        27
          C2       0.00      0.00      0.00         1

    accuracy                           0.63       203
   macro avg       0.58      0.47      0.48       203
weighted avg       0.65      0.63      0.62       203


K-fold scores
[0.6571356907924363, 0.6680011830419803, 0.6948020654474109, 0.6728931129681475, 0.6162399376720762]
SKF f1 score mean 0.6618143979844102

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3414
3414
Fold 0
[[ 6  1  3  0  0  0]
 [ 4 20 18  1  0  0]
 [ 2 21 35 12  0  0]
 [ 0  1 10 37  7  0]
 [ 0  0  0  9 19  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.50      0.60      0.55        10
          A2       0.47      0.47      0.47        43
          B1       0.53      0.50      0.51        70
          B2       0.62      0.67      0.64        55
          C1       0.73      0.68      0.70        28
          C2       0.00      0.00      0.00         1

    accuracy                           0.57       207
   macro avg       0.47      0.49      0.48       207
weighted avg       0.56      0.57      0.56       207


Fold 1
[[ 3  5  2  0  0  0]
 [ 1 28 14  0  0  0]
 [ 2 20 42  6  0  0]
 [ 0  0 12 36  6  0]
 [ 0  0  0 11 17  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.50      0.30      0.37        10
          A2       0.53      0.65      0.58        43
          B1       0.60      0.60      0.60        70
          B2       0.68      0.67      0.67        54
          C1       0.71      0.61      0.65        28
          C2       0.00      0.00      0.00         1

    accuracy                           0.61       206
   macro avg       0.50      0.47      0.48       206
weighted avg       0.61      0.61      0.61       206


Fold 2
[[ 5  4  1  0  0  0]
 [ 2 22 18  1  0  0]
 [ 3 21 42  4  0  0]
 [ 0  1  8 38  7  0]
 [ 0  0  0  9 19  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.50      0.50      0.50        10
          A2       0.46      0.51      0.48        43
          B1       0.61      0.60      0.60        70
          B2       0.73      0.70      0.72        54
          C1       0.70      0.68      0.69        28
          C2       0.00      0.00      0.00         1

    accuracy                           0.61       206
   macro avg       0.50      0.50      0.50       206
weighted avg       0.61      0.61      0.61       206


Fold 3
[[ 3  4  3  0  0  0]
 [ 5 25 11  1  0  0]
 [ 2 18 42  8  0  0]
 [ 0  1 14 33  6  0]
 [ 0  0  0  6 21  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.30      0.30      0.30        10
          A2       0.52      0.60      0.56        42
          B1       0.60      0.60      0.60        70
          B2       0.69      0.61      0.65        54
          C1       0.75      0.78      0.76        27
          C2       0.00      0.00      0.00         1

    accuracy                           0.61       204
   macro avg       0.48      0.48      0.48       204
weighted avg       0.61      0.61      0.61       204


Fold 4
[[ 3  7  0  0  0  0]
 [ 2 23 17  0  0  0]
 [ 1 19 36 12  1  0]
 [ 0  1 14 30  8  1]
 [ 0  1  2  6 18  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.50      0.30      0.37        10
          A2       0.45      0.55      0.49        42
          B1       0.52      0.52      0.52        69
          B2       0.62      0.56      0.59        54
          C1       0.64      0.67      0.65        27
          C2       0.00      0.00      0.00         1

    accuracy                           0.54       203
   macro avg       0.46      0.43      0.44       203
weighted avg       0.55      0.54      0.54       203


K-fold scores
[0.5631834071052229, 0.6091139526172644, 0.6124068094277675, 0.6073171157946246, 0.5416828901648636]
SKF f1 score mean 0.5867408350219486

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3414
3414
Fold 0
[[ 5  3  2  0  0  0]
 [ 4 21 17  1  0  0]
 [ 3 20 37 10  0  0]
 [ 0  1  7 38  8  1]
 [ 0  0  0  8 20  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.42      0.50      0.45        10
          A2       0.47      0.49      0.48        43
          B1       0.59      0.53      0.56        70
          B2       0.66      0.69      0.67        55
          C1       0.71      0.71      0.71        28
          C2       0.00      0.00      0.00         1

    accuracy                           0.58       207
   macro avg       0.47      0.49      0.48       207
weighted avg       0.59      0.58      0.58       207


Fold 1
[[ 3  5  2  0  0  0]
 [ 2 26 15  0  0  0]
 [ 1 14 50  5  0  0]
 [ 0  1  9 38  6  0]
 [ 0  0  0 10 18  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.50      0.30      0.37        10
          A2       0.57      0.60      0.58        43
          B1       0.66      0.71      0.68        70
          B2       0.72      0.70      0.71        54
          C1       0.72      0.64      0.68        28
          C2       0.00      0.00      0.00         1

    accuracy                           0.66       206
   macro avg       0.53      0.49      0.51       206
weighted avg       0.65      0.66      0.65       206


Fold 2
[[ 5  3  2  0  0  0]
 [ 1 21 20  1  0  0]
 [ 4 20 44  2  0  0]
 [ 0  0  8 37  9  0]
 [ 0  0  0  8 20  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.50      0.50      0.50        10
          A2       0.48      0.49      0.48        43
          B1       0.59      0.63      0.61        70
          B2       0.77      0.69      0.73        54
          C1       0.67      0.71      0.69        28
          C2       0.00      0.00      0.00         1

    accuracy                           0.62       206
   macro avg       0.50      0.50      0.50       206
weighted avg       0.62      0.62      0.62       206


Fold 3
[[ 3  4  3  0  0  0]
 [ 4 24 14  0  0  0]
 [ 0 17 45  8  0  0]
 [ 0  1 11 34  8  0]
 [ 0  0  0  7 20  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.43      0.30      0.35        10
          A2       0.52      0.57      0.55        42
          B1       0.62      0.64      0.63        70
          B2       0.69      0.63      0.66        54
          C1       0.69      0.74      0.71        27
          C2       0.00      0.00      0.00         1

    accuracy                           0.62       204
   macro avg       0.49      0.48      0.48       204
weighted avg       0.61      0.62      0.61       204


Fold 4
[[ 4  6  0  0  0  0]
 [ 2 21 19  0  0  0]
 [ 0 13 44 11  1  0]
 [ 0  0 13 31  9  1]
 [ 0  0  1  6 20  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.67      0.40      0.50        10
          A2       0.53      0.50      0.51        42
          B1       0.57      0.64      0.60        69
          B2       0.65      0.57      0.61        54
          C1       0.65      0.74      0.69        27
          C2       0.00      0.00      0.00         1

    accuracy                           0.59       203
   macro avg       0.51      0.48      0.49       203
weighted avg       0.59      0.59      0.59       203


K-fold scores
[0.584573433244987, 0.651421403381137, 0.6166175431225944, 0.614856110060531, 0.5888938683979248]
SKF f1 score mean 0.6112724716414348

SAME LANG EVAL DONE FOR THIS LANG
With POS ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
6373
6373
Fold 0
[[ 4  5  1  0  0  0]
 [ 0 29 14  0  0  0]
 [ 1 13 50  6  0  0]
 [ 0  0  3 45  7  0]
 [ 0  0  1  9 18  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.80      0.40      0.53        10
          A2       0.62      0.67      0.64        43
          B1       0.72      0.71      0.72        70
          B2       0.75      0.82      0.78        55
          C1       0.69      0.64      0.67        28
          C2       0.00      0.00      0.00         1

    accuracy                           0.71       207
   macro avg       0.60      0.54      0.56       207
weighted avg       0.70      0.71      0.70       207


Fold 1
[[ 3  6  1  0  0  0]
 [ 0 32 11  0  0  0]
 [ 2 14 52  2  0  0]
 [ 0  1  6 38  9  0]
 [ 0  0  1 10 17  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.60      0.30      0.40        10
          A2       0.60      0.74      0.67        43
          B1       0.73      0.74      0.74        70
          B2       0.76      0.70      0.73        54
          C1       0.63      0.61      0.62        28
          C2       0.00      0.00      0.00         1

    accuracy                           0.69       206
   macro avg       0.55      0.52      0.53       206
weighted avg       0.69      0.69      0.68       206


Fold 2
[[ 4  6  0  0  0  0]
 [ 1 30 12  0  0  0]
 [ 0 12 53  5  0  0]
 [ 0  0  4 41  9  0]
 [ 0  0  1 14 13  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.80      0.40      0.53        10
          A2       0.62      0.70      0.66        43
          B1       0.76      0.76      0.76        70
          B2       0.68      0.76      0.72        54
          C1       0.57      0.46      0.51        28
          C2       0.00      0.00      0.00         1

    accuracy                           0.68       206
   macro avg       0.57      0.51      0.53       206
weighted avg       0.68      0.68      0.68       206


Fold 3
[[ 3  7  0  0  0  0]
 [ 0 30 12  0  0  0]
 [ 0 10 53  7  0  0]
 [ 0  1  9 37  7  0]
 [ 0  0  0  8 19  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       1.00      0.30      0.46        10
          A2       0.62      0.71      0.67        42
          B1       0.72      0.76      0.74        70
          B2       0.71      0.69      0.70        54
          C1       0.70      0.70      0.70        27
          C2       0.00      0.00      0.00         1

    accuracy                           0.70       204
   macro avg       0.63      0.53      0.54       204
weighted avg       0.70      0.70      0.69       204


Fold 4
[[ 2  8  0  0  0  0]
 [ 0 28 14  0  0  0]
 [ 0 10 50  9  0  0]
 [ 0  0 15 33  6  0]
 [ 0  0  0 13 14  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       1.00      0.20      0.33        10
          A2       0.61      0.67      0.64        42
          B1       0.63      0.72      0.68        69
          B2       0.59      0.61      0.60        54
          C1       0.70      0.52      0.60        27
          C2       0.00      0.00      0.00         1

    accuracy                           0.63       203
   macro avg       0.59      0.45      0.47       203
weighted avg       0.64      0.63      0.62       203


K-fold scores
[0.7010352734405627, 0.6847985519954121, 0.678648527922627, 0.6903984098073996, 0.6165878525379626]
SKF f1 score mean 0.6742937231407928

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
6373
6373
Fold 0
[[ 4  4  1  0  1  0]
 [ 2 24 16  1  0  0]
 [ 1 19 42  8  0  0]
 [ 0  0 12 32 11  0]
 [ 0  0  2  9 17  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.57      0.40      0.47        10
          A2       0.51      0.56      0.53        43
          B1       0.58      0.60      0.59        70
          B2       0.64      0.58      0.61        55
          C1       0.57      0.61      0.59        28
          C2       0.00      0.00      0.00         1

    accuracy                           0.57       207
   macro avg       0.48      0.46      0.46       207
weighted avg       0.57      0.57      0.57       207


Fold 1
[[ 2  5  3  0  0  0]
 [ 4 23 14  1  1  0]
 [ 1 24 36  8  1  0]
 [ 0  6 11 28  9  0]
 [ 0  1  2  9 16  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.29      0.20      0.24        10
          A2       0.39      0.53      0.45        43
          B1       0.55      0.51      0.53        70
          B2       0.61      0.52      0.56        54
          C1       0.57      0.57      0.57        28
          C2       0.00      0.00      0.00         1

    accuracy                           0.51       206
   macro avg       0.40      0.39      0.39       206
weighted avg       0.52      0.51      0.51       206


Fold 2
[[ 4  4  2  0  0  0]
 [ 6 18 18  1  0  0]
 [ 2 14 47  7  0  0]
 [ 0  0 15 31  8  0]
 [ 0  1  2 14 11  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.33      0.40      0.36        10
          A2       0.49      0.42      0.45        43
          B1       0.56      0.67      0.61        70
          B2       0.58      0.57      0.58        54
          C1       0.55      0.39      0.46        28
          C2       0.00      0.00      0.00         1

    accuracy                           0.54       206
   macro avg       0.42      0.41      0.41       206
weighted avg       0.54      0.54      0.53       206


Fold 3
[[ 2  7  1  0  0  0]
 [ 7 20 14  1  0  0]
 [ 1 22 33 14  0  0]
 [ 0  5 15 29  5  0]
 [ 0  1  1 10 15  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.20      0.20      0.20        10
          A2       0.36      0.48      0.41        42
          B1       0.52      0.47      0.49        70
          B2       0.54      0.54      0.54        54
          C1       0.71      0.56      0.63        27
          C2       0.00      0.00      0.00         1

    accuracy                           0.49       204
   macro avg       0.39      0.37      0.38       204
weighted avg       0.50      0.49      0.49       204


Fold 4
[[ 2  7  1  0  0  0]
 [ 3 18 21  0  0  0]
 [ 1 11 45 12  0  0]
 [ 0  2 17 30  5  0]
 [ 0  1  0 12 14  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.33      0.20      0.25        10
          A2       0.46      0.43      0.44        42
          B1       0.54      0.65      0.59        69
          B2       0.55      0.56      0.55        54
          C1       0.74      0.52      0.61        27
          C2       0.00      0.00      0.00         1

    accuracy                           0.54       203
   macro avg       0.44      0.39      0.41       203
weighted avg       0.54      0.54      0.53       203


K-fold scores
[0.5734091760019973, 0.5099219493622691, 0.5331878122482423, 0.4885892135743357, 0.5315983015353783]
SKF f1 score mean 0.5273412905444446

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
6373
6373
Fold 0
[[ 2  5  2  0  1  0]
 [ 2 24 16  1  0  0]
 [ 0 17 46  7  0  0]
 [ 0  0  9 36 10  0]
 [ 0  0  1  8 18  1]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.50      0.20      0.29        10
          A2       0.52      0.56      0.54        43
          B1       0.62      0.66      0.64        70
          B2       0.69      0.65      0.67        55
          C1       0.60      0.64      0.62        28
          C2       0.00      0.00      0.00         1

    accuracy                           0.61       207
   macro avg       0.49      0.45      0.46       207
weighted avg       0.61      0.61      0.60       207


Fold 1
[[ 1  6  3  0  0  0]
 [ 4 23 14  1  1  0]
 [ 0 23 39  7  1  0]
 [ 0  4 10 29 11  0]
 [ 0  0  1  9 18  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.20      0.10      0.13        10
          A2       0.41      0.53      0.46        43
          B1       0.58      0.56      0.57        70
          B2       0.63      0.54      0.58        54
          C1       0.56      0.64      0.60        28
          C2       0.00      0.00      0.00         1

    accuracy                           0.53       206
   macro avg       0.40      0.40      0.39       206
weighted avg       0.53      0.53      0.53       206


Fold 2
[[ 4  5  1  0  0  0]
 [ 6 18 18  1  0  0]
 [ 2 13 48  7  0  0]
 [ 0  0  9 36  9  0]
 [ 0  1  1 13 13  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.33      0.40      0.36        10
          A2       0.49      0.42      0.45        43
          B1       0.62      0.69      0.65        70
          B2       0.63      0.67      0.65        54
          C1       0.57      0.46      0.51        28
          C2       0.00      0.00      0.00         1

    accuracy                           0.58       206
   macro avg       0.44      0.44      0.44       206
weighted avg       0.57      0.58      0.57       206


Fold 3
[[ 3  6  1  0  0  0]
 [ 7 19 16  0  0  0]
 [ 1 21 36 12  0  0]
 [ 0  2 14 31  7  0]
 [ 0  1  0  8 18  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.27      0.30      0.29        10
          A2       0.39      0.45      0.42        42
          B1       0.54      0.51      0.53        70
          B2       0.61      0.57      0.59        54
          C1       0.69      0.67      0.68        27
          C2       0.00      0.00      0.00         1

    accuracy                           0.52       204
   macro avg       0.42      0.42      0.42       204
weighted avg       0.53      0.52      0.53       204


Fold 4
[[ 2  7  1  0  0  0]
 [ 1 21 20  0  0  0]
 [ 1 10 48 10  0  0]
 [ 0  1 14 32  7  0]
 [ 0  1  0 11 15  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.50      0.20      0.29        10
          A2       0.53      0.50      0.51        42
          B1       0.58      0.70      0.63        69
          B2       0.59      0.59      0.59        54
          C1       0.68      0.56      0.61        27
          C2       0.00      0.00      0.00         1

    accuracy                           0.58       203
   macro avg       0.48      0.42      0.44       203
weighted avg       0.58      0.58      0.57       203


K-fold scores
[0.6046330070267645, 0.530520125784813, 0.5728261465126114, 0.5265159926015262, 0.57378767286903]
SKF f1 score mean 0.5616565889589491

SAME LANG EVAL DONE FOR THIS LANG
Dep ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5787
5787
Fold 0
[[ 4  5  1  0  0  0]
 [ 0 27 16  0  0  0]
 [ 2 15 43 10  0  0]
 [ 0  1  8 42  4  0]
 [ 0  0  0 17 11  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.67      0.40      0.50        10
          A2       0.56      0.63      0.59        43
          B1       0.63      0.61      0.62        70
          B2       0.61      0.76      0.68        55
          C1       0.69      0.39      0.50        28
          C2       0.00      0.00      0.00         1

    accuracy                           0.61       207
   macro avg       0.53      0.47      0.48       207
weighted avg       0.62      0.61      0.61       207


Fold 1
[[ 4  4  2  0  0  0]
 [ 0 28 15  0  0  0]
 [ 2 14 53  1  0  0]
 [ 0  1 10 37  6  0]
 [ 0  0  1 19  8  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.67      0.40      0.50        10
          A2       0.60      0.65      0.62        43
          B1       0.65      0.76      0.70        70
          B2       0.65      0.69      0.67        54
          C1       0.53      0.29      0.37        28
          C2       0.00      0.00      0.00         1

    accuracy                           0.63       206
   macro avg       0.52      0.46      0.48       206
weighted avg       0.62      0.63      0.62       206


Fold 2
[[ 4  6  0  0  0  0]
 [ 4 26 13  0  0  0]
 [ 0 13 53  4  0  0]
 [ 0  0  4 42  8  0]
 [ 0  0  2 16 10  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.50      0.40      0.44        10
          A2       0.58      0.60      0.59        43
          B1       0.74      0.76      0.75        70
          B2       0.68      0.78      0.72        54
          C1       0.53      0.36      0.43        28
          C2       0.00      0.00      0.00         1

    accuracy                           0.66       206
   macro avg       0.50      0.48      0.49       206
weighted avg       0.64      0.66      0.65       206


Fold 3
[[ 3  6  1  0  0  0]
 [ 2 28 12  0  0  0]
 [ 0  9 54  7  0  0]
 [ 0  1 11 37  5  0]
 [ 0  0  2 12 13  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.60      0.30      0.40        10
          A2       0.64      0.67      0.65        42
          B1       0.68      0.77      0.72        70
          B2       0.66      0.69      0.67        54
          C1       0.68      0.48      0.57        27
          C2       0.00      0.00      0.00         1

    accuracy                           0.66       204
   macro avg       0.54      0.48      0.50       204
weighted avg       0.66      0.66      0.65       204


Fold 4
[[ 3  7  0  0  0  0]
 [ 0 25 17  0  0  0]
 [ 0 11 51  7  0  0]
 [ 0  0 16 35  3  0]
 [ 0  0  2 14 11  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       1.00      0.30      0.46        10
          A2       0.58      0.60      0.59        42
          B1       0.59      0.74      0.66        69
          B2       0.62      0.65      0.64        54
          C1       0.73      0.41      0.52        27
          C2       0.00      0.00      0.00         1

    accuracy                           0.62       203
   macro avg       0.59      0.45      0.48       203
weighted avg       0.63      0.62      0.61       203


K-fold scores
[0.6057861663691289, 0.6180254031767013, 0.6462397979280546, 0.6536126446166296, 0.607065084175973]
SKF f1 score mean 0.6261458192532976

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5787
5787
Fold 0
[[ 5  3  1  1  0  0]
 [ 6 23 13  0  1  0]
 [ 2 18 37 13  0  0]
 [ 0  0 10 40  5  0]
 [ 0  0  1 13 14  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.38      0.50      0.43        10
          A2       0.52      0.53      0.53        43
          B1       0.60      0.53      0.56        70
          B2       0.59      0.73      0.65        55
          C1       0.70      0.50      0.58        28
          C2       0.00      0.00      0.00         1

    accuracy                           0.57       207
   macro avg       0.47      0.47      0.46       207
weighted avg       0.58      0.57      0.57       207


Fold 1
[[ 3  4  3  0  0  0]
 [ 4 17 22  0  0  0]
 [ 4 22 37  7  0  0]
 [ 0  1 15 31  7  0]
 [ 0  1  2 11 14  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.27      0.30      0.29        10
          A2       0.38      0.40      0.39        43
          B1       0.47      0.53      0.50        70
          B2       0.62      0.57      0.60        54
          C1       0.67      0.50      0.57        28
          C2       0.00      0.00      0.00         1

    accuracy                           0.50       206
   macro avg       0.40      0.38      0.39       206
weighted avg       0.50      0.50      0.50       206


Fold 2
[[ 4  6  0  0  0  0]
 [ 6 19 17  1  0  0]
 [ 0 20 43  7  0  0]
 [ 0  3 14 31  6  0]
 [ 0  0  3 10 15  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.40      0.40      0.40        10
          A2       0.40      0.44      0.42        43
          B1       0.56      0.61      0.59        70
          B2       0.63      0.57      0.60        54
          C1       0.68      0.54      0.60        28
          C2       0.00      0.00      0.00         1

    accuracy                           0.54       206
   macro avg       0.44      0.43      0.43       206
weighted avg       0.55      0.54      0.54       206


Fold 3
[[ 4  5  1  0  0  0]
 [10 13 18  1  0  0]
 [ 1 15 47  7  0  0]
 [ 0  3 14 31  6  0]
 [ 0  0  0 12 15  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.27      0.40      0.32        10
          A2       0.36      0.31      0.33        42
          B1       0.59      0.67      0.63        70
          B2       0.61      0.57      0.59        54
          C1       0.68      0.56      0.61        27
          C2       0.00      0.00      0.00         1

    accuracy                           0.54       204
   macro avg       0.42      0.42      0.41       204
weighted avg       0.54      0.54      0.54       204


Fold 4
[[ 1  7  2  0  0  0]
 [ 3 21 16  2  0  0]
 [ 1 23 37  7  1  0]
 [ 0  1 18 29  6  0]
 [ 0  1  1  8 17  0]
 [ 0  0  1  0  0  0]]
              precision    recall  f1-score   support

          A1       0.20      0.10      0.13        10
          A2       0.40      0.50      0.44        42
          B1       0.49      0.54      0.51        69
          B2       0.63      0.54      0.58        54
          C1       0.71      0.63      0.67        27
          C2       0.00      0.00      0.00         1

    accuracy                           0.52       203
   macro avg       0.40      0.38      0.39       203
weighted avg       0.52      0.52      0.52       203


K-fold scores
[0.5721332056531372, 0.49722421157392166, 0.5447246566861987, 0.5366813392023475, 0.5156654567453116]
SKF f1 score mean 0.5332857739721834

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5787
5787
Fold 0
[[ 5  3  1  1  0  0]
 [ 6 22 14  0  1  0]
 [ 2 18 38 12  0  0]
 [ 0  0  9 41  5  0]
 [ 0  0  1 10 17  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.38      0.50      0.43        10
          A2       0.51      0.51      0.51        43
          B1       0.60      0.54      0.57        70
          B2       0.63      0.75      0.68        55
          C1       0.74      0.61      0.67        28
          C2       0.00      0.00      0.00         1

    accuracy                           0.59       207
   macro avg       0.48      0.48      0.48       207
weighted avg       0.60      0.59      0.59       207


Fold 1
[[ 2  4  4  0  0  0]
 [ 5 17 21  0  0  0]
 [ 3 18 42  7  0  0]
 [ 0  1 13 31  9  0]
 [ 0  1  2 11 14  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.20      0.20      0.20        10
          A2       0.41      0.40      0.40        43
          B1       0.51      0.60      0.55        70
          B2       0.62      0.57      0.60        54
          C1       0.61      0.50      0.55        28
          C2       0.00      0.00      0.00         1

    accuracy                           0.51       206
   macro avg       0.39      0.38      0.38       206
weighted avg       0.52      0.51      0.51       206


Fold 2
[[ 4  6  0  0  0  0]
 [ 5 18 19  1  0  0]
 [ 0 20 45  5  0  0]
 [ 0  0 15 32  7  0]
 [ 0  0  1 11 16  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.44      0.40      0.42        10
          A2       0.41      0.42      0.41        43
          B1       0.56      0.64      0.60        70
          B2       0.65      0.59      0.62        54
          C1       0.67      0.57      0.62        28
          C2       0.00      0.00      0.00         1

    accuracy                           0.56       206
   macro avg       0.46      0.44      0.45       206
weighted avg       0.56      0.56      0.56       206


Fold 3
[[ 3  6  1  0  0  0]
 [ 7 18 17  0  0  0]
 [ 0 14 46  9  1  0]
 [ 0  1 15 30  8  0]
 [ 0  0  0 10 17  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.30      0.30      0.30        10
          A2       0.46      0.43      0.44        42
          B1       0.58      0.66      0.62        70
          B2       0.61      0.56      0.58        54
          C1       0.63      0.63      0.63        27
          C2       0.00      0.00      0.00         1

    accuracy                           0.56       204
   macro avg       0.43      0.43      0.43       204
weighted avg       0.55      0.56      0.56       204


Fold 4
[[ 1  7  2  0  0  0]
 [ 3 20 17  2  0  0]
 [ 1 23 37  7  1  0]
 [ 0  0 18 29  7  0]
 [ 0  0  1  9 17  0]
 [ 0  0  1  0  0  0]]
              precision    recall  f1-score   support

          A1       0.20      0.10      0.13        10
          A2       0.40      0.48      0.43        42
          B1       0.49      0.54      0.51        69
          B2       0.62      0.54      0.57        54
          C1       0.68      0.63      0.65        27
          C2       0.00      0.00      0.00         1

    accuracy                           0.51       203
   macro avg       0.40      0.38      0.38       203
weighted avg       0.51      0.51      0.51       203


K-fold scores
[0.5922600294055871, 0.5128826657426856, 0.5572223157916846, 0.555610067923262, 0.5097130204233703]
SKF f1 score mean 0.5455376198573179

SAME LANG EVAL DONE FOR THIS LANG
************for dimension:  Vocabularycontrol  ***************
Extracted all features: 
Printing class statistics
Counter({'B1': 319, 'B2': 278, 'A2': 259, 'C1': 82, 'A1': 77, 'C2': 11})
With Word ngrams: 
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3414
3414
Fold 0
[[ 9  6  1  0  0  0]
 [ 5 38  9  0  0  0]
 [ 0 19 35 10  0  0]
 [ 0  2 10 43  1  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  3  0  0]]
              precision    recall  f1-score   support

          A1       0.64      0.56      0.60        16
          A2       0.58      0.73      0.65        52
          B1       0.64      0.55      0.59        64
          B2       0.59      0.77      0.67        56
          C1       0.00      0.00      0.00        17
          C2       0.00      0.00      0.00         3

    accuracy                           0.60       208
   macro avg       0.41      0.43      0.42       208
weighted avg       0.55      0.60      0.57       208


Fold 1
[[ 8  7  1  0  0  0]
 [ 1 37 14  0  0  0]
 [ 1 16 31 16  0  0]
 [ 0  1  8 47  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.80      0.50      0.62        16
          A2       0.61      0.71      0.65        52
          B1       0.57      0.48      0.53        64
          B2       0.57      0.84      0.68        56
          C1       0.00      0.00      0.00        17
          C2       0.00      0.00      0.00         2

    accuracy                           0.59       207
   macro avg       0.43      0.42      0.41       207
weighted avg       0.55      0.59      0.56       207


Fold 2
[[ 5  9  1  0  0  0]
 [ 0 45  7  0  0  0]
 [ 1 24 25 13  1  0]
 [ 0  3  9 41  3  0]
 [ 0  0  0 15  1  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.83      0.33      0.48        15
          A2       0.56      0.87      0.68        52
          B1       0.60      0.39      0.47        64
          B2       0.58      0.73      0.65        56
          C1       0.20      0.06      0.10        16
          C2       0.00      0.00      0.00         2

    accuracy                           0.57       205
   macro avg       0.46      0.40      0.39       205
weighted avg       0.56      0.57      0.54       205


Fold 3
[[ 3 12  0  0  0  0]
 [ 2 38 12  0  0  0]
 [ 1 18 33 12  0  0]
 [ 0  3 10 42  0  0]
 [ 0  0  1 15  0  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.50      0.20      0.29        15
          A2       0.54      0.73      0.62        52
          B1       0.59      0.52      0.55        64
          B2       0.59      0.76      0.67        55
          C1       0.00      0.00      0.00        16
          C2       0.00      0.00      0.00         2

    accuracy                           0.57       204
   macro avg       0.37      0.37      0.35       204
weighted avg       0.52      0.57      0.53       204


Fold 4
[[ 6  9  0  0  0  0]
 [ 2 35 13  1  0  0]
 [ 1 15 34 13  0  0]
 [ 0  0  5 50  0  0]
 [ 0  0  0 15  1  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.67      0.40      0.50        15
          A2       0.59      0.69      0.64        51
          B1       0.65      0.54      0.59        63
          B2       0.62      0.91      0.74        55
          C1       1.00      0.06      0.12        16
          C2       0.00      0.00      0.00         2

    accuracy                           0.62       202
   macro avg       0.59      0.43      0.43       202
weighted avg       0.65      0.62      0.59       202


K-fold scores
[0.5690296631473102, 0.5587985380269868, 0.5375648592766943, 0.530796383594088, 0.5917339048482853]
SKF f1 score mean 0.5575846697786729

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3414
3414
Fold 0
[[10  3  3  0  0  0]
 [ 6 32 13  0  1  0]
 [ 2 19 34  7  2  0]
 [ 0  4 10 36  6  0]
 [ 0  0  3 11  3  0]
 [ 0  0  0  2  1  0]]
              precision    recall  f1-score   support

          A1       0.56      0.62      0.59        16
          A2       0.55      0.62      0.58        52
          B1       0.54      0.53      0.54        64
          B2       0.64      0.64      0.64        56
          C1       0.23      0.18      0.20        17
          C2       0.00      0.00      0.00         3

    accuracy                           0.55       208
   macro avg       0.42      0.43      0.42       208
weighted avg       0.54      0.55      0.54       208


Fold 1
[[ 7  9  0  0  0  0]
 [ 1 27 20  4  0  0]
 [ 3 11 38 11  1  0]
 [ 0  0 18 31  6  1]
 [ 0  0  2 13  2  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.64      0.44      0.52        16
          A2       0.57      0.52      0.55        52
          B1       0.49      0.59      0.54        64
          B2       0.51      0.55      0.53        56
          C1       0.22      0.12      0.15        17
          C2       0.00      0.00      0.00         2

    accuracy                           0.51       207
   macro avg       0.40      0.37      0.38       207
weighted avg       0.50      0.51      0.50       207


Fold 2
[[ 6  4  5  0  0  0]
 [ 3 31 16  2  0  0]
 [ 3 16 35  8  2  0]
 [ 0  1 20 28  7  0]
 [ 0  1  2  3 10  0]
 [ 0  0  1  1  0  0]]
              precision    recall  f1-score   support

          A1       0.50      0.40      0.44        15
          A2       0.58      0.60      0.59        52
          B1       0.44      0.55      0.49        64
          B2       0.67      0.50      0.57        56
          C1       0.53      0.62      0.57        16
          C2       0.00      0.00      0.00         2

    accuracy                           0.54       205
   macro avg       0.45      0.44      0.44       205
weighted avg       0.55      0.54      0.54       205


Fold 3
[[ 5 10  0  0  0  0]
 [ 3 35 13  1  0  0]
 [ 5 16 32  9  2  0]
 [ 0  2 21 29  3  0]
 [ 0  0  1 12  3  0]
 [ 0  0  0  1  0  1]]
              precision    recall  f1-score   support

          A1       0.38      0.33      0.36        15
          A2       0.56      0.67      0.61        52
          B1       0.48      0.50      0.49        64
          B2       0.56      0.53      0.54        55
          C1       0.38      0.19      0.25        16
          C2       1.00      0.50      0.67         2

    accuracy                           0.51       204
   macro avg       0.56      0.45      0.49       204
weighted avg       0.51      0.51      0.51       204


Fold 4
[[ 7  7  1  0  0  0]
 [ 2 34 15  0  0  0]
 [ 1 21 24 16  1  0]
 [ 0  2 16 30  7  0]
 [ 0  1  1  9  5  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.70      0.47      0.56        15
          A2       0.52      0.67      0.59        51
          B1       0.42      0.38      0.40        63
          B2       0.53      0.55      0.54        55
          C1       0.38      0.31      0.34        16
          C2       0.00      0.00      0.00         2

    accuracy                           0.50       202
   macro avg       0.43      0.40      0.40       202
weighted avg       0.49      0.50      0.49       202


K-fold scores
[0.5448751283454851, 0.49857029989029716, 0.5358193026485709, 0.5069750479813152, 0.48751524167195043]
SKF f1 score mean 0.5147510041075238

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3414
3414
Fold 0
[[10  4  2  0  0  0]
 [ 6 32 14  0  0  0]
 [ 2 17 38  6  1  0]
 [ 0  3  7 41  5  0]
 [ 0  0  1 12  4  0]
 [ 0  0  0  2  1  0]]
              precision    recall  f1-score   support

          A1       0.56      0.62      0.59        16
          A2       0.57      0.62      0.59        52
          B1       0.61      0.59      0.60        64
          B2       0.67      0.73      0.70        56
          C1       0.36      0.24      0.29        17
          C2       0.00      0.00      0.00         3

    accuracy                           0.60       208
   macro avg       0.46      0.47      0.46       208
weighted avg       0.58      0.60      0.59       208


Fold 1
[[ 8  8  0  0  0  0]
 [ 1 30 19  2  0  0]
 [ 2 14 38 10  0  0]
 [ 0  0 16 33  6  1]
 [ 0  0  1 14  2  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.73      0.50      0.59        16
          A2       0.58      0.58      0.58        52
          B1       0.51      0.59      0.55        64
          B2       0.54      0.59      0.56        56
          C1       0.25      0.12      0.16        17
          C2       0.00      0.00      0.00         2

    accuracy                           0.54       207
   macro avg       0.43      0.40      0.41       207
weighted avg       0.53      0.54      0.53       207


Fold 2
[[ 6  5  4  0  0  0]
 [ 3 30 18  1  0  0]
 [ 2 17 33 10  2  0]
 [ 0  1 17 29  9  0]
 [ 0  0  2  6  8  0]
 [ 0  0  1  1  0  0]]
              precision    recall  f1-score   support

          A1       0.55      0.40      0.46        15
          A2       0.57      0.58      0.57        52
          B1       0.44      0.52      0.47        64
          B2       0.62      0.52      0.56        56
          C1       0.42      0.50      0.46        16
          C2       0.00      0.00      0.00         2

    accuracy                           0.52       205
   macro avg       0.43      0.42      0.42       205
weighted avg       0.52      0.52      0.52       205


Fold 3
[[ 5 10  0  0  0  0]
 [ 3 33 15  1  0  0]
 [ 3 16 35  8  2  0]
 [ 0  3 16 33  3  0]
 [ 0  0  1 12  3  0]
 [ 0  0  0  1  0  1]]
              precision    recall  f1-score   support

          A1       0.45      0.33      0.38        15
          A2       0.53      0.63      0.58        52
          B1       0.52      0.55      0.53        64
          B2       0.60      0.60      0.60        55
          C1       0.38      0.19      0.25        16
          C2       1.00      0.50      0.67         2

    accuracy                           0.54       204
   macro avg       0.58      0.47      0.50       204
weighted avg       0.53      0.54      0.53       204


Fold 4
[[ 8  7  0  0  0  0]
 [ 3 33 15  0  0  0]
 [ 2 17 28 15  1  0]
 [ 0  0 12 37  6  0]
 [ 0  0  1  9  6  0]
 [ 0  0  0  1  0  1]]
              precision    recall  f1-score   support

          A1       0.62      0.53      0.57        15
          A2       0.58      0.65      0.61        51
          B1       0.50      0.44      0.47        63
          B2       0.60      0.67      0.63        55
          C1       0.46      0.38      0.41        16
          C2       1.00      0.50      0.67         2

    accuracy                           0.56       202
   macro avg       0.63      0.53      0.56       202
weighted avg       0.56      0.56      0.56       202


K-fold scores
[0.5910325011003744, 0.5267516999170013, 0.5164591128917352, 0.5314034340338313, 0.5550767417646296]
SKF f1 score mean 0.5441446979415143

SAME LANG EVAL DONE FOR THIS LANG
With POS ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
6373
6373
Fold 0
[[ 8  8  0  0  0  0]
 [ 3 38 11  0  0  0]
 [ 0 20 31 13  0  0]
 [ 0  3  7 46  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  3  0  0]]
              precision    recall  f1-score   support

          A1       0.73      0.50      0.59        16
          A2       0.55      0.73      0.63        52
          B1       0.63      0.48      0.55        64
          B2       0.58      0.82      0.68        56
          C1       0.00      0.00      0.00        17
          C2       0.00      0.00      0.00         3

    accuracy                           0.59       208
   macro avg       0.42      0.42      0.41       208
weighted avg       0.55      0.59      0.55       208


Fold 1
[[ 7  8  1  0  0  0]
 [ 1 31 20  0  0  0]
 [ 2 14 29 19  0  0]
 [ 0  2  8 46  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.70      0.44      0.54        16
          A2       0.56      0.60      0.58        52
          B1       0.50      0.45      0.48        64
          B2       0.55      0.82      0.66        56
          C1       0.00      0.00      0.00        17
          C2       0.00      0.00      0.00         2

    accuracy                           0.55       207
   macro avg       0.39      0.38      0.38       207
weighted avg       0.50      0.55      0.51       207


Fold 2
[[ 3 12  0  0  0  0]
 [ 3 42  6  1  0  0]
 [ 1 20 31 12  0  0]
 [ 0  2 11 43  0  0]
 [ 0  1  1 14  0  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.43      0.20      0.27        15
          A2       0.55      0.81      0.65        52
          B1       0.63      0.48      0.55        64
          B2       0.60      0.77      0.67        56
          C1       0.00      0.00      0.00        16
          C2       0.00      0.00      0.00         2

    accuracy                           0.58       205
   macro avg       0.37      0.38      0.36       205
weighted avg       0.53      0.58      0.54       205


Fold 3
[[ 3 12  0  0  0  0]
 [ 2 39 11  0  0  0]
 [ 2 21 31 10  0  0]
 [ 0  3  7 45  0  0]
 [ 0  0  1 15  0  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.43      0.20      0.27        15
          A2       0.52      0.75      0.61        52
          B1       0.62      0.48      0.54        64
          B2       0.62      0.82      0.71        55
          C1       0.00      0.00      0.00        16
          C2       0.00      0.00      0.00         2

    accuracy                           0.58       204
   macro avg       0.37      0.38      0.36       204
weighted avg       0.53      0.58      0.54       204


Fold 4
[[ 8  7  0  0  0  0]
 [ 2 31 18  0  0  0]
 [ 0 14 40  9  0  0]
 [ 0  0  5 50  0  0]
 [ 0  0  1 15  0  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.80      0.53      0.64        15
          A2       0.60      0.61      0.60        51
          B1       0.62      0.63      0.63        63
          B2       0.66      0.91      0.76        55
          C1       0.00      0.00      0.00        16
          C2       0.00      0.00      0.00         2

    accuracy                           0.64       202
   macro avg       0.45      0.45      0.44       202
weighted avg       0.58      0.64      0.60       202


K-fold scores
[0.5549069505626224, 0.5119442282370853, 0.5399581388047598, 0.5382907473604921, 0.6038059471322763]
SKF f1 score mean 0.5497812024194472

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
6373
6373
Fold 0
[[ 6  9  1  0  0  0]
 [12 25 14  1  0  0]
 [ 3 25 26 10  0  0]
 [ 0  1 19 30  6  0]
 [ 0  0  3  9  5  0]
 [ 0  0  0  3  0  0]]
              precision    recall  f1-score   support

          A1       0.29      0.38      0.32        16
          A2       0.42      0.48      0.45        52
          B1       0.41      0.41      0.41        64
          B2       0.57      0.54      0.55        56
          C1       0.45      0.29      0.36        17
          C2       0.00      0.00      0.00         3

    accuracy                           0.44       208
   macro avg       0.36      0.35      0.35       208
weighted avg       0.44      0.44      0.44       208


Fold 1
[[ 8  6  2  0  0  0]
 [ 3 28 19  2  0  0]
 [ 1 18 31 13  1  0]
 [ 0  4 16 31  5  0]
 [ 0  0  2 13  2  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.67      0.50      0.57        16
          A2       0.50      0.54      0.52        52
          B1       0.44      0.48      0.46        64
          B2       0.51      0.55      0.53        56
          C1       0.25      0.12      0.16        17
          C2       0.00      0.00      0.00         2

    accuracy                           0.48       207
   macro avg       0.39      0.37      0.37       207
weighted avg       0.47      0.48      0.47       207


Fold 2
[[ 2 10  3  0  0  0]
 [ 5 29 13  4  1  0]
 [ 3 18 25 17  1  0]
 [ 0  2 15 31  8  0]
 [ 0  1  7  6  2  0]
 [ 0  0  1  1  0  0]]
              precision    recall  f1-score   support

          A1       0.20      0.13      0.16        15
          A2       0.48      0.56      0.52        52
          B1       0.39      0.39      0.39        64
          B2       0.53      0.55      0.54        56
          C1       0.17      0.12      0.14        16
          C2       0.00      0.00      0.00         2

    accuracy                           0.43       205
   macro avg       0.29      0.29      0.29       205
weighted avg       0.42      0.43      0.42       205


Fold 3
[[ 3  6  4  1  1  0]
 [ 2 30 17  2  1  0]
 [ 4 18 32  8  1  1]
 [ 1  6 15 26  7  0]
 [ 0  1  4  9  2  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.30      0.20      0.24        15
          A2       0.49      0.58      0.53        52
          B1       0.44      0.50      0.47        64
          B2       0.54      0.47      0.50        55
          C1       0.17      0.12      0.14        16
          C2       0.00      0.00      0.00         2

    accuracy                           0.46       204
   macro avg       0.32      0.31      0.31       204
weighted avg       0.45      0.46      0.45       204


Fold 4
[[ 8  4  2  1  0  0]
 [ 5 26 16  4  0  0]
 [ 1 20 30 12  0  0]
 [ 0  0 18 30  6  1]
 [ 0  0  2  9  4  1]
 [ 0  1  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.57      0.53      0.55        15
          A2       0.51      0.51      0.51        51
          B1       0.44      0.48      0.46        63
          B2       0.53      0.55      0.54        55
          C1       0.40      0.25      0.31        16
          C2       0.00      0.00      0.00         2

    accuracy                           0.49       202
   macro avg       0.41      0.39      0.39       202
weighted avg       0.48      0.49      0.48       202


K-fold scores
[0.4399294036430134, 0.4739757201910205, 0.4234419027420088, 0.44794593678836525, 0.4827632996949961]
SKF f1 score mean 0.45361125261188084

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
6373
6373
Fold 0
[[ 6  9  1  0  0  0]
 [10 29 12  1  0  0]
 [ 1 26 27 10  0  0]
 [ 0  2 18 29  7  0]
 [ 0  0  3  9  5  0]
 [ 0  0  0  3  0  0]]
              precision    recall  f1-score   support

          A1       0.35      0.38      0.36        16
          A2       0.44      0.56      0.49        52
          B1       0.44      0.42      0.43        64
          B2       0.56      0.52      0.54        56
          C1       0.42      0.29      0.34        17
          C2       0.00      0.00      0.00         3

    accuracy                           0.46       208
   macro avg       0.37      0.36      0.36       208
weighted avg       0.46      0.46      0.46       208


Fold 1
[[ 9  5  2  0  0  0]
 [ 3 28 19  2  0  0]
 [ 1 17 31 14  1  0]
 [ 0  4 15 31  6  0]
 [ 0  0  2 13  2  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.69      0.56      0.62        16
          A2       0.52      0.54      0.53        52
          B1       0.45      0.48      0.47        64
          B2       0.50      0.55      0.53        56
          C1       0.22      0.12      0.15        17
          C2       0.00      0.00      0.00         2

    accuracy                           0.49       207
   macro avg       0.40      0.38      0.38       207
weighted avg       0.48      0.49      0.48       207


Fold 2
[[ 3 10  2  0  0  0]
 [ 6 29 13  3  1  0]
 [ 3 16 30 14  1  0]
 [ 0  3 12 33  8  0]
 [ 0  1  5  7  3  0]
 [ 0  0  2  0  0  0]]
              precision    recall  f1-score   support

          A1       0.25      0.20      0.22        15
          A2       0.49      0.56      0.52        52
          B1       0.47      0.47      0.47        64
          B2       0.58      0.59      0.58        56
          C1       0.23      0.19      0.21        16
          C2       0.00      0.00      0.00         2

    accuracy                           0.48       205
   macro avg       0.34      0.33      0.33       205
weighted avg       0.47      0.48      0.47       205


Fold 3
[[ 2  9  3  1  0  0]
 [ 3 30 17  2  0  0]
 [ 2 16 36  8  1  1]
 [ 1  3 15 28  8  0]
 [ 0  0  4 10  2  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.25      0.13      0.17        15
          A2       0.52      0.58      0.55        52
          B1       0.48      0.56      0.52        64
          B2       0.55      0.51      0.53        55
          C1       0.18      0.12      0.15        16
          C2       0.00      0.00      0.00         2

    accuracy                           0.48       204
   macro avg       0.33      0.32      0.32       204
weighted avg       0.46      0.48      0.47       204


Fold 4
[[ 6  6  2  1  0  0]
 [ 6 24 17  4  0  0]
 [ 1 20 34  8  0  0]
 [ 0  1 18 30  5  1]
 [ 0  0  1 10  4  1]
 [ 0  1  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.46      0.40      0.43        15
          A2       0.46      0.47      0.47        51
          B1       0.47      0.54      0.50        63
          B2       0.56      0.55      0.55        55
          C1       0.44      0.25      0.32        16
          C2       0.00      0.00      0.00         2

    accuracy                           0.49       202
   macro avg       0.40      0.37      0.38       202
weighted avg       0.48      0.49      0.48       202


K-fold scores
[0.45654637928688185, 0.479596292227418, 0.4708429947993202, 0.46838424164929504, 0.4818025960938865]
SKF f1 score mean 0.4714345008113603

SAME LANG EVAL DONE FOR THIS LANG
Dep ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5787
5787
Fold 0
[[ 7  9  0  0  0  0]
 [ 3 40  9  0  0  0]
 [ 0 23 28 13  0  0]
 [ 0  3  7 46  0  0]
 [ 0  0  1 16  0  0]
 [ 0  0  1  2  0  0]]
              precision    recall  f1-score   support

          A1       0.70      0.44      0.54        16
          A2       0.53      0.77      0.63        52
          B1       0.61      0.44      0.51        64
          B2       0.60      0.82      0.69        56
          C1       0.00      0.00      0.00        17
          C2       0.00      0.00      0.00         3

    accuracy                           0.58       208
   macro avg       0.41      0.41      0.39       208
weighted avg       0.54      0.58      0.54       208


Fold 1
[[ 5 10  1  0  0  0]
 [ 1 33 18  0  0  0]
 [ 1 13 33 17  0  0]
 [ 0  1  9 46  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.71      0.31      0.43        16
          A2       0.58      0.63      0.61        52
          B1       0.54      0.52      0.53        64
          B2       0.56      0.82      0.67        56
          C1       0.00      0.00      0.00        17
          C2       0.00      0.00      0.00         2

    accuracy                           0.57       207
   macro avg       0.40      0.38      0.37       207
weighted avg       0.52      0.57      0.53       207


Fold 2
[[ 5  8  2  0  0  0]
 [ 2 41  9  0  0  0]
 [ 2 25 22 15  0  0]
 [ 0  2 14 40  0  0]
 [ 0  1  1 14  0  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.56      0.33      0.42        15
          A2       0.53      0.79      0.64        52
          B1       0.46      0.34      0.39        64
          B2       0.56      0.71      0.63        56
          C1       0.00      0.00      0.00        16
          C2       0.00      0.00      0.00         2

    accuracy                           0.53       205
   macro avg       0.35      0.36      0.35       205
weighted avg       0.47      0.53      0.49       205


Fold 3
[[ 5 10  0  0  0  0]
 [ 2 42  8  0  0  0]
 [ 2 19 32 11  0  0]
 [ 0  4  8 43  0  0]
 [ 0  0  1 15  0  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.56      0.33      0.42        15
          A2       0.56      0.81      0.66        52
          B1       0.65      0.50      0.57        64
          B2       0.61      0.78      0.68        55
          C1       0.00      0.00      0.00        16
          C2       0.00      0.00      0.00         2

    accuracy                           0.60       204
   macro avg       0.40      0.40      0.39       204
weighted avg       0.55      0.60      0.56       204


Fold 4
[[ 6  9  0  0  0  0]
 [ 2 34 15  0  0  0]
 [ 0 15 37 11  0  0]
 [ 0  0  8 47  0  0]
 [ 0  0  3 13  0  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.75      0.40      0.52        15
          A2       0.59      0.67      0.62        51
          B1       0.59      0.59      0.59        63
          B2       0.64      0.85      0.73        55
          C1       0.00      0.00      0.00        16
          C2       0.00      0.00      0.00         2

    accuracy                           0.61       202
   macro avg       0.43      0.42      0.41       202
weighted avg       0.56      0.61      0.58       202


K-fold scores
[0.541778607760947, 0.5293144618578448, 0.4864522500484377, 0.5609371124397233, 0.5793724045445031]
SKF f1 score mean 0.5395709673302912

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5787
5787
Fold 0
[[ 5  7  4  0  0  0]
 [12 23 15  2  0  0]
 [ 1 21 35  7  0  0]
 [ 2  2 19 26  7  0]
 [ 0  1  4 10  2  0]
 [ 0  0  0  3  0  0]]
              precision    recall  f1-score   support

          A1       0.25      0.31      0.28        16
          A2       0.43      0.44      0.43        52
          B1       0.45      0.55      0.50        64
          B2       0.54      0.46      0.50        56
          C1       0.22      0.12      0.15        17
          C2       0.00      0.00      0.00         3

    accuracy                           0.44       208
   macro avg       0.32      0.31      0.31       208
weighted avg       0.43      0.44      0.43       208


Fold 1
[[ 7  7  1  1  0  0]
 [ 4 19 22  6  1  0]
 [ 1 18 31 12  2  0]
 [ 0  1 20 27  8  0]
 [ 0  1  1 12  3  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.58      0.44      0.50        16
          A2       0.41      0.37      0.39        52
          B1       0.41      0.48      0.45        64
          B2       0.45      0.48      0.47        56
          C1       0.21      0.18      0.19        17
          C2       0.00      0.00      0.00         2

    accuracy                           0.42       207
   macro avg       0.35      0.32      0.33       207
weighted avg       0.42      0.42      0.42       207


Fold 2
[[ 4  8  2  1  0  0]
 [ 8 35  5  3  0  1]
 [ 5 21 22 15  1  0]
 [ 0 10 12 26  6  2]
 [ 0  1  6  7  2  0]
 [ 0  0  1  1  0  0]]
              precision    recall  f1-score   support

          A1       0.24      0.27      0.25        15
          A2       0.47      0.67      0.55        52
          B1       0.46      0.34      0.39        64
          B2       0.49      0.46      0.48        56
          C1       0.22      0.12      0.16        16
          C2       0.00      0.00      0.00         2

    accuracy                           0.43       205
   macro avg       0.31      0.31      0.31       205
weighted avg       0.43      0.43      0.42       205


Fold 3
[[ 7  6  1  1  0  0]
 [10 25 17  0  0  0]
 [ 2 12 42  7  1  0]
 [ 0  7 18 26  4  0]
 [ 0  0  3  8  5  0]
 [ 0  0  1  1  0  0]]
              precision    recall  f1-score   support

          A1       0.37      0.47      0.41        15
          A2       0.50      0.48      0.49        52
          B1       0.51      0.66      0.58        64
          B2       0.60      0.47      0.53        55
          C1       0.50      0.31      0.38        16
          C2       0.00      0.00      0.00         2

    accuracy                           0.51       204
   macro avg       0.41      0.40      0.40       204
weighted avg       0.52      0.51      0.51       204


Fold 4
[[ 6  8  1  0  0  0]
 [ 6 30 12  3  0  0]
 [ 3 17 30 12  1  0]
 [ 0  1 16 34  4  0]
 [ 0  0  1 10  5  0]
 [ 0  1  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.40      0.40      0.40        15
          A2       0.53      0.59      0.56        51
          B1       0.50      0.48      0.49        63
          B2       0.57      0.62      0.59        55
          C1       0.50      0.31      0.38        16
          C2       0.00      0.00      0.00         2

    accuracy                           0.52       202
   macro avg       0.42      0.40      0.40       202
weighted avg       0.51      0.52      0.51       202


K-fold scores
[0.42980248288975087, 0.4157937970721597, 0.4235603453461806, 0.5089514906770576, 0.513567455122215]
SKF f1 score mean 0.4583351142214728

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5787
5787
Fold 0
[[ 5  7  4  0  0  0]
 [12 24 14  2  0  0]
 [ 0 21 34  9  0  0]
 [ 1  2 17 30  6  0]
 [ 0  1  3 11  2  0]
 [ 0  0  0  2  1  0]]
              precision    recall  f1-score   support

          A1       0.28      0.31      0.29        16
          A2       0.44      0.46      0.45        52
          B1       0.47      0.53      0.50        64
          B2       0.56      0.54      0.55        56
          C1       0.22      0.12      0.15        17
          C2       0.00      0.00      0.00         3

    accuracy                           0.46       208
   macro avg       0.33      0.33      0.32       208
weighted avg       0.44      0.46      0.45       208


Fold 1
[[ 7  7  1  1  0  0]
 [ 3 21 20  7  1  0]
 [ 3 14 35 10  2  0]
 [ 0  1 22 26  7  0]
 [ 0  0  1 14  2  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.54      0.44      0.48        16
          A2       0.49      0.40      0.44        52
          B1       0.44      0.55      0.49        64
          B2       0.43      0.46      0.45        56
          C1       0.17      0.12      0.14        17
          C2       0.00      0.00      0.00         2

    accuracy                           0.44       207
   macro avg       0.34      0.33      0.33       207
weighted avg       0.43      0.44      0.43       207


Fold 2
[[ 5  7  2  1  0  0]
 [ 5 35  6  5  0  1]
 [ 5 19 22 17  1  0]
 [ 0  7  9 32  6  2]
 [ 0  1  3  8  4  0]
 [ 0  0  1  1  0  0]]
              precision    recall  f1-score   support

          A1       0.33      0.33      0.33        15
          A2       0.51      0.67      0.58        52
          B1       0.51      0.34      0.41        64
          B2       0.50      0.57      0.53        56
          C1       0.36      0.25      0.30        16
          C2       0.00      0.00      0.00         2

    accuracy                           0.48       205
   macro avg       0.37      0.36      0.36       205
weighted avg       0.48      0.48      0.47       205


Fold 3
[[ 7  6  1  1  0  0]
 [ 6 28 18  0  0  0]
 [ 1 13 42  7  1  0]
 [ 0  8 18 25  4  0]
 [ 0  0  3  9  4  0]
 [ 0  0  1  1  0  0]]
              precision    recall  f1-score   support

          A1       0.50      0.47      0.48        15
          A2       0.51      0.54      0.52        52
          B1       0.51      0.66      0.57        64
          B2       0.58      0.45      0.51        55
          C1       0.44      0.25      0.32        16
          C2       0.00      0.00      0.00         2

    accuracy                           0.52       204
   macro avg       0.42      0.39      0.40       204
weighted avg       0.52      0.52      0.51       204


Fold 4
[[ 6  8  1  0  0  0]
 [ 4 29 14  4  0  0]
 [ 3 15 33 11  1  0]
 [ 0  1 14 37  3  0]
 [ 0  0  2 10  4  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.46      0.40      0.43        15
          A2       0.55      0.57      0.56        51
          B1       0.52      0.52      0.52        63
          B2       0.58      0.67      0.62        55
          C1       0.50      0.25      0.33        16
          C2       0.00      0.00      0.00         2

    accuracy                           0.54       202
   macro avg       0.43      0.40      0.41       202
weighted avg       0.53      0.54      0.53       202


K-fold scores
[0.4480472322957627, 0.43232154012553287, 0.4683307758285481, 0.5108283609931242, 0.5304259931029521]
SKF f1 score mean 0.477990780469184

SAME LANG EVAL DONE FOR THIS LANG
************for dimension:  CoherenceCohesion  ***************
Extracted all features: 
Printing class statistics
Counter({'B1': 356, 'B2': 269, 'A2': 227, 'C1': 85, 'A1': 84, 'C2': 5})
With Word ngrams: 
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3414
3414
Fold 0
[[ 6 10  1  0  0  0]
 [ 5 19 22  0  0  0]
 [ 1 11 49 11  0  0]
 [ 0  0  6 47  1  0]
 [ 0  0  0 15  2  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.50      0.35      0.41        17
          A2       0.47      0.41      0.44        46
          B1       0.63      0.68      0.65        72
          B2       0.64      0.87      0.73        54
          C1       0.67      0.12      0.20        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.59       207
   macro avg       0.48      0.41      0.41       207
weighted avg       0.59      0.59      0.57       207


Fold 1
[[ 3 12  2  0  0  0]
 [ 7 19 20  0  0  0]
 [ 0  9 49 13  0  0]
 [ 0  0  6 48  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.30      0.18      0.22        17
          A2       0.47      0.41      0.44        46
          B1       0.64      0.69      0.66        71
          B2       0.61      0.89      0.72        54
          C1       0.00      0.00      0.00        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.58       206
   macro avg       0.34      0.36      0.34       206
weighted avg       0.51      0.58      0.53       206


Fold 2
[[ 5 11  1  0  0  0]
 [ 4 25 16  0  0  0]
 [ 0 11 50 10  0  0]
 [ 0  2 10 41  1  0]
 [ 0  0  1 15  1  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.56      0.29      0.38        17
          A2       0.51      0.56      0.53        45
          B1       0.64      0.70      0.67        71
          B2       0.61      0.76      0.68        54
          C1       0.50      0.06      0.11        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.60       205
   macro avg       0.47      0.40      0.40       205
weighted avg       0.58      0.60      0.57       205


Fold 3
[[ 5 10  2  0  0  0]
 [ 2 23 20  0  0  0]
 [ 1 10 49 11  0  0]
 [ 0  1 12 41  0  0]
 [ 0  0  1 16  0  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.62      0.29      0.40        17
          A2       0.52      0.51      0.52        45
          B1       0.58      0.69      0.63        71
          B2       0.59      0.76      0.67        54
          C1       0.00      0.00      0.00        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.58       205
   macro avg       0.39      0.38      0.37       205
weighted avg       0.53      0.58      0.54       205


Fold 4
[[ 9  7  0  0  0  0]
 [ 6 29 10  0  0  0]
 [ 1 15 47  8  0  0]
 [ 0  0 11 42  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.56      0.56      0.56        16
          A2       0.57      0.64      0.60        45
          B1       0.69      0.66      0.68        71
          B2       0.62      0.79      0.69        53
          C1       0.00      0.00      0.00        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.63       203
   macro avg       0.41      0.44      0.42       203
weighted avg       0.57      0.63      0.60       203


K-fold scores
[0.5674218075070993, 0.5344384286197218, 0.56834221204469, 0.5412134124240843, 0.5960358468914321]
SKF f1 score mean 0.5614903414974055

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3414
3414
Fold 0
[[ 6  6  5  0  0  0]
 [ 5 23 16  2  0  0]
 [ 4 19 41  8  0  0]
 [ 0  1 10 29 14  0]
 [ 0  0  2  8  7  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.40      0.35      0.38        17
          A2       0.47      0.50      0.48        46
          B1       0.55      0.57      0.56        72
          B2       0.60      0.54      0.57        54
          C1       0.33      0.41      0.37        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.51       207
   macro avg       0.39      0.40      0.39       207
weighted avg       0.51      0.51      0.51       207


Fold 1
[[ 6  9  2  0  0  0]
 [ 6 20 18  2  0  0]
 [ 2 12 45 10  2  0]
 [ 0  2 10 36  6  0]
 [ 0  0  1 12  4  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.43      0.35      0.39        17
          A2       0.47      0.43      0.45        46
          B1       0.59      0.63      0.61        71
          B2       0.60      0.67      0.63        54
          C1       0.31      0.24      0.27        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.54       206
   macro avg       0.40      0.39      0.39       206
weighted avg       0.53      0.54      0.53       206


Fold 2
[[ 6  8  3  0  0  0]
 [ 6 23 16  0  0  0]
 [ 1 22 36 12  0  0]
 [ 0  5 11 29  9  0]
 [ 0  1  2 10  4  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.46      0.35      0.40        17
          A2       0.39      0.51      0.44        45
          B1       0.53      0.51      0.52        71
          B2       0.57      0.54      0.55        54
          C1       0.29      0.24      0.26        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.48       205
   macro avg       0.37      0.36      0.36       205
weighted avg       0.48      0.48      0.48       205


Fold 3
[[ 5 11  1  0  0  0]
 [ 5 16 22  2  0  0]
 [ 3 13 44 11  0  0]
 [ 0  3 11 32  8  0]
 [ 0  0  2 11  4  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.38      0.29      0.33        17
          A2       0.37      0.36      0.36        45
          B1       0.55      0.62      0.58        71
          B2       0.57      0.59      0.58        54
          C1       0.31      0.24      0.27        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.49       205
   macro avg       0.36      0.35      0.35       205
weighted avg       0.48      0.49      0.48       205


Fold 4
[[ 8  8  0  0  0  0]
 [ 9 27  8  1  0  0]
 [ 5 18 39  7  2  0]
 [ 1  2 18 29  3  0]
 [ 0  0  2  9  6  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.35      0.50      0.41        16
          A2       0.49      0.60      0.54        45
          B1       0.58      0.55      0.57        71
          B2       0.62      0.55      0.58        53
          C1       0.55      0.35      0.43        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.54       203
   macro avg       0.43      0.42      0.42       203
weighted avg       0.55      0.54      0.54       203


K-fold scores
[0.5123482155680574, 0.5308873141310254, 0.4765682574335479, 0.484679520124521, 0.5370455745439683]
SKF f1 score mean 0.508305776360224

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3414
3414
Fold 0
[[ 6  6  5  0  0  0]
 [ 5 22 18  1  0  0]
 [ 2 13 47 10  0  0]
 [ 0  1 10 33 10  0]
 [ 0  0  2  8  7  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.46      0.35      0.40        17
          A2       0.52      0.48      0.50        46
          B1       0.57      0.65      0.61        72
          B2       0.62      0.61      0.62        54
          C1       0.41      0.41      0.41        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.56       207
   macro avg       0.43      0.42      0.42       207
weighted avg       0.55      0.56      0.55       207


Fold 1
[[ 5 10  2  0  0  0]
 [ 9 21 14  2  0  0]
 [ 1 13 44 11  2  0]
 [ 0  1  8 37  8  0]
 [ 0  0  0 12  5  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.33      0.29      0.31        17
          A2       0.47      0.46      0.46        46
          B1       0.65      0.62      0.63        71
          B2       0.60      0.69      0.64        54
          C1       0.31      0.29      0.30        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.54       206
   macro avg       0.39      0.39      0.39       206
weighted avg       0.54      0.54      0.54       206


Fold 2
[[ 6  7  4  0  0  0]
 [ 7 22 15  1  0  0]
 [ 1 19 42  8  1  0]
 [ 0  4  9 33  8  0]
 [ 0  1  1 11  4  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.43      0.35      0.39        17
          A2       0.42      0.49      0.45        45
          B1       0.59      0.59      0.59        71
          B2       0.62      0.61      0.62        54
          C1       0.29      0.24      0.26        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.52       205
   macro avg       0.39      0.38      0.38       205
weighted avg       0.52      0.52      0.52       205


Fold 3
[[ 5 10  2  0  0  0]
 [ 1 20 23  1  0  0]
 [ 0 14 45 11  1  0]
 [ 0  3  9 34  8  0]
 [ 0  0  2 12  3  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.83      0.29      0.43        17
          A2       0.43      0.44      0.43        45
          B1       0.56      0.63      0.59        71
          B2       0.59      0.63      0.61        54
          C1       0.23      0.18      0.20        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.52       205
   macro avg       0.44      0.36      0.38       205
weighted avg       0.53      0.52      0.51       205


Fold 4
[[ 8  8  0  0  0  0]
 [ 6 28 11  0  0  0]
 [ 2 14 49  5  1  0]
 [ 1  2 12 34  4  0]
 [ 0  0  0  9  8  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.47      0.50      0.48        16
          A2       0.54      0.62      0.58        45
          B1       0.68      0.69      0.69        71
          B2       0.69      0.64      0.67        53
          C1       0.62      0.47      0.53        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.63       203
   macro avg       0.50      0.49      0.49       203
weighted avg       0.63      0.63      0.62       203


K-fold scores
[0.5509974065845663, 0.5392849540394328, 0.5194157794297083, 0.5130815107758793, 0.6246024623557506]
SKF f1 score mean 0.5494764226370674

SAME LANG EVAL DONE FOR THIS LANG
With POS ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
6373
6373
Fold 0
[[ 7  9  1  0  0  0]
 [ 3 27 16  0  0  0]
 [ 4 11 45 12  0  0]
 [ 0  1  8 44  1  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.50      0.41      0.45        17
          A2       0.56      0.59      0.57        46
          B1       0.64      0.62      0.63        72
          B2       0.59      0.81      0.69        54
          C1       0.00      0.00      0.00        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.59       207
   macro avg       0.38      0.41      0.39       207
weighted avg       0.54      0.59      0.56       207


Fold 1
[[ 3 14  0  0  0  0]
 [ 3 27 16  0  0  0]
 [ 1  9 48 13  0  0]
 [ 0  0  8 46  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.43      0.18      0.25        17
          A2       0.54      0.59      0.56        46
          B1       0.67      0.68      0.67        71
          B2       0.60      0.85      0.70        54
          C1       0.00      0.00      0.00        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.60       206
   macro avg       0.37      0.38      0.36       206
weighted avg       0.54      0.60      0.56       206


Fold 2
[[ 4 13  0  0  0  0]
 [ 2 33 10  0  0  0]
 [ 0 15 49  7  0  0]
 [ 0  2  4 47  1  0]
 [ 0  0  1 16  0  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.67      0.24      0.35        17
          A2       0.52      0.73      0.61        45
          B1       0.77      0.69      0.73        71
          B2       0.66      0.87      0.75        54
          C1       0.00      0.00      0.00        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.65       205
   macro avg       0.44      0.42      0.41       205
weighted avg       0.61      0.65      0.61       205


Fold 3
[[ 4 12  1  0  0  0]
 [ 3 28 13  1  0  0]
 [ 2  8 48 13  0  0]
 [ 0  1 11 42  0  0]
 [ 0  0  1 16  0  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.44      0.24      0.31        17
          A2       0.57      0.62      0.60        45
          B1       0.65      0.68      0.66        71
          B2       0.58      0.78      0.66        54
          C1       0.00      0.00      0.00        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.60       205
   macro avg       0.37      0.39      0.37       205
weighted avg       0.54      0.60      0.56       205


Fold 4
[[11  5  0  0  0  0]
 [ 5 31  9  0  0  0]
 [ 1 18 42 10  0  0]
 [ 0  1 11 41  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.65      0.69      0.67        16
          A2       0.56      0.69      0.62        45
          B1       0.68      0.59      0.63        71
          B2       0.59      0.77      0.67        53
          C1       0.00      0.00      0.00        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.62       203
   macro avg       0.41      0.46      0.43       203
weighted avg       0.57      0.62      0.59       203


K-fold scores
[0.5645495366504045, 0.5617135911964711, 0.6124965083853737, 0.5598181066042054, 0.5863631662536782]
SKF f1 score mean 0.5769881818180266

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
6373
6373
Fold 0
[[ 9  6  2  0  0  0]
 [10 16 17  3  0  0]
 [ 4 15 44  9  0  0]
 [ 2  2 17 22 11  0]
 [ 0  0  5  5  7  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.36      0.53      0.43        17
          A2       0.41      0.35      0.38        46
          B1       0.52      0.61      0.56        72
          B2       0.56      0.41      0.47        54
          C1       0.37      0.41      0.39        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.47       207
   macro avg       0.37      0.38      0.37       207
weighted avg       0.48      0.47      0.47       207


Fold 1
[[ 3  9  4  1  0  0]
 [ 8 14 22  2  0  0]
 [ 1 12 45 13  0  0]
 [ 0  1 15 34  4  0]
 [ 0  0  0 15  2  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.25      0.18      0.21        17
          A2       0.39      0.30      0.34        46
          B1       0.52      0.63      0.57        71
          B2       0.52      0.63      0.57        54
          C1       0.29      0.12      0.17        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.48       206
   macro avg       0.33      0.31      0.31       206
weighted avg       0.45      0.48      0.45       206


Fold 2
[[ 3 12  2  0  0  0]
 [ 7 25 12  1  0  0]
 [ 3 22 38  7  1  0]
 [ 0  4 18 26  6  0]
 [ 0  0  4  8  5  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.23      0.18      0.20        17
          A2       0.40      0.56      0.46        45
          B1       0.51      0.54      0.52        71
          B2       0.60      0.48      0.54        54
          C1       0.42      0.29      0.34        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.47       205
   macro avg       0.36      0.34      0.34       205
weighted avg       0.48      0.47      0.47       205


Fold 3
[[ 3 12  2  0  0  0]
 [ 7 18 20  0  0  0]
 [ 1 19 37 12  2  0]
 [ 0  2 22 23  7  0]
 [ 0  0  2 12  3  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.27      0.18      0.21        17
          A2       0.35      0.40      0.38        45
          B1       0.45      0.52      0.48        71
          B2       0.48      0.43      0.45        54
          C1       0.25      0.18      0.21        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.41       205
   macro avg       0.30      0.28      0.29       205
weighted avg       0.40      0.41      0.40       205


Fold 4
[[10  4  2  0  0  0]
 [ 9 24 11  1  0  0]
 [ 6 22 28 15  0  0]
 [ 0  1 15 33  4  0]
 [ 0  1  2  9  5  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.40      0.62      0.49        16
          A2       0.46      0.53      0.49        45
          B1       0.48      0.39      0.43        71
          B2       0.57      0.62      0.59        53
          C1       0.50      0.29      0.37        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.49       203
   macro avg       0.40      0.41      0.40       203
weighted avg       0.49      0.49      0.49       203


K-fold scores
[0.4691765577319435, 0.4544450076972448, 0.46954950736851847, 0.40246303812449363, 0.4862287414879817]
SKF f1 score mean 0.4563725704820364

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
6373
6373
Fold 0
[[ 8  7  2  0  0  0]
 [ 8 16 20  2  0  0]
 [ 3 16 43 10  0  0]
 [ 1  1 16 25 11  0]
 [ 0  0  3  7  7  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.40      0.47      0.43        17
          A2       0.40      0.35      0.37        46
          B1       0.51      0.60      0.55        72
          B2       0.57      0.46      0.51        54
          C1       0.37      0.41      0.39        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.48       207
   macro avg       0.37      0.38      0.38       207
weighted avg       0.48      0.48      0.47       207


Fold 1
[[ 3 11  3  0  0  0]
 [ 6 17 21  2  0  0]
 [ 1 11 46 13  0  0]
 [ 0  0 12 38  4  0]
 [ 0  0  0 12  5  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.30      0.18      0.22        17
          A2       0.44      0.37      0.40        46
          B1       0.56      0.65      0.60        71
          B2       0.58      0.70      0.64        54
          C1       0.50      0.29      0.37        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.53       206
   macro avg       0.40      0.37      0.37       206
weighted avg       0.51      0.53      0.51       206


Fold 2
[[ 2 13  2  0  0  0]
 [ 6 24 14  1  0  0]
 [ 3 19 39  9  1  0]
 [ 0  3 18 26  7  0]
 [ 0  0  3  8  6  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.18      0.12      0.14        17
          A2       0.41      0.53      0.46        45
          B1       0.51      0.55      0.53        71
          B2       0.58      0.48      0.53        54
          C1       0.43      0.35      0.39        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.47       205
   macro avg       0.35      0.34      0.34       205
weighted avg       0.47      0.47      0.47       205


Fold 3
[[ 3 11  3  0  0  0]
 [ 7 17 21  0  0  0]
 [ 0 17 41 12  1  0]
 [ 0  1 20 26  7  0]
 [ 0  0  1 13  3  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.30      0.18      0.22        17
          A2       0.37      0.38      0.37        45
          B1       0.48      0.58      0.52        71
          B2       0.50      0.48      0.49        54
          C1       0.27      0.18      0.21        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.44       205
   macro avg       0.32      0.30      0.30       205
weighted avg       0.43      0.44      0.43       205


Fold 4
[[10  5  1  0  0  0]
 [10 25 10  0  0  0]
 [ 4 26 27 14  0  0]
 [ 0  2 13 34  4  0]
 [ 0  1  1  9  6  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.42      0.62      0.50        16
          A2       0.42      0.56      0.48        45
          B1       0.52      0.38      0.44        71
          B2       0.60      0.64      0.62        53
          C1       0.55      0.35      0.43        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.50       203
   macro avg       0.42      0.43      0.41       203
weighted avg       0.51      0.50      0.50       203


K-fold scores
[0.474985843636236, 0.5128848518911341, 0.46739294200236087, 0.4283277576552599, 0.4968211711393257]
SKF f1 score mean 0.4760825132648633

SAME LANG EVAL DONE FOR THIS LANG
Dep ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5787
5787
Fold 0
[[ 7  9  1  0  0  0]
 [ 5 20 21  0  0  0]
 [ 4 13 48  7  0  0]
 [ 0  1 10 43  0  0]
 [ 0  0  1 16  0  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.44      0.41      0.42        17
          A2       0.47      0.43      0.45        46
          B1       0.59      0.67      0.63        72
          B2       0.64      0.80      0.71        54
          C1       0.00      0.00      0.00        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.57       207
   macro avg       0.36      0.38      0.37       207
weighted avg       0.51      0.57      0.54       207


Fold 1
[[ 6  9  2  0  0  0]
 [ 3 28 15  0  0  0]
 [ 0 12 47 12  0  0]
 [ 0  0  7 47  0  0]
 [ 0  0  0 17  0  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.67      0.35      0.46        17
          A2       0.57      0.61      0.59        46
          B1       0.66      0.66      0.66        71
          B2       0.61      0.87      0.72        54
          C1       0.00      0.00      0.00        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.62       206
   macro avg       0.42      0.42      0.41       206
weighted avg       0.57      0.62      0.59       206


Fold 2
[[ 5 11  1  0  0  0]
 [ 1 36  8  0  0  0]
 [ 0 16 44 11  0  0]
 [ 0  2  9 43  0  0]
 [ 0  0  1 16  0  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.83      0.29      0.43        17
          A2       0.55      0.80      0.65        45
          B1       0.70      0.62      0.66        71
          B2       0.61      0.80      0.69        54
          C1       0.00      0.00      0.00        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.62       205
   macro avg       0.45      0.42      0.41       205
weighted avg       0.59      0.62      0.59       205


Fold 3
[[ 3 14  0  0  0  0]
 [ 2 28 15  0  0  0]
 [ 2  8 49 12  0  0]
 [ 0  2  9 43  0  0]
 [ 0  0  1 16  0  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.43      0.18      0.25        17
          A2       0.54      0.62      0.58        45
          B1       0.66      0.69      0.68        71
          B2       0.60      0.80      0.68        54
          C1       0.00      0.00      0.00        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.60       205
   macro avg       0.37      0.38      0.36       205
weighted avg       0.54      0.60      0.56       205


Fold 4
[[ 8  8  0  0  0  0]
 [ 2 31 12  0  0  0]
 [ 1 20 42  8  0  0]
 [ 0  2 11 40  0  0]
 [ 0  0  1 16  0  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.73      0.50      0.59        16
          A2       0.51      0.69      0.58        45
          B1       0.64      0.59      0.61        71
          B2       0.62      0.75      0.68        53
          C1       0.00      0.00      0.00        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.60       203
   macro avg       0.41      0.42      0.41       203
weighted avg       0.55      0.60      0.57       203


K-fold scores
[0.5383715671158623, 0.5859710433150072, 0.5884132462146991, 0.5613303960828954, 0.5678191443528157]
SKF f1 score mean 0.5683810794162559

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5787
5787
Fold 0
[[ 7  7  1  2  0  0]
 [12 16 17  1  0  0]
 [ 4 17 34 17  0  0]
 [ 1  3 14 25 11  0]
 [ 0  1  0 12  4  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.29      0.41      0.34        17
          A2       0.36      0.35      0.36        46
          B1       0.52      0.47      0.49        72
          B2       0.44      0.46      0.45        54
          C1       0.25      0.24      0.24        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.42       207
   macro avg       0.31      0.32      0.31       207
weighted avg       0.42      0.42      0.42       207


Fold 1
[[ 5  6  6  0  0  0]
 [ 6 20 20  0  0  0]
 [ 0 18 38 14  1  0]
 [ 0  0 14 30 10  0]
 [ 0  0  2 11  4  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.45      0.29      0.36        17
          A2       0.45      0.43      0.44        46
          B1       0.47      0.54      0.50        71
          B2       0.54      0.56      0.55        54
          C1       0.27      0.24      0.25        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.47       206
   macro avg       0.36      0.34      0.35       206
weighted avg       0.47      0.47      0.47       206


Fold 2
[[ 6  8  3  0  0  0]
 [ 8 24 10  3  0  0]
 [ 5 19 34 10  3  0]
 [ 0  6 19 24  5  0]
 [ 0  2  1 10  4  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.32      0.35      0.33        17
          A2       0.41      0.53      0.46        45
          B1       0.51      0.48      0.49        71
          B2       0.51      0.44      0.48        54
          C1       0.31      0.24      0.27        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.45       205
   macro avg       0.34      0.34      0.34       205
weighted avg       0.45      0.45      0.45       205


Fold 3
[[ 4 10  2  1  0  0]
 [ 4 20 18  3  0  0]
 [ 0 21 40 10  0  0]
 [ 1  2 21 25  5  0]
 [ 0  0  3 12  2  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.44      0.24      0.31        17
          A2       0.38      0.44      0.41        45
          B1       0.48      0.56      0.52        71
          B2       0.48      0.46      0.47        54
          C1       0.29      0.12      0.17        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.44       205
   macro avg       0.34      0.30      0.31       205
weighted avg       0.43      0.44      0.43       205


Fold 4
[[ 7  7  2  0  0  0]
 [10 15 19  1  0  0]
 [ 6 18 42  4  1  0]
 [ 0  3 19 27  4  0]
 [ 0  0  1 10  6  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.30      0.44      0.36        16
          A2       0.35      0.33      0.34        45
          B1       0.51      0.59      0.55        71
          B2       0.63      0.51      0.56        53
          C1       0.55      0.35      0.43        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.48       203
   macro avg       0.39      0.37      0.37       203
weighted avg       0.49      0.48      0.48       203


K-fold scores
[0.4158658498523561, 0.46580348450566417, 0.4469175822060549, 0.43194297027516193, 0.4773890928447579]
SKF f1 score mean 0.447583795936799

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5787
5787
Fold 0
[[ 7  6  2  2  0  0]
 [12 16 17  1  0  0]
 [ 3 16 36 17  0  0]
 [ 1  2 13 25 13  0]
 [ 0  1  0 12  4  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.30      0.41      0.35        17
          A2       0.39      0.35      0.37        46
          B1       0.53      0.50      0.51        72
          B2       0.44      0.46      0.45        54
          C1       0.22      0.24      0.23        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.43       207
   macro avg       0.31      0.33      0.32       207
weighted avg       0.43      0.43      0.43       207


Fold 1
[[ 4  8  5  0  0  0]
 [ 6 18 21  1  0  0]
 [ 0 15 41 12  3  0]
 [ 0  0 12 33  9  0]
 [ 0  0  0 13  4  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.40      0.24      0.30        17
          A2       0.44      0.39      0.41        46
          B1       0.52      0.58      0.55        71
          B2       0.56      0.61      0.58        54
          C1       0.24      0.24      0.24        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.49       206
   macro avg       0.36      0.34      0.35       206
weighted avg       0.48      0.49      0.48       206


Fold 2
[[ 5  9  3  0  0  0]
 [ 5 26 13  1  0  0]
 [ 6 18 35 10  2  0]
 [ 0  3 18 27  6  0]
 [ 0  1  2 10  4  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.31      0.29      0.30        17
          A2       0.46      0.58      0.51        45
          B1       0.49      0.49      0.49        71
          B2       0.56      0.50      0.53        54
          C1       0.31      0.24      0.27        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.47       205
   macro avg       0.36      0.35      0.35       205
weighted avg       0.47      0.47      0.47       205


Fold 3
[[ 3 12  2  0  0  0]
 [ 6 19 18  2  0  0]
 [ 0 20 40 11  0  0]
 [ 1  1 21 27  4  0]
 [ 0  0  3 11  3  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.30      0.18      0.22        17
          A2       0.37      0.42      0.39        45
          B1       0.48      0.56      0.52        71
          B2       0.52      0.50      0.51        54
          C1       0.43      0.18      0.25        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.45       205
   macro avg       0.35      0.31      0.31       205
weighted avg       0.44      0.45      0.44       205


Fold 4
[[ 7  7  2  0  0  0]
 [ 9 15 19  2  0  0]
 [ 5 16 43  7  0  0]
 [ 0  2 18 28  5  0]
 [ 0  0  2  9  6  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.33      0.44      0.38        16
          A2       0.38      0.33      0.35        45
          B1       0.51      0.61      0.55        71
          B2       0.60      0.53      0.56        53
          C1       0.55      0.35      0.43        17
          C2       0.00      0.00      0.00         1

    accuracy                           0.49       203
   macro avg       0.39      0.38      0.38       203
weighted avg       0.49      0.49      0.48       203


K-fold scores
[0.4256432380120536, 0.4777896899895176, 0.469337854875875, 0.43810360492559, 0.4842151215174463]
SKF f1 score mean 0.4590179018640964

SAME LANG EVAL DONE FOR THIS LANG
************for dimension:  Sociolinguisticappropriateness  ***************
Extracted all features: 
Printing class statistics
Counter({'B2': 346, 'B1': 299, 'A2': 258, 'C1': 69, 'A1': 54})
With Word ngrams: 
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3414
3414
Fold 0
[[ 4  7  0  0  0]
 [ 0 43  9  0  0]
 [ 1 16 26 17  0]
 [ 0  6  7 57  0]
 [ 0  0  0 14  0]]
              precision    recall  f1-score   support

          A1       0.80      0.36      0.50        11
          A2       0.60      0.83      0.69        52
          B1       0.62      0.43      0.51        60
          B2       0.65      0.81      0.72        70
          C1       0.00      0.00      0.00        14

    accuracy                           0.63       207
   macro avg       0.53      0.49      0.48       207
weighted avg       0.59      0.63      0.59       207


Fold 1
[[ 6  5  0  0  0]
 [ 2 37 12  1  0]
 [ 0 21 25 14  0]
 [ 0  2 10 57  0]
 [ 0  0  0 14  0]]
              precision    recall  f1-score   support

          A1       0.75      0.55      0.63        11
          A2       0.57      0.71      0.63        52
          B1       0.53      0.42      0.47        60
          B2       0.66      0.83      0.74        69
          C1       0.00      0.00      0.00        14

    accuracy                           0.61       206
   macro avg       0.50      0.50      0.49       206
weighted avg       0.56      0.61      0.58       206


Fold 2
[[ 0 11  0  0  0]
 [ 0 43  9  0  0]
 [ 1 17 27 15  0]
 [ 0  5 14 50  0]
 [ 0  0  0 14  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00        11
          A2       0.57      0.83      0.67        52
          B1       0.54      0.45      0.49        60
          B2       0.63      0.72      0.68        69
          C1       0.00      0.00      0.00        14

    accuracy                           0.58       206
   macro avg       0.35      0.40      0.37       206
weighted avg       0.51      0.58      0.54       206


Fold 3
[[ 3  8  0  0  0]
 [ 1 34 16  0  0]
 [ 1 24 24 11  0]
 [ 0  2 11 56  0]
 [ 0  0  0 14  0]]
              precision    recall  f1-score   support

          A1       0.60      0.27      0.37        11
          A2       0.50      0.67      0.57        51
          B1       0.47      0.40      0.43        60
          B2       0.69      0.81      0.75        69
          C1       0.00      0.00      0.00        14

    accuracy                           0.57       205
   macro avg       0.45      0.43      0.43       205
weighted avg       0.53      0.57      0.54       205


Fold 4
[[ 2  5  3  0  0]
 [ 1 34 16  0  0]
 [ 0 18 23 18  0]
 [ 0  3 12 54  0]
 [ 0  0  0 13  0]]
              precision    recall  f1-score   support

          A1       0.67      0.20      0.31        10
          A2       0.57      0.67      0.61        51
          B1       0.43      0.39      0.41        59
          B2       0.64      0.78      0.70        69
          C1       0.00      0.00      0.00        13

    accuracy                           0.56       202
   macro avg       0.46      0.41      0.41       202
weighted avg       0.52      0.56      0.53       202


K-fold scores
[0.5925559446235457, 0.5758350853606107, 0.538901296486248, 0.5401648931161126, 0.5283538407169355]
SKF f1 score mean 0.5551622120606904

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3414
3414
Fold 0
[[ 4  7  0  0  0]
 [ 8 30  9  5  0]
 [ 3 19 26 11  1]
 [ 0  5 14 46  5]
 [ 0  0  1  6  7]]
              precision    recall  f1-score   support

          A1       0.27      0.36      0.31        11
          A2       0.49      0.58      0.53        52
          B1       0.52      0.43      0.47        60
          B2       0.68      0.66      0.67        70
          C1       0.54      0.50      0.52        14

    accuracy                           0.55       207
   macro avg       0.50      0.51      0.50       207
weighted avg       0.55      0.55      0.55       207


Fold 1
[[ 4  6  1  0  0]
 [ 4 29 16  3  0]
 [ 0 22 24 14  0]
 [ 0  4 14 43  8]
 [ 0  0  2 10  2]]
              precision    recall  f1-score   support

          A1       0.50      0.36      0.42        11
          A2       0.48      0.56      0.51        52
          B1       0.42      0.40      0.41        60
          B2       0.61      0.62      0.62        69
          C1       0.20      0.14      0.17        14

    accuracy                           0.50       206
   macro avg       0.44      0.42      0.43       206
weighted avg       0.49      0.50      0.49       206


Fold 2
[[ 2  7  2  0  0]
 [ 5 30 17  0  0]
 [ 0 19 27 13  1]
 [ 0  7 15 43  4]
 [ 0  0  2  8  4]]
              precision    recall  f1-score   support

          A1       0.29      0.18      0.22        11
          A2       0.48      0.58      0.52        52
          B1       0.43      0.45      0.44        60
          B2       0.67      0.62      0.65        69
          C1       0.44      0.29      0.35        14

    accuracy                           0.51       206
   macro avg       0.46      0.42      0.44       206
weighted avg       0.52      0.51      0.51       206


Fold 3
[[ 2  7  2  0  0]
 [ 3 30 16  2  0]
 [ 2 14 28 14  2]
 [ 0  1 19 42  7]
 [ 0  0  0 10  4]]
              precision    recall  f1-score   support

          A1       0.29      0.18      0.22        11
          A2       0.58      0.59      0.58        51
          B1       0.43      0.47      0.45        60
          B2       0.62      0.61      0.61        69
          C1       0.31      0.29      0.30        14

    accuracy                           0.52       205
   macro avg       0.44      0.43      0.43       205
weighted avg       0.51      0.52      0.51       205


Fold 4
[[ 2  5  3  0  0]
 [ 7 27 15  2  0]
 [ 4 21 16 17  1]
 [ 1  4 10 43 11]
 [ 1  0  0  8  4]]
              precision    recall  f1-score   support

          A1       0.13      0.20      0.16        10
          A2       0.47      0.53      0.50        51
          B1       0.36      0.27      0.31        59
          B2       0.61      0.62      0.62        69
          C1       0.25      0.31      0.28        13

    accuracy                           0.46       202
   macro avg       0.37      0.39      0.37       202
weighted avg       0.46      0.46      0.45       202


K-fold scores
[0.5472695514164455, 0.49010296050886604, 0.5116623748177769, 0.5145751209743218, 0.45399480923735025]
SKF f1 score mean 0.5035209633909521

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3414
3414
Fold 0
[[ 3  8  0  0  0]
 [ 7 31  9  5  0]
 [ 3 17 30  9  1]
 [ 0  4 12 46  8]
 [ 0  0  0  8  6]]
              precision    recall  f1-score   support

          A1       0.23      0.27      0.25        11
          A2       0.52      0.60      0.55        52
          B1       0.59      0.50      0.54        60
          B2       0.68      0.66      0.67        70
          C1       0.40      0.43      0.41        14

    accuracy                           0.56       207
   macro avg       0.48      0.49      0.48       207
weighted avg       0.57      0.56      0.56       207


Fold 1
[[ 4  6  1  0  0]
 [ 4 28 16  4  0]
 [ 1 23 22 14  0]
 [ 0  4  8 50  7]
 [ 0  0  3  9  2]]
              precision    recall  f1-score   support

          A1       0.44      0.36      0.40        11
          A2       0.46      0.54      0.50        52
          B1       0.44      0.37      0.40        60
          B2       0.65      0.72      0.68        69
          C1       0.22      0.14      0.17        14

    accuracy                           0.51       206
   macro avg       0.44      0.43      0.43       206
weighted avg       0.50      0.51      0.50       206


Fold 2
[[ 2  6  3  0  0]
 [ 4 32 16  0  0]
 [ 0 16 29 14  1]
 [ 0  6 13 44  6]
 [ 0  0  1  7  6]]
              precision    recall  f1-score   support

          A1       0.33      0.18      0.24        11
          A2       0.53      0.62      0.57        52
          B1       0.47      0.48      0.48        60
          B2       0.68      0.64      0.66        69
          C1       0.46      0.43      0.44        14

    accuracy                           0.55       206
   macro avg       0.49      0.47      0.48       206
weighted avg       0.55      0.55      0.55       206


Fold 3
[[ 2  9  0  0  0]
 [ 4 30 16  1  0]
 [ 2 16 31 11  0]
 [ 0  2 14 46  7]
 [ 0  0  0 10  4]]
              precision    recall  f1-score   support

          A1       0.25      0.18      0.21        11
          A2       0.53      0.59      0.56        51
          B1       0.51      0.52      0.51        60
          B2       0.68      0.67      0.67        69
          C1       0.36      0.29      0.32        14

    accuracy                           0.55       205
   macro avg       0.46      0.45      0.45       205
weighted avg       0.55      0.55      0.55       205


Fold 4
[[ 2  5  3  0  0]
 [ 3 28 17  3  0]
 [ 2 19 22 16  0]
 [ 0  3 12 46  8]
 [ 0  0  0  9  4]]
              precision    recall  f1-score   support

          A1       0.29      0.20      0.24        10
          A2       0.51      0.55      0.53        51
          B1       0.41      0.37      0.39        59
          B2       0.62      0.67      0.64        69
          C1       0.33      0.31      0.32        13

    accuracy                           0.50       202
   macro avg       0.43      0.42      0.42       202
weighted avg       0.50      0.50      0.50       202


K-fold scores
[0.5624537045076775, 0.5041988742025397, 0.5454503215067035, 0.5473594677232575, 0.49911582733627574]
SKF f1 score mean 0.5317156390552908

SAME LANG EVAL DONE FOR THIS LANG
With POS ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
6373
6373
Fold 0
[[ 3  8  0  0  0]
 [ 1 40 11  0  0]
 [ 0 13 30 17  0]
 [ 0  3 11 56  0]
 [ 0  0  1 13  0]]
              precision    recall  f1-score   support

          A1       0.75      0.27      0.40        11
          A2       0.62      0.77      0.69        52
          B1       0.57      0.50      0.53        60
          B2       0.65      0.80      0.72        70
          C1       0.00      0.00      0.00        14

    accuracy                           0.62       207
   macro avg       0.52      0.47      0.47       207
weighted avg       0.58      0.62      0.59       207


Fold 1
[[ 5  6  0  0  0]
 [ 1 39  9  3  0]
 [ 0 18 26 16  0]
 [ 0  1 10 58  0]
 [ 0  0  0 14  0]]
              precision    recall  f1-score   support

          A1       0.83      0.45      0.59        11
          A2       0.61      0.75      0.67        52
          B1       0.58      0.43      0.50        60
          B2       0.64      0.84      0.72        69
          C1       0.00      0.00      0.00        14

    accuracy                           0.62       206
   macro avg       0.53      0.50      0.50       206
weighted avg       0.58      0.62      0.59       206


Fold 2
[[ 1 10  0  0  0]
 [ 0 44  8  0  0]
 [ 1 19 23 17  0]
 [ 0  7 10 52  0]
 [ 0  0  0 14  0]]
              precision    recall  f1-score   support

          A1       0.50      0.09      0.15        11
          A2       0.55      0.85      0.67        52
          B1       0.56      0.38      0.46        60
          B2       0.63      0.75      0.68        69
          C1       0.00      0.00      0.00        14

    accuracy                           0.58       206
   macro avg       0.45      0.41      0.39       206
weighted avg       0.54      0.58      0.54       206


Fold 3
[[ 2  9  0  0  0]
 [ 1 35 15  0  0]
 [ 0 19 30 11  0]
 [ 0  1  7 61  0]
 [ 0  0  0 14  0]]
              precision    recall  f1-score   support

          A1       0.67      0.18      0.29        11
          A2       0.55      0.69      0.61        51
          B1       0.58      0.50      0.54        60
          B2       0.71      0.88      0.79        69
          C1       0.00      0.00      0.00        14

    accuracy                           0.62       205
   macro avg       0.50      0.45      0.44       205
weighted avg       0.58      0.62      0.59       205


Fold 4
[[ 1  8  1  0  0]
 [ 0 39 12  0  0]
 [ 0 15 27 17  0]
 [ 0  2 11 56  0]
 [ 0  0  0 13  0]]
              precision    recall  f1-score   support

          A1       1.00      0.10      0.18        10
          A2       0.61      0.76      0.68        51
          B1       0.53      0.46      0.49        59
          B2       0.65      0.81      0.72        69
          C1       0.00      0.00      0.00        13

    accuracy                           0.61       202
   macro avg       0.56      0.43      0.41       202
weighted avg       0.58      0.61      0.57       202


K-fold scores
[0.5911926874472151, 0.5882300543250444, 0.5383312298448114, 0.5884822925167741, 0.5704514210186797]
SKF f1 score mean 0.5753375370305049

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
6373
6373
Fold 0
[[ 2  7  2  0  0]
 [ 2 30 16  4  0]
 [ 0 21 26 13  0]
 [ 0  6 23 35  6]
 [ 0  0  0  9  5]]
              precision    recall  f1-score   support

          A1       0.50      0.18      0.27        11
          A2       0.47      0.58      0.52        52
          B1       0.39      0.43      0.41        60
          B2       0.57      0.50      0.53        70
          C1       0.45      0.36      0.40        14

    accuracy                           0.47       207
   macro avg       0.48      0.41      0.43       207
weighted avg       0.48      0.47      0.47       207


Fold 1
[[ 5  5  1  0  0]
 [ 5 23 19  5  0]
 [ 2 23 20 15  0]
 [ 0  6 22 37  4]
 [ 0  1  1 12  0]]
              precision    recall  f1-score   support

          A1       0.42      0.45      0.43        11
          A2       0.40      0.44      0.42        52
          B1       0.32      0.33      0.33        60
          B2       0.54      0.54      0.54        69
          C1       0.00      0.00      0.00        14

    accuracy                           0.41       206
   macro avg       0.33      0.35      0.34       206
weighted avg       0.39      0.41      0.40       206


Fold 2
[[ 2  8  1  0  0]
 [ 6 25 17  4  0]
 [ 0 26 16 18  0]
 [ 2  9 15 35  8]
 [ 0  0  2 10  2]]
              precision    recall  f1-score   support

          A1       0.20      0.18      0.19        11
          A2       0.37      0.48      0.42        52
          B1       0.31      0.27      0.29        60
          B2       0.52      0.51      0.51        69
          C1       0.20      0.14      0.17        14

    accuracy                           0.39       206
   macro avg       0.32      0.32      0.32       206
weighted avg       0.38      0.39      0.38       206


Fold 3
[[ 2  6  3  0  0]
 [ 3 22 19  7  0]
 [ 0 20 26 13  1]
 [ 0  4 21 40  4]
 [ 0  0  2 12  0]]
              precision    recall  f1-score   support

          A1       0.40      0.18      0.25        11
          A2       0.42      0.43      0.43        51
          B1       0.37      0.43      0.40        60
          B2       0.56      0.58      0.57        69
          C1       0.00      0.00      0.00        14

    accuracy                           0.44       205
   macro avg       0.35      0.33      0.33       205
weighted avg       0.42      0.44      0.43       205


Fold 4
[[ 1  6  2  1  0]
 [ 3 25 18  5  0]
 [ 1 22 18 16  2]
 [ 0 11 13 39  6]
 [ 0  0  2  7  4]]
              precision    recall  f1-score   support

          A1       0.20      0.10      0.13        10
          A2       0.39      0.49      0.43        51
          B1       0.34      0.31      0.32        59
          B2       0.57      0.57      0.57        69
          C1       0.33      0.31      0.32        13

    accuracy                           0.43       202
   macro avg       0.37      0.35      0.36       202
weighted avg       0.43      0.43      0.43       202


K-fold scores
[0.4705381369275425, 0.4031080503061065, 0.38304486055771036, 0.4268396967333309, 0.42532774071259405]
SKF f1 score mean 0.4217716970474569

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
6373
6373
Fold 0
[[ 2  7  2  0  0]
 [ 2 29 16  4  1]
 [ 0 18 31 11  0]
 [ 0  3 21 40  6]
 [ 0  0  0  9  5]]
              precision    recall  f1-score   support

          A1       0.50      0.18      0.27        11
          A2       0.51      0.56      0.53        52
          B1       0.44      0.52      0.48        60
          B2       0.62      0.57      0.60        70
          C1       0.42      0.36      0.38        14

    accuracy                           0.52       207
   macro avg       0.50      0.44      0.45       207
weighted avg       0.52      0.52      0.51       207


Fold 1
[[ 4  7  0  0  0]
 [ 4 23 20  4  1]
 [ 1 22 22 15  0]
 [ 0  6 20 40  3]
 [ 0  1  1 12  0]]
              precision    recall  f1-score   support

          A1       0.44      0.36      0.40        11
          A2       0.39      0.44      0.41        52
          B1       0.35      0.37      0.36        60
          B2       0.56      0.58      0.57        69
          C1       0.00      0.00      0.00        14

    accuracy                           0.43       206
   macro avg       0.35      0.35      0.35       206
weighted avg       0.41      0.43      0.42       206


Fold 2
[[ 2  8  1  0  0]
 [ 6 28 15  3  0]
 [ 0 24 19 17  0]
 [ 2  9 13 37  8]
 [ 0  0  2 11  1]]
              precision    recall  f1-score   support

          A1       0.20      0.18      0.19        11
          A2       0.41      0.54      0.46        52
          B1       0.38      0.32      0.35        60
          B2       0.54      0.54      0.54        69
          C1       0.11      0.07      0.09        14

    accuracy                           0.42       206
   macro avg       0.33      0.33      0.33       206
weighted avg       0.41      0.42      0.41       206


Fold 3
[[ 2  6  3  0  0]
 [ 3 22 20  6  0]
 [ 0 20 26 13  1]
 [ 0  1 21 44  3]
 [ 0  0  2 12  0]]
              precision    recall  f1-score   support

          A1       0.40      0.18      0.25        11
          A2       0.45      0.43      0.44        51
          B1       0.36      0.43      0.39        60
          B2       0.59      0.64      0.61        69
          C1       0.00      0.00      0.00        14

    accuracy                           0.46       205
   macro avg       0.36      0.34      0.34       205
weighted avg       0.44      0.46      0.44       205


Fold 4
[[ 1  6  2  1  0]
 [ 3 26 19  3  0]
 [ 1 18 21 16  3]
 [ 0  5 15 43  6]
 [ 0  0  2  7  4]]
              precision    recall  f1-score   support

          A1       0.20      0.10      0.13        10
          A2       0.47      0.51      0.49        51
          B1       0.36      0.36      0.36        59
          B2       0.61      0.62      0.62        69
          C1       0.31      0.31      0.31        13

    accuracy                           0.47       202
   macro avg       0.39      0.38      0.38       202
weighted avg       0.46      0.47      0.47       202


K-fold scores
[0.513981173333792, 0.4215608524867346, 0.41444704282543066, 0.4438684405025868, 0.46555865712809913]
SKF f1 score mean 0.4518832332553286

SAME LANG EVAL DONE FOR THIS LANG
Dep ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5787
5787
Fold 0
[[ 4  7  0  0  0]
 [ 2 44  5  1  0]
 [ 0 13 27 20  0]
 [ 0  4  7 59  0]
 [ 0  0  1 13  0]]
              precision    recall  f1-score   support

          A1       0.67      0.36      0.47        11
          A2       0.65      0.85      0.73        52
          B1       0.68      0.45      0.54        60
          B2       0.63      0.84      0.72        70
          C1       0.00      0.00      0.00        14

    accuracy                           0.65       207
   macro avg       0.52      0.50      0.49       207
weighted avg       0.61      0.65      0.61       207


Fold 1
[[ 6  5  0  0  0]
 [ 2 39  8  3  0]
 [ 1 19 24 16  0]
 [ 0  3 10 56  0]
 [ 0  0  0 14  0]]
              precision    recall  f1-score   support

          A1       0.67      0.55      0.60        11
          A2       0.59      0.75      0.66        52
          B1       0.57      0.40      0.47        60
          B2       0.63      0.81      0.71        69
          C1       0.00      0.00      0.00        14

    accuracy                           0.61       206
   macro avg       0.49      0.50      0.49       206
weighted avg       0.56      0.61      0.57       206


Fold 2
[[ 1 10  0  0  0]
 [ 0 45  7  0  0]
 [ 1 21 22 16  0]
 [ 0  7  8 54  0]
 [ 0  0  0 14  0]]
              precision    recall  f1-score   support

          A1       0.50      0.09      0.15        11
          A2       0.54      0.87      0.67        52
          B1       0.59      0.37      0.45        60
          B2       0.64      0.78      0.71        69
          C1       0.00      0.00      0.00        14

    accuracy                           0.59       206
   macro avg       0.46      0.42      0.40       206
weighted avg       0.55      0.59      0.55       206


Fold 3
[[ 3  8  0  0  0]
 [ 1 36 14  0  0]
 [ 0 17 32 11  0]
 [ 0  2  8 59  0]
 [ 0  0  0 14  0]]
              precision    recall  f1-score   support

          A1       0.75      0.27      0.40        11
          A2       0.57      0.71      0.63        51
          B1       0.59      0.53      0.56        60
          B2       0.70      0.86      0.77        69
          C1       0.00      0.00      0.00        14

    accuracy                           0.63       205
   macro avg       0.52      0.47      0.47       205
weighted avg       0.59      0.63      0.60       205


Fold 4
[[ 1  7  2  0  0]
 [ 1 39  9  2  0]
 [ 0 19 24 16  0]
 [ 0  2 11 56  0]
 [ 0  0  0 13  0]]
              precision    recall  f1-score   support

          A1       0.50      0.10      0.17        10
          A2       0.58      0.76      0.66        51
          B1       0.52      0.41      0.46        59
          B2       0.64      0.81      0.72        69
          C1       0.00      0.00      0.00        13

    accuracy                           0.59       202
   macro avg       0.45      0.42      0.40       202
weighted avg       0.54      0.59      0.55       202


K-fold scores
[0.6105538673784324, 0.5733959605759348, 0.5450551046469726, 0.602489868861537, 0.5539030751650318]
SKF f1 score mean 0.5770795753255816

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5787
5787
Fold 0
[[ 3  7  1  0  0]
 [ 5 29 15  3  0]
 [ 1 15 28 15  1]
 [ 0  7 19 39  5]
 [ 0  0  3  7  4]]
              precision    recall  f1-score   support

          A1       0.33      0.27      0.30        11
          A2       0.50      0.56      0.53        52
          B1       0.42      0.47      0.44        60
          B2       0.61      0.56      0.58        70
          C1       0.40      0.29      0.33        14

    accuracy                           0.50       207
   macro avg       0.45      0.43      0.44       207
weighted avg       0.50      0.50      0.50       207


Fold 1
[[ 3  8  0  0  0]
 [ 8 22 20  2  0]
 [ 0 24 22 14  0]
 [ 0  6 20 37  6]
 [ 0  0  3  9  2]]
              precision    recall  f1-score   support

          A1       0.27      0.27      0.27        11
          A2       0.37      0.42      0.39        52
          B1       0.34      0.37      0.35        60
          B2       0.60      0.54      0.56        69
          C1       0.25      0.14      0.18        14

    accuracy                           0.42       206
   macro avg       0.36      0.35      0.35       206
weighted avg       0.42      0.42      0.42       206


Fold 2
[[ 2  8  1  0  0]
 [ 5 28 17  2  0]
 [ 1 20 23 16  0]
 [ 0 12 16 35  6]
 [ 0  0  2 10  2]]
              precision    recall  f1-score   support

          A1       0.25      0.18      0.21        11
          A2       0.41      0.54      0.47        52
          B1       0.39      0.38      0.39        60
          B2       0.56      0.51      0.53        69
          C1       0.25      0.14      0.18        14

    accuracy                           0.44       206
   macro avg       0.37      0.35      0.36       206
weighted avg       0.43      0.44      0.43       206


Fold 3
[[ 3  7  1  0  0]
 [ 1 27 19  4  0]
 [ 1 17 25 16  1]
 [ 0  3 23 42  1]
 [ 0  1  1 11  1]]
              precision    recall  f1-score   support

          A1       0.60      0.27      0.37        11
          A2       0.49      0.53      0.51        51
          B1       0.36      0.42      0.39        60
          B2       0.58      0.61      0.59        69
          C1       0.33      0.07      0.12        14

    accuracy                           0.48       205
   macro avg       0.47      0.38      0.40       205
weighted avg       0.48      0.48      0.47       205


Fold 4
[[ 2  5  2  1  0]
 [ 5 20 17  8  1]
 [ 1 13 27 17  1]
 [ 0  4 22 38  5]
 [ 0  0  2  9  2]]
              precision    recall  f1-score   support

          A1       0.25      0.20      0.22        10
          A2       0.48      0.39      0.43        51
          B1       0.39      0.46      0.42        59
          B2       0.52      0.55      0.54        69
          C1       0.22      0.15      0.18        13

    accuracy                           0.44       202
   macro avg       0.37      0.35      0.36       202
weighted avg       0.44      0.44      0.44       202


K-fold scores
[0.4966076512474955, 0.41782099616828733, 0.4316121217846548, 0.467443445175318, 0.43637917990207775]
SKF f1 score mean 0.4499726788555666

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5787
5787
Fold 0
[[ 3  7  1  0  0]
 [ 5 29 16  2  0]
 [ 1 15 29 14  1]
 [ 0  6 18 39  7]
 [ 0  0  3  7  4]]
              precision    recall  f1-score   support

          A1       0.33      0.27      0.30        11
          A2       0.51      0.56      0.53        52
          B1       0.43      0.48      0.46        60
          B2       0.63      0.56      0.59        70
          C1       0.33      0.29      0.31        14

    accuracy                           0.50       207
   macro avg       0.45      0.43      0.44       207
weighted avg       0.51      0.50      0.50       207


Fold 1
[[ 2  9  0  0  0]
 [ 7 27 15  3  0]
 [ 0 21 25 14  0]
 [ 0  6 18 39  6]
 [ 0  0  3  9  2]]
              precision    recall  f1-score   support

          A1       0.22      0.18      0.20        11
          A2       0.43      0.52      0.47        52
          B1       0.41      0.42      0.41        60
          B2       0.60      0.57      0.58        69
          C1       0.25      0.14      0.18        14

    accuracy                           0.46       206
   macro avg       0.38      0.37      0.37       206
weighted avg       0.46      0.46      0.46       206


Fold 2
[[ 1  8  2  0  0]
 [ 2 32 15  3  0]
 [ 1 20 22 17  0]
 [ 0  9 16 38  6]
 [ 0  0  3 10  1]]
              precision    recall  f1-score   support

          A1       0.25      0.09      0.13        11
          A2       0.46      0.62      0.53        52
          B1       0.38      0.37      0.37        60
          B2       0.56      0.55      0.55        69
          C1       0.14      0.07      0.10        14

    accuracy                           0.46       206
   macro avg       0.36      0.34      0.34       206
weighted avg       0.44      0.46      0.44       206


Fold 3
[[ 3  7  1  0  0]
 [ 1 30 18  2  0]
 [ 0 19 27 13  1]
 [ 0  2 23 43  1]
 [ 0  0  2 11  1]]
              precision    recall  f1-score   support

          A1       0.75      0.27      0.40        11
          A2       0.52      0.59      0.55        51
          B1       0.38      0.45      0.41        60
          B2       0.62      0.62      0.62        69
          C1       0.33      0.07      0.12        14

    accuracy                           0.51       205
   macro avg       0.52      0.40      0.42       205
weighted avg       0.51      0.51      0.50       205


Fold 4
[[ 1  6  2  1  0]
 [ 7 22 17  5  0]
 [ 1 15 23 19  1]
 [ 0  3 21 42  3]
 [ 0  0  2  8  3]]
              precision    recall  f1-score   support

          A1       0.11      0.10      0.11        10
          A2       0.48      0.43      0.45        51
          B1       0.35      0.39      0.37        59
          B2       0.56      0.61      0.58        69
          C1       0.43      0.23      0.30        13

    accuracy                           0.45       202
   macro avg       0.39      0.35      0.36       202
weighted avg       0.45      0.45      0.45       202


K-fold scores
[0.5026213924885047, 0.45689521060209737, 0.4415261449034509, 0.4968452572996252, 0.4466522226321665]
SKF f1 score mean 0.46890804558516885

SAME LANG EVAL DONE FOR THIS LANG
Doing monolingual classification for  IT
************for dimension:  OverallCEFRrating  ***************
Extracted all features: 
Printing class statistics
Counter({'B1': 392, 'A2': 380, 'A1': 28})
With Word ngrams: 
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2688
2688
Fold 0
[[ 0  6  0]
 [ 0 70  6]
 [ 0 13 66]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         6
          A2       0.79      0.92      0.85        76
          B1       0.92      0.84      0.87        79

    accuracy                           0.84       161
   macro avg       0.57      0.59      0.57       161
weighted avg       0.82      0.84      0.83       161


Fold 1
[[ 1  5  0]
 [ 0 60 16]
 [ 0  7 72]]
              precision    recall  f1-score   support

          A1       1.00      0.17      0.29         6
          A2       0.83      0.79      0.81        76
          B1       0.82      0.91      0.86        79

    accuracy                           0.83       161
   macro avg       0.88      0.62      0.65       161
weighted avg       0.83      0.83      0.82       161


Fold 2
[[ 1  5  0]
 [ 0 68  8]
 [ 0 10 68]]
              precision    recall  f1-score   support

          A1       1.00      0.17      0.29         6
          A2       0.82      0.89      0.86        76
          B1       0.89      0.87      0.88        78

    accuracy                           0.86       160
   macro avg       0.90      0.64      0.67       160
weighted avg       0.86      0.86      0.85       160


Fold 3
[[ 1  4  0]
 [ 0 66 10]
 [ 0  5 73]]
              precision    recall  f1-score   support

          A1       1.00      0.20      0.33         5
          A2       0.88      0.87      0.87        76
          B1       0.88      0.94      0.91        78

    accuracy                           0.88       159
   macro avg       0.92      0.67      0.70       159
weighted avg       0.88      0.88      0.87       159


Fold 4
[[ 0  5  0]
 [ 1 67  8]
 [ 0 12 66]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         5
          A2       0.80      0.88      0.84        76
          B1       0.89      0.85      0.87        78

    accuracy                           0.84       159
   macro avg       0.56      0.58      0.57       159
weighted avg       0.82      0.84      0.83       159


K-fold scores
[0.8294686405829509, 0.8164948311487532, 0.8475230744098667, 0.873186616440505, 0.826332340284674]
SKF f1 score mean 0.8386011005733499

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2688
2688
Fold 0
[[ 1  5  0]
 [ 3 64  9]
 [ 0 14 65]]
              precision    recall  f1-score   support

          A1       0.25      0.17      0.20         6
          A2       0.77      0.84      0.81        76
          B1       0.88      0.82      0.85        79

    accuracy                           0.81       161
   macro avg       0.63      0.61      0.62       161
weighted avg       0.80      0.81      0.80       161


Fold 1
[[ 2  4  0]
 [ 1 58 17]
 [ 0 11 68]]
              precision    recall  f1-score   support

          A1       0.67      0.33      0.44         6
          A2       0.79      0.76      0.78        76
          B1       0.80      0.86      0.83        79

    accuracy                           0.80       161
   macro avg       0.75      0.65      0.68       161
weighted avg       0.79      0.80      0.79       161


Fold 2
[[ 2  4  0]
 [ 0 66 10]
 [ 1 17 60]]
              precision    recall  f1-score   support

          A1       0.67      0.33      0.44         6
          A2       0.76      0.87      0.81        76
          B1       0.86      0.77      0.81        78

    accuracy                           0.80       160
   macro avg       0.76      0.66      0.69       160
weighted avg       0.80      0.80      0.80       160


Fold 3
[[ 2  2  1]
 [ 0 62 14]
 [ 0  7 71]]
              precision    recall  f1-score   support

          A1       1.00      0.40      0.57         5
          A2       0.87      0.82      0.84        76
          B1       0.83      0.91      0.87        78

    accuracy                           0.85       159
   macro avg       0.90      0.71      0.76       159
weighted avg       0.85      0.85      0.85       159


Fold 4
[[ 2  3  0]
 [ 5 60 11]
 [ 0 18 60]]
              precision    recall  f1-score   support

          A1       0.29      0.40      0.33         5
          A2       0.74      0.79      0.76        76
          B1       0.85      0.77      0.81        78

    accuracy                           0.77       159
   macro avg       0.62      0.65      0.63       159
weighted avg       0.78      0.77      0.77       159


K-fold scores
[0.8043886518238689, 0.790972962878989, 0.7965995136240535, 0.8459281242793175, 0.7709096263364633]
SKF f1 score mean 0.8017597757885385

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2688
2688
Fold 0
[[ 2  4  0]
 [ 1 67  8]
 [ 0 12 67]]
              precision    recall  f1-score   support

          A1       0.67      0.33      0.44         6
          A2       0.81      0.88      0.84        76
          B1       0.89      0.85      0.87        79

    accuracy                           0.84       161
   macro avg       0.79      0.69      0.72       161
weighted avg       0.84      0.84      0.84       161


Fold 1
[[ 1  5  0]
 [ 0 60 16]
 [ 0  6 73]]
              precision    recall  f1-score   support

          A1       1.00      0.17      0.29         6
          A2       0.85      0.79      0.82        76
          B1       0.82      0.92      0.87        79

    accuracy                           0.83       161
   macro avg       0.89      0.63      0.66       161
weighted avg       0.84      0.83      0.82       161


Fold 2
[[ 2  4  0]
 [ 0 68  8]
 [ 1 13 64]]
              precision    recall  f1-score   support

          A1       0.67      0.33      0.44         6
          A2       0.80      0.89      0.84        76
          B1       0.89      0.82      0.85        78

    accuracy                           0.84       160
   macro avg       0.79      0.68      0.71       160
weighted avg       0.84      0.84      0.83       160


Fold 3
[[ 3  2  0]
 [ 0 61 15]
 [ 0  5 73]]
              precision    recall  f1-score   support

          A1       1.00      0.60      0.75         5
          A2       0.90      0.80      0.85        76
          B1       0.83      0.94      0.88        78

    accuracy                           0.86       159
   macro avg       0.91      0.78      0.83       159
weighted avg       0.87      0.86      0.86       159


Fold 4
[[ 2  3  0]
 [ 4 60 12]
 [ 0 14 64]]
              precision    recall  f1-score   support

          A1       0.33      0.40      0.36         5
          A2       0.78      0.79      0.78        76
          B1       0.84      0.82      0.83        78

    accuracy                           0.79       159
   macro avg       0.65      0.67      0.66       159
weighted avg       0.80      0.79      0.79       159


K-fold scores
[0.8413493221884174, 0.822421515189927, 0.8339089026915115, 0.860008166839265, 0.7940704011736199]
SKF f1 score mean 0.8303516616165482

SAME LANG EVAL DONE FOR THIS LANG
With POS ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
4765
4765
Fold 0
[[ 0  6  0]
 [ 0 68  8]
 [ 0 15 64]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         6
          A2       0.76      0.89      0.82        76
          B1       0.89      0.81      0.85        79

    accuracy                           0.82       161
   macro avg       0.55      0.57      0.56       161
weighted avg       0.80      0.82      0.81       161


Fold 1
[[ 0  6  0]
 [ 1 65 10]
 [ 0  8 71]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         6
          A2       0.82      0.86      0.84        76
          B1       0.88      0.90      0.89        79

    accuracy                           0.84       161
   macro avg       0.57      0.58      0.58       161
weighted avg       0.82      0.84      0.83       161


Fold 2
[[ 1  5  0]
 [ 0 69  7]
 [ 0 15 63]]
              precision    recall  f1-score   support

          A1       1.00      0.17      0.29         6
          A2       0.78      0.91      0.84        76
          B1       0.90      0.81      0.85        78

    accuracy                           0.83       160
   macro avg       0.89      0.63      0.66       160
weighted avg       0.84      0.83      0.82       160


Fold 3
[[ 1  4  0]
 [ 0 66 10]
 [ 0  7 71]]
              precision    recall  f1-score   support

          A1       1.00      0.20      0.33         5
          A2       0.86      0.87      0.86        76
          B1       0.88      0.91      0.89        78

    accuracy                           0.87       159
   macro avg       0.91      0.66      0.70       159
weighted avg       0.87      0.87      0.86       159


Fold 4
[[ 0  5  0]
 [ 0 68  8]
 [ 0 17 61]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         5
          A2       0.76      0.89      0.82        76
          B1       0.88      0.78      0.83        78

    accuracy                           0.81       159
   macro avg       0.55      0.56      0.55       159
weighted avg       0.79      0.81      0.80       159


K-fold scores
[0.8050267804946758, 0.8313940092165898, 0.8230207967707969, 0.860979065887658, 0.798740591940345]
SKF f1 score mean 0.8238322488620131

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
4765
4765
Fold 0
[[ 2  3  1]
 [ 2 63 11]
 [ 0 14 65]]
              precision    recall  f1-score   support

          A1       0.50      0.33      0.40         6
          A2       0.79      0.83      0.81        76
          B1       0.84      0.82      0.83        79

    accuracy                           0.81       161
   macro avg       0.71      0.66      0.68       161
weighted avg       0.80      0.81      0.81       161


Fold 1
[[ 0  6  0]
 [ 0 58 18]
 [ 0 14 65]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         6
          A2       0.74      0.76      0.75        76
          B1       0.78      0.82      0.80        79

    accuracy                           0.76       161
   macro avg       0.51      0.53      0.52       161
weighted avg       0.74      0.76      0.75       161


Fold 2
[[ 1  5  0]
 [ 0 62 14]
 [ 0 14 64]]
              precision    recall  f1-score   support

          A1       1.00      0.17      0.29         6
          A2       0.77      0.82      0.79        76
          B1       0.82      0.82      0.82        78

    accuracy                           0.79       160
   macro avg       0.86      0.60      0.63       160
weighted avg       0.80      0.79      0.79       160


Fold 3
[[ 1  4  0]
 [ 1 59 16]
 [ 0 20 58]]
              precision    recall  f1-score   support

          A1       0.50      0.20      0.29         5
          A2       0.71      0.78      0.74        76
          B1       0.78      0.74      0.76        78

    accuracy                           0.74       159
   macro avg       0.66      0.57      0.60       159
weighted avg       0.74      0.74      0.74       159


Fold 4
[[ 0  5  0]
 [ 1 58 17]
 [ 0 25 53]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         5
          A2       0.66      0.76      0.71        76
          B1       0.76      0.68      0.72        78

    accuracy                           0.70       159
   macro avg       0.47      0.48      0.47       159
weighted avg       0.69      0.70      0.69       159


K-fold scores
[0.8050804268195572, 0.7493280433239026, 0.7858735213830755, 0.7380968738447519, 0.6894400152568584]
SKF f1 score mean 0.7535637761256291

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
4765
4765
Fold 0
[[ 1  4  1]
 [ 2 66  8]
 [ 0 12 67]]
              precision    recall  f1-score   support

          A1       0.33      0.17      0.22         6
          A2       0.80      0.87      0.84        76
          B1       0.88      0.85      0.86        79

    accuracy                           0.83       161
   macro avg       0.67      0.63      0.64       161
weighted avg       0.82      0.83      0.83       161


Fold 1
[[ 0  6  0]
 [ 1 59 16]
 [ 0 11 68]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         6
          A2       0.78      0.78      0.78        76
          B1       0.81      0.86      0.83        79

    accuracy                           0.79       161
   macro avg       0.53      0.55      0.54       161
weighted avg       0.76      0.79      0.78       161


Fold 2
[[ 1  5  0]
 [ 0 62 14]
 [ 0 10 68]]
              precision    recall  f1-score   support

          A1       1.00      0.17      0.29         6
          A2       0.81      0.82      0.81        76
          B1       0.83      0.87      0.85        78

    accuracy                           0.82       160
   macro avg       0.88      0.62      0.65       160
weighted avg       0.82      0.82      0.81       160


Fold 3
[[ 1  4  0]
 [ 0 58 18]
 [ 0 18 60]]
              precision    recall  f1-score   support

          A1       1.00      0.20      0.33         5
          A2       0.72      0.76      0.74        76
          B1       0.77      0.77      0.77        78

    accuracy                           0.75       159
   macro avg       0.83      0.58      0.62       159
weighted avg       0.76      0.75      0.74       159


Fold 4
[[ 0  5  0]
 [ 1 62 13]
 [ 0 21 57]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         5
          A2       0.70      0.82      0.76        76
          B1       0.81      0.73      0.77        78

    accuracy                           0.75       159
   macro avg       0.51      0.52      0.51       159
weighted avg       0.74      0.75      0.74       159


K-fold scores
[0.8268557665401098, 0.7758640399344587, 0.8100566059757238, 0.7432672149653282, 0.7392735579574052]
SKF f1 score mean 0.7790634370746051

SAME LANG EVAL DONE FOR THIS LANG
Dep ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5018
5018
Fold 0
[[ 0  6  0]
 [ 0 70  6]
 [ 0 17 62]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         6
          A2       0.75      0.92      0.83        76
          B1       0.91      0.78      0.84        79

    accuracy                           0.82       161
   macro avg       0.55      0.57      0.56       161
weighted avg       0.80      0.82      0.80       161


Fold 1
[[ 0  6  0]
 [ 0 63 13]
 [ 0 11 68]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         6
          A2       0.79      0.83      0.81        76
          B1       0.84      0.86      0.85        79

    accuracy                           0.81       161
   macro avg       0.54      0.56      0.55       161
weighted avg       0.78      0.81      0.80       161


Fold 2
[[ 1  5  0]
 [ 0 70  6]
 [ 0 16 62]]
              precision    recall  f1-score   support

          A1       1.00      0.17      0.29         6
          A2       0.77      0.92      0.84        76
          B1       0.91      0.79      0.85        78

    accuracy                           0.83       160
   macro avg       0.89      0.63      0.66       160
weighted avg       0.85      0.83      0.82       160


Fold 3
[[ 1  4  0]
 [ 0 69  7]
 [ 0  8 70]]
              precision    recall  f1-score   support

          A1       1.00      0.20      0.33         5
          A2       0.85      0.91      0.88        76
          B1       0.91      0.90      0.90        78

    accuracy                           0.88       159
   macro avg       0.92      0.67      0.71       159
weighted avg       0.88      0.88      0.87       159


Fold 4
[[ 0  5  0]
 [ 0 65 11]
 [ 0 20 58]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         5
          A2       0.72      0.86      0.78        76
          B1       0.84      0.74      0.79        78

    accuracy                           0.77       159
   macro avg       0.52      0.53      0.52       159
weighted avg       0.76      0.77      0.76       159


K-fold scores
[0.8049567432544702, 0.7983516483516484, 0.822958974419068, 0.8737158952238561, 0.761440834574339]
SKF f1 score mean 0.8122848191646763

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5018
5018
Fold 0
[[ 1  5  0]
 [ 0 65 11]
 [ 0 18 61]]
              precision    recall  f1-score   support

          A1       1.00      0.17      0.29         6
          A2       0.74      0.86      0.79        76
          B1       0.85      0.77      0.81        79

    accuracy                           0.79       161
   macro avg       0.86      0.60      0.63       161
weighted avg       0.80      0.79      0.78       161


Fold 1
[[ 0  6  0]
 [ 2 50 24]
 [ 0 14 65]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         6
          A2       0.71      0.66      0.68        76
          B1       0.73      0.82      0.77        79

    accuracy                           0.71       161
   macro avg       0.48      0.49      0.49       161
weighted avg       0.70      0.71      0.70       161


Fold 2
[[ 1  5  0]
 [ 0 62 14]
 [ 0 17 61]]
              precision    recall  f1-score   support

          A1       1.00      0.17      0.29         6
          A2       0.74      0.82      0.78        76
          B1       0.81      0.78      0.80        78

    accuracy                           0.78       160
   macro avg       0.85      0.59      0.62       160
weighted avg       0.78      0.78      0.77       160


Fold 3
[[ 0  4  1]
 [ 2 58 16]
 [ 0 13 65]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         5
          A2       0.77      0.76      0.77        76
          B1       0.79      0.83      0.81        78

    accuracy                           0.77       159
   macro avg       0.52      0.53      0.53       159
weighted avg       0.76      0.77      0.77       159


Fold 4
[[ 0  4  1]
 [ 1 64 11]
 [ 0 24 54]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         5
          A2       0.70      0.84      0.76        76
          B1       0.82      0.69      0.75        78

    accuracy                           0.74       159
   macro avg       0.50      0.51      0.50       159
weighted avg       0.73      0.74      0.73       159


K-fold scores
[0.7812795200176802, 0.703017061500002, 0.7675647759103641, 0.765780540630597, 0.7321054207846662]
SKF f1 score mean 0.7499494637686619

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5018
5018
Fold 0
[[ 1  5  0]
 [ 0 65 11]
 [ 0 16 63]]
              precision    recall  f1-score   support

          A1       1.00      0.17      0.29         6
          A2       0.76      0.86      0.80        76
          B1       0.85      0.80      0.82        79

    accuracy                           0.80       161
   macro avg       0.87      0.61      0.64       161
weighted avg       0.81      0.80      0.79       161


Fold 1
[[ 0  6  0]
 [ 2 49 25]
 [ 0 11 68]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         6
          A2       0.74      0.64      0.69        76
          B1       0.73      0.86      0.79        79

    accuracy                           0.73       161
   macro avg       0.49      0.50      0.49       161
weighted avg       0.71      0.73      0.71       161


Fold 2
[[ 1  5  0]
 [ 0 61 15]
 [ 0 13 65]]
              precision    recall  f1-score   support

          A1       1.00      0.17      0.29         6
          A2       0.77      0.80      0.79        76
          B1       0.81      0.83      0.82        78

    accuracy                           0.79       160
   macro avg       0.86      0.60      0.63       160
weighted avg       0.80      0.79      0.79       160


Fold 3
[[ 1  3  1]
 [ 2 58 16]
 [ 0 13 65]]
              precision    recall  f1-score   support

          A1       0.33      0.20      0.25         5
          A2       0.78      0.76      0.77        76
          B1       0.79      0.83      0.81        78

    accuracy                           0.78       159
   macro avg       0.64      0.60      0.61       159
weighted avg       0.77      0.78      0.78       159


Fold 4
[[ 0  5  0]
 [ 0 64 12]
 [ 0 20 58]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         5
          A2       0.72      0.84      0.78        76
          B1       0.83      0.74      0.78        78

    accuracy                           0.77       159
   macro avg       0.52      0.53      0.52       159
weighted avg       0.75      0.77      0.76       159


K-fold scores
[0.7935451153086033, 0.7137628602759124, 0.78569284839293, 0.7760901467505241, 0.7553000685076157]
SKF f1 score mean 0.764878207847117

SAME LANG EVAL DONE FOR THIS LANG
************for dimension:  Grammaticalaccuracy  ***************
Extracted all features: 
Printing class statistics
Counter({'B1': 376, 'A2': 242, 'B2': 111, 'A1': 71})
With Word ngrams: 
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2688
2688
Fold 0
[[ 5 10  0  0]
 [ 0 38 11  0]
 [ 1 14 57  4]
 [ 0  2 19  2]]
              precision    recall  f1-score   support

          A1       0.83      0.33      0.48        15
          A2       0.59      0.78      0.67        49
          B1       0.66      0.75      0.70        76
          B2       0.33      0.09      0.14        23

    accuracy                           0.63       163
   macro avg       0.60      0.49      0.50       163
weighted avg       0.61      0.63      0.59       163


Fold 1
[[ 3 10  1  0]
 [ 1 28 20  0]
 [ 0 19 55  1]
 [ 0  0 19  3]]
              precision    recall  f1-score   support

          A1       0.75      0.21      0.33        14
          A2       0.49      0.57      0.53        49
          B1       0.58      0.73      0.65        75
          B2       0.75      0.14      0.23        22

    accuracy                           0.56       160
   macro avg       0.64      0.41      0.43       160
weighted avg       0.59      0.56      0.53       160


Fold 2
[[ 2 10  2  0]
 [ 2 33 13  0]
 [ 0 11 64  0]
 [ 0  1 18  3]]
              precision    recall  f1-score   support

          A1       0.50      0.14      0.22        14
          A2       0.60      0.69      0.64        48
          B1       0.66      0.85      0.74        75
          B2       1.00      0.14      0.24        22

    accuracy                           0.64       159
   macro avg       0.69      0.46      0.46       159
weighted avg       0.67      0.64      0.60       159


Fold 3
[[ 1 12  1  0]
 [ 0 35 13  0]
 [ 0 16 59  0]
 [ 0  0 19  3]]
              precision    recall  f1-score   support

          A1       1.00      0.07      0.13        14
          A2       0.56      0.73      0.63        48
          B1       0.64      0.79      0.71        75
          B2       1.00      0.14      0.24        22

    accuracy                           0.62       159
   macro avg       0.80      0.43      0.43       159
weighted avg       0.70      0.62      0.57       159


Fold 4
[[ 3 11  0  0]
 [ 1 33 14  0]
 [ 0 19 56  0]
 [ 0  1 18  3]]
              precision    recall  f1-score   support

          A1       0.75      0.21      0.33        14
          A2       0.52      0.69      0.59        48
          B1       0.64      0.75      0.69        75
          B2       1.00      0.14      0.24        22

    accuracy                           0.60       159
   macro avg       0.73      0.45      0.46       159
weighted avg       0.66      0.60      0.56       159


K-fold scores
[0.5915607201317717, 0.5259987122570363, 0.5972474600810143, 0.5686223202074518, 0.5645668131048382]
SKF f1 score mean 0.5695992051564225

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2688
2688
Fold 0
[[ 8  4  3  0]
 [ 4 28 15  2]
 [ 2 20 42 12]
 [ 0  3 11  9]]
              precision    recall  f1-score   support

          A1       0.57      0.53      0.55        15
          A2       0.51      0.57      0.54        49
          B1       0.59      0.55      0.57        76
          B2       0.39      0.39      0.39        23

    accuracy                           0.53       163
   macro avg       0.52      0.51      0.51       163
weighted avg       0.54      0.53      0.53       163


Fold 1
[[ 4  9  1  0]
 [ 6 23 18  2]
 [ 3 22 36 14]
 [ 0  3 12  7]]
              precision    recall  f1-score   support

          A1       0.31      0.29      0.30        14
          A2       0.40      0.47      0.43        49
          B1       0.54      0.48      0.51        75
          B2       0.30      0.32      0.31        22

    accuracy                           0.44       160
   macro avg       0.39      0.39      0.39       160
weighted avg       0.44      0.44      0.44       160


Fold 2
[[ 4 10  0  0]
 [ 5 21 22  0]
 [ 2  9 51 13]
 [ 0  2 13  7]]
              precision    recall  f1-score   support

          A1       0.36      0.29      0.32        14
          A2       0.50      0.44      0.47        48
          B1       0.59      0.68      0.63        75
          B2       0.35      0.32      0.33        22

    accuracy                           0.52       159
   macro avg       0.45      0.43      0.44       159
weighted avg       0.51      0.52      0.51       159


Fold 3
[[ 5  7  2  0]
 [ 7 24 15  2]
 [ 0 21 42 12]
 [ 0  1 15  6]]
              precision    recall  f1-score   support

          A1       0.42      0.36      0.38        14
          A2       0.45      0.50      0.48        48
          B1       0.57      0.56      0.56        75
          B2       0.30      0.27      0.29        22

    accuracy                           0.48       159
   macro avg       0.43      0.42      0.43       159
weighted avg       0.48      0.48      0.48       159


Fold 4
[[ 5  7  2  0]
 [ 5 26 16  1]
 [ 1 23 41 10]
 [ 0  3 10  9]]
              precision    recall  f1-score   support

          A1       0.45      0.36      0.40        14
          A2       0.44      0.54      0.49        48
          B1       0.59      0.55      0.57        75
          B2       0.45      0.41      0.43        22

    accuracy                           0.51       159
   macro avg       0.48      0.46      0.47       159
weighted avg       0.52      0.51      0.51       159


K-fold scores
[0.5342886434487872, 0.43928070343795833, 0.5140179954946156, 0.4827930192017569, 0.5098365255700823]
SKF f1 score mean 0.49604337743064003

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2688
2688
Fold 0
[[ 8  5  2  0]
 [ 5 26 18  0]
 [ 3 15 44 14]
 [ 0  3 10 10]]
              precision    recall  f1-score   support

          A1       0.50      0.53      0.52        15
          A2       0.53      0.53      0.53        49
          B1       0.59      0.58      0.59        76
          B2       0.42      0.43      0.43        23

    accuracy                           0.54       163
   macro avg       0.51      0.52      0.51       163
weighted avg       0.54      0.54      0.54       163


Fold 1
[[ 2 11  1  0]
 [ 6 25 15  3]
 [ 2 20 41 12]
 [ 0  0 15  7]]
              precision    recall  f1-score   support

          A1       0.20      0.14      0.17        14
          A2       0.45      0.51      0.48        49
          B1       0.57      0.55      0.56        75
          B2       0.32      0.32      0.32        22

    accuracy                           0.47       160
   macro avg       0.38      0.38      0.38       160
weighted avg       0.46      0.47      0.47       160


Fold 2
[[ 5  9  0  0]
 [ 8 21 19  0]
 [ 0  9 52 14]
 [ 0  0 16  6]]
              precision    recall  f1-score   support

          A1       0.38      0.36      0.37        14
          A2       0.54      0.44      0.48        48
          B1       0.60      0.69      0.64        75
          B2       0.30      0.27      0.29        22

    accuracy                           0.53       159
   macro avg       0.46      0.44      0.45       159
weighted avg       0.52      0.53      0.52       159


Fold 3
[[ 5  8  1  0]
 [ 6 28 13  1]
 [ 0 15 47 13]
 [ 0  0 14  8]]
              precision    recall  f1-score   support

          A1       0.45      0.36      0.40        14
          A2       0.55      0.58      0.57        48
          B1       0.63      0.63      0.63        75
          B2       0.36      0.36      0.36        22

    accuracy                           0.55       159
   macro avg       0.50      0.48      0.49       159
weighted avg       0.55      0.55      0.55       159


Fold 4
[[ 6  7  1  0]
 [ 2 29 16  1]
 [ 2 22 42  9]
 [ 0  2 10 10]]
              precision    recall  f1-score   support

          A1       0.60      0.43      0.50        14
          A2       0.48      0.60      0.54        48
          B1       0.61      0.56      0.58        75
          B2       0.50      0.45      0.48        22

    accuracy                           0.55       159
   macro avg       0.55      0.51      0.52       159
weighted avg       0.56      0.55      0.55       159


K-fold scores
[0.5405879521048517, 0.46564625850340136, 0.5207010151707613, 0.5518963217076425, 0.5471947688928821]
SKF f1 score mean 0.5252052632759078

SAME LANG EVAL DONE FOR THIS LANG
With POS ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
4765
4765
Fold 0
[[ 3 12  0  0]
 [ 1 39  9  0]
 [ 0 18 57  1]
 [ 0  2 21  0]]
              precision    recall  f1-score   support

          A1       0.75      0.20      0.32        15
          A2       0.55      0.80      0.65        49
          B1       0.66      0.75      0.70        76
          B2       0.00      0.00      0.00        23

    accuracy                           0.61       163
   macro avg       0.49      0.44      0.42       163
weighted avg       0.54      0.61      0.55       163


Fold 1
[[ 2 10  2  0]
 [ 2 31 16  0]
 [ 0 18 57  0]
 [ 0  0 21  1]]
              precision    recall  f1-score   support

          A1       0.50      0.14      0.22        14
          A2       0.53      0.63      0.57        49
          B1       0.59      0.76      0.67        75
          B2       1.00      0.05      0.09        22

    accuracy                           0.57       160
   macro avg       0.65      0.40      0.39       160
weighted avg       0.62      0.57      0.52       160


Fold 2
[[ 2  8  4  0]
 [ 1 33 14  0]
 [ 0 10 65  0]
 [ 0  2 19  1]]
              precision    recall  f1-score   support

          A1       0.67      0.14      0.24        14
          A2       0.62      0.69      0.65        48
          B1       0.64      0.87      0.73        75
          B2       1.00      0.05      0.09        22

    accuracy                           0.64       159
   macro avg       0.73      0.44      0.43       159
weighted avg       0.69      0.64      0.58       159


Fold 3
[[ 2 12  0  0]
 [ 3 28 17  0]
 [ 0 12 62  1]
 [ 0  0 22  0]]
              precision    recall  f1-score   support

          A1       0.40      0.14      0.21        14
          A2       0.54      0.58      0.56        48
          B1       0.61      0.83      0.70        75
          B2       0.00      0.00      0.00        22

    accuracy                           0.58       159
   macro avg       0.39      0.39      0.37       159
weighted avg       0.49      0.58      0.52       159


Fold 4
[[ 4 10  0  0]
 [ 1 29 18  0]
 [ 0 22 53  0]
 [ 0  0 22  0]]
              precision    recall  f1-score   support

          A1       0.80      0.29      0.42        14
          A2       0.48      0.60      0.53        48
          B1       0.57      0.71      0.63        75
          B2       0.00      0.00      0.00        22

    accuracy                           0.54       159
   macro avg       0.46      0.40      0.40       159
weighted avg       0.48      0.54      0.50       159


K-fold scores
[0.5505534744686625, 0.5197111513687601, 0.5764669403880184, 0.5199262736601366, 0.49532987306417336]
SKF f1 score mean 0.5323975425899502

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
4765
4765
Fold 0
[[ 5  9  1  0]
 [ 7 29 10  3]
 [ 6 18 37 15]
 [ 0  5 12  6]]
              precision    recall  f1-score   support

          A1       0.28      0.33      0.30        15
          A2       0.48      0.59      0.53        49
          B1       0.62      0.49      0.54        76
          B2       0.25      0.26      0.26        23

    accuracy                           0.47       163
   macro avg       0.40      0.42      0.41       163
weighted avg       0.49      0.47      0.48       163


Fold 1
[[ 2  8  4  0]
 [ 7 22 19  1]
 [ 0 22 41 12]
 [ 0  1 17  4]]
              precision    recall  f1-score   support

          A1       0.22      0.14      0.17        14
          A2       0.42      0.45      0.43        49
          B1       0.51      0.55      0.53        75
          B2       0.24      0.18      0.21        22

    accuracy                           0.43       160
   macro avg       0.34      0.33      0.33       160
weighted avg       0.42      0.43      0.42       160


Fold 2
[[ 5  6  3  0]
 [ 8 25 14  1]
 [ 3 22 39 11]
 [ 0  5  9  8]]
              precision    recall  f1-score   support

          A1       0.31      0.36      0.33        14
          A2       0.43      0.52      0.47        48
          B1       0.60      0.52      0.56        75
          B2       0.40      0.36      0.38        22

    accuracy                           0.48       159
   macro avg       0.44      0.44      0.44       159
weighted avg       0.50      0.48      0.49       159


Fold 3
[[ 4  5  5  0]
 [ 4 24 19  1]
 [ 2 18 48  7]
 [ 0  1 13  8]]
              precision    recall  f1-score   support

          A1       0.40      0.29      0.33        14
          A2       0.50      0.50      0.50        48
          B1       0.56      0.64      0.60        75
          B2       0.50      0.36      0.42        22

    accuracy                           0.53       159
   macro avg       0.49      0.45      0.46       159
weighted avg       0.52      0.53      0.52       159


Fold 4
[[ 3 10  1  0]
 [ 4 25 18  1]
 [ 1 19 49  6]
 [ 0  5 13  4]]
              precision    recall  f1-score   support

          A1       0.38      0.21      0.27        14
          A2       0.42      0.52      0.47        48
          B1       0.60      0.65      0.63        75
          B2       0.36      0.18      0.24        22

    accuracy                           0.51       159
   macro avg       0.44      0.39      0.40       159
weighted avg       0.50      0.51      0.49       159


K-fold scores
[0.47611717658785696, 0.4219245934159617, 0.4872631620584628, 0.5215712236566258, 0.49494846735192344]
SKF f1 score mean 0.4803649246141661

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
4765
4765
Fold 0
[[ 7  7  1  0]
 [ 7 29 11  2]
 [ 5 15 39 17]
 [ 0  4 13  6]]
              precision    recall  f1-score   support

          A1       0.37      0.47      0.41        15
          A2       0.53      0.59      0.56        49
          B1       0.61      0.51      0.56        76
          B2       0.24      0.26      0.25        23

    accuracy                           0.50       163
   macro avg       0.44      0.46      0.44       163
weighted avg       0.51      0.50      0.50       163


Fold 1
[[ 2  8  4  0]
 [ 7 19 21  2]
 [ 0 23 42 10]
 [ 0  2 16  4]]
              precision    recall  f1-score   support

          A1       0.22      0.14      0.17        14
          A2       0.37      0.39      0.38        49
          B1       0.51      0.56      0.53        75
          B2       0.25      0.18      0.21        22

    accuracy                           0.42       160
   macro avg       0.34      0.32      0.32       160
weighted avg       0.40      0.42      0.41       160


Fold 2
[[ 4  6  4  0]
 [10 26 11  1]
 [ 1 21 40 13]
 [ 0  5  8  9]]
              precision    recall  f1-score   support

          A1       0.27      0.29      0.28        14
          A2       0.45      0.54      0.49        48
          B1       0.63      0.53      0.58        75
          B2       0.39      0.41      0.40        22

    accuracy                           0.50       159
   macro avg       0.44      0.44      0.44       159
weighted avg       0.51      0.50      0.50       159


Fold 3
[[ 4  6  4  0]
 [ 2 26 19  1]
 [ 2 16 54  3]
 [ 0  0 12 10]]
              precision    recall  f1-score   support

          A1       0.50      0.29      0.36        14
          A2       0.54      0.54      0.54        48
          B1       0.61      0.72      0.66        75
          B2       0.71      0.45      0.56        22

    accuracy                           0.59       159
   macro avg       0.59      0.50      0.53       159
weighted avg       0.59      0.59      0.58       159


Fold 4
[[ 3 10  1  0]
 [ 4 25 18  1]
 [ 0 21 48  6]
 [ 0  5 12  5]]
              precision    recall  f1-score   support

          A1       0.43      0.21      0.29        14
          A2       0.41      0.52      0.46        48
          B1       0.61      0.64      0.62        75
          B2       0.42      0.23      0.29        22

    accuracy                           0.51       159
   macro avg       0.47      0.40      0.42       159
weighted avg       0.51      0.51      0.50       159


K-fold scores
[0.5005904957546964, 0.40859639276212184, 0.5011792430591397, 0.5830400956954109, 0.49837851328549737]
SKF f1 score mean 0.4983569481113732

SAME LANG EVAL DONE FOR THIS LANG
Dep ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5018
5018
Fold 0
[[ 4 11  0  0]
 [ 2 36 11  0]
 [ 3 16 56  1]
 [ 0  2 20  1]]
              precision    recall  f1-score   support

          A1       0.44      0.27      0.33        15
          A2       0.55      0.73      0.63        49
          B1       0.64      0.74      0.69        76
          B2       0.50      0.04      0.08        23

    accuracy                           0.60       163
   macro avg       0.54      0.45      0.43       163
weighted avg       0.58      0.60      0.55       163


Fold 1
[[ 1 10  3  0]
 [ 2 27 20  0]
 [ 1 18 56  0]
 [ 0  0 22  0]]
              precision    recall  f1-score   support

          A1       0.25      0.07      0.11        14
          A2       0.49      0.55      0.52        49
          B1       0.55      0.75      0.64        75
          B2       0.00      0.00      0.00        22

    accuracy                           0.53       160
   macro avg       0.32      0.34      0.32       160
weighted avg       0.43      0.53      0.47       160


Fold 2
[[ 4  8  2  0]
 [ 1 33 14  0]
 [ 0 10 65  0]
 [ 0  2 19  1]]
              precision    recall  f1-score   support

          A1       0.80      0.29      0.42        14
          A2       0.62      0.69      0.65        48
          B1       0.65      0.87      0.74        75
          B2       1.00      0.05      0.09        22

    accuracy                           0.65       159
   macro avg       0.77      0.47      0.48       159
weighted avg       0.70      0.65      0.60       159


Fold 3
[[ 0 14  0  0]
 [ 3 25 20  0]
 [ 1 13 61  0]
 [ 0  0 21  1]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00        14
          A2       0.48      0.52      0.50        48
          B1       0.60      0.81      0.69        75
          B2       1.00      0.05      0.09        22

    accuracy                           0.55       159
   macro avg       0.52      0.34      0.32       159
weighted avg       0.57      0.55      0.49       159


Fold 4
[[ 3 10  1  0]
 [ 2 31 15  0]
 [ 0 20 55  0]
 [ 0  1 21  0]]
              precision    recall  f1-score   support

          A1       0.60      0.21      0.32        14
          A2       0.50      0.65      0.56        48
          B1       0.60      0.73      0.66        75
          B2       0.00      0.00      0.00        22

    accuracy                           0.56       159
   macro avg       0.42      0.40      0.38       159
weighted avg       0.49      0.56      0.51       159


K-fold scores
[0.5521977135997432, 0.4670320998445999, 0.5967824067189637, 0.48810036938678086, 0.5086590923956906]
SKF f1 score mean 0.5225543363891557

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5018
5018
Fold 0
[[ 5  9  1  0]
 [ 4 31 12  2]
 [ 5 20 41 10]
 [ 0  5 15  3]]
              precision    recall  f1-score   support

          A1       0.36      0.33      0.34        15
          A2       0.48      0.63      0.54        49
          B1       0.59      0.54      0.57        76
          B2       0.20      0.13      0.16        23

    accuracy                           0.49       163
   macro avg       0.41      0.41      0.40       163
weighted avg       0.48      0.49      0.48       163


Fold 1
[[ 3  6  5  0]
 [ 6 24 16  3]
 [ 2 24 38 11]
 [ 1  2 13  6]]
              precision    recall  f1-score   support

          A1       0.25      0.21      0.23        14
          A2       0.43      0.49      0.46        49
          B1       0.53      0.51      0.52        75
          B2       0.30      0.27      0.29        22

    accuracy                           0.44       160
   macro avg       0.38      0.37      0.37       160
weighted avg       0.44      0.44      0.44       160


Fold 2
[[ 5  7  2  0]
 [11 17 19  1]
 [ 3 16 46 10]
 [ 0  1 16  5]]
              precision    recall  f1-score   support

          A1       0.26      0.36      0.30        14
          A2       0.41      0.35      0.38        48
          B1       0.55      0.61      0.58        75
          B2       0.31      0.23      0.26        22

    accuracy                           0.46       159
   macro avg       0.39      0.39      0.38       159
weighted avg       0.45      0.46      0.45       159


Fold 3
[[ 2 10  2  0]
 [ 3 25 17  3]
 [ 1 20 45  9]
 [ 0  1 11 10]]
              precision    recall  f1-score   support

          A1       0.33      0.14      0.20        14
          A2       0.45      0.52      0.48        48
          B1       0.60      0.60      0.60        75
          B2       0.45      0.45      0.45        22

    accuracy                           0.52       159
   macro avg       0.46      0.43      0.43       159
weighted avg       0.51      0.52      0.51       159


Fold 4
[[ 1 10  3  0]
 [ 6 21 21  0]
 [ 2 24 43  6]
 [ 0  2 16  4]]
              precision    recall  f1-score   support

          A1       0.11      0.07      0.09        14
          A2       0.37      0.44      0.40        48
          B1       0.52      0.57      0.54        75
          B2       0.40      0.18      0.25        22

    accuracy                           0.43       159
   macro avg       0.35      0.32      0.32       159
weighted avg       0.42      0.43      0.42       159


K-fold scores
[0.4811805269467301, 0.4418249607535321, 0.45308089719666367, 0.508659893565554, 0.4197495353111598]
SKF f1 score mean 0.460899162754728

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5018
5018
Fold 0
[[ 7  7  1  0]
 [ 5 30 12  2]
 [ 4 21 39 12]
 [ 0  4 14  5]]
              precision    recall  f1-score   support

          A1       0.44      0.47      0.45        15
          A2       0.48      0.61      0.54        49
          B1       0.59      0.51      0.55        76
          B2       0.26      0.22      0.24        23

    accuracy                           0.50       163
   macro avg       0.44      0.45      0.44       163
weighted avg       0.50      0.50      0.49       163


Fold 1
[[ 3  8  3  0]
 [ 6 23 17  3]
 [ 3 25 38  9]
 [ 0  2 13  7]]
              precision    recall  f1-score   support

          A1       0.25      0.21      0.23        14
          A2       0.40      0.47      0.43        49
          B1       0.54      0.51      0.52        75
          B2       0.37      0.32      0.34        22

    accuracy                           0.44       160
   macro avg       0.39      0.38      0.38       160
weighted avg       0.44      0.44      0.44       160


Fold 2
[[ 6  6  2  0]
 [12 17 18  1]
 [ 2 16 47 10]
 [ 0  1 16  5]]
              precision    recall  f1-score   support

          A1       0.30      0.43      0.35        14
          A2       0.42      0.35      0.39        48
          B1       0.57      0.63      0.59        75
          B2       0.31      0.23      0.26        22

    accuracy                           0.47       159
   macro avg       0.40      0.41      0.40       159
weighted avg       0.47      0.47      0.46       159


Fold 3
[[ 3 10  1  0]
 [ 3 25 19  1]
 [ 1 16 50  8]
 [ 0  1 13  8]]
              precision    recall  f1-score   support

          A1       0.43      0.21      0.29        14
          A2       0.48      0.52      0.50        48
          B1       0.60      0.67      0.63        75
          B2       0.47      0.36      0.41        22

    accuracy                           0.54       159
   macro avg       0.50      0.44      0.46       159
weighted avg       0.53      0.54      0.53       159


Fold 4
[[ 2  9  3  0]
 [ 6 25 17  0]
 [ 1 24 42  8]
 [ 0  2 16  4]]
              precision    recall  f1-score   support

          A1       0.22      0.14      0.17        14
          A2       0.42      0.52      0.46        48
          B1       0.54      0.56      0.55        75
          B2       0.33      0.18      0.24        22

    accuracy                           0.46       159
   macro avg       0.38      0.35      0.36       159
weighted avg       0.45      0.46      0.45       159


K-fold scores
[0.49376287965830373, 0.44280925502424423, 0.46475696770320923, 0.5314087764529608, 0.4466034340087325]
SKF f1 score mean 0.47586826256949016

SAME LANG EVAL DONE FOR THIS LANG
************for dimension:  Orthography  ***************
Extracted all features: 
Printing class statistics
Counter({'C1': 262, 'B1': 220, 'A2': 143, 'B2': 77, 'C2': 76, 'A1': 22})
With Word ngrams: 
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2688
2688
Fold 0
[[ 0  3  2  0  0  0]
 [ 0 10 18  0  1  0]
 [ 0  4 37  0  3  0]
 [ 0  0  5  0 10  1]
 [ 0  0  0  0 52  1]
 [ 0  0  1  0 15  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         5
          A2       0.59      0.34      0.43        29
          B1       0.59      0.84      0.69        44
          B2       0.00      0.00      0.00        16
          C1       0.64      0.98      0.78        53
          C2       0.00      0.00      0.00        16

    accuracy                           0.61       163
   macro avg       0.30      0.36      0.32       163
weighted avg       0.47      0.61      0.52       163


Fold 1
[[ 0  5  0  0  0  0]
 [ 0 16 12  0  1  0]
 [ 1  6 33  0  4  0]
 [ 0  0  4  0 12  0]
 [ 0  0  4  0 49  0]
 [ 0  0  3  0 12  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         5
          A2       0.59      0.55      0.57        29
          B1       0.59      0.75      0.66        44
          B2       0.00      0.00      0.00        16
          C1       0.63      0.92      0.75        53
          C2       0.00      0.00      0.00        15

    accuracy                           0.60       162
   macro avg       0.30      0.37      0.33       162
weighted avg       0.47      0.60      0.53       162


Fold 2
[[ 0  4  0  0  0  0]
 [ 0 19  8  0  2  0]
 [ 0 11 26  0  7  0]
 [ 0  0  1  0 14  0]
 [ 0  1  3  0 48  0]
 [ 0  0  2  0 13  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.54      0.66      0.59        29
          B1       0.65      0.59      0.62        44
          B2       0.00      0.00      0.00        15
          C1       0.57      0.92      0.71        52
          C2       0.00      0.00      0.00        15

    accuracy                           0.58       159
   macro avg       0.29      0.36      0.32       159
weighted avg       0.47      0.58      0.51       159


Fold 3
[[ 0  2  2  0  0  0]
 [ 0 15 13  0  0  0]
 [ 0  7 30  0  7  0]
 [ 0  0  1  0 14  0]
 [ 0  1  1  0 50  0]
 [ 0  1  2  0 12  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.58      0.54      0.56        28
          B1       0.61      0.68      0.65        44
          B2       0.00      0.00      0.00        15
          C1       0.60      0.96      0.74        52
          C2       0.00      0.00      0.00        15

    accuracy                           0.60       158
   macro avg       0.30      0.36      0.32       158
weighted avg       0.47      0.60      0.52       158


Fold 4
[[ 0  4  0  0  0  0]
 [ 0  9 19  0  0  0]
 [ 0  7 33  0  4  0]
 [ 0  0  5  0 10  0]
 [ 0  1  2  0 45  4]
 [ 0  0  1  0 13  1]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.43      0.32      0.37        28
          B1       0.55      0.75      0.63        44
          B2       0.00      0.00      0.00        15
          C1       0.62      0.87      0.73        52
          C2       0.20      0.07      0.10        15

    accuracy                           0.56       158
   macro avg       0.30      0.33      0.30       158
weighted avg       0.45      0.56      0.49       158


K-fold scores
[0.5163983469474782, 0.5262980465016088, 0.5104574062329336, 0.5219061446092887, 0.4901944727383682]
SKF f1 score mean 0.5130508834059355

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2688
2688
Fold 0
[[ 0  3  2  0  0  0]
 [ 2 13 12  1  0  1]
 [ 1 12 27  2  2  0]
 [ 0  1  3  3  8  1]
 [ 0  0  2  5 38  8]
 [ 0  2  1  2 10  1]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         5
          A2       0.42      0.45      0.43        29
          B1       0.57      0.61      0.59        44
          B2       0.23      0.19      0.21        16
          C1       0.66      0.72      0.68        53
          C2       0.09      0.06      0.07        16

    accuracy                           0.50       163
   macro avg       0.33      0.34      0.33       163
weighted avg       0.47      0.50      0.49       163


Fold 1
[[ 0  5  0  0  0  0]
 [ 1 18  8  0  2  0]
 [ 1 15 22  4  2  0]
 [ 0  2  3  1 10  0]
 [ 0  2  4  3 37  7]
 [ 0  1  3  2  8  1]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         5
          A2       0.42      0.62      0.50        29
          B1       0.55      0.50      0.52        44
          B2       0.10      0.06      0.08        16
          C1       0.63      0.70      0.66        53
          C2       0.12      0.07      0.09        15

    accuracy                           0.49       162
   macro avg       0.30      0.32      0.31       162
weighted avg       0.45      0.49      0.46       162


Fold 2
[[ 0  4  0  0  0  0]
 [ 1 14 11  2  0  1]
 [ 0 14 24  3  3  0]
 [ 0  0  1  1  9  4]
 [ 0  2  6  5 29 10]
 [ 0  0  4  1  9  1]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.41      0.48      0.44        29
          B1       0.52      0.55      0.53        44
          B2       0.08      0.07      0.07        15
          C1       0.58      0.56      0.57        52
          C2       0.06      0.07      0.06        15

    accuracy                           0.43       159
   macro avg       0.28      0.29      0.28       159
weighted avg       0.42      0.43      0.43       159


Fold 3
[[ 0  1  3  0  0  0]
 [ 1 15 10  1  0  1]
 [ 0  6 30  3  5  0]
 [ 0  1  0  4  8  2]
 [ 0  1  3  7 30 11]
 [ 0  0  3  2  9  1]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.62      0.54      0.58        28
          B1       0.61      0.68      0.65        44
          B2       0.24      0.27      0.25        15
          C1       0.58      0.58      0.58        52
          C2       0.07      0.07      0.07        15

    accuracy                           0.51       158
   macro avg       0.35      0.35      0.35       158
weighted avg       0.50      0.51      0.50       158


Fold 4
[[ 0  3  1  0  0  0]
 [ 2 14 12  0  0  0]
 [ 1 10 29  1  2  1]
 [ 0  2  3  1  6  3]
 [ 0  0  5  6 35  6]
 [ 0  1  1  1 11  1]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.47      0.50      0.48        28
          B1       0.57      0.66      0.61        44
          B2       0.11      0.07      0.08        15
          C1       0.65      0.67      0.66        52
          C2       0.09      0.07      0.08        15

    accuracy                           0.51       158
   macro avg       0.31      0.33      0.32       158
weighted avg       0.47      0.51      0.49       158


K-fold scores
[0.4874869636663586, 0.4635839089341505, 0.42769205064862226, 0.5018414109369601, 0.4881257472747012]
SKF f1 score mean 0.4737460162921586

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2688
2688
Fold 0
[[ 0  2  3  0  0  0]
 [ 1 14 12  1  0  1]
 [ 1 12 28  2  1  0]
 [ 0  1  3  3  8  1]
 [ 0  0  2  6 37  8]
 [ 0  0  0  3 12  1]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         5
          A2       0.48      0.48      0.48        29
          B1       0.58      0.64      0.61        44
          B2       0.20      0.19      0.19        16
          C1       0.64      0.70      0.67        53
          C2       0.09      0.06      0.07        16

    accuracy                           0.51       163
   macro avg       0.33      0.34      0.34       163
weighted avg       0.48      0.51      0.49       163


Fold 1
[[ 1  4  0  0  0  0]
 [ 0 17  9  1  1  1]
 [ 1 14 23  3  2  1]
 [ 0  2  2  1 11  0]
 [ 0  2  1  5 40  5]
 [ 0  1  3  2  7  2]]
              precision    recall  f1-score   support

          A1       0.50      0.20      0.29         5
          A2       0.42      0.59      0.49        29
          B1       0.61      0.52      0.56        44
          B2       0.08      0.06      0.07        16
          C1       0.66      0.75      0.70        53
          C2       0.22      0.13      0.17        15

    accuracy                           0.52       162
   macro avg       0.42      0.38      0.38       162
weighted avg       0.50      0.52      0.50       162


Fold 2
[[ 0  4  0  0  0  0]
 [ 1 15  9  2  1  1]
 [ 0 11 25  4  3  1]
 [ 0  0  1  0 10  4]
 [ 0  1  3  3 40  5]
 [ 0  1  1  0 11  2]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.47      0.52      0.49        29
          B1       0.64      0.57      0.60        44
          B2       0.00      0.00      0.00        15
          C1       0.62      0.77      0.68        52
          C2       0.15      0.13      0.14        15

    accuracy                           0.52       159
   macro avg       0.31      0.33      0.32       159
weighted avg       0.48      0.52      0.49       159


Fold 3
[[ 0  1  3  0  0  0]
 [ 1 16 10  1  0  0]
 [ 1  7 30  1  5  0]
 [ 0  1  3  2  8  1]
 [ 0  1  2  4 38  7]
 [ 0  1  2  2  9  1]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.59      0.57      0.58        28
          B1       0.60      0.68      0.64        44
          B2       0.20      0.13      0.16        15
          C1       0.63      0.73      0.68        52
          C2       0.11      0.07      0.08        15

    accuracy                           0.55       158
   macro avg       0.36      0.36      0.36       158
weighted avg       0.51      0.55      0.53       158


Fold 4
[[ 0  3  1  0  0  0]
 [ 2 11 14  1  0  0]
 [ 1  8 29  2  2  2]
 [ 0  2  3  1  7  2]
 [ 0  0  5  5 36  6]
 [ 0  1  1  1 11  1]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.44      0.39      0.42        28
          B1       0.55      0.66      0.60        44
          B2       0.10      0.07      0.08        15
          C1       0.64      0.69      0.67        52
          C2       0.09      0.07      0.08        15

    accuracy                           0.49       158
   macro avg       0.30      0.31      0.31       158
weighted avg       0.46      0.49      0.47       158


K-fold scores
[0.4932386589430618, 0.501464153884574, 0.4935014583444368, 0.5272894288582411, 0.4743824853193813]
SKF f1 score mean 0.49797523706993907

SAME LANG EVAL DONE FOR THIS LANG
With POS ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
4765
4765
Fold 0
[[ 0  3  1  0  1  0]
 [ 0  3 24  0  2  0]
 [ 0  4 35  0  5  0]
 [ 0  0  4  0 12  0]
 [ 0  0  6  0 47  0]
 [ 0  0  1  0 15  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         5
          A2       0.30      0.10      0.15        29
          B1       0.49      0.80      0.61        44
          B2       0.00      0.00      0.00        16
          C1       0.57      0.89      0.70        53
          C2       0.00      0.00      0.00        16

    accuracy                           0.52       163
   macro avg       0.23      0.30      0.24       163
weighted avg       0.37      0.52      0.42       163


Fold 1
[[ 0  3  2  0  0  0]
 [ 0  8 20  0  1  0]
 [ 0  9 29  0  6  0]
 [ 0  1  5  0 10  0]
 [ 0  0  4  0 49  0]
 [ 0  1  4  0 10  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         5
          A2       0.36      0.28      0.31        29
          B1       0.45      0.66      0.54        44
          B2       0.00      0.00      0.00        16
          C1       0.64      0.92      0.76        53
          C2       0.00      0.00      0.00        15

    accuracy                           0.53       162
   macro avg       0.24      0.31      0.27       162
weighted avg       0.40      0.53      0.45       162


Fold 2
[[ 0  4  0  0  0  0]
 [ 0 11 15  0  3  0]
 [ 0 16 23  0  5  0]
 [ 0  0  2  0 13  0]
 [ 0  0  7  0 45  0]
 [ 0  2  3  0 10  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.33      0.38      0.35        29
          B1       0.46      0.52      0.49        44
          B2       0.00      0.00      0.00        15
          C1       0.59      0.87      0.70        52
          C2       0.00      0.00      0.00        15

    accuracy                           0.50       159
   macro avg       0.23      0.29      0.26       159
weighted avg       0.38      0.50      0.43       159


Fold 3
[[ 0  1  3  0  0  0]
 [ 0 11 14  0  3  0]
 [ 0  3 31  0 10  0]
 [ 0  0  3  0 12  0]
 [ 0  0  8  0 44  0]
 [ 0  0  5  0 10  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.73      0.39      0.51        28
          B1       0.48      0.70      0.57        44
          B2       0.00      0.00      0.00        15
          C1       0.56      0.85      0.67        52
          C2       0.00      0.00      0.00        15

    accuracy                           0.54       158
   macro avg       0.30      0.32      0.29       158
weighted avg       0.45      0.54      0.47       158


Fold 4
[[ 0  2  2  0  0  0]
 [ 0  5 23  0  0  0]
 [ 0  7 32  0  5  0]
 [ 0  0  3  0 12  0]
 [ 0  1  6  0 45  0]
 [ 0  0  2  0 13  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.33      0.18      0.23        28
          B1       0.47      0.73      0.57        44
          B2       0.00      0.00      0.00        15
          C1       0.60      0.87      0.71        52
          C2       0.00      0.00      0.00        15

    accuracy                           0.52       158
   macro avg       0.23      0.30      0.25       158
weighted avg       0.39      0.52      0.43       158


K-fold scores
[0.41808497460671373, 0.450563177387585, 0.4300926885173722, 0.47162112888944147, 0.4335751819659575]
SKF f1 score mean 0.44078743027341394

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
4765
4765
Fold 0
[[ 0  2  2  0  0  1]
 [ 0 13 10  3  0  3]
 [ 0 12 28  1  1  2]
 [ 1  4  3  1  6  1]
 [ 0  5  5  1 39  3]
 [ 0  0  1  1 13  1]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         5
          A2       0.36      0.45      0.40        29
          B1       0.57      0.64      0.60        44
          B2       0.14      0.06      0.09        16
          C1       0.66      0.74      0.70        53
          C2       0.09      0.06      0.07        16

    accuracy                           0.50       163
   macro avg       0.30      0.32      0.31       163
weighted avg       0.46      0.50      0.48       163


Fold 1
[[ 1  3  1  0  0  0]
 [ 0  9 17  1  1  1]
 [ 0 14 21  2  5  2]
 [ 0  1  5  3  7  0]
 [ 0  5  6  3 33  6]
 [ 0  3  3  1  8  0]]
              precision    recall  f1-score   support

          A1       1.00      0.20      0.33         5
          A2       0.26      0.31      0.28        29
          B1       0.40      0.48      0.43        44
          B2       0.30      0.19      0.23        16
          C1       0.61      0.62      0.62        53
          C2       0.00      0.00      0.00        15

    accuracy                           0.41       162
   macro avg       0.43      0.30      0.32       162
weighted avg       0.41      0.41      0.40       162


Fold 2
[[ 0  2  2  0  0  0]
 [ 0 13 15  0  1  0]
 [ 1 11 26  3  2  1]
 [ 0  1  0  1 11  2]
 [ 0  4  4  6 32  6]
 [ 0  2  3  3  5  2]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.39      0.45      0.42        29
          B1       0.52      0.59      0.55        44
          B2       0.08      0.07      0.07        15
          C1       0.63      0.62      0.62        52
          C2       0.18      0.13      0.15        15

    accuracy                           0.47       159
   macro avg       0.30      0.31      0.30       159
weighted avg       0.45      0.47      0.45       159


Fold 3
[[ 0  1  2  1  0  0]
 [ 0 12 11  2  2  1]
 [ 1 14 24  2  3  0]
 [ 0  1  1  3  5  5]
 [ 0  1  4  5 38  4]
 [ 0  1  2  0 10  2]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.40      0.43      0.41        28
          B1       0.55      0.55      0.55        44
          B2       0.23      0.20      0.21        15
          C1       0.66      0.73      0.69        52
          C2       0.17      0.13      0.15        15

    accuracy                           0.50       158
   macro avg       0.33      0.34      0.34       158
weighted avg       0.48      0.50      0.49       158


Fold 4
[[ 0  2  1  0  1  0]
 [ 2 10 15  0  1  0]
 [ 2 12 24  0  4  2]
 [ 0  1  3  1  7  3]
 [ 0  1  5  5 35  6]
 [ 0  1  3  1  6  4]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.37      0.36      0.36        28
          B1       0.47      0.55      0.51        44
          B2       0.14      0.07      0.09        15
          C1       0.65      0.67      0.66        52
          C2       0.27      0.27      0.27        15

    accuracy                           0.47       158
   macro avg       0.32      0.32      0.31       158
weighted avg       0.45      0.47      0.46       158


K-fold scores
[0.47596213174625485, 0.4028293798504547, 0.4540346939953893, 0.48702523772362283, 0.4564345324957747]
SKF f1 score mean 0.45525719516229923

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
4765
4765
Fold 0
[[ 0  2  2  0  0  1]
 [ 0 16  8  3  0  2]
 [ 0 11 29  0  2  2]
 [ 0  3  2  1  8  2]
 [ 0  2  3  1 43  4]
 [ 0  0  1  1 13  1]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         5
          A2       0.47      0.55      0.51        29
          B1       0.64      0.66      0.65        44
          B2       0.17      0.06      0.09        16
          C1       0.65      0.81      0.72        53
          C2       0.08      0.06      0.07        16

    accuracy                           0.55       163
   macro avg       0.34      0.36      0.34       163
weighted avg       0.49      0.55      0.52       163


Fold 1
[[ 1  3  1  0  0  0]
 [ 0  8 18  1  1  1]
 [ 0 15 21  2  5  1]
 [ 0  1  5  3  7  0]
 [ 0  4  5  3 36  5]
 [ 0  2  3  1  9  0]]
              precision    recall  f1-score   support

          A1       1.00      0.20      0.33         5
          A2       0.24      0.28      0.26        29
          B1       0.40      0.48      0.43        44
          B2       0.30      0.19      0.23        16
          C1       0.62      0.68      0.65        53
          C2       0.00      0.00      0.00        15

    accuracy                           0.43       162
   macro avg       0.43      0.30      0.32       162
weighted avg       0.41      0.43      0.41       162


Fold 2
[[ 0  2  2  0  0  0]
 [ 0 13 16  0  0  0]
 [ 1 11 27  3  2  0]
 [ 0  1  0  1 12  1]
 [ 0  3  3  4 36  6]
 [ 0  3  3  2  5  2]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.39      0.45      0.42        29
          B1       0.53      0.61      0.57        44
          B2       0.10      0.07      0.08        15
          C1       0.65      0.69      0.67        52
          C2       0.22      0.13      0.17        15

    accuracy                           0.50       159
   macro avg       0.32      0.33      0.32       159
weighted avg       0.46      0.50      0.48       159


Fold 3
[[ 0  1  2  1  0  0]
 [ 0 11 10  2  2  3]
 [ 1 13 25  2  3  0]
 [ 0  1  0  2  8  4]
 [ 0  0  3  6 40  3]
 [ 0  1  2  0 10  2]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.41      0.39      0.40        28
          B1       0.60      0.57      0.58        44
          B2       0.15      0.13      0.14        15
          C1       0.63      0.77      0.70        52
          C2       0.17      0.13      0.15        15

    accuracy                           0.51       158
   macro avg       0.33      0.33      0.33       158
weighted avg       0.48      0.51      0.49       158


Fold 4
[[ 0  2  1  0  1  0]
 [ 1  9 17  0  1  0]
 [ 2 13 24  0  3  2]
 [ 0  1  3  0  7  4]
 [ 0  0  5  3 35  9]
 [ 0  1  3  0  8  3]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.35      0.32      0.33        28
          B1       0.45      0.55      0.49        44
          B2       0.00      0.00      0.00        15
          C1       0.64      0.67      0.65        52
          C2       0.17      0.20      0.18        15

    accuracy                           0.45       158
   macro avg       0.27      0.29      0.28       158
weighted avg       0.41      0.45      0.43       158


K-fold scores
[0.5172039241677023, 0.40909117343732865, 0.4771224581382239, 0.4893695427683218, 0.4294461615549353]
SKF f1 score mean 0.46444665201330243

SAME LANG EVAL DONE FOR THIS LANG
Dep ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5018
5018
Fold 0
[[ 0  3  2  0  0  0]
 [ 0  5 23  0  1  0]
 [ 0  6 35  0  3  0]
 [ 0  1  2  0 13  0]
 [ 0  1  6  0 46  0]
 [ 0  0  2  0 14  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         5
          A2       0.31      0.17      0.22        29
          B1       0.50      0.80      0.61        44
          B2       0.00      0.00      0.00        16
          C1       0.60      0.87      0.71        53
          C2       0.00      0.00      0.00        16

    accuracy                           0.53       163
   macro avg       0.23      0.31      0.26       163
weighted avg       0.38      0.53      0.44       163


Fold 1
[[ 0  3  2  0  0  0]
 [ 0 12 15  0  2  0]
 [ 0 12 26  0  6  0]
 [ 0  1  6  0  9  0]
 [ 0  0  6  0 47  0]
 [ 0  1  4  0 10  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         5
          A2       0.41      0.41      0.41        29
          B1       0.44      0.59      0.50        44
          B2       0.00      0.00      0.00        16
          C1       0.64      0.89      0.74        53
          C2       0.00      0.00      0.00        15

    accuracy                           0.52       162
   macro avg       0.25      0.32      0.28       162
weighted avg       0.40      0.52      0.45       162


Fold 2
[[ 0  4  0  0  0  0]
 [ 0 11 15  0  3  0]
 [ 0  8 33  0  3  0]
 [ 0  0  3  0 12  0]
 [ 0  2  4  0 46  0]
 [ 0  2  2  0 11  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.41      0.38      0.39        29
          B1       0.58      0.75      0.65        44
          B2       0.00      0.00      0.00        15
          C1       0.61      0.88      0.72        52
          C2       0.00      0.00      0.00        15

    accuracy                           0.57       159
   macro avg       0.27      0.34      0.30       159
weighted avg       0.44      0.57      0.49       159


Fold 3
[[ 0  1  3  0  0  0]
 [ 0 11 16  0  1  0]
 [ 0  6 30  0  8  0]
 [ 0  0  2  0 13  0]
 [ 0  1  6  0 45  0]
 [ 0  0  5  0 10  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.58      0.39      0.47        28
          B1       0.48      0.68      0.57        44
          B2       0.00      0.00      0.00        15
          C1       0.58      0.87      0.70        52
          C2       0.00      0.00      0.00        15

    accuracy                           0.54       158
   macro avg       0.27      0.32      0.29       158
weighted avg       0.43      0.54      0.47       158


Fold 4
[[ 0  2  2  0  0  0]
 [ 0 10 18  0  0  0]
 [ 0 12 27  0  5  0]
 [ 0  0  5  0 10  0]
 [ 0  0  6  0 46  0]
 [ 0  0  3  0 12  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.42      0.36      0.38        28
          B1       0.44      0.61      0.51        44
          B2       0.00      0.00      0.00        15
          C1       0.63      0.88      0.74        52
          C2       0.00      0.00      0.00        15

    accuracy                           0.53       158
   macro avg       0.25      0.31      0.27       158
weighted avg       0.40      0.53      0.45       158


K-fold scores
[0.4353968135692385, 0.4533453005537199, 0.48940014923877084, 0.4701969185033149, 0.45360634302406455]
SKF f1 score mean 0.4603891049778217

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5018
5018
Fold 0
[[ 1  1  2  1  0  0]
 [ 0 13 12  1  1  2]
 [ 1 14 21  2  5  1]
 [ 1  2  1  2  8  2]
 [ 0  5  7  3 31  7]
 [ 0  1  4  0  8  3]]
              precision    recall  f1-score   support

          A1       0.33      0.20      0.25         5
          A2       0.36      0.45      0.40        29
          B1       0.45      0.48      0.46        44
          B2       0.22      0.12      0.16        16
          C1       0.58      0.58      0.58        53
          C2       0.20      0.19      0.19        16

    accuracy                           0.44       163
   macro avg       0.36      0.34      0.34       163
weighted avg       0.43      0.44      0.43       163


Fold 1
[[ 0  4  0  1  0  0]
 [ 1 11 11  1  3  2]
 [ 0 14 22  1  4  3]
 [ 0  2  5  3  5  1]
 [ 1  2  5  5 37  3]
 [ 0  2  2  2  6  3]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         5
          A2       0.31      0.38      0.34        29
          B1       0.49      0.50      0.49        44
          B2       0.23      0.19      0.21        16
          C1       0.67      0.70      0.69        53
          C2       0.25      0.20      0.22        15

    accuracy                           0.47       162
   macro avg       0.33      0.33      0.33       162
weighted avg       0.46      0.47      0.46       162


Fold 2
[[ 0  3  0  0  0  1]
 [ 1  8 17  0  3  0]
 [ 0 14 25  2  3  0]
 [ 1  1  1  4  8  0]
 [ 0  0  4  4 40  4]
 [ 0  5  1  2  5  2]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.26      0.28      0.27        29
          B1       0.52      0.57      0.54        44
          B2       0.33      0.27      0.30        15
          C1       0.68      0.77      0.72        52
          C2       0.29      0.13      0.18        15

    accuracy                           0.50       159
   macro avg       0.35      0.34      0.33       159
weighted avg       0.47      0.50      0.48       159


Fold 3
[[ 0  0  3  0  1  0]
 [ 2  8 13  1  3  1]
 [ 0 10 25  1  6  2]
 [ 0  3  1  0  8  3]
 [ 0  2  5  1 40  4]
 [ 0  3  2  0  9  1]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.31      0.29      0.30        28
          B1       0.51      0.57      0.54        44
          B2       0.00      0.00      0.00        15
          C1       0.60      0.77      0.67        52
          C2       0.09      0.07      0.08        15

    accuracy                           0.47       158
   macro avg       0.25      0.28      0.26       158
weighted avg       0.40      0.47      0.43       158


Fold 4
[[ 0  2  1  0  1  0]
 [ 1 11 14  2  0  0]
 [ 1 10 29  0  3  1]
 [ 0  1  3  1  9  1]
 [ 0  3  7  3 30  9]
 [ 1  0  3  3  6  2]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.41      0.39      0.40        28
          B1       0.51      0.66      0.57        44
          B2       0.11      0.07      0.08        15
          C1       0.61      0.58      0.59        52
          C2       0.15      0.13      0.14        15

    accuracy                           0.46       158
   macro avg       0.30      0.30      0.30       158
weighted avg       0.44      0.46      0.45       158


K-fold scores
[0.4283096104370595, 0.46098797508949624, 0.47984636138860914, 0.4307850608981924, 0.4477928669901348]
SKF f1 score mean 0.44954437496069843

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5018
5018
Fold 0
[[ 0  2  2  1  0  0]
 [ 0 13 12  1  1  2]
 [ 0 11 23  2  5  3]
 [ 0  2  2  3  8  1]
 [ 0  4  4  3 34  8]
 [ 0  1  2  0 11  2]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         5
          A2       0.39      0.45      0.42        29
          B1       0.51      0.52      0.52        44
          B2       0.30      0.19      0.23        16
          C1       0.58      0.64      0.61        53
          C2       0.12      0.12      0.12        16

    accuracy                           0.46       163
   macro avg       0.32      0.32      0.32       163
weighted avg       0.44      0.46      0.45       163


Fold 1
[[ 0  4  0  1  0  0]
 [ 1 10 12  0  4  2]
 [ 0 14 21  0  6  3]
 [ 0  3  3  4  5  1]
 [ 0  2  4  5 39  3]
 [ 0  1  2  1  8  3]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         5
          A2       0.29      0.34      0.32        29
          B1       0.50      0.48      0.49        44
          B2       0.36      0.25      0.30        16
          C1       0.63      0.74      0.68        53
          C2       0.25      0.20      0.22        15

    accuracy                           0.48       162
   macro avg       0.34      0.33      0.33       162
weighted avg       0.45      0.48      0.46       162


Fold 2
[[ 0  3  0  0  0  1]
 [ 0  9 18  0  2  0]
 [ 0 12 27  1  3  1]
 [ 0  1  0  2 11  1]
 [ 0  1  2  5 40  4]
 [ 0  3  1  1  7  3]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.31      0.31      0.31        29
          B1       0.56      0.61      0.59        44
          B2       0.22      0.13      0.17        15
          C1       0.63      0.77      0.70        52
          C2       0.30      0.20      0.24        15

    accuracy                           0.51       159
   macro avg       0.34      0.34      0.33       159
weighted avg       0.47      0.51      0.48       159


Fold 3
[[ 0  0  3  0  1  0]
 [ 2  8 13  1  3  1]
 [ 0 11 25  1  6  1]
 [ 0  3  1  0  8  3]
 [ 0  2  4  2 40  4]
 [ 0  2  2  0  9  2]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.31      0.29      0.30        28
          B1       0.52      0.57      0.54        44
          B2       0.00      0.00      0.00        15
          C1       0.60      0.77      0.67        52
          C2       0.18      0.13      0.15        15

    accuracy                           0.47       158
   macro avg       0.27      0.29      0.28       158
weighted avg       0.41      0.47      0.44       158


Fold 4
[[ 0  2  1  0  1  0]
 [ 1  8 16  3  0  0]
 [ 2 10 27  0  4  1]
 [ 0  1  4  1  9  0]
 [ 0  2  4  4 34  8]
 [ 1  0  3  3  6  2]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         4
          A2       0.35      0.29      0.31        28
          B1       0.49      0.61      0.55        44
          B2       0.09      0.07      0.08        15
          C1       0.63      0.65      0.64        52
          C2       0.18      0.13      0.15        15

    accuracy                           0.46       158
   macro avg       0.29      0.29      0.29       158
weighted avg       0.43      0.46      0.44       158


K-fold scores
[0.4464646777740345, 0.4612137127185374, 0.48490566037735844, 0.439715286553967, 0.44053381489282534]
SKF f1 score mean 0.45456663046334456

SAME LANG EVAL DONE FOR THIS LANG
************for dimension:  Vocabularyrange  ***************
Extracted all features: 
Printing class statistics
Counter({'B1': 328, 'A2': 243, 'B2': 188, 'A1': 39, 'C1': 2})
With Word ngrams: 
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2688
2688
Fold 0
[[ 1  6  1  0]
 [ 0 39 10  0]
 [ 0  6 49 11]
 [ 0  0  6 32]]
              precision    recall  f1-score   support

          A1       1.00      0.12      0.22         8
          A2       0.76      0.80      0.78        49
          B1       0.74      0.74      0.74        66
          B2       0.74      0.84      0.79        38

    accuracy                           0.75       161
   macro avg       0.81      0.63      0.63       161
weighted avg       0.76      0.75      0.74       161


Fold 1
[[ 2  6  0  0  0]
 [ 0 40  9  0  0]
 [ 0 10 53  3  0]
 [ 0  0 11 27  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       1.00      0.25      0.40         8
          A2       0.71      0.82      0.76        49
          B1       0.73      0.80      0.76        66
          B2       0.87      0.71      0.78        38
          C1       0.00      0.00      0.00         1

    accuracy                           0.75       162
   macro avg       0.66      0.52      0.54       162
weighted avg       0.77      0.75      0.74       162


Fold 2
[[ 0  8  0  0]
 [ 1 38 10  0]
 [ 0 15 45  6]
 [ 0  0  6 32]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.62      0.78      0.69        49
          B1       0.74      0.68      0.71        66
          B2       0.84      0.84      0.84        38

    accuracy                           0.71       161
   macro avg       0.55      0.57      0.56       161
weighted avg       0.69      0.71      0.70       161


Fold 3
[[ 2  6  0  0  0]
 [ 0 40  8  0  0]
 [ 0  9 49  7  0]
 [ 0  0 10 27  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       1.00      0.25      0.40         8
          A2       0.73      0.83      0.78        48
          B1       0.73      0.75      0.74        65
          B2       0.77      0.73      0.75        37
          C1       0.00      0.00      0.00         1

    accuracy                           0.74       159
   macro avg       0.65      0.51      0.53       159
weighted avg       0.75      0.74      0.73       159


Fold 4
[[ 3  4  0  0]
 [ 2 34 11  1]
 [ 0 11 44 10]
 [ 0  0 10 27]]
              precision    recall  f1-score   support

          A1       0.60      0.43      0.50         7
          A2       0.69      0.71      0.70        48
          B1       0.68      0.68      0.68        65
          B2       0.71      0.73      0.72        37

    accuracy                           0.69       157
   macro avg       0.67      0.64      0.65       157
weighted avg       0.69      0.69      0.69       157


K-fold scores
[0.7392699946323134, 0.744465426048855, 0.6995416086823139, 0.7326360324253712, 0.6865572263444744]
SKF f1 score mean 0.7204940576266655

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2688
2688
Fold 0
[[ 2  6  0  0]
 [ 4 34 11  0]
 [ 1 13 39 13]
 [ 0  0  9 29]]
              precision    recall  f1-score   support

          A1       0.29      0.25      0.27         8
          A2       0.64      0.69      0.67        49
          B1       0.66      0.59      0.62        66
          B2       0.69      0.76      0.72        38

    accuracy                           0.65       161
   macro avg       0.57      0.57      0.57       161
weighted avg       0.64      0.65      0.64       161


Fold 1
[[ 5  3  0  0  0]
 [ 6 29 13  1  0]
 [ 0 13 47  6  0]
 [ 0  0 13 25  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.45      0.62      0.53         8
          A2       0.64      0.59      0.62        49
          B1       0.64      0.71      0.68        66
          B2       0.76      0.66      0.70        38
          C1       0.00      0.00      0.00         1

    accuracy                           0.65       162
   macro avg       0.50      0.52      0.50       162
weighted avg       0.66      0.65      0.65       162


Fold 2
[[ 0  6  2  0]
 [ 1 33 14  1]
 [ 0 19 38  9]
 [ 0  0  9 29]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.57      0.67      0.62        49
          B1       0.60      0.58      0.59        66
          B2       0.74      0.76      0.75        38

    accuracy                           0.62       161
   macro avg       0.48      0.50      0.49       161
weighted avg       0.60      0.62      0.61       161


Fold 3
[[ 4  4  0  0  0]
 [ 2 38  8  0  0]
 [ 0 12 42 11  0]
 [ 0  0 15 22  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.67      0.50      0.57         8
          A2       0.70      0.79      0.75        48
          B1       0.65      0.65      0.65        65
          B2       0.65      0.59      0.62        37
          C1       0.00      0.00      0.00         1

    accuracy                           0.67       159
   macro avg       0.53      0.51      0.52       159
weighted avg       0.66      0.67      0.66       159


Fold 4
[[ 3  3  1  0]
 [ 9 24 13  2]
 [ 1 16 38 10]
 [ 0  0  9 28]]
              precision    recall  f1-score   support

          A1       0.23      0.43      0.30         7
          A2       0.56      0.50      0.53        48
          B1       0.62      0.58      0.60        65
          B2       0.70      0.76      0.73        37

    accuracy                           0.59       157
   macro avg       0.53      0.57      0.54       157
weighted avg       0.60      0.59      0.60       157


K-fold scores
[0.6430683229813665, 0.6533223813237192, 0.6070273081927314, 0.6620485026325172, 0.5957587352491811]
SKF f1 score mean 0.6322450500759031

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2688
2688
Fold 0
[[ 2  6  0  0]
 [ 2 34 13  0]
 [ 0 10 40 16]
 [ 0  0  8 30]]
              precision    recall  f1-score   support

          A1       0.50      0.25      0.33         8
          A2       0.68      0.69      0.69        49
          B1       0.66      0.61      0.63        66
          B2       0.65      0.79      0.71        38

    accuracy                           0.66       161
   macro avg       0.62      0.58      0.59       161
weighted avg       0.65      0.66      0.65       161


Fold 1
[[ 5  3  0  0  0]
 [ 6 31 11  1  0]
 [ 0  9 52  5  0]
 [ 0  0 12 26  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.45      0.62      0.53         8
          A2       0.72      0.63      0.67        49
          B1       0.69      0.79      0.74        66
          B2       0.79      0.68      0.73        38
          C1       0.00      0.00      0.00         1

    accuracy                           0.70       162
   macro avg       0.53      0.55      0.53       162
weighted avg       0.71      0.70      0.70       162


Fold 2
[[ 0  6  2  0]
 [ 2 35 11  1]
 [ 0 17 39 10]
 [ 0  0  8 29]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.60      0.71      0.65        49
          B1       0.65      0.59      0.62        66
          B2       0.72      0.76      0.74        38
          C1       0.00      0.00      0.00         0

    accuracy                           0.64       161
   macro avg       0.40      0.41      0.40       161
weighted avg       0.62      0.64      0.63       161


Fold 3
[[ 3  5  0  0  0]
 [ 2 39  7  0  0]
 [ 1  8 46 10  0]
 [ 0  0 12 25  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.50      0.38      0.43         8
          A2       0.75      0.81      0.78        48
          B1       0.71      0.71      0.71        65
          B2       0.69      0.68      0.68        37
          C1       0.00      0.00      0.00         1

    accuracy                           0.71       159
   macro avg       0.53      0.51      0.52       159
weighted avg       0.70      0.71      0.71       159


Fold 4
[[ 3  4  0  0]
 [ 6 32  9  1]
 [ 0 13 43  9]
 [ 0  0  9 28]]
              precision    recall  f1-score   support

          A1       0.33      0.43      0.38         7
          A2       0.65      0.67      0.66        48
          B1       0.70      0.66      0.68        65
          B2       0.74      0.76      0.75        37

    accuracy                           0.68       157
   macro avg       0.61      0.63      0.62       157
weighted avg       0.68      0.68      0.68       157


K-fold scores
[0.6524279044453153, 0.7021240890465864, 0.6283827818627841, 0.7057297935974597, 0.6769863001562391]
SKF f1 score mean 0.673130173821677

SAME LANG EVAL DONE FOR THIS LANG
With POS ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
4765
4765
Fold 0
[[ 0  7  1  0]
 [ 1 39  9  0]
 [ 0 11 42 13]
 [ 0  1  7 30]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.67      0.80      0.73        49
          B1       0.71      0.64      0.67        66
          B2       0.70      0.79      0.74        38

    accuracy                           0.69       161
   macro avg       0.52      0.56      0.54       161
weighted avg       0.66      0.69      0.67       161


Fold 1
[[ 1  6  1  0  0]
 [ 0 37 12  0  0]
 [ 0 10 52  4  0]
 [ 0  0 16 22  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       1.00      0.12      0.22         8
          A2       0.70      0.76      0.73        49
          B1       0.64      0.79      0.71        66
          B2       0.81      0.58      0.68        38
          C1       0.00      0.00      0.00         1

    accuracy                           0.69       162
   macro avg       0.63      0.45      0.47       162
weighted avg       0.71      0.69      0.68       162


Fold 2
[[ 0  6  2  0]
 [ 0 36 13  0]
 [ 1 15 43  7]
 [ 0  0  7 31]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.63      0.73      0.68        49
          B1       0.66      0.65      0.66        66
          B2       0.82      0.82      0.82        38

    accuracy                           0.68       161
   macro avg       0.53      0.55      0.54       161
weighted avg       0.66      0.68      0.67       161


Fold 3
[[ 1  7  0  0  0]
 [ 0 37 11  0  0]
 [ 0 10 47  8  0]
 [ 0  0 10 27  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       1.00      0.12      0.22         8
          A2       0.69      0.77      0.73        48
          B1       0.69      0.72      0.71        65
          B2       0.75      0.73      0.74        37
          C1       0.00      0.00      0.00         1

    accuracy                           0.70       159
   macro avg       0.63      0.47      0.48       159
weighted avg       0.71      0.70      0.69       159


Fold 4
[[ 1  4  2  0]
 [ 3 32 12  1]
 [ 0 13 41 11]
 [ 0  0 13 24]]
              precision    recall  f1-score   support

          A1       0.25      0.14      0.18         7
          A2       0.65      0.67      0.66        48
          B1       0.60      0.63      0.62        65
          B2       0.67      0.65      0.66        37

    accuracy                           0.62       157
   macro avg       0.54      0.52      0.53       157
weighted avg       0.62      0.62      0.62       157


K-fold scores
[0.6721725112818164, 0.6774305670352977, 0.6683929387747434, 0.6912642756434042, 0.6200432194823138]
SKF f1 score mean 0.6658607024435153

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
4765
4765
Fold 0
[[ 2  3  3  0]
 [ 4 33 11  1]
 [ 2 11 38 15]
 [ 0  4 13 21]]
              precision    recall  f1-score   support

          A1       0.25      0.25      0.25         8
          A2       0.65      0.67      0.66        49
          B1       0.58      0.58      0.58        66
          B2       0.57      0.55      0.56        38

    accuracy                           0.58       161
   macro avg       0.51      0.51      0.51       161
weighted avg       0.58      0.58      0.58       161


Fold 1
[[ 0  5  2  1  0]
 [ 2 27 19  1  0]
 [ 3 15 40  8  0]
 [ 0  2 12 24  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.55      0.55      0.55        49
          B1       0.55      0.61      0.58        66
          B2       0.69      0.63      0.66        38
          C1       0.00      0.00      0.00         1

    accuracy                           0.56       162
   macro avg       0.36      0.36      0.36       162
weighted avg       0.55      0.56      0.56       162


Fold 2
[[ 0  8  0  0]
 [ 3 30 12  3]
 [ 3 15 38 10]
 [ 1  1 13 23]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.56      0.61      0.58        49
          B1       0.60      0.58      0.59        66
          B2       0.64      0.61      0.62        38
          C1       0.00      0.00      0.00         0

    accuracy                           0.57       161
   macro avg       0.36      0.36      0.36       161
weighted avg       0.57      0.57      0.57       161


Fold 3
[[ 1  5  2  0  0]
 [ 2 35 10  1  0]
 [ 0 18 39  8  0]
 [ 0  3 14 20  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.33      0.12      0.18         8
          A2       0.57      0.73      0.64        48
          B1       0.60      0.60      0.60        65
          B2       0.67      0.54      0.60        37
          C1       0.00      0.00      0.00         1

    accuracy                           0.60       159
   macro avg       0.43      0.40      0.40       159
weighted avg       0.59      0.60      0.59       159


Fold 4
[[ 2  4  0  1]
 [ 1 34 11  2]
 [ 0 13 42 10]
 [ 0  2 14 21]]
              precision    recall  f1-score   support

          A1       0.67      0.29      0.40         7
          A2       0.64      0.71      0.67        48
          B1       0.63      0.65      0.64        65
          B2       0.62      0.57      0.59        37

    accuracy                           0.63       157
   macro avg       0.64      0.55      0.58       157
weighted avg       0.63      0.63      0.63       157


K-fold scores
[0.583292399601726, 0.5553821782756443, 0.565521937094674, 0.5872313570235591, 0.6265464457991506]
SKF f1 score mean 0.5835948635589508

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
4765
4765
Fold 0
[[ 1  4  3  0]
 [ 3 33 12  1]
 [ 2  9 41 14]
 [ 0  2 13 23]]
              precision    recall  f1-score   support

          A1       0.17      0.12      0.14         8
          A2       0.69      0.67      0.68        49
          B1       0.59      0.62      0.61        66
          B2       0.61      0.61      0.61        38

    accuracy                           0.61       161
   macro avg       0.51      0.51      0.51       161
weighted avg       0.60      0.61      0.61       161


Fold 1
[[ 1  4  3  0  0]
 [ 4 28 16  1  0]
 [ 4 12 42  8  0]
 [ 0  0 10 28  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.11      0.12      0.12         8
          A2       0.64      0.57      0.60        49
          B1       0.59      0.64      0.61        66
          B2       0.74      0.74      0.74        38
          C1       0.00      0.00      0.00         1

    accuracy                           0.61       162
   macro avg       0.42      0.41      0.41       162
weighted avg       0.61      0.61      0.61       162


Fold 2
[[ 0  8  0  0]
 [ 2 32 11  3]
 [ 1 14 40 11]
 [ 0  0 10 28]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.59      0.65      0.62        49
          B1       0.66      0.61      0.63        66
          B2       0.67      0.74      0.70        38
          C1       0.00      0.00      0.00         0

    accuracy                           0.62       161
   macro avg       0.38      0.40      0.39       161
weighted avg       0.61      0.62      0.61       161


Fold 3
[[ 2  4  2  0  0]
 [ 1 35 10  2  0]
 [ 0 15 42  8  0]
 [ 0  2 14 21  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.67      0.25      0.36         8
          A2       0.62      0.73      0.67        48
          B1       0.62      0.65      0.63        65
          B2       0.66      0.57      0.61        37
          C1       0.00      0.00      0.00         1

    accuracy                           0.63       159
   macro avg       0.51      0.44      0.46       159
weighted avg       0.63      0.63      0.62       159


Fold 4
[[ 2  4  0  1]
 [ 1 34 11  2]
 [ 0 13 42 10]
 [ 0  1 11 25]]
              precision    recall  f1-score   support

          A1       0.67      0.29      0.40         7
          A2       0.65      0.71      0.68        48
          B1       0.66      0.65      0.65        65
          B2       0.66      0.68      0.67        37

    accuracy                           0.66       157
   macro avg       0.66      0.58      0.60       157
weighted avg       0.66      0.66      0.65       157


K-fold scores
[0.6060369702938699, 0.610578432712011, 0.6125553111264215, 0.6213280121142476, 0.652434701031946]
SKF f1 score mean 0.6205866854556993

SAME LANG EVAL DONE FOR THIS LANG
Dep ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5018
5018
Fold 0
[[ 0  7  1  0]
 [ 1 39  9  0]
 [ 0 13 43 10]
 [ 0  1  8 29]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.65      0.80      0.72        49
          B1       0.70      0.65      0.68        66
          B2       0.74      0.76      0.75        38

    accuracy                           0.69       161
   macro avg       0.52      0.55      0.54       161
weighted avg       0.66      0.69      0.67       161


Fold 1
[[ 0  7  1  0  0]
 [ 1 35 13  0  0]
 [ 0 11 51  4  0]
 [ 0  0 14 24  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.66      0.71      0.69        49
          B1       0.65      0.77      0.70        66
          B2       0.83      0.63      0.72        38
          C1       0.00      0.00      0.00         1

    accuracy                           0.68       162
   macro avg       0.43      0.42      0.42       162
weighted avg       0.66      0.68      0.66       162


Fold 2
[[ 0  7  1  0]
 [ 0 38 11  0]
 [ 0 21 39  6]
 [ 0  0  9 29]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.58      0.78      0.66        49
          B1       0.65      0.59      0.62        66
          B2       0.83      0.76      0.79        38

    accuracy                           0.66       161
   macro avg       0.51      0.53      0.52       161
weighted avg       0.64      0.66      0.64       161


Fold 3
[[ 0  8  0  0  0]
 [ 0 36 12  0  0]
 [ 0 13 44  8  0]
 [ 0  0  9 28  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.63      0.75      0.69        48
          B1       0.68      0.68      0.68        65
          B2       0.76      0.76      0.76        37
          C1       0.00      0.00      0.00         1

    accuracy                           0.68       159
   macro avg       0.41      0.44      0.42       159
weighted avg       0.64      0.68      0.66       159


Fold 4
[[ 1  4  2  0]
 [ 4 30 14  0]
 [ 0 14 43  8]
 [ 0  0 12 25]]
              precision    recall  f1-score   support

          A1       0.20      0.14      0.17         7
          A2       0.62      0.62      0.62        48
          B1       0.61      0.66      0.63        65
          B2       0.76      0.68      0.71        37

    accuracy                           0.63       157
   macro avg       0.55      0.53      0.53       157
weighted avg       0.63      0.63      0.63       157


K-fold scores
[0.6731708707620474, 0.6622155418784174, 0.6424318781038065, 0.6598382749326145, 0.6286508233866795]
SKF f1 score mean 0.653261477812713

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5018
5018
Fold 0
[[ 0  7  0  1]
 [ 2 30 14  3]
 [ 2 13 37 14]
 [ 0  1 15 22]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.59      0.61      0.60        49
          B1       0.56      0.56      0.56        66
          B2       0.55      0.58      0.56        38

    accuracy                           0.55       161
   macro avg       0.42      0.44      0.43       161
weighted avg       0.54      0.55      0.55       161


Fold 1
[[ 2  4  2  0  0]
 [ 0 29 20  0  0]
 [ 0 18 40  8  0]
 [ 1  0 13 24  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.67      0.25      0.36         8
          A2       0.57      0.59      0.58        49
          B1       0.53      0.61      0.57        66
          B2       0.73      0.63      0.68        38
          C1       0.00      0.00      0.00         1

    accuracy                           0.59       162
   macro avg       0.50      0.42      0.44       162
weighted avg       0.59      0.59      0.58       162


Fold 2
[[ 1  5  2  0]
 [ 1 27 19  2]
 [ 0 21 38  7]
 [ 0  0 13 25]]
              precision    recall  f1-score   support

          A1       0.50      0.12      0.20         8
          A2       0.51      0.55      0.53        49
          B1       0.53      0.58      0.55        66
          B2       0.74      0.66      0.69        38

    accuracy                           0.57       161
   macro avg       0.57      0.48      0.49       161
weighted avg       0.57      0.57      0.56       161


Fold 3
[[ 1  5  1  1  0]
 [ 2 30 15  1  0]
 [ 2 15 30 18  0]
 [ 0  0 15 22  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.20      0.12      0.15         8
          A2       0.60      0.62      0.61        48
          B1       0.49      0.46      0.48        65
          B2       0.51      0.59      0.55        37
          C1       0.00      0.00      0.00         1

    accuracy                           0.52       159
   macro avg       0.36      0.36      0.36       159
weighted avg       0.51      0.52      0.52       159


Fold 4
[[ 2  3  2  0]
 [ 5 26 16  1]
 [ 1 14 40 10]
 [ 1  4 12 20]]
              precision    recall  f1-score   support

          A1       0.22      0.29      0.25         7
          A2       0.55      0.54      0.55        48
          B1       0.57      0.62      0.59        65
          B2       0.65      0.54      0.59        37

    accuracy                           0.56       157
   macro avg       0.50      0.50      0.49       157
weighted avg       0.57      0.56      0.56       157


K-fold scores
[0.545564580347189, 0.5831237053382693, 0.5607322450089046, 0.5152258194037169, 0.5624643860598585]
SKF f1 score mean 0.5534221472315877

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5018
5018
Fold 0
[[ 0  7  0  1]
 [ 2 31 13  3]
 [ 1 12 41 12]
 [ 0  1 14 23]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.61      0.63      0.62        49
          B1       0.60      0.62      0.61        66
          B2       0.59      0.61      0.60        38

    accuracy                           0.59       161
   macro avg       0.45      0.46      0.46       161
weighted avg       0.57      0.59      0.58       161


Fold 1
[[ 1  5  2  0  0]
 [ 0 32 17  0  0]
 [ 1 16 41  8  0]
 [ 0  0 12 26  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.50      0.12      0.20         8
          A2       0.60      0.65      0.63        49
          B1       0.57      0.62      0.59        66
          B2       0.74      0.68      0.71        38
          C1       0.00      0.00      0.00         1

    accuracy                           0.62       162
   macro avg       0.48      0.42      0.43       162
weighted avg       0.61      0.62      0.61       162


Fold 2
[[ 1  5  2  0]
 [ 1 26 19  3]
 [ 0 20 39  7]
 [ 0  0 12 26]]
              precision    recall  f1-score   support

          A1       0.50      0.12      0.20         8
          A2       0.51      0.53      0.52        49
          B1       0.54      0.59      0.57        66
          B2       0.72      0.68      0.70        38

    accuracy                           0.57       161
   macro avg       0.57      0.48      0.50       161
weighted avg       0.57      0.57      0.57       161


Fold 3
[[ 1  6  1  0  0]
 [ 1 33 12  2  0]
 [ 1 17 33 14  0]
 [ 0  1 12 24  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.33      0.12      0.18         8
          A2       0.58      0.69      0.63        48
          B1       0.57      0.51      0.54        65
          B2       0.59      0.65      0.62        37
          C1       0.00      0.00      0.00         1

    accuracy                           0.57       159
   macro avg       0.41      0.39      0.39       159
weighted avg       0.56      0.57      0.56       159


Fold 4
[[ 2  3  2  0]
 [ 5 26 16  1]
 [ 0 14 39 12]
 [ 0  3 12 22]]
              precision    recall  f1-score   support

          A1       0.29      0.29      0.29         7
          A2       0.57      0.54      0.55        48
          B1       0.57      0.60      0.58        65
          B2       0.63      0.59      0.61        37

    accuracy                           0.57       157
   macro avg       0.51      0.51      0.51       157
weighted avg       0.57      0.57      0.57       157


K-fold scores
[0.5805550211353493, 0.6088332252731384, 0.5657580778185693, 0.5614670036207081, 0.5668797674904154]
SKF f1 score mean 0.5766986190676361

SAME LANG EVAL DONE FOR THIS LANG
************for dimension:  Vocabularycontrol  ***************
Extracted all features: 
Printing class statistics
Counter({'B1': 327, 'A2': 204, 'B2': 196, 'A1': 70, 'C1': 3})
With Word ngrams: 
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2688
2688
Fold 0
[[ 7  6  1  0]
 [ 4 26 11  0]
 [ 1  9 39 17]
 [ 0  0 11 29]]
              precision    recall  f1-score   support

          A1       0.58      0.50      0.54        14
          A2       0.63      0.63      0.63        41
          B1       0.63      0.59      0.61        66
          B2       0.63      0.72      0.67        40

    accuracy                           0.63       161
   macro avg       0.62      0.61      0.61       161
weighted avg       0.63      0.63      0.63       161


Fold 1
[[ 3 10  1  0  0]
 [ 2 28 11  0  0]
 [ 1  8 48  9  0]
 [ 0  0 14 25  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.50      0.21      0.30        14
          A2       0.61      0.68      0.64        41
          B1       0.65      0.73      0.69        66
          B2       0.71      0.64      0.68        39
          C1       0.00      0.00      0.00         1

    accuracy                           0.65       161
   macro avg       0.49      0.45      0.46       161
weighted avg       0.64      0.65      0.63       161


Fold 2
[[ 5  7  2  0]
 [ 2 29 10  0]
 [ 0 20 30 15]
 [ 0  0 15 24]]
              precision    recall  f1-score   support

          A1       0.71      0.36      0.48        14
          A2       0.52      0.71      0.60        41
          B1       0.53      0.46      0.49        65
          B2       0.62      0.62      0.62        39

    accuracy                           0.55       159
   macro avg       0.59      0.54      0.55       159
weighted avg       0.56      0.55      0.55       159


Fold 3
[[ 2  8  4  0  0]
 [ 3 20 17  1  0]
 [ 1 10 43 11  0]
 [ 0  0 13 26  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.33      0.14      0.20        14
          A2       0.53      0.49      0.51        41
          B1       0.56      0.66      0.61        65
          B2       0.67      0.67      0.67        39
          C1       0.00      0.00      0.00         1

    accuracy                           0.57       160
   macro avg       0.42      0.39      0.40       160
weighted avg       0.55      0.57      0.56       160


Fold 4
[[ 1 12  1  0  0]
 [ 2 34  4  0  0]
 [ 1 14 41  9  0]
 [ 0  0 10 29  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.25      0.07      0.11        14
          A2       0.57      0.85      0.68        40
          B1       0.73      0.63      0.68        65
          B2       0.74      0.74      0.74        39
          C1       0.00      0.00      0.00         1

    accuracy                           0.66       159
   macro avg       0.46      0.46      0.44       159
weighted avg       0.65      0.66      0.64       159


K-fold scores
[0.6256767436304849, 0.6347782534546296, 0.5481090798675883, 0.5557855678374042, 0.6402839140403462]
SKF f1 score mean 0.6009267117660906

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2688
2688
Fold 0
[[ 8  4  2  0]
 [ 7 22 11  1]
 [ 3 11 37 15]
 [ 0  5 11 23]]
              precision    recall  f1-score   support

          A1       0.44      0.57      0.50        14
          A2       0.52      0.54      0.53        41
          B1       0.61      0.56      0.58        66
          B2       0.59      0.57      0.58        40
          C1       0.00      0.00      0.00         0

    accuracy                           0.56       161
   macro avg       0.43      0.45      0.44       161
weighted avg       0.57      0.56      0.56       161


Fold 1
[[ 6  6  2  0  0]
 [ 3 22 15  1  0]
 [ 6  9 39 12  0]
 [ 0  0 15 24  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.40      0.43      0.41        14
          A2       0.59      0.54      0.56        41
          B1       0.55      0.59      0.57        66
          B2       0.63      0.62      0.62        39
          C1       0.00      0.00      0.00         1

    accuracy                           0.57       161
   macro avg       0.44      0.43      0.43       161
weighted avg       0.56      0.57      0.56       161


Fold 2
[[ 5  7  2  0]
 [ 5 28  8  0]
 [ 2 19 30 14]
 [ 0  0 12 27]]
              precision    recall  f1-score   support

          A1       0.42      0.36      0.38        14
          A2       0.52      0.68      0.59        41
          B1       0.58      0.46      0.51        65
          B2       0.66      0.69      0.68        39

    accuracy                           0.57       159
   macro avg       0.54      0.55      0.54       159
weighted avg       0.57      0.57      0.56       159


Fold 3
[[ 2  8  3  1  0]
 [ 9 20  9  3  0]
 [ 0 15 36 14  0]
 [ 0  0 19 20  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.18      0.14      0.16        14
          A2       0.47      0.49      0.48        41
          B1       0.54      0.55      0.55        65
          B2       0.51      0.51      0.51        39
          C1       0.00      0.00      0.00         1

    accuracy                           0.49       160
   macro avg       0.34      0.34      0.34       160
weighted avg       0.48      0.49      0.48       160


Fold 4
[[ 3 10  1  0  0]
 [ 7 22 11  0  0]
 [ 2 12 40 11  0]
 [ 0  1 18 20  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.25      0.21      0.23        14
          A2       0.49      0.55      0.52        40
          B1       0.57      0.62      0.59        65
          B2       0.62      0.51      0.56        39
          C1       0.00      0.00      0.00         1

    accuracy                           0.53       159
   macro avg       0.39      0.38      0.38       159
weighted avg       0.53      0.53      0.53       159


K-fold scores
[0.5620047944902379, 0.5640350262356256, 0.5610777972992471, 0.4826147186147186, 0.5309874282273235]
SKF f1 score mean 0.5401439529734305

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2688
2688
Fold 0
[[ 7  5  2  0]
 [ 7 23 11  0]
 [ 3  9 38 16]
 [ 0  2 11 26]]
              precision    recall  f1-score   support

          A1       0.41      0.50      0.45        14
          A2       0.59      0.56      0.57        41
          B1       0.61      0.58      0.59        66
          B2       0.62      0.65      0.63        40
          C1       0.00      0.00      0.00         0

    accuracy                           0.58       161
   macro avg       0.45      0.46      0.45       161
weighted avg       0.59      0.58      0.59       161


Fold 1
[[ 5  7  2  0  0]
 [ 5 20 15  1  0]
 [ 6 10 39 11  0]
 [ 0  0 13 26  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.31      0.36      0.33        14
          A2       0.54      0.49      0.51        41
          B1       0.57      0.59      0.58        66
          B2       0.67      0.67      0.67        39
          C1       0.00      0.00      0.00         1

    accuracy                           0.56       161
   macro avg       0.42      0.42      0.42       161
weighted avg       0.56      0.56      0.56       161


Fold 2
[[ 4  6  4  0]
 [ 4 27 10  0]
 [ 2 15 32 16]
 [ 0  0 13 26]]
              precision    recall  f1-score   support

          A1       0.40      0.29      0.33        14
          A2       0.56      0.66      0.61        41
          B1       0.54      0.49      0.52        65
          B2       0.62      0.67      0.64        39

    accuracy                           0.56       159
   macro avg       0.53      0.53      0.52       159
weighted avg       0.55      0.56      0.55       159


Fold 3
[[ 3  7  3  1  0]
 [10 18 10  3  0]
 [ 0 12 41 12  0]
 [ 0  0 17 22  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.23      0.21      0.22        14
          A2       0.49      0.44      0.46        41
          B1       0.58      0.63      0.60        65
          B2       0.56      0.56      0.56        39
          C1       0.00      0.00      0.00         1

    accuracy                           0.53       160
   macro avg       0.37      0.37      0.37       160
weighted avg       0.52      0.53      0.52       160


Fold 4
[[ 3 10  1  0  0]
 [ 4 30  6  0  0]
 [ 3 12 41  9  0]
 [ 0  0 17 22  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.30      0.21      0.25        14
          A2       0.58      0.75      0.65        40
          B1       0.63      0.63      0.63        65
          B2       0.69      0.56      0.62        39
          C1       0.00      0.00      0.00         1

    accuracy                           0.60       159
   macro avg       0.44      0.43      0.43       159
weighted avg       0.60      0.60      0.60       159


K-fold scores
[0.5866517658614776, 0.5579232361841058, 0.5542672660054083, 0.5201585281548516, 0.5959495006682148]
SKF f1 score mean 0.5629900593748116

SAME LANG EVAL DONE FOR THIS LANG
With POS ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
4765
4765
Fold 0
[[ 3  7  4  0]
 [ 3 30  8  0]
 [ 0 11 34 21]
 [ 0  0 14 26]]
              precision    recall  f1-score   support

          A1       0.50      0.21      0.30        14
          A2       0.62      0.73      0.67        41
          B1       0.57      0.52      0.54        66
          B2       0.55      0.65      0.60        40

    accuracy                           0.58       161
   macro avg       0.56      0.53      0.53       161
weighted avg       0.57      0.58      0.57       161


Fold 1
[[ 2  9  3  0  0]
 [ 2 26 13  0  0]
 [ 0  8 49  9  0]
 [ 0  0 15 24  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.50      0.14      0.22        14
          A2       0.60      0.63      0.62        41
          B1       0.61      0.74      0.67        66
          B2       0.71      0.62      0.66        39
          C1       0.00      0.00      0.00         1

    accuracy                           0.63       161
   macro avg       0.48      0.43      0.43       161
weighted avg       0.62      0.63      0.61       161


Fold 2
[[ 2  7  5  0]
 [ 1 30 10  0]
 [ 1 19 28 17]
 [ 0  0 12 27]]
              precision    recall  f1-score   support

          A1       0.50      0.14      0.22        14
          A2       0.54      0.73      0.62        41
          B1       0.51      0.43      0.47        65
          B2       0.61      0.69      0.65        39

    accuracy                           0.55       159
   macro avg       0.54      0.50      0.49       159
weighted avg       0.54      0.55      0.53       159


Fold 3
[[ 1  8  5  0  0]
 [ 1 22 17  1  0]
 [ 0 12 39 14  0]
 [ 0  1 14 24  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.50      0.07      0.12        14
          A2       0.51      0.54      0.52        41
          B1       0.52      0.60      0.56        65
          B2       0.60      0.62      0.61        39
          C1       0.00      0.00      0.00         1

    accuracy                           0.54       160
   macro avg       0.43      0.36      0.36       160
weighted avg       0.53      0.54      0.52       160


Fold 4
[[ 1  9  4  0  0]
 [ 0 31  9  0  0]
 [ 1 15 41  8  0]
 [ 0  0 12 27  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.50      0.07      0.12        14
          A2       0.56      0.78      0.65        40
          B1       0.62      0.63      0.63        65
          B2       0.75      0.69      0.72        39
          C1       0.00      0.00      0.00         1

    accuracy                           0.63       159
   macro avg       0.49      0.43      0.42       159
weighted avg       0.62      0.63      0.61       159


K-fold scores
[0.5675002672935489, 0.61141160851873, 0.5294261834126799, 0.5196042420132609, 0.6076873337022523]
SKF f1 score mean 0.5671259269880944

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
4765
4765
Fold 0
[[ 1  7  6  0]
 [ 8 14 18  1]
 [ 1  9 39 17]
 [ 1  3 22 14]]
              precision    recall  f1-score   support

          A1       0.09      0.07      0.08        14
          A2       0.42      0.34      0.38        41
          B1       0.46      0.59      0.52        66
          B2       0.44      0.35      0.39        40

    accuracy                           0.42       161
   macro avg       0.35      0.34      0.34       161
weighted avg       0.41      0.42      0.41       161


Fold 1
[[ 2  6  6  0  0]
 [ 4 21 16  0  0]
 [ 3  9 38 16  0]
 [ 0  2 14 23  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.22      0.14      0.17        14
          A2       0.55      0.51      0.53        41
          B1       0.51      0.58      0.54        66
          B2       0.57      0.59      0.58        39
          C1       0.00      0.00      0.00         1

    accuracy                           0.52       161
   macro avg       0.37      0.36      0.37       161
weighted avg       0.51      0.52      0.51       161


Fold 2
[[ 6  7  1  0]
 [ 5 28  7  1]
 [ 1 23 24 17]
 [ 1  6 12 19]]
              precision    recall  f1-score   support

          A1       0.46      0.43      0.44        14
          A2       0.44      0.68      0.53        41
          B1       0.55      0.37      0.44        65
          B2       0.51      0.49      0.50        39
          C1       0.00      0.00      0.00         0

    accuracy                           0.48       159
   macro avg       0.39      0.39      0.38       159
weighted avg       0.50      0.48      0.48       159


Fold 3
[[ 1  9  4  0  0]
 [ 8 18 11  4  0]
 [ 5 14 28 18  0]
 [ 0  2 13 24  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.07      0.07      0.07        14
          A2       0.42      0.44      0.43        41
          B1       0.50      0.43      0.46        65
          B2       0.51      0.62      0.56        39
          C1       0.00      0.00      0.00         1

    accuracy                           0.44       160
   macro avg       0.30      0.31      0.30       160
weighted avg       0.44      0.44      0.44       160


Fold 4
[[ 1  7  6  0  0]
 [ 3 25 11  1  0]
 [ 4 23 25 13  0]
 [ 1  4 17 17  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.11      0.07      0.09        14
          A2       0.42      0.62      0.51        40
          B1       0.42      0.38      0.40        65
          B2       0.53      0.44      0.48        39
          C1       0.00      0.00      0.00         1

    accuracy                           0.43       159
   macro avg       0.30      0.30      0.29       159
weighted avg       0.42      0.43      0.42       159


K-fold scores
[0.4116881012475599, 0.5140974108769636, 0.4793254220119375, 0.44013446912495535, 0.41701349222484935]
SKF f1 score mean 0.45245177909725315

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
4765
4765
Fold 0
[[ 2  7  5  0]
 [ 4 19 16  2]
 [ 1 10 37 18]
 [ 0  2 22 16]]
              precision    recall  f1-score   support

          A1       0.29      0.14      0.19        14
          A2       0.50      0.46      0.48        41
          B1       0.46      0.56      0.51        66
          B2       0.44      0.40      0.42        40

    accuracy                           0.46       161
   macro avg       0.42      0.39      0.40       161
weighted avg       0.45      0.46      0.45       161


Fold 1
[[ 2  8  4  0  0]
 [ 5 22 14  0  0]
 [ 5 10 38 13  0]
 [ 0  2 16 21  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.17      0.14      0.15        14
          A2       0.52      0.54      0.53        41
          B1       0.53      0.58      0.55        66
          B2       0.60      0.54      0.57        39
          C1       0.00      0.00      0.00         1

    accuracy                           0.52       161
   macro avg       0.36      0.36      0.36       161
weighted avg       0.51      0.52      0.51       161


Fold 2
[[ 5  8  1  0]
 [ 4 26 10  1]
 [ 1 25 22 17]
 [ 1  3 13 21]]
              precision    recall  f1-score   support

          A1       0.45      0.36      0.40        14
          A2       0.42      0.63      0.50        41
          B1       0.48      0.34      0.40        65
          B2       0.54      0.54      0.54        39
          C1       0.00      0.00      0.00         0

    accuracy                           0.47       159
   macro avg       0.38      0.37      0.37       159
weighted avg       0.48      0.47      0.46       159


Fold 3
[[ 1  9  4  0  0]
 [ 7 19 11  4  0]
 [ 4 14 29 18  0]
 [ 0  2 14 23  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.08      0.07      0.08        14
          A2       0.43      0.46      0.45        41
          B1       0.50      0.45      0.47        65
          B2       0.50      0.59      0.54        39
          C1       0.00      0.00      0.00         1

    accuracy                           0.45       160
   macro avg       0.30      0.31      0.31       160
weighted avg       0.44      0.45      0.44       160


Fold 4
[[ 1  8  5  0  0]
 [ 3 24 11  2  0]
 [ 4 19 28 14  0]
 [ 1  4 13 21  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.11      0.07      0.09        14
          A2       0.44      0.60      0.51        40
          B1       0.49      0.43      0.46        65
          B2       0.55      0.54      0.55        39
          C1       0.00      0.00      0.00         1

    accuracy                           0.47       159
   macro avg       0.32      0.33      0.32       159
weighted avg       0.46      0.47      0.46       159


K-fold scores
[0.4514431410663781, 0.5116257586023227, 0.4595270118992413, 0.4447663981164699, 0.4562057262052519]
SKF f1 score mean 0.46471360717793286

SAME LANG EVAL DONE FOR THIS LANG
Dep ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5018
5018
Fold 0
[[ 1  9  4  0]
 [ 5 24 12  0]
 [ 1 10 35 20]
 [ 0  0 13 27]]
              precision    recall  f1-score   support

          A1       0.14      0.07      0.10        14
          A2       0.56      0.59      0.57        41
          B1       0.55      0.53      0.54        66
          B2       0.57      0.68      0.62        40

    accuracy                           0.54       161
   macro avg       0.46      0.47      0.46       161
weighted avg       0.52      0.54      0.53       161


Fold 1
[[ 1  9  4  0  0]
 [ 3 24 14  0  0]
 [ 0  9 48  9  0]
 [ 0  1 16 22  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.25      0.07      0.11        14
          A2       0.56      0.59      0.57        41
          B1       0.59      0.73      0.65        66
          B2       0.69      0.56      0.62        39
          C1       0.00      0.00      0.00         1

    accuracy                           0.59       161
   macro avg       0.42      0.39      0.39       161
weighted avg       0.57      0.59      0.57       161


Fold 2
[[ 2  8  4  0]
 [ 3 27 11  0]
 [ 2 18 32 13]
 [ 0  0 10 29]]
              precision    recall  f1-score   support

          A1       0.29      0.14      0.19        14
          A2       0.51      0.66      0.57        41
          B1       0.56      0.49      0.52        65
          B2       0.69      0.74      0.72        39

    accuracy                           0.57       159
   macro avg       0.51      0.51      0.50       159
weighted avg       0.56      0.57      0.55       159


Fold 3
[[ 2  7  5  0  0]
 [ 1 22 18  0  0]
 [ 0 14 37 14  0]
 [ 0  1 15 23  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.67      0.14      0.24        14
          A2       0.50      0.54      0.52        41
          B1       0.49      0.57      0.53        65
          B2       0.61      0.59      0.60        39
          C1       0.00      0.00      0.00         1

    accuracy                           0.53       160
   macro avg       0.45      0.37      0.38       160
weighted avg       0.53      0.53      0.51       160


Fold 4
[[ 2  9  3  0  0]
 [ 0 29 11  0  0]
 [ 1 13 43  8  0]
 [ 0  0 12 27  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.67      0.14      0.24        14
          A2       0.57      0.72      0.64        40
          B1       0.62      0.66      0.64        65
          B2       0.75      0.69      0.72        39
          C1       0.00      0.00      0.00         1

    accuracy                           0.64       159
   macro avg       0.52      0.44      0.45       159
weighted avg       0.64      0.64      0.62       159


K-fold scores
[0.5287450466289618, 0.5712046700586636, 0.5549946209917737, 0.513584320091673, 0.6200317047296356]
SKF f1 score mean 0.5577120725001415

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5018
5018
Fold 0
[[ 5  6  3  0]
 [ 5 20 13  3]
 [ 2 11 33 20]
 [ 0  3 20 17]]
              precision    recall  f1-score   support

          A1       0.42      0.36      0.38        14
          A2       0.50      0.49      0.49        41
          B1       0.48      0.50      0.49        66
          B2       0.42      0.42      0.42        40

    accuracy                           0.47       161
   macro avg       0.45      0.44      0.45       161
weighted avg       0.47      0.47      0.47       161


Fold 1
[[ 2 11  0  1  0]
 [ 6 20 14  1  0]
 [ 2 13 37 14  0]
 [ 0  4 14 21  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.20      0.14      0.17        14
          A2       0.42      0.49      0.45        41
          B1       0.57      0.56      0.56        66
          B2       0.55      0.54      0.55        39
          C1       0.00      0.00      0.00         1

    accuracy                           0.50       161
   macro avg       0.35      0.35      0.35       161
weighted avg       0.49      0.50      0.49       161


Fold 2
[[ 6  4  3  1]
 [ 4 25 10  2]
 [ 5 15 29 16]
 [ 0  1 20 18]]
              precision    recall  f1-score   support

          A1       0.40      0.43      0.41        14
          A2       0.56      0.61      0.58        41
          B1       0.47      0.45      0.46        65
          B2       0.49      0.46      0.47        39

    accuracy                           0.49       159
   macro avg       0.48      0.49      0.48       159
weighted avg       0.49      0.49      0.49       159


Fold 3
[[ 2  8  4  0  0]
 [ 8 16 17  0  0]
 [ 1 16 32 16  0]
 [ 0  3 18 18  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.18      0.14      0.16        14
          A2       0.37      0.39      0.38        41
          B1       0.45      0.49      0.47        65
          B2       0.51      0.46      0.49        39
          C1       0.00      0.00      0.00         1

    accuracy                           0.42       160
   macro avg       0.30      0.30      0.30       160
weighted avg       0.42      0.42      0.42       160


Fold 4
[[ 2  6  6  0  0]
 [ 1 25 12  2  0]
 [ 4 17 30 14  0]
 [ 0  0 23 16  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.29      0.14      0.19        14
          A2       0.52      0.62      0.57        40
          B1       0.42      0.46      0.44        65
          B2       0.48      0.41      0.44        39
          C1       0.00      0.00      0.00         1

    accuracy                           0.46       159
   macro avg       0.34      0.33      0.33       159
weighted avg       0.45      0.46      0.45       159


K-fold scores
[0.4652061840467637, 0.4926426686104785, 0.4892392222088459, 0.42137659928836396, 0.4490801466384152]
SKF f1 score mean 0.46350896415857346

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5018
5018
Fold 0
[[ 5  7  2  0]
 [ 4 18 16  3]
 [ 1  8 36 21]
 [ 0  2 18 20]]
              precision    recall  f1-score   support

          A1       0.50      0.36      0.42        14
          A2       0.51      0.44      0.47        41
          B1       0.50      0.55      0.52        66
          B2       0.45      0.50      0.48        40

    accuracy                           0.49       161
   macro avg       0.49      0.46      0.47       161
weighted avg       0.49      0.49      0.49       161


Fold 1
[[ 2 10  0  2  0]
 [ 5 23 13  0  0]
 [ 2 12 41 11  0]
 [ 0  1 16 22  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.22      0.14      0.17        14
          A2       0.50      0.56      0.53        41
          B1       0.59      0.62      0.60        66
          B2       0.61      0.56      0.59        39
          C1       0.00      0.00      0.00         1

    accuracy                           0.55       161
   macro avg       0.38      0.38      0.38       161
weighted avg       0.53      0.55      0.54       161


Fold 2
[[ 6  4  3  1]
 [ 6 26  8  1]
 [ 5 16 25 19]
 [ 0  1 19 19]]
              precision    recall  f1-score   support

          A1       0.35      0.43      0.39        14
          A2       0.55      0.63      0.59        41
          B1       0.45      0.38      0.42        65
          B2       0.47      0.49      0.48        39

    accuracy                           0.48       159
   macro avg       0.46      0.48      0.47       159
weighted avg       0.48      0.48      0.47       159


Fold 3
[[ 2  8  4  0  0]
 [ 8 18 14  1  0]
 [ 0 16 31 18  0]
 [ 0  2 17 20  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.20      0.14      0.17        14
          A2       0.41      0.44      0.42        41
          B1       0.47      0.48      0.47        65
          B2       0.50      0.51      0.51        39
          C1       0.00      0.00      0.00         1

    accuracy                           0.44       160
   macro avg       0.32      0.31      0.31       160
weighted avg       0.44      0.44      0.44       160


Fold 4
[[ 1  8  5  0  0]
 [ 1 24 12  3  0]
 [ 3 14 35 13  0]
 [ 0  1 21 17  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.20      0.07      0.11        14
          A2       0.51      0.60      0.55        40
          B1       0.48      0.54      0.51        65
          B2       0.50      0.44      0.47        39
          C1       0.00      0.00      0.00         1

    accuracy                           0.48       159
   macro avg       0.34      0.33      0.33       159
weighted avg       0.46      0.48      0.47       159


K-fold scores
[0.48904837031818, 0.5390500694117683, 0.47477644383774725, 0.43880145898343875, 0.46967325649160474]
SKF f1 score mean 0.4822699198085478

SAME LANG EVAL DONE FOR THIS LANG
************for dimension:  CoherenceCohesion  ***************
Extracted all features: 
Printing class statistics
Counter({'A2': 334, 'B1': 300, 'A1': 109, 'B2': 57})
With Word ngrams: 
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2688
2688
Fold 0
[[ 3 19  0  0]
 [ 0 56 11  0]
 [ 0 12 48  0]
 [ 0  0 12  0]]
              precision    recall  f1-score   support

          A1       1.00      0.14      0.24        22
          A2       0.64      0.84      0.73        67
          B1       0.68      0.80      0.73        60
          B2       0.00      0.00      0.00        12

    accuracy                           0.66       161
   macro avg       0.58      0.44      0.43       161
weighted avg       0.66      0.66      0.61       161


Fold 1
[[ 7 14  1  0]
 [ 4 57  6  0]
 [ 0 10 50  0]
 [ 0  1 11  0]]
              precision    recall  f1-score   support

          A1       0.64      0.32      0.42        22
          A2       0.70      0.85      0.77        67
          B1       0.74      0.83      0.78        60
          B2       0.00      0.00      0.00        12

    accuracy                           0.71       161
   macro avg       0.52      0.50      0.49       161
weighted avg       0.65      0.71      0.67       161


Fold 2
[[ 9 13  0  0]
 [ 4 55  8  0]
 [ 0  8 52  0]
 [ 0  0 11  0]]
              precision    recall  f1-score   support

          A1       0.69      0.41      0.51        22
          A2       0.72      0.82      0.77        67
          B1       0.73      0.87      0.79        60
          B2       0.00      0.00      0.00        11

    accuracy                           0.73       160
   macro avg       0.54      0.52      0.52       160
weighted avg       0.67      0.72      0.69       160


Fold 3
[[ 4 18  0  0]
 [ 1 58  8  0]
 [ 0 15 45  0]
 [ 0  0 11  0]]
              precision    recall  f1-score   support

          A1       0.80      0.18      0.30        22
          A2       0.64      0.87      0.73        67
          B1       0.70      0.75      0.73        60
          B2       0.00      0.00      0.00        11

    accuracy                           0.67       160
   macro avg       0.54      0.45      0.44       160
weighted avg       0.64      0.67      0.62       160


Fold 4
[[ 5 16  0  0]
 [ 2 52 12  0]
 [ 0 10 50  0]
 [ 0  1  9  1]]
              precision    recall  f1-score   support

          A1       0.71      0.24      0.36        21
          A2       0.66      0.79      0.72        66
          B1       0.70      0.83      0.76        60
          B2       1.00      0.09      0.17        11

    accuracy                           0.68       158
   macro avg       0.77      0.49      0.50       158
weighted avg       0.71      0.68      0.65       158


K-fold scores
[0.6085511700380603, 0.6675160142843247, 0.6905395939937925, 0.620354868956339, 0.648561968894768]
SKF f1 score mean 0.6471047232334568

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2688
2688
Fold 0
[[10 11  1  0]
 [ 8 42 17  0]
 [ 0 18 34  8]
 [ 0  0  8  4]]
              precision    recall  f1-score   support

          A1       0.56      0.45      0.50        22
          A2       0.59      0.63      0.61        67
          B1       0.57      0.57      0.57        60
          B2       0.33      0.33      0.33        12

    accuracy                           0.56       161
   macro avg       0.51      0.50      0.50       161
weighted avg       0.56      0.56      0.56       161


Fold 1
[[10 11  1  0]
 [11 47  9  0]
 [ 1 16 36  7]
 [ 1  0  9  2]]
              precision    recall  f1-score   support

          A1       0.43      0.45      0.44        22
          A2       0.64      0.70      0.67        67
          B1       0.65      0.60      0.63        60
          B2       0.22      0.17      0.19        12

    accuracy                           0.59       161
   macro avg       0.49      0.48      0.48       161
weighted avg       0.58      0.59      0.59       161


Fold 2
[[12  9  1  0]
 [15 38 13  1]
 [ 0 11 48  1]
 [ 0  0  8  3]]
              precision    recall  f1-score   support

          A1       0.44      0.55      0.49        22
          A2       0.66      0.57      0.61        67
          B1       0.69      0.80      0.74        60
          B2       0.60      0.27      0.37        11

    accuracy                           0.63       160
   macro avg       0.60      0.55      0.55       160
weighted avg       0.63      0.63      0.62       160


Fold 3
[[ 7 14  1  0]
 [12 47  8  0]
 [ 0 22 36  2]
 [ 0  1  8  2]]
              precision    recall  f1-score   support

          A1       0.37      0.32      0.34        22
          A2       0.56      0.70      0.62        67
          B1       0.68      0.60      0.64        60
          B2       0.50      0.18      0.27        11

    accuracy                           0.57       160
   macro avg       0.53      0.45      0.47       160
weighted avg       0.57      0.57      0.56       160


Fold 4
[[11  8  2  0]
 [10 42 14  0]
 [ 0 14 40  6]
 [ 0  1  8  2]]
              precision    recall  f1-score   support

          A1       0.52      0.52      0.52        21
          A2       0.65      0.64      0.64        66
          B1       0.62      0.67      0.65        60
          B2       0.25      0.18      0.21        11

    accuracy                           0.60       158
   macro avg       0.51      0.50      0.51       158
weighted avg       0.59      0.60      0.60       158


K-fold scores
[0.55765595463138, 0.5856855659718204, 0.6246512656985871, 0.5649014138898935, 0.5971270732915314]
SKF f1 score mean 0.5860042546966425

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2688
2688
Fold 0
[[10 11  1  0]
 [ 8 43 16  0]
 [ 0 13 40  7]
 [ 0  0  8  4]]
              precision    recall  f1-score   support

          A1       0.56      0.45      0.50        22
          A2       0.64      0.64      0.64        67
          B1       0.62      0.67      0.64        60
          B2       0.36      0.33      0.35        12

    accuracy                           0.60       161
   macro avg       0.54      0.52      0.53       161
weighted avg       0.60      0.60      0.60       161


Fold 1
[[10 10  2  0]
 [10 47 10  0]
 [ 1 15 38  6]
 [ 1  0  9  2]]
              precision    recall  f1-score   support

          A1       0.45      0.45      0.45        22
          A2       0.65      0.70      0.68        67
          B1       0.64      0.63      0.64        60
          B2       0.25      0.17      0.20        12

    accuracy                           0.60       161
   macro avg       0.50      0.49      0.49       161
weighted avg       0.59      0.60      0.60       161


Fold 2
[[11 10  1  0]
 [15 37 15  0]
 [ 0  7 51  2]
 [ 0  0  8  3]]
              precision    recall  f1-score   support

          A1       0.42      0.50      0.46        22
          A2       0.69      0.55      0.61        67
          B1       0.68      0.85      0.76        60
          B2       0.60      0.27      0.37        11

    accuracy                           0.64       160
   macro avg       0.60      0.54      0.55       160
weighted avg       0.64      0.64      0.63       160


Fold 3
[[ 6 15  1  0]
 [11 50  6  0]
 [ 0 15 43  2]
 [ 0  0  9  2]]
              precision    recall  f1-score   support

          A1       0.35      0.27      0.31        22
          A2       0.62      0.75      0.68        67
          B1       0.73      0.72      0.72        60
          B2       0.50      0.18      0.27        11

    accuracy                           0.63       160
   macro avg       0.55      0.48      0.49       160
weighted avg       0.62      0.63      0.62       160


Fold 4
[[ 9  9  3  0]
 [ 9 44 13  0]
 [ 1 12 41  6]
 [ 0  0  8  3]]
              precision    recall  f1-score   support

          A1       0.47      0.43      0.45        21
          A2       0.68      0.67      0.67        66
          B1       0.63      0.68      0.66        60
          B2       0.33      0.27      0.30        11

    accuracy                           0.61       158
   macro avg       0.53      0.51      0.52       158
weighted avg       0.61      0.61      0.61       158


K-fold scores
[0.5998379692141508, 0.5964514301184973, 0.6282304579889807, 0.6165133745806015, 0.6104169484974393]
SKF f1 score mean 0.6102900360799339

SAME LANG EVAL DONE FOR THIS LANG
With POS ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
4765
4765
Fold 0
[[ 2 20  0  0]
 [ 0 54 13  0]
 [ 0 12 48  0]
 [ 0  0 12  0]]
              precision    recall  f1-score   support

          A1       1.00      0.09      0.17        22
          A2       0.63      0.81      0.71        67
          B1       0.66      0.80      0.72        60
          B2       0.00      0.00      0.00        12

    accuracy                           0.65       161
   macro avg       0.57      0.42      0.40       161
weighted avg       0.64      0.65      0.59       161


Fold 1
[[ 3 17  2  0]
 [ 3 58  6  0]
 [ 0 10 50  0]
 [ 0  1 11  0]]
              precision    recall  f1-score   support

          A1       0.50      0.14      0.21        22
          A2       0.67      0.87      0.76        67
          B1       0.72      0.83      0.78        60
          B2       0.00      0.00      0.00        12

    accuracy                           0.69       161
   macro avg       0.47      0.46      0.44       161
weighted avg       0.62      0.69      0.63       161


Fold 2
[[ 4 18  0  0]
 [ 4 56  7  0]
 [ 0 10 50  0]
 [ 0  0 11  0]]
              precision    recall  f1-score   support

          A1       0.50      0.18      0.27        22
          A2       0.67      0.84      0.74        67
          B1       0.74      0.83      0.78        60
          B2       0.00      0.00      0.00        11

    accuracy                           0.69       160
   macro avg       0.48      0.46      0.45       160
weighted avg       0.62      0.69      0.64       160


Fold 3
[[ 3 18  1  0]
 [ 3 57  7  0]
 [ 0 13 47  0]
 [ 0  0 11  0]]
              precision    recall  f1-score   support

          A1       0.50      0.14      0.21        22
          A2       0.65      0.85      0.74        67
          B1       0.71      0.78      0.75        60
          B2       0.00      0.00      0.00        11

    accuracy                           0.67       160
   macro avg       0.46      0.44      0.42       160
weighted avg       0.61      0.67      0.62       160


Fold 4
[[ 2 17  2  0]
 [ 5 51 10  0]
 [ 0  9 51  0]
 [ 0  0 11  0]]
              precision    recall  f1-score   support

          A1       0.29      0.10      0.14        21
          A2       0.66      0.77      0.71        66
          B1       0.69      0.85      0.76        60
          B2       0.00      0.00      0.00        11

    accuracy                           0.66       158
   macro avg       0.41      0.43      0.40       158
weighted avg       0.58      0.66      0.61       158


K-fold scores
[0.5855220806858211, 0.6336850884607298, 0.6402314431567329, 0.6172100614439323, 0.6060035751137206]
SKF f1 score mean 0.6165304497721873

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
4765
4765
Fold 0
[[ 3 19  0  0]
 [ 8 47 11  1]
 [ 3 17 35  5]
 [ 0  1  9  2]]
              precision    recall  f1-score   support

          A1       0.21      0.14      0.17        22
          A2       0.56      0.70      0.62        67
          B1       0.64      0.58      0.61        60
          B2       0.25      0.17      0.20        12

    accuracy                           0.54       161
   macro avg       0.42      0.40      0.40       161
weighted avg       0.52      0.54      0.52       161


Fold 1
[[ 5 16  1  0]
 [12 47  8  0]
 [ 1 11 47  1]
 [ 0  2  8  2]]
              precision    recall  f1-score   support

          A1       0.28      0.23      0.25        22
          A2       0.62      0.70      0.66        67
          B1       0.73      0.78      0.76        60
          B2       0.67      0.17      0.27        12

    accuracy                           0.63       161
   macro avg       0.57      0.47      0.48       161
weighted avg       0.62      0.63      0.61       161


Fold 2
[[ 5 16  1  0]
 [ 9 42 15  1]
 [ 2 15 39  4]
 [ 0  3  8  0]]
              precision    recall  f1-score   support

          A1       0.31      0.23      0.26        22
          A2       0.55      0.63      0.59        67
          B1       0.62      0.65      0.63        60
          B2       0.00      0.00      0.00        11

    accuracy                           0.54       160
   macro avg       0.37      0.38      0.37       160
weighted avg       0.51      0.54      0.52       160


Fold 3
[[ 9 10  3  0]
 [15 44  8  0]
 [ 4 13 39  4]
 [ 0  0  9  2]]
              precision    recall  f1-score   support

          A1       0.32      0.41      0.36        22
          A2       0.66      0.66      0.66        67
          B1       0.66      0.65      0.66        60
          B2       0.33      0.18      0.24        11

    accuracy                           0.59       160
   macro avg       0.49      0.47      0.48       160
weighted avg       0.59      0.59      0.59       160


Fold 4
[[ 5 14  2  0]
 [14 38 13  1]
 [ 0 15 39  6]
 [ 0  1  8  2]]
              precision    recall  f1-score   support

          A1       0.26      0.24      0.25        21
          A2       0.56      0.58      0.57        66
          B1       0.63      0.65      0.64        60
          B2       0.22      0.18      0.20        11

    accuracy                           0.53       158
   macro avg       0.42      0.41      0.41       158
weighted avg       0.52      0.53      0.53       158


K-fold scores
[0.5235839445256188, 0.6100983168304347, 0.5199681095541172, 0.5864747899159665, 0.5268575415101293]
SKF f1 score mean 0.5533965404672533

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
4765
4765
Fold 0
[[ 4 17  1  0]
 [ 4 51 11  1]
 [ 3 14 37  6]
 [ 0  1  9  2]]
              precision    recall  f1-score   support

          A1       0.36      0.18      0.24        22
          A2       0.61      0.76      0.68        67
          B1       0.64      0.62      0.63        60
          B2       0.22      0.17      0.19        12

    accuracy                           0.58       161
   macro avg       0.46      0.43      0.44       161
weighted avg       0.56      0.58      0.56       161


Fold 1
[[ 4 17  1  0]
 [11 45 11  0]
 [ 1 11 47  1]
 [ 0  1  9  2]]
              precision    recall  f1-score   support

          A1       0.25      0.18      0.21        22
          A2       0.61      0.67      0.64        67
          B1       0.69      0.78      0.73        60
          B2       0.67      0.17      0.27        12

    accuracy                           0.61       161
   macro avg       0.55      0.45      0.46       161
weighted avg       0.59      0.61      0.59       161


Fold 2
[[ 5 17  0  0]
 [ 9 43 13  2]
 [ 1 12 42  5]
 [ 0  2  8  1]]
              precision    recall  f1-score   support

          A1       0.33      0.23      0.27        22
          A2       0.58      0.64      0.61        67
          B1       0.67      0.70      0.68        60
          B2       0.12      0.09      0.11        11

    accuracy                           0.57       160
   macro avg       0.43      0.41      0.42       160
weighted avg       0.55      0.57      0.56       160


Fold 3
[[ 7 11  4  0]
 [14 48  4  1]
 [ 3 11 42  4]
 [ 0  0  9  2]]
              precision    recall  f1-score   support

          A1       0.29      0.32      0.30        22
          A2       0.69      0.72      0.70        67
          B1       0.71      0.70      0.71        60
          B2       0.29      0.18      0.22        11

    accuracy                           0.62       160
   macro avg       0.49      0.48      0.48       160
weighted avg       0.61      0.62      0.62       160


Fold 4
[[ 5 14  2  0]
 [13 40 13  0]
 [ 0 15 40  5]
 [ 0  0  9  2]]
              precision    recall  f1-score   support

          A1       0.28      0.24      0.26        21
          A2       0.58      0.61      0.59        66
          B1       0.62      0.67      0.65        60
          B2       0.29      0.18      0.22        11

    accuracy                           0.55       158
   macro avg       0.44      0.42      0.43       158
weighted avg       0.54      0.55      0.54       158


K-fold scores
[0.5640134550504063, 0.5879505366097947, 0.5559043666614748, 0.615262143151982, 0.5420876478450999]
SKF f1 score mean 0.5730436298637516

SAME LANG EVAL DONE FOR THIS LANG
Dep ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5018
5018
Fold 0
[[ 2 20  0  0]
 [ 0 55 12  0]
 [ 0 16 44  0]
 [ 0  0 12  0]]
              precision    recall  f1-score   support

          A1       1.00      0.09      0.17        22
          A2       0.60      0.82      0.70        67
          B1       0.65      0.73      0.69        60
          B2       0.00      0.00      0.00        12

    accuracy                           0.63       161
   macro avg       0.56      0.41      0.39       161
weighted avg       0.63      0.63      0.57       161


Fold 1
[[ 4 17  1  0]
 [ 5 57  5  0]
 [ 0 13 47  0]
 [ 0  1 11  0]]
              precision    recall  f1-score   support

          A1       0.44      0.18      0.26        22
          A2       0.65      0.85      0.74        67
          B1       0.73      0.78      0.76        60
          B2       0.00      0.00      0.00        12

    accuracy                           0.67       161
   macro avg       0.46      0.45      0.44       161
weighted avg       0.60      0.67      0.62       161


Fold 2
[[ 3 19  0  0]
 [ 3 56  8  0]
 [ 0 10 50  0]
 [ 0  0 11  0]]
              precision    recall  f1-score   support

          A1       0.50      0.14      0.21        22
          A2       0.66      0.84      0.74        67
          B1       0.72      0.83      0.78        60
          B2       0.00      0.00      0.00        11

    accuracy                           0.68       160
   macro avg       0.47      0.45      0.43       160
weighted avg       0.62      0.68      0.63       160


Fold 3
[[ 3 19  0  0]
 [ 2 58  7  0]
 [ 0 16 44  0]
 [ 0  0 11  0]]
              precision    recall  f1-score   support

          A1       0.60      0.14      0.22        22
          A2       0.62      0.87      0.72        67
          B1       0.71      0.73      0.72        60
          B2       0.00      0.00      0.00        11

    accuracy                           0.66       160
   macro avg       0.48      0.43      0.42       160
weighted avg       0.61      0.66      0.60       160


Fold 4
[[ 1 19  1  0]
 [ 5 53  8  0]
 [ 0 12 48  0]
 [ 0  1 10  0]]
              precision    recall  f1-score   support

          A1       0.17      0.05      0.07        21
          A2       0.62      0.80      0.70        66
          B1       0.72      0.80      0.76        60
          B2       0.00      0.00      0.00        11

    accuracy                           0.65       158
   macro avg       0.38      0.41      0.38       158
weighted avg       0.55      0.65      0.59       158


K-fold scores
[0.5687095421547815, 0.6238429172510518, 0.6287145917118377, 0.604641108834244, 0.5901329879242503]
SKF f1 score mean 0.6032082295752332

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5018
5018
Fold 0
[[ 5 16  1  0]
 [ 6 51 10  0]
 [ 4 19 34  3]
 [ 0  1 10  1]]
              precision    recall  f1-score   support

          A1       0.33      0.23      0.27        22
          A2       0.59      0.76      0.66        67
          B1       0.62      0.57      0.59        60
          B2       0.25      0.08      0.12        12

    accuracy                           0.57       161
   macro avg       0.45      0.41      0.41       161
weighted avg       0.54      0.57      0.54       161


Fold 1
[[ 5 16  0  1]
 [12 44 10  1]
 [ 1 19 37  3]
 [ 0  2  7  3]]
              precision    recall  f1-score   support

          A1       0.28      0.23      0.25        22
          A2       0.54      0.66      0.59        67
          B1       0.69      0.62      0.65        60
          B2       0.38      0.25      0.30        12

    accuracy                           0.55       161
   macro avg       0.47      0.44      0.45       161
weighted avg       0.55      0.55      0.55       161


Fold 2
[[ 1 19  2  0]
 [10 39 16  2]
 [ 2 10 44  4]
 [ 0  0 10  1]]
              precision    recall  f1-score   support

          A1       0.08      0.05      0.06        22
          A2       0.57      0.58      0.58        67
          B1       0.61      0.73      0.67        60
          B2       0.14      0.09      0.11        11

    accuracy                           0.53       160
   macro avg       0.35      0.36      0.35       160
weighted avg       0.49      0.53      0.51       160


Fold 3
[[10 11  1  0]
 [12 42 13  0]
 [ 5 15 38  2]
 [ 1  0  8  2]]
              precision    recall  f1-score   support

          A1       0.36      0.45      0.40        22
          A2       0.62      0.63      0.62        67
          B1       0.63      0.63      0.63        60
          B2       0.50      0.18      0.27        11

    accuracy                           0.57       160
   macro avg       0.53      0.47      0.48       160
weighted avg       0.58      0.57      0.57       160


Fold 4
[[ 5 15  1  0]
 [13 42 11  0]
 [ 2 15 41  2]
 [ 0  2  7  2]]
              precision    recall  f1-score   support

          A1       0.25      0.24      0.24        21
          A2       0.57      0.64      0.60        66
          B1       0.68      0.68      0.68        60
          B2       0.50      0.18      0.27        11

    accuracy                           0.57       158
   macro avg       0.50      0.43      0.45       158
weighted avg       0.56      0.57      0.56       158


K-fold scores
[0.5422411813176058, 0.5458708463285122, 0.5074404761904763, 0.571388888888889, 0.5611093959040857]
SKF f1 score mean 0.5456101577259138

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5018
5018
Fold 0
[[ 4 17  1  0]
 [ 4 51 12  0]
 [ 3 18 36  3]
 [ 0  1 10  1]]
              precision    recall  f1-score   support

          A1       0.36      0.18      0.24        22
          A2       0.59      0.76      0.66        67
          B1       0.61      0.60      0.61        60
          B2       0.25      0.08      0.12        12

    accuracy                           0.57       161
   macro avg       0.45      0.41      0.41       161
weighted avg       0.54      0.57      0.54       161


Fold 1
[[ 5 16  0  1]
 [11 46  9  1]
 [ 0 19 37  4]
 [ 0  2  7  3]]
              precision    recall  f1-score   support

          A1       0.31      0.23      0.26        22
          A2       0.55      0.69      0.61        67
          B1       0.70      0.62      0.65        60
          B2       0.33      0.25      0.29        12

    accuracy                           0.57       161
   macro avg       0.47      0.45      0.45       161
weighted avg       0.56      0.57      0.56       161


Fold 2
[[ 3 17  2  0]
 [10 40 15  2]
 [ 1 11 45  3]
 [ 0  0 10  1]]
              precision    recall  f1-score   support

          A1       0.21      0.14      0.17        22
          A2       0.59      0.60      0.59        67
          B1       0.62      0.75      0.68        60
          B2       0.17      0.09      0.12        11

    accuracy                           0.56       160
   macro avg       0.40      0.39      0.39       160
weighted avg       0.52      0.56      0.53       160


Fold 3
[[10 11  1  0]
 [ 9 43 14  1]
 [ 4 15 38  3]
 [ 1  0  8  2]]
              precision    recall  f1-score   support

          A1       0.42      0.45      0.43        22
          A2       0.62      0.64      0.63        67
          B1       0.62      0.63      0.63        60
          B2       0.33      0.18      0.24        11

    accuracy                           0.58       160
   macro avg       0.50      0.48      0.48       160
weighted avg       0.57      0.58      0.58       160


Fold 4
[[ 5 14  2  0]
 [12 43 11  0]
 [ 2 14 42  2]
 [ 0  2  6  3]]
              precision    recall  f1-score   support

          A1       0.26      0.24      0.25        21
          A2       0.59      0.65      0.62        66
          B1       0.69      0.70      0.69        60
          B2       0.60      0.27      0.37        11

    accuracy                           0.59       158
   macro avg       0.54      0.47      0.48       158
weighted avg       0.58      0.59      0.58       158


K-fold scores
[0.543555762225839, 0.55654294313258, 0.5348348682907507, 0.5762940634841792, 0.5814077527600264]
SKF f1 score mean 0.5585270779786751

SAME LANG EVAL DONE FOR THIS LANG
************for dimension:  Sociolinguisticappropriateness  ***************
Extracted all features: 
Printing class statistics
Counter({'B1': 372, 'A2': 336, 'A1': 56, 'B2': 36})
With Word ngrams: 
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2688
2688
Fold 0
[[ 0 11  1  0]
 [ 1 52 15  0]
 [ 0 14 61  0]
 [ 0  0  8  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00        12
          A2       0.68      0.76      0.72        68
          B1       0.72      0.81      0.76        75
          B2       0.00      0.00      0.00         8

    accuracy                           0.69       163
   macro avg       0.35      0.39      0.37       163
weighted avg       0.61      0.69      0.65       163


Fold 1
[[ 0 11  0  0]
 [ 0 55 12  0]
 [ 0  5 70  0]
 [ 0  0  7  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00        11
          A2       0.77      0.82      0.80        67
          B1       0.79      0.93      0.85        75
          B2       0.00      0.00      0.00         7

    accuracy                           0.78       160
   macro avg       0.39      0.44      0.41       160
weighted avg       0.69      0.78      0.73       160


Fold 2
[[ 1  9  1  0]
 [ 1 53 13  0]
 [ 0  6 68  0]
 [ 0  0  7  0]]
              precision    recall  f1-score   support

          A1       0.50      0.09      0.15        11
          A2       0.78      0.79      0.79        67
          B1       0.76      0.92      0.83        74
          B2       0.00      0.00      0.00         7

    accuracy                           0.77       159
   macro avg       0.51      0.45      0.44       159
weighted avg       0.72      0.77      0.73       159


Fold 3
[[ 0 10  1  0]
 [ 1 44 22  0]
 [ 0 13 61  0]
 [ 0  1  6  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00        11
          A2       0.65      0.66      0.65        67
          B1       0.68      0.82      0.74        74
          B2       0.00      0.00      0.00         7

    accuracy                           0.66       159
   macro avg       0.33      0.37      0.35       159
weighted avg       0.59      0.66      0.62       159


Fold 4
[[ 0 11  0  0]
 [ 0 53 14  0]
 [ 0 12 62  0]
 [ 0  0  7  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00        11
          A2       0.70      0.79      0.74        67
          B1       0.75      0.84      0.79        74
          B2       0.00      0.00      0.00         7

    accuracy                           0.72       159
   macro avg       0.36      0.41      0.38       159
weighted avg       0.64      0.72      0.68       159


K-fold scores
[0.6500608208165856, 0.7339386709084483, 0.7298241911198657, 0.6208984563640186, 0.6799383367103594]
SKF f1 score mean 0.6829320951838556

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2688
2688
Fold 0
[[ 0 11  1  0]
 [13 44 10  1]
 [ 2 19 50  4]
 [ 0  0  7  1]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00        12
          A2       0.59      0.65      0.62        68
          B1       0.74      0.67      0.70        75
          B2       0.17      0.12      0.14         8

    accuracy                           0.58       163
   macro avg       0.37      0.36      0.37       163
weighted avg       0.59      0.58      0.59       163


Fold 1
[[ 2  9  0  0]
 [ 3 47 17  0]
 [ 3 16 50  6]
 [ 0  0  7  0]]
              precision    recall  f1-score   support

          A1       0.25      0.18      0.21        11
          A2       0.65      0.70      0.68        67
          B1       0.68      0.67      0.67        75
          B2       0.00      0.00      0.00         7

    accuracy                           0.62       160
   macro avg       0.39      0.39      0.39       160
weighted avg       0.61      0.62      0.61       160


Fold 2
[[ 3  7  1  0]
 [ 8 41 18  0]
 [ 0 16 53  5]
 [ 0  0  7  0]]
              precision    recall  f1-score   support

          A1       0.27      0.27      0.27        11
          A2       0.64      0.61      0.63        67
          B1       0.67      0.72      0.69        74
          B2       0.00      0.00      0.00         7

    accuracy                           0.61       159
   macro avg       0.40      0.40      0.40       159
weighted avg       0.60      0.61      0.61       159


Fold 3
[[ 5  5  1  0]
 [ 5 43 18  1]
 [ 0 18 54  2]
 [ 0  2  4  1]]
              precision    recall  f1-score   support

          A1       0.50      0.45      0.48        11
          A2       0.63      0.64      0.64        67
          B1       0.70      0.73      0.72        74
          B2       0.25      0.14      0.18         7

    accuracy                           0.65       159
   macro avg       0.52      0.49      0.50       159
weighted avg       0.64      0.65      0.64       159


Fold 4
[[ 3  7  1  0]
 [ 6 49 12  0]
 [ 0 24 48  2]
 [ 0  0  6  1]]
              precision    recall  f1-score   support

          A1       0.33      0.27      0.30        11
          A2       0.61      0.73      0.67        67
          B1       0.72      0.65      0.68        74
          B2       0.33      0.14      0.20         7

    accuracy                           0.64       159
   macro avg       0.50      0.45      0.46       159
weighted avg       0.63      0.64      0.63       159


K-fold scores
[0.5873083108026511, 0.612254452884178, 0.605074875181881, 0.642260731510596, 0.6273562603149115]
SKF f1 score mean 0.6148509261388435

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2688
2688
Fold 0
[[ 0 11  1  0]
 [ 8 46 14  0]
 [ 1 15 55  4]
 [ 0  0  7  1]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00        12
          A2       0.64      0.68      0.66        68
          B1       0.71      0.73      0.72        75
          B2       0.20      0.12      0.15         8

    accuracy                           0.63       163
   macro avg       0.39      0.38      0.38       163
weighted avg       0.61      0.63      0.61       163


Fold 1
[[ 2  9  0  0]
 [ 3 51 13  0]
 [ 0 13 57  5]
 [ 0  0  7  0]]
              precision    recall  f1-score   support

          A1       0.40      0.18      0.25        11
          A2       0.70      0.76      0.73        67
          B1       0.74      0.76      0.75        75
          B2       0.00      0.00      0.00         7

    accuracy                           0.69       160
   macro avg       0.46      0.43      0.43       160
weighted avg       0.67      0.69      0.67       160


Fold 2
[[ 3  7  1  0]
 [ 7 44 16  0]
 [ 0 11 57  6]
 [ 0  0  7  0]]
              precision    recall  f1-score   support

          A1       0.30      0.27      0.29        11
          A2       0.71      0.66      0.68        67
          B1       0.70      0.77      0.74        74
          B2       0.00      0.00      0.00         7

    accuracy                           0.65       159
   macro avg       0.43      0.42      0.43       159
weighted avg       0.65      0.65      0.65       159


Fold 3
[[ 3  7  1  0]
 [ 6 41 19  1]
 [ 0 16 56  2]
 [ 0  2  4  1]]
              precision    recall  f1-score   support

          A1       0.33      0.27      0.30        11
          A2       0.62      0.61      0.62        67
          B1       0.70      0.76      0.73        74
          B2       0.25      0.14      0.18         7

    accuracy                           0.64       159
   macro avg       0.48      0.45      0.46       159
weighted avg       0.62      0.64      0.63       159


Fold 4
[[ 2  8  1  0]
 [ 6 49 12  0]
 [ 0 21 51  2]
 [ 0  0  6  1]]
              precision    recall  f1-score   support

          A1       0.25      0.18      0.21        11
          A2       0.63      0.73      0.68        67
          B1       0.73      0.69      0.71        74
          B2       0.33      0.14      0.20         7

    accuracy                           0.65       159
   macro avg       0.49      0.44      0.45       159
weighted avg       0.64      0.65      0.64       159


K-fold scores
[0.6146797503432957, 0.6738392857142858, 0.6495225783085485, 0.6270388664628982, 0.6378315393776135]
SKF f1 score mean 0.6405824040413284

SAME LANG EVAL DONE FOR THIS LANG
With POS ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
4765
4765
Fold 0
[[ 1 10  1  0]
 [ 2 52 14  0]
 [ 0 18 57  0]
 [ 0  0  8  0]]
              precision    recall  f1-score   support

          A1       0.33      0.08      0.13        12
          A2       0.65      0.76      0.70        68
          B1       0.71      0.76      0.74        75
          B2       0.00      0.00      0.00         8

    accuracy                           0.67       163
   macro avg       0.42      0.40      0.39       163
weighted avg       0.62      0.67      0.64       163


Fold 1
[[ 1 10  0  0]
 [ 1 57  9  0]
 [ 0 11 64  0]
 [ 0  0  7  0]]
              precision    recall  f1-score   support

          A1       0.50      0.09      0.15        11
          A2       0.73      0.85      0.79        67
          B1       0.80      0.85      0.83        75
          B2       0.00      0.00      0.00         7

    accuracy                           0.76       160
   macro avg       0.51      0.45      0.44       160
weighted avg       0.72      0.76      0.73       160


Fold 2
[[ 0 11  0  0]
 [ 0 56 11  0]
 [ 0 14 60  0]
 [ 0  0  7  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00        11
          A2       0.69      0.84      0.76        67
          B1       0.77      0.81      0.79        74
          B2       0.00      0.00      0.00         7

    accuracy                           0.73       159
   macro avg       0.37      0.41      0.39       159
weighted avg       0.65      0.73      0.69       159


Fold 3
[[ 0 10  1  0]
 [ 1 52 14  0]
 [ 0 22 52  0]
 [ 0  1  6  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00        11
          A2       0.61      0.78      0.68        67
          B1       0.71      0.70      0.71        74
          B2       0.00      0.00      0.00         7

    accuracy                           0.65       159
   macro avg       0.33      0.37      0.35       159
weighted avg       0.59      0.65      0.62       159


Fold 4
[[ 0 11  0  0]
 [ 0 55 12  0]
 [ 0 20 54  0]
 [ 0  1  6  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00        11
          A2       0.63      0.82      0.71        67
          B1       0.75      0.73      0.74        74
          B2       0.00      0.00      0.00         7

    accuracy                           0.69       159
   macro avg       0.35      0.39      0.36       159
weighted avg       0.62      0.69      0.65       159


K-fold scores
[0.641380822738432, 0.726897835201506, 0.6863129266307021, 0.6175839418852612, 0.6452633263178623]
SKF f1 score mean 0.6634877705547527

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
4765
4765
Fold 0
[[ 2  8  2  0]
 [ 5 39 23  1]
 [ 3 19 49  4]
 [ 0  0  6  2]]
              precision    recall  f1-score   support

          A1       0.20      0.17      0.18        12
          A2       0.59      0.57      0.58        68
          B1       0.61      0.65      0.63        75
          B2       0.29      0.25      0.27         8

    accuracy                           0.56       163
   macro avg       0.42      0.41      0.42       163
weighted avg       0.56      0.56      0.56       163


Fold 1
[[ 2  6  3  0]
 [ 3 42 21  1]
 [ 1 25 48  1]
 [ 0  0  7  0]]
              precision    recall  f1-score   support

          A1       0.33      0.18      0.24        11
          A2       0.58      0.63      0.60        67
          B1       0.61      0.64      0.62        75
          B2       0.00      0.00      0.00         7

    accuracy                           0.57       160
   macro avg       0.38      0.36      0.36       160
weighted avg       0.55      0.57      0.56       160


Fold 2
[[ 0  9  2  0]
 [ 3 47 17  0]
 [ 1 18 51  4]
 [ 0  1  5  1]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00        11
          A2       0.63      0.70      0.66        67
          B1       0.68      0.69      0.68        74
          B2       0.20      0.14      0.17         7

    accuracy                           0.62       159
   macro avg       0.38      0.38      0.38       159
weighted avg       0.59      0.62      0.60       159


Fold 3
[[ 5  3  3  0]
 [ 7 37 20  3]
 [ 2 25 44  3]
 [ 0  1  5  1]]
              precision    recall  f1-score   support

          A1       0.36      0.45      0.40        11
          A2       0.56      0.55      0.56        67
          B1       0.61      0.59      0.60        74
          B2       0.14      0.14      0.14         7

    accuracy                           0.55       159
   macro avg       0.42      0.44      0.43       159
weighted avg       0.55      0.55      0.55       159


Fold 4
[[ 2  6  3  0]
 [ 2 40 25  0]
 [ 1 27 46  0]
 [ 0  1  6  0]]
              precision    recall  f1-score   support

          A1       0.40      0.18      0.25        11
          A2       0.54      0.60      0.57        67
          B1       0.57      0.62      0.60        74
          B2       0.00      0.00      0.00         7

    accuracy                           0.55       159
   macro avg       0.38      0.35      0.35       159
weighted avg       0.52      0.55      0.53       159


K-fold scores
[0.5602245147613496, 0.5596342627960273, 0.6048836318460904, 0.5489366994638314, 0.5344149472009768]
SKF f1 score mean 0.5616188112136551

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
4765
4765
Fold 0
[[ 3  8  1  0]
 [ 7 41 20  0]
 [ 3 18 51  3]
 [ 0  0  7  1]]
              precision    recall  f1-score   support

          A1       0.23      0.25      0.24        12
          A2       0.61      0.60      0.61        68
          B1       0.65      0.68      0.66        75
          B2       0.25      0.12      0.17         8

    accuracy                           0.59       163
   macro avg       0.43      0.41      0.42       163
weighted avg       0.58      0.59      0.58       163


Fold 1
[[ 2  7  2  0]
 [ 3 47 17  0]
 [ 1 23 50  1]
 [ 0  0  7  0]]
              precision    recall  f1-score   support

          A1       0.33      0.18      0.24        11
          A2       0.61      0.70      0.65        67
          B1       0.66      0.67      0.66        75
          B2       0.00      0.00      0.00         7

    accuracy                           0.62       160
   macro avg       0.40      0.39      0.39       160
weighted avg       0.59      0.62      0.60       160


Fold 2
[[ 1  9  1  0]
 [ 2 50 15  0]
 [ 0 18 52  4]
 [ 0  0  6  1]]
              precision    recall  f1-score   support

          A1       0.33      0.09      0.14        11
          A2       0.65      0.75      0.69        67
          B1       0.70      0.70      0.70        74
          B2       0.20      0.14      0.17         7

    accuracy                           0.65       159
   macro avg       0.47      0.42      0.43       159
weighted avg       0.63      0.65      0.64       159


Fold 3
[[ 4  4  3  0]
 [ 6 39 20  2]
 [ 2 25 45  2]
 [ 0  1  5  1]]
              precision    recall  f1-score   support

          A1       0.33      0.36      0.35        11
          A2       0.57      0.58      0.57        67
          B1       0.62      0.61      0.61        74
          B2       0.20      0.14      0.17         7

    accuracy                           0.56       159
   macro avg       0.43      0.42      0.43       159
weighted avg       0.56      0.56      0.56       159


Fold 4
[[ 1  7  3  0]
 [ 2 43 21  1]
 [ 0 24 49  1]
 [ 0  1  6  0]]
              precision    recall  f1-score   support

          A1       0.33      0.09      0.14        11
          A2       0.57      0.64      0.61        67
          B1       0.62      0.66      0.64        74
          B2       0.00      0.00      0.00         7

    accuracy                           0.58       159
   macro avg       0.38      0.35      0.35       159
weighted avg       0.55      0.58      0.56       159


K-fold scores
[0.5840022190942437, 0.5999576286088386, 0.6368922831186983, 0.5580210481786372, 0.5631923658529464]
SKF f1 score mean 0.5884131089706728

SAME LANG EVAL DONE FOR THIS LANG
Dep ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5018
5018
Fold 0
[[ 0 12  0  0]
 [ 3 53 12  0]
 [ 0 18 57  0]
 [ 0  0  8  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00        12
          A2       0.64      0.78      0.70        68
          B1       0.74      0.76      0.75        75
          B2       0.00      0.00      0.00         8

    accuracy                           0.67       163
   macro avg       0.34      0.38      0.36       163
weighted avg       0.61      0.67      0.64       163


Fold 1
[[ 1 10  0  0]
 [ 0 53 14  0]
 [ 0 11 64  0]
 [ 0  0  7  0]]
              precision    recall  f1-score   support

          A1       1.00      0.09      0.17        11
          A2       0.72      0.79      0.75        67
          B1       0.75      0.85      0.80        75
          B2       0.00      0.00      0.00         7

    accuracy                           0.74       160
   macro avg       0.62      0.43      0.43       160
weighted avg       0.72      0.74      0.70       160


Fold 2
[[ 0 10  1  0]
 [ 0 58  9  0]
 [ 0 11 63  0]
 [ 0  0  7  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00        11
          A2       0.73      0.87      0.79        67
          B1       0.79      0.85      0.82        74
          B2       0.00      0.00      0.00         7

    accuracy                           0.76       159
   macro avg       0.38      0.43      0.40       159
weighted avg       0.68      0.76      0.72       159


Fold 3
[[ 0 10  1  0]
 [ 2 47 18  0]
 [ 0 26 48  0]
 [ 0  1  6  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00        11
          A2       0.56      0.70      0.62        67
          B1       0.66      0.65      0.65        74
          B2       0.00      0.00      0.00         7

    accuracy                           0.60       159
   macro avg       0.30      0.34      0.32       159
weighted avg       0.54      0.60      0.57       159


Fold 4
[[ 0 11  0  0]
 [ 0 55 12  0]
 [ 0 24 50  0]
 [ 0  1  6  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00        11
          A2       0.60      0.82      0.70        67
          B1       0.74      0.68      0.70        74
          B2       0.00      0.00      0.00         7

    accuracy                           0.66       159
   macro avg       0.33      0.37      0.35       159
weighted avg       0.60      0.66      0.62       159


K-fold scores
[0.6379453947101126, 0.7012632978723404, 0.7155869890426623, 0.5662587414073463, 0.6211210419911396]
SKF f1 score mean 0.6484350930047202

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5018
5018
Fold 0
[[ 0  7  5  0]
 [ 7 39 20  2]
 [ 2 20 52  1]
 [ 0  1  7  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00        12
          A2       0.58      0.57      0.58        68
          B1       0.62      0.69      0.65        75
          B2       0.00      0.00      0.00         8

    accuracy                           0.56       163
   macro avg       0.30      0.32      0.31       163
weighted avg       0.53      0.56      0.54       163


Fold 1
[[ 1  9  1  0]
 [ 5 49 13  0]
 [ 1 18 54  2]
 [ 0  1  6  0]]
              precision    recall  f1-score   support

          A1       0.14      0.09      0.11        11
          A2       0.64      0.73      0.68        67
          B1       0.73      0.72      0.72        75
          B2       0.00      0.00      0.00         7

    accuracy                           0.65       160
   macro avg       0.38      0.39      0.38       160
weighted avg       0.62      0.65      0.63       160


Fold 2
[[ 0 10  1  0]
 [ 6 46 15  0]
 [ 0 22 51  1]
 [ 0  1  6  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00        11
          A2       0.58      0.69      0.63        67
          B1       0.70      0.69      0.69        74
          B2       0.00      0.00      0.00         7

    accuracy                           0.61       159
   macro avg       0.32      0.34      0.33       159
weighted avg       0.57      0.61      0.59       159


Fold 3
[[ 3  6  2  0]
 [ 6 39 20  2]
 [ 0 27 44  3]
 [ 0  1  5  1]]
              precision    recall  f1-score   support

          A1       0.33      0.27      0.30        11
          A2       0.53      0.58      0.56        67
          B1       0.62      0.59      0.61        74
          B2       0.17      0.14      0.15         7

    accuracy                           0.55       159
   macro avg       0.41      0.40      0.40       159
weighted avg       0.54      0.55      0.54       159


Fold 4
[[ 2  9  0  0]
 [ 5 45 17  0]
 [ 0 23 51  0]
 [ 0  1  6  0]]
              precision    recall  f1-score   support

          A1       0.29      0.18      0.22        11
          A2       0.58      0.67      0.62        67
          B1       0.69      0.69      0.69        74
          B2       0.00      0.00      0.00         7

    accuracy                           0.62       159
   macro avg       0.39      0.39      0.38       159
weighted avg       0.58      0.62      0.60       159


K-fold scores
[0.5419968874998393, 0.6323866284489188, 0.588466143759132, 0.544753706497363, 0.5976770524590954]
SKF f1 score mean 0.5810560837328697

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
5018
5018
Fold 0
[[ 0  8  4  0]
 [ 6 40 20  2]
 [ 0 18 56  1]
 [ 0  0  8  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00        12
          A2       0.61      0.59      0.60        68
          B1       0.64      0.75      0.69        75
          B2       0.00      0.00      0.00         8

    accuracy                           0.59       163
   macro avg       0.31      0.33      0.32       163
weighted avg       0.55      0.59      0.57       163


Fold 1
[[ 1  8  2  0]
 [ 3 53 11  0]
 [ 0 14 59  2]
 [ 0  1  6  0]]
              precision    recall  f1-score   support

          A1       0.25      0.09      0.13        11
          A2       0.70      0.79      0.74        67
          B1       0.76      0.79      0.77        75
          B2       0.00      0.00      0.00         7

    accuracy                           0.71       160
   macro avg       0.43      0.42      0.41       160
weighted avg       0.66      0.71      0.68       160


Fold 2
[[ 1 10  0  0]
 [ 3 52 12  0]
 [ 0 17 54  3]
 [ 0  1  6  0]]
              precision    recall  f1-score   support

          A1       0.25      0.09      0.13        11
          A2       0.65      0.78      0.71        67
          B1       0.75      0.73      0.74        74
          B2       0.00      0.00      0.00         7

    accuracy                           0.67       159
   macro avg       0.41      0.40      0.40       159
weighted avg       0.64      0.67      0.65       159


Fold 3
[[ 3  5  2  1]
 [ 6 42 17  2]
 [ 0 23 48  3]
 [ 0  2  4  1]]
              precision    recall  f1-score   support

          A1       0.33      0.27      0.30        11
          A2       0.58      0.63      0.60        67
          B1       0.68      0.65      0.66        74
          B2       0.14      0.14      0.14         7

    accuracy                           0.59       159
   macro avg       0.43      0.42      0.43       159
weighted avg       0.59      0.59      0.59       159


Fold 4
[[ 2  9  0  0]
 [ 3 44 20  0]
 [ 1 20 52  1]
 [ 0  0  7  0]]
              precision    recall  f1-score   support

          A1       0.33      0.18      0.24        11
          A2       0.60      0.66      0.63        67
          B1       0.66      0.70      0.68        74
          B2       0.00      0.00      0.00         7

    accuracy                           0.62       159
   macro avg       0.40      0.39      0.39       159
weighted avg       0.58      0.62      0.60       159


K-fold scores
[0.5652193696727698, 0.6810883724119018, 0.6516210895489409, 0.5898258621469773, 0.5975042427872617]
SKF f1 score mean 0.6170517873135702

SAME LANG EVAL DONE FOR THIS LANG
Doing monolingual classification for  CZ
************for dimension:  OverallCEFRrating  ***************
Extracted all features: 
Printing class statistics
Counter({'A2': 188, 'B1': 165, 'B2': 81})
With Word ngrams: 
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
1161
1161
Fold 0
[[37  1  0]
 [ 8 22  3]
 [ 0 11  6]]
              precision    recall  f1-score   support

          A2       0.82      0.97      0.89        38
          B1       0.65      0.67      0.66        33
          B2       0.67      0.35      0.46        17

    accuracy                           0.74        88
   macro avg       0.71      0.66      0.67        88
weighted avg       0.73      0.74      0.72        88


Fold 1
[[34  4  0]
 [10 18  5]
 [ 0  7  9]]
              precision    recall  f1-score   support

          A2       0.77      0.89      0.83        38
          B1       0.62      0.55      0.58        33
          B2       0.64      0.56      0.60        16

    accuracy                           0.70        87
   macro avg       0.68      0.67      0.67        87
weighted avg       0.69      0.70      0.69        87


Fold 2
[[35  3  0]
 [ 8 20  5]
 [ 0  8  8]]
              precision    recall  f1-score   support

          A2       0.81      0.92      0.86        38
          B1       0.65      0.61      0.62        33
          B2       0.62      0.50      0.55        16

    accuracy                           0.72        87
   macro avg       0.69      0.68      0.68        87
weighted avg       0.71      0.72      0.72        87


Fold 3
[[28  9  0]
 [ 7 24  2]
 [ 0  7  9]]
              precision    recall  f1-score   support

          A2       0.80      0.76      0.78        37
          B1       0.60      0.73      0.66        33
          B2       0.82      0.56      0.67        16

    accuracy                           0.71        86
   macro avg       0.74      0.68      0.70        86
weighted avg       0.73      0.71      0.71        86


Fold 4
[[35  2  0]
 [ 7 25  1]
 [ 1  6  9]]
              precision    recall  f1-score   support

          A2       0.81      0.95      0.88        37
          B1       0.76      0.76      0.76        33
          B2       0.90      0.56      0.69        16

    accuracy                           0.80        86
   macro avg       0.82      0.76      0.77        86
weighted avg       0.81      0.80      0.80        86


K-fold scores
[0.7204240194259975, 0.6927986832704813, 0.7160010618360465, 0.7109659835050087, 0.7959525939177102]
SKF f1 score mean 0.7272284683910489

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
1161
1161
Fold 0
[[32  6  0]
 [13 15  5]
 [ 0  8  9]]
              precision    recall  f1-score   support

          A2       0.71      0.84      0.77        38
          B1       0.52      0.45      0.48        33
          B2       0.64      0.53      0.58        17

    accuracy                           0.64        88
   macro avg       0.62      0.61      0.61        88
weighted avg       0.63      0.64      0.63        88


Fold 1
[[31  6  1]
 [11 18  4]
 [ 1  5 10]]
              precision    recall  f1-score   support

          A2       0.72      0.82      0.77        38
          B1       0.62      0.55      0.58        33
          B2       0.67      0.62      0.65        16

    accuracy                           0.68        87
   macro avg       0.67      0.66      0.66        87
weighted avg       0.67      0.68      0.67        87


Fold 2
[[34  4  0]
 [ 8 18  7]
 [ 0  4 12]]
              precision    recall  f1-score   support

          A2       0.81      0.89      0.85        38
          B1       0.69      0.55      0.61        33
          B2       0.63      0.75      0.69        16

    accuracy                           0.74        87
   macro avg       0.71      0.73      0.72        87
weighted avg       0.73      0.74      0.73        87


Fold 3
[[30  7  0]
 [ 8 19  6]
 [ 0  8  8]]
              precision    recall  f1-score   support

          A2       0.79      0.81      0.80        37
          B1       0.56      0.58      0.57        33
          B2       0.57      0.50      0.53        16

    accuracy                           0.66        86
   macro avg       0.64      0.63      0.63        86
weighted avg       0.66      0.66      0.66        86


Fold 4
[[30  7  0]
 [ 9 18  6]
 [ 0 10  6]]
              precision    recall  f1-score   support

          A2       0.77      0.81      0.79        37
          B1       0.51      0.55      0.53        33
          B2       0.50      0.38      0.43        16

    accuracy                           0.63        86
   macro avg       0.59      0.58      0.58        86
weighted avg       0.62      0.63      0.62        86


K-fold scores
[0.6265899374624598, 0.6732217324233143, 0.7288163424341099, 0.6610436191137337, 0.6225378768398424]
SKF f1 score mean 0.6624419016546921

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
1161
1161
Fold 0
[[34  4  0]
 [ 9 19  5]
 [ 0  7 10]]
              precision    recall  f1-score   support

          A2       0.79      0.89      0.84        38
          B1       0.63      0.58      0.60        33
          B2       0.67      0.59      0.62        17

    accuracy                           0.72        88
   macro avg       0.70      0.69      0.69        88
weighted avg       0.71      0.72      0.71        88


Fold 1
[[31  6  1]
 [ 9 17  7]
 [ 0  5 11]]
              precision    recall  f1-score   support

          A2       0.78      0.82      0.79        38
          B1       0.61      0.52      0.56        33
          B2       0.58      0.69      0.63        16

    accuracy                           0.68        87
   macro avg       0.65      0.67      0.66        87
weighted avg       0.68      0.68      0.67        87


Fold 2
[[34  4  0]
 [ 9 17  7]
 [ 0  4 12]]
              precision    recall  f1-score   support

          A2       0.79      0.89      0.84        38
          B1       0.68      0.52      0.59        33
          B2       0.63      0.75      0.69        16

    accuracy                           0.72        87
   macro avg       0.70      0.72      0.70        87
weighted avg       0.72      0.72      0.72        87


Fold 3
[[30  7  0]
 [ 7 19  7]
 [ 0  7  9]]
              precision    recall  f1-score   support

          A2       0.81      0.81      0.81        37
          B1       0.58      0.58      0.58        33
          B2       0.56      0.56      0.56        16

    accuracy                           0.67        86
   macro avg       0.65      0.65      0.65        86
weighted avg       0.67      0.67      0.67        86


Fold 4
[[30  7  0]
 [ 7 18  8]
 [ 0  9  7]]
              precision    recall  f1-score   support

          A2       0.81      0.81      0.81        37
          B1       0.53      0.55      0.54        33
          B2       0.47      0.44      0.45        16

    accuracy                           0.64        86
   macro avg       0.60      0.60      0.60        86
weighted avg       0.64      0.64      0.64        86


K-fold scores
[0.7094431417348084, 0.674203605577263, 0.7151435715578932, 0.6744186046511628, 0.6390366248278488]
SKF f1 score mean 0.6824491096697953

SAME LANG EVAL DONE FOR THIS LANG
With POS ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3975
3975
Fold 0
[[35  3  0]
 [ 9 19  5]
 [ 0 11  6]]
              precision    recall  f1-score   support

          A2       0.80      0.92      0.85        38
          B1       0.58      0.58      0.58        33
          B2       0.55      0.35      0.43        17

    accuracy                           0.68        88
   macro avg       0.64      0.62      0.62        88
weighted avg       0.66      0.68      0.67        88


Fold 1
[[34  3  1]
 [ 9 22  2]
 [ 1  7  8]]
              precision    recall  f1-score   support

          A2       0.77      0.89      0.83        38
          B1       0.69      0.67      0.68        33
          B2       0.73      0.50      0.59        16

    accuracy                           0.74        87
   macro avg       0.73      0.69      0.70        87
weighted avg       0.73      0.74      0.73        87


Fold 2
[[34  4  0]
 [ 3 27  3]
 [ 0 10  6]]
              precision    recall  f1-score   support

          A2       0.92      0.89      0.91        38
          B1       0.66      0.82      0.73        33
          B2       0.67      0.38      0.48        16

    accuracy                           0.77        87
   macro avg       0.75      0.70      0.71        87
weighted avg       0.77      0.77      0.76        87


Fold 3
[[28  9  0]
 [ 7 24  2]
 [ 0  9  7]]
              precision    recall  f1-score   support

          A2       0.80      0.76      0.78        37
          B1       0.57      0.73      0.64        33
          B2       0.78      0.44      0.56        16

    accuracy                           0.69        86
   macro avg       0.72      0.64      0.66        86
weighted avg       0.71      0.69      0.68        86


Fold 4
[[32  5  0]
 [ 7 24  2]
 [ 0  8  8]]
              precision    recall  f1-score   support

          A2       0.82      0.86      0.84        37
          B1       0.65      0.73      0.69        33
          B2       0.80      0.50      0.62        16

    accuracy                           0.74        86
   macro avg       0.76      0.70      0.71        86
weighted avg       0.75      0.74      0.74        86


K-fold scores
[0.6673265758631612, 0.7279556108263706, 0.7610852231541887, 0.6843927648578811, 0.739914186181016]
SKF f1 score mean 0.7161348721765235

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3975
3975
Fold 0
[[31  6  1]
 [17  8  8]
 [ 1  7  9]]
              precision    recall  f1-score   support

          A2       0.63      0.82      0.71        38
          B1       0.38      0.24      0.30        33
          B2       0.50      0.53      0.51        17

    accuracy                           0.55        88
   macro avg       0.50      0.53      0.51        88
weighted avg       0.51      0.55      0.52        88


Fold 1
[[29  7  2]
 [11 15  7]
 [ 0  6 10]]
              precision    recall  f1-score   support

          A2       0.72      0.76      0.74        38
          B1       0.54      0.45      0.49        33
          B2       0.53      0.62      0.57        16

    accuracy                           0.62        87
   macro avg       0.60      0.61      0.60        87
weighted avg       0.62      0.62      0.62        87


Fold 2
[[25 13  0]
 [ 9 20  4]
 [ 1  7  8]]
              precision    recall  f1-score   support

          A2       0.71      0.66      0.68        38
          B1       0.50      0.61      0.55        33
          B2       0.67      0.50      0.57        16

    accuracy                           0.61        87
   macro avg       0.63      0.59      0.60        87
weighted avg       0.62      0.61      0.61        87


Fold 3
[[23 14  0]
 [15 13  5]
 [ 2  7  7]]
              precision    recall  f1-score   support

          A2       0.57      0.62      0.60        37
          B1       0.38      0.39      0.39        33
          B2       0.58      0.44      0.50        16

    accuracy                           0.50        86
   macro avg       0.51      0.48      0.50        86
weighted avg       0.50      0.50      0.50        86


Fold 4
[[27  9  1]
 [11 15  7]
 [ 2  6  8]]
              precision    recall  f1-score   support

          A2       0.68      0.73      0.70        37
          B1       0.50      0.45      0.48        33
          B2       0.50      0.50      0.50        16

    accuracy                           0.58        86
   macro avg       0.56      0.56      0.56        86
weighted avg       0.58      0.58      0.58        86


K-fold scores
[0.5181942578494302, 0.6164227079998703, 0.6120970825741728, 0.4989519331761608, 0.5774690425853216]
SKF f1 score mean 0.5646270048369911

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3975
3975
Fold 0
[[31  6  1]
 [15 10  8]
 [ 0  8  9]]
              precision    recall  f1-score   support

          A2       0.67      0.82      0.74        38
          B1       0.42      0.30      0.35        33
          B2       0.50      0.53      0.51        17

    accuracy                           0.57        88
   macro avg       0.53      0.55      0.53        88
weighted avg       0.54      0.57      0.55        88


Fold 1
[[29  6  3]
 [11 16  6]
 [ 0  6 10]]
              precision    recall  f1-score   support

          A2       0.72      0.76      0.74        38
          B1       0.57      0.48      0.52        33
          B2       0.53      0.62      0.57        16

    accuracy                           0.63        87
   macro avg       0.61      0.62      0.61        87
weighted avg       0.63      0.63      0.63        87


Fold 2
[[25 13  0]
 [ 9 20  4]
 [ 0  7  9]]
              precision    recall  f1-score   support

          A2       0.74      0.66      0.69        38
          B1       0.50      0.61      0.55        33
          B2       0.69      0.56      0.62        16

    accuracy                           0.62        87
   macro avg       0.64      0.61      0.62        87
weighted avg       0.64      0.62      0.63        87


Fold 3
[[25 12  0]
 [11 13  9]
 [ 2  5  9]]
              precision    recall  f1-score   support

          A2       0.66      0.68      0.67        37
          B1       0.43      0.39      0.41        33
          B2       0.50      0.56      0.53        16

    accuracy                           0.55        86
   macro avg       0.53      0.54      0.54        86
weighted avg       0.54      0.55      0.54        86


Fold 4
[[27  9  1]
 [11 15  7]
 [ 2  6  8]]
              precision    recall  f1-score   support

          A2       0.68      0.73      0.70        37
          B1       0.50      0.45      0.48        33
          B2       0.50      0.50      0.50        16

    accuracy                           0.58        86
   macro avg       0.56      0.56      0.56        86
weighted avg       0.58      0.58      0.58        86


K-fold scores
[0.5496525404420143, 0.6288591127483157, 0.625311668419189, 0.5436779362907953, 0.5774690425853216]
SKF f1 score mean 0.5849940600971272

SAME LANG EVAL DONE FOR THIS LANG
Dep ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2943
2943
Fold 0
[[37  1  0]
 [ 9 21  3]
 [ 0  3 14]]
              precision    recall  f1-score   support

          A2       0.80      0.97      0.88        38
          B1       0.84      0.64      0.72        33
          B2       0.82      0.82      0.82        17

    accuracy                           0.82        88
   macro avg       0.82      0.81      0.81        88
weighted avg       0.82      0.82      0.81        88


Fold 1
[[33  5  0]
 [ 8 22  3]
 [ 1  7  8]]
              precision    recall  f1-score   support

          A2       0.79      0.87      0.82        38
          B1       0.65      0.67      0.66        33
          B2       0.73      0.50      0.59        16

    accuracy                           0.72        87
   macro avg       0.72      0.68      0.69        87
weighted avg       0.72      0.72      0.72        87


Fold 2
[[30  8  0]
 [ 4 27  2]
 [ 0 10  6]]
              precision    recall  f1-score   support

          A2       0.88      0.79      0.83        38
          B1       0.60      0.82      0.69        33
          B2       0.75      0.38      0.50        16

    accuracy                           0.72        87
   macro avg       0.74      0.66      0.68        87
weighted avg       0.75      0.72      0.72        87


Fold 3
[[26 11  0]
 [ 6 26  1]
 [ 0 10  6]]
              precision    recall  f1-score   support

          A2       0.81      0.70      0.75        37
          B1       0.55      0.79      0.65        33
          B2       0.86      0.38      0.52        16

    accuracy                           0.67        86
   macro avg       0.74      0.62      0.64        86
weighted avg       0.72      0.67      0.67        86


Fold 4
[[30  7  0]
 [ 5 25  3]
 [ 0 11  5]]
              precision    recall  f1-score   support

          A2       0.86      0.81      0.83        37
          B1       0.58      0.76      0.66        33
          B2       0.62      0.31      0.42        16

    accuracy                           0.70        86
   macro avg       0.69      0.63      0.64        86
weighted avg       0.71      0.70      0.69        86


K-fold scores
[0.8110538886400955, 0.7184267042819109, 0.7185381668140288, 0.6707195820694303, 0.6884944920440637]
SKF f1 score mean 0.7214465667699059

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2943
2943
Fold 0
[[29  9  0]
 [13 13  7]
 [ 1  7  9]]
              precision    recall  f1-score   support

          A2       0.67      0.76      0.72        38
          B1       0.45      0.39      0.42        33
          B2       0.56      0.53      0.55        17

    accuracy                           0.58        88
   macro avg       0.56      0.56      0.56        88
weighted avg       0.57      0.58      0.57        88


Fold 1
[[29  7  2]
 [10 17  6]
 [ 2  4 10]]
              precision    recall  f1-score   support

          A2       0.71      0.76      0.73        38
          B1       0.61      0.52      0.56        33
          B2       0.56      0.62      0.59        16

    accuracy                           0.64        87
   macro avg       0.62      0.63      0.63        87
weighted avg       0.64      0.64      0.64        87


Fold 2
[[29  8  1]
 [13 16  4]
 [ 0  6 10]]
              precision    recall  f1-score   support

          A2       0.69      0.76      0.72        38
          B1       0.53      0.48      0.51        33
          B2       0.67      0.62      0.65        16

    accuracy                           0.63        87
   macro avg       0.63      0.62      0.63        87
weighted avg       0.63      0.63      0.63        87


Fold 3
[[27 10  0]
 [10 18  5]
 [ 1  7  8]]
              precision    recall  f1-score   support

          A2       0.71      0.73      0.72        37
          B1       0.51      0.55      0.53        33
          B2       0.62      0.50      0.55        16

    accuracy                           0.62        86
   macro avg       0.61      0.59      0.60        86
weighted avg       0.62      0.62      0.62        86


Fold 4
[[29  8  0]
 [ 9 18  6]
 [ 0  7  9]]
              precision    recall  f1-score   support

          A2       0.76      0.78      0.77        37
          B1       0.55      0.55      0.55        33
          B2       0.60      0.56      0.58        16

    accuracy                           0.65        86
   macro avg       0.64      0.63      0.63        86
weighted avg       0.65      0.65      0.65        86


K-fold scores
[0.5718331078790512, 0.6402751897246938, 0.6279825908858168, 0.6155601679324497, 0.6500425106276569]
SKF f1 score mean 0.6211387134099338

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2943
2943
Fold 0
[[30  8  0]
 [13 13  7]
 [ 0  6 11]]
              precision    recall  f1-score   support

          A2       0.70      0.79      0.74        38
          B1       0.48      0.39      0.43        33
          B2       0.61      0.65      0.63        17

    accuracy                           0.61        88
   macro avg       0.60      0.61      0.60        88
weighted avg       0.60      0.61      0.60        88


Fold 1
[[29  7  2]
 [ 9 17  7]
 [ 2  4 10]]
              precision    recall  f1-score   support

          A2       0.72      0.76      0.74        38
          B1       0.61      0.52      0.56        33
          B2       0.53      0.62      0.57        16

    accuracy                           0.64        87
   macro avg       0.62      0.63      0.62        87
weighted avg       0.64      0.64      0.64        87


Fold 2
[[28  9  1]
 [14 15  4]
 [ 0  6 10]]
              precision    recall  f1-score   support

          A2       0.67      0.74      0.70        38
          B1       0.50      0.45      0.48        33
          B2       0.67      0.62      0.65        16

    accuracy                           0.61        87
   macro avg       0.61      0.61      0.61        87
weighted avg       0.60      0.61      0.61        87


Fold 3
[[28  9  0]
 [ 9 19  5]
 [ 1  6  9]]
              precision    recall  f1-score   support

          A2       0.74      0.76      0.75        37
          B1       0.56      0.58      0.57        33
          B2       0.64      0.56      0.60        16

    accuracy                           0.65        86
   macro avg       0.65      0.63      0.64        86
weighted avg       0.65      0.65      0.65        86


Fold 4
[[29  8  0]
 [ 6 20  7]
 [ 0  7  9]]
              precision    recall  f1-score   support

          A2       0.83      0.78      0.81        37
          B1       0.57      0.61      0.59        33
          B2       0.56      0.56      0.56        16

    accuracy                           0.67        86
   macro avg       0.65      0.65      0.65        86
weighted avg       0.68      0.67      0.68        86


K-fold scores
[0.6037938912938913, 0.6412955174967611, 0.6050214524074368, 0.6505009834548189, 0.6769455844353245]
SKF f1 score mean 0.6355114858176465

SAME LANG EVAL DONE FOR THIS LANG
************for dimension:  Grammaticalaccuracy  ***************
Extracted all features: 
Printing class statistics
Counter({'A2': 185, 'B1': 156, 'B2': 82, 'A1': 6, 'C1': 5})
With Word ngrams: 
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
1161
1161
Fold 0
[[ 0  2  0  0  0]
 [ 0 28  7  2  0]
 [ 0 12 13  7  0]
 [ 0  3  2 12  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.62      0.76      0.68        37
          B1       0.59      0.41      0.48        32
          B2       0.55      0.71      0.62        17
          C1       0.00      0.00      0.00         1

    accuracy                           0.60        89
   macro avg       0.35      0.37      0.36        89
weighted avg       0.58      0.60      0.57        89


Fold 1
[[ 0  1  0  0  0]
 [ 0 30  7  0  0]
 [ 0 17 12  2  0]
 [ 0  2  2 13  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.60      0.81      0.69        37
          B1       0.57      0.39      0.46        31
          B2       0.81      0.76      0.79        17
          C1       0.00      0.00      0.00         1

    accuracy                           0.63        87
   macro avg       0.40      0.39      0.39        87
weighted avg       0.62      0.63      0.61        87


Fold 2
[[ 0  1  0  0  0]
 [ 0 30  6  1  0]
 [ 0 12 12  7  0]
 [ 0  1  7  8  0]
 [ 0  1  0  0  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.67      0.81      0.73        37
          B1       0.48      0.39      0.43        31
          B2       0.50      0.50      0.50        16
          C1       0.00      0.00      0.00         1

    accuracy                           0.58        86
   macro avg       0.33      0.34      0.33        86
weighted avg       0.55      0.58      0.56        86


Fold 3
[[ 0  1  0  0  0]
 [ 0 27  9  1  0]
 [ 0 16  8  7  0]
 [ 0  2  2 12  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.59      0.73      0.65        37
          B1       0.42      0.26      0.32        31
          B2       0.57      0.75      0.65        16
          C1       0.00      0.00      0.00         1

    accuracy                           0.55        86
   macro avg       0.32      0.35      0.32        86
weighted avg       0.51      0.55      0.52        86


Fold 4
[[ 0  1  0  0  0]
 [ 0 34  3  0  0]
 [ 0 11 10 10  0]
 [ 0  1  1 14  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.72      0.92      0.81        37
          B1       0.71      0.32      0.44        31
          B2       0.56      0.88      0.68        16
          C1       0.00      0.00      0.00         1

    accuracy                           0.67        86
   macro avg       0.40      0.42      0.39        86
weighted avg       0.67      0.67      0.64        86


K-fold scores
[0.5745757140659854, 0.6117111848384144, 0.5623126164816465, 0.5159379945930801, 0.6355463720750164]
SKF f1 score mean 0.5800167764108285

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
1161
1161
Fold 0
[[ 0  2  0  0  0]
 [ 0 29  6  2  0]
 [ 0  8 19  5  0]
 [ 0  3  6  8  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.69      0.78      0.73        37
          B1       0.61      0.59      0.60        32
          B2       0.50      0.47      0.48        17
          C1       0.00      0.00      0.00         1

    accuracy                           0.63        89
   macro avg       0.36      0.37      0.36        89
weighted avg       0.60      0.63      0.61        89


Fold 1
[[ 0  1  0  0  0]
 [ 0 30  7  0  0]
 [ 0 13 16  2  0]
 [ 0  4  4  9  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.62      0.81      0.71        37
          B1       0.59      0.52      0.55        31
          B2       0.75      0.53      0.62        17
          C1       0.00      0.00      0.00         1

    accuracy                           0.63        87
   macro avg       0.39      0.37      0.38        87
weighted avg       0.62      0.63      0.62        87


Fold 2
[[ 0  0  1  0  0]
 [ 0 22 13  2  0]
 [ 0  9 15  7  0]
 [ 0  2  9  5  0]
 [ 0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.67      0.59      0.63        37
          B1       0.38      0.48      0.43        31
          B2       0.36      0.31      0.33        16
          C1       0.00      0.00      0.00         1

    accuracy                           0.49        86
   macro avg       0.28      0.28      0.28        86
weighted avg       0.49      0.49      0.49        86


Fold 3
[[ 0  1  0  0  0]
 [ 0 26 10  1  0]
 [ 0 11 16  4  0]
 [ 0  1  5 10  0]
 [ 0  0  0  0  1]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.67      0.70      0.68        37
          B1       0.52      0.52      0.52        31
          B2       0.67      0.62      0.65        16
          C1       1.00      1.00      1.00         1

    accuracy                           0.62        86
   macro avg       0.57      0.57      0.57        86
weighted avg       0.61      0.62      0.61        86


Fold 4
[[ 0  1  0  0  0]
 [ 1 26  8  2  0]
 [ 0  9 13  8  1]
 [ 0  2  9  5  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.68      0.70      0.69        37
          B1       0.43      0.42      0.43        31
          B2       0.31      0.31      0.31        16
          C1       0.00      0.00      0.00         1

    accuracy                           0.51        86
   macro avg       0.29      0.29      0.29        86
weighted avg       0.51      0.51      0.51        86


K-fold scores
[0.6147030169217625, 0.6180783847427199, 0.48693244739756375, 0.6120740711493663, 0.5100749777608337]
SKF f1 score mean 0.5683725795944492

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
1161
1161
Fold 0
[[ 0  2  0  0  0]
 [ 0 29  6  2  0]
 [ 0  7 19  6  0]
 [ 0  1  5 11  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.74      0.78      0.76        37
          B1       0.63      0.59      0.61        32
          B2       0.55      0.65      0.59        17
          C1       0.00      0.00      0.00         1

    accuracy                           0.66        89
   macro avg       0.39      0.40      0.39        89
weighted avg       0.64      0.66      0.65        89


Fold 1
[[ 0  1  0  0  0]
 [ 0 31  6  0  0]
 [ 0 14 15  2  0]
 [ 0  4  3 10  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.62      0.84      0.71        37
          B1       0.62      0.48      0.55        31
          B2       0.77      0.59      0.67        17
          C1       0.00      0.00      0.00         1

    accuracy                           0.64        87
   macro avg       0.40      0.38      0.38        87
weighted avg       0.64      0.64      0.63        87


Fold 2
[[ 0  1  0  0  0]
 [ 0 23 12  2  0]
 [ 0  9 15  7  0]
 [ 0  1  8  7  0]
 [ 0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.68      0.62      0.65        37
          B1       0.42      0.48      0.45        31
          B2       0.44      0.44      0.44        16
          C1       0.00      0.00      0.00         1

    accuracy                           0.52        86
   macro avg       0.31      0.31      0.31        86
weighted avg       0.52      0.52      0.52        86


Fold 3
[[ 0  1  0  0  0]
 [ 0 22 14  1  0]
 [ 0 11 14  6  0]
 [ 0  1  4 11  0]
 [ 0  0  0  0  1]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.63      0.59      0.61        37
          B1       0.44      0.45      0.44        31
          B2       0.61      0.69      0.65        16
          C1       1.00      1.00      1.00         1

    accuracy                           0.56        86
   macro avg       0.54      0.55      0.54        86
weighted avg       0.55      0.56      0.56        86


Fold 4
[[ 0  1  0  0  0]
 [ 0 30  5  2  0]
 [ 0  7 15  8  1]
 [ 0  1  7  7  1]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.77      0.81      0.79        37
          B1       0.56      0.48      0.52        31
          B2       0.39      0.44      0.41        16
          C1       0.00      0.00      0.00         1

    accuracy                           0.60        86
   macro avg       0.34      0.35      0.34        86
weighted avg       0.60      0.60      0.60        86


K-fold scores
[0.6512118363952554, 0.6277039118894053, 0.5215398604748938, 0.5551375588995288, 0.6027121438200908]
SKF f1 score mean 0.5916610622958348

SAME LANG EVAL DONE FOR THIS LANG
With POS ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3975
3975
Fold 0
[[ 0  2  0  0  0]
 [ 0 27 10  0  0]
 [ 0 11 15  6  0]
 [ 0  2 10  5  0]
 [ 0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.64      0.73      0.68        37
          B1       0.42      0.47      0.44        32
          B2       0.45      0.29      0.36        17
          C1       0.00      0.00      0.00         1

    accuracy                           0.53        89
   macro avg       0.30      0.30      0.30        89
weighted avg       0.50      0.53      0.51        89


Fold 1
[[ 0  1  0  0  0]
 [ 0 29  8  0  0]
 [ 0 12 17  2  0]
 [ 0  2  8  7  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.66      0.78      0.72        37
          B1       0.52      0.55      0.53        31
          B2       0.70      0.41      0.52        17
          C1       0.00      0.00      0.00         1

    accuracy                           0.61        87
   macro avg       0.37      0.35      0.35        87
weighted avg       0.60      0.61      0.60        87


Fold 2
[[ 0  0  1  0  0]
 [ 0 28  8  1  0]
 [ 0 11 15  5  0]
 [ 0  1  5 10  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.70      0.76      0.73        37
          B1       0.52      0.48      0.50        31
          B2       0.59      0.62      0.61        16
          C1       0.00      0.00      0.00         1

    accuracy                           0.62        86
   macro avg       0.36      0.37      0.37        86
weighted avg       0.60      0.62      0.61        86


Fold 3
[[ 0  1  0  0  0]
 [ 0 28  7  2  0]
 [ 0 16  7  8  0]
 [ 0  1  5 10  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.61      0.76      0.67        37
          B1       0.37      0.23      0.28        31
          B2       0.48      0.62      0.54        16
          C1       0.00      0.00      0.00         1

    accuracy                           0.52        86
   macro avg       0.29      0.32      0.30        86
weighted avg       0.48      0.52      0.49        86


Fold 4
[[ 0  1  0  0  0]
 [ 0 33  3  1  0]
 [ 0  6 18  7  0]
 [ 0  1  3 12  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.80      0.89      0.85        37
          B1       0.75      0.58      0.65        31
          B2       0.57      0.75      0.65        16
          C1       0.00      0.00      0.00         1

    accuracy                           0.73        86
   macro avg       0.43      0.44      0.43        86
weighted avg       0.72      0.73      0.72        86


K-fold scores
[0.5110136502332407, 0.5951424364978004, 0.6058844256518675, 0.49177330314343487, 0.7206625555462766]
SKF f1 score mean 0.584895274214524

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3975
3975
Fold 0
[[ 0  2  0  0  0]
 [ 0 26 10  1  0]
 [ 0 11 15  6  0]
 [ 0  4  6  7  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.60      0.70      0.65        37
          B1       0.48      0.47      0.48        32
          B2       0.47      0.41      0.44        17
          C1       0.00      0.00      0.00         1

    accuracy                           0.54        89
   macro avg       0.31      0.32      0.31        89
weighted avg       0.51      0.54      0.53        89


Fold 1
[[ 0  1  0  0  0]
 [ 0 28  8  1  0]
 [ 0 14 16  1  0]
 [ 0  4  3 10  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.60      0.76      0.67        37
          B1       0.59      0.52      0.55        31
          B2       0.77      0.59      0.67        17
          C1       0.00      0.00      0.00         1

    accuracy                           0.62        87
   macro avg       0.39      0.37      0.38        87
weighted avg       0.61      0.62      0.61        87


Fold 2
[[ 0  1  0  0  0]
 [ 0 26  8  3  0]
 [ 0 14 12  5  0]
 [ 0  4  4  8  0]
 [ 0  0  0  0  1]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.58      0.70      0.63        37
          B1       0.50      0.39      0.44        31
          B2       0.50      0.50      0.50        16
          C1       1.00      1.00      1.00         1

    accuracy                           0.55        86
   macro avg       0.52      0.52      0.51        86
weighted avg       0.53      0.55      0.53        86


Fold 3
[[ 0  1  0  0  0]
 [ 0 21 15  1  0]
 [ 0  7 18  6  0]
 [ 0  3  7  6  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.66      0.57      0.61        37
          B1       0.45      0.58      0.51        31
          B2       0.43      0.38      0.40        16
          C1       0.00      0.00      0.00         1

    accuracy                           0.52        86
   macro avg       0.31      0.30      0.30        86
weighted avg       0.52      0.52      0.52        86


Fold 4
[[ 0  0  1  0  0]
 [ 0 27  7  3  0]
 [ 0  8 18  5  0]
 [ 0  1  5 10  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.75      0.73      0.74        37
          B1       0.58      0.58      0.58        31
          B2       0.53      0.62      0.57        16
          C1       0.00      0.00      0.00         1

    accuracy                           0.64        86
   macro avg       0.37      0.39      0.38        86
weighted avg       0.63      0.64      0.63        86


K-fold scores
[0.5250066880684858, 0.6103844629409434, 0.5347754344351053, 0.5190703370882525, 0.6338688390297181]
SKF f1 score mean 0.5646211523125011

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3975
3975
Fold 0
[[ 0  2  0  0  0]
 [ 0 26 10  1  0]
 [ 0 11 15  6  0]
 [ 0  4  6  7  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.60      0.70      0.65        37
          B1       0.48      0.47      0.48        32
          B2       0.47      0.41      0.44        17
          C1       0.00      0.00      0.00         1

    accuracy                           0.54        89
   macro avg       0.31      0.32      0.31        89
weighted avg       0.51      0.54      0.53        89


Fold 1
[[ 0  1  0  0  0]
 [ 0 28  7  2  0]
 [ 0 12 17  2  0]
 [ 0  4  3 10  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.62      0.76      0.68        37
          B1       0.63      0.55      0.59        31
          B2       0.67      0.59      0.62        17
          C1       0.00      0.00      0.00         1

    accuracy                           0.63        87
   macro avg       0.38      0.38      0.38        87
weighted avg       0.62      0.63      0.62        87


Fold 2
[[ 0  1  0  0  0]
 [ 0 27  8  2  0]
 [ 0 15 11  5  0]
 [ 0  2  7  7  0]
 [ 0  0  0  0  1]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.60      0.73      0.66        37
          B1       0.42      0.35      0.39        31
          B2       0.50      0.44      0.47        16
          C1       1.00      1.00      1.00         1

    accuracy                           0.53        86
   macro avg       0.50      0.50      0.50        86
weighted avg       0.52      0.53      0.52        86


Fold 3
[[ 0  1  0  0  0]
 [ 0 23 13  1  0]
 [ 0  7 18  6  0]
 [ 0  3  7  6  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.68      0.62      0.65        37
          B1       0.47      0.58      0.52        31
          B2       0.43      0.38      0.40        16
          C1       0.00      0.00      0.00         1

    accuracy                           0.55        86
   macro avg       0.32      0.32      0.31        86
weighted avg       0.54      0.55      0.54        86


Fold 4
[[ 0  0  1  0  0]
 [ 0 25  9  3  0]
 [ 0  8 18  5  0]
 [ 0  0  3 13  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.76      0.68      0.71        37
          B1       0.58      0.58      0.58        31
          B2       0.59      0.81      0.68        16
          C1       0.00      0.00      0.00         1

    accuracy                           0.65        86
   macro avg       0.39      0.41      0.40        86
weighted avg       0.65      0.65      0.64        86


K-fold scores
[0.5250066880684858, 0.6214449020233366, 0.5209003791384303, 0.5412295817371366, 0.64390627732121]
SKF f1 score mean 0.5704975656577199

SAME LANG EVAL DONE FOR THIS LANG
Dep ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2943
2943
Fold 0
[[ 0  2  0  0  0]
 [ 0 29  8  0  0]
 [ 0 10 16  6  0]
 [ 0  3  5  9  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.66      0.78      0.72        37
          B1       0.55      0.50      0.52        32
          B2       0.56      0.53      0.55        17
          C1       0.00      0.00      0.00         1

    accuracy                           0.61        89
   macro avg       0.35      0.36      0.36        89
weighted avg       0.58      0.61      0.59        89


Fold 1
[[ 0  1  0  0  0]
 [ 0 32  5  0  0]
 [ 0 14 15  2  0]
 [ 0  2  7  8  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.65      0.86      0.74        37
          B1       0.56      0.48      0.52        31
          B2       0.73      0.47      0.57        17
          C1       0.00      0.00      0.00         1

    accuracy                           0.63        87
   macro avg       0.39      0.36      0.37        87
weighted avg       0.62      0.63      0.61        87


Fold 2
[[ 0  1  0  0  0]
 [ 0 28  8  1  0]
 [ 0 16 11  4  0]
 [ 0  1  3 12  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.61      0.76      0.67        37
          B1       0.50      0.35      0.42        31
          B2       0.67      0.75      0.71        16
          C1       0.00      0.00      0.00         1

    accuracy                           0.59        86
   macro avg       0.36      0.37      0.36        86
weighted avg       0.57      0.59      0.57        86


Fold 3
[[ 0  1  0  0  0]
 [ 0 29  8  0  0]
 [ 0 17  9  5  0]
 [ 0  1  7  8  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.60      0.78      0.68        37
          B1       0.38      0.29      0.33        31
          B2       0.57      0.50      0.53        16
          C1       0.00      0.00      0.00         1

    accuracy                           0.53        86
   macro avg       0.31      0.31      0.31        86
weighted avg       0.50      0.53      0.51        86


Fold 4
[[ 0  1  0  0  0]
 [ 0 32  5  0  0]
 [ 0  9 20  2  0]
 [ 0  1  8  7  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.74      0.86      0.80        37
          B1       0.61      0.65      0.62        31
          B2       0.70      0.44      0.54        16
          C1       0.00      0.00      0.00         1

    accuracy                           0.69        86
   macro avg       0.41      0.39      0.39        86
weighted avg       0.67      0.69      0.67        86


K-fold scores
[0.590488086282278, 0.6124557723429499, 0.5712313674075287, 0.5107656593292708, 0.6696556350626118]
SKF f1 score mean 0.5909193040849279

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2943
2943
Fold 0
[[ 0  2  0  0  0]
 [ 0 24 13  0  0]
 [ 0 11 14  7  0]
 [ 0  1  7  9  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.63      0.65      0.64        37
          B1       0.41      0.44      0.42        32
          B2       0.53      0.53      0.53        17
          C1       0.00      0.00      0.00         1

    accuracy                           0.53        89
   macro avg       0.31      0.32      0.32        89
weighted avg       0.51      0.53      0.52        89


Fold 1
[[ 0  1  0  0  0]
 [ 0 27 10  0  0]
 [ 0 14 13  4  0]
 [ 0  1  6 10  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.63      0.73      0.68        37
          B1       0.45      0.42      0.43        31
          B2       0.67      0.59      0.62        17
          C1       0.00      0.00      0.00         1

    accuracy                           0.57        87
   macro avg       0.35      0.35      0.35        87
weighted avg       0.56      0.57      0.56        87


Fold 2
[[ 0  0  1  0  0]
 [ 0 21 14  2  0]
 [ 0 14 11  6  0]
 [ 0  0  6 10  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.60      0.57      0.58        37
          B1       0.34      0.35      0.35        31
          B2       0.53      0.62      0.57        16
          C1       0.00      0.00      0.00         1

    accuracy                           0.49        86
   macro avg       0.29      0.31      0.30        86
weighted avg       0.48      0.49      0.48        86


Fold 3
[[ 0  1  0  0  0]
 [ 1 20 15  1  0]
 [ 0 15 10  6  0]
 [ 0  5  4  7  0]
 [ 0  0  0  0  1]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.49      0.54      0.51        37
          B1       0.34      0.32      0.33        31
          B2       0.50      0.44      0.47        16
          C1       1.00      1.00      1.00         1

    accuracy                           0.44        86
   macro avg       0.47      0.46      0.46        86
weighted avg       0.44      0.44      0.44        86


Fold 4
[[ 0  1  0  0  0]
 [ 0 26 10  1  0]
 [ 0 12 15  4  0]
 [ 0  1  4 11  0]
 [ 0  1  0  0  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.63      0.70      0.67        37
          B1       0.52      0.48      0.50        31
          B2       0.69      0.69      0.69        16
          C1       0.00      0.00      0.00         1

    accuracy                           0.60        86
   macro avg       0.37      0.37      0.37        86
weighted avg       0.59      0.60      0.59        86


K-fold scores
[0.5197276132107593, 0.5636015325670498, 0.48315799187892217, 0.4392367322599881, 0.5949612403100776]
SKF f1 score mean 0.5201370220453594

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2943
2943
Fold 0
[[ 0  2  0  0  0]
 [ 0 28  9  0  0]
 [ 0 12 12  8  0]
 [ 0  3  5  9  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.62      0.76      0.68        37
          B1       0.46      0.38      0.41        32
          B2       0.50      0.53      0.51        17
          C1       0.00      0.00      0.00         1

    accuracy                           0.55        89
   macro avg       0.32      0.33      0.32        89
weighted avg       0.52      0.55      0.53        89


Fold 1
[[ 0  1  0  0  0]
 [ 0 28  9  0  0]
 [ 0 13 14  4  0]
 [ 0  1  5 11  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.65      0.76      0.70        37
          B1       0.50      0.45      0.47        31
          B2       0.69      0.65      0.67        17
          C1       0.00      0.00      0.00         1

    accuracy                           0.61        87
   macro avg       0.37      0.37      0.37        87
weighted avg       0.59      0.61      0.60        87


Fold 2
[[ 0  0  1  0  0]
 [ 1 20 14  2  0]
 [ 0 12 12  7  0]
 [ 0  0  7  9  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.62      0.54      0.58        37
          B1       0.35      0.39      0.37        31
          B2       0.47      0.56      0.51        16
          C1       0.00      0.00      0.00         1

    accuracy                           0.48        86
   macro avg       0.29      0.30      0.29        86
weighted avg       0.48      0.48      0.48        86


Fold 3
[[ 0  1  0  0  0]
 [ 0 20 17  0  0]
 [ 0 15 10  6  0]
 [ 0  4  4  8  0]
 [ 0  0  0  0  1]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.50      0.54      0.52        37
          B1       0.32      0.32      0.32        31
          B2       0.57      0.50      0.53        16
          C1       1.00      1.00      1.00         1

    accuracy                           0.45        86
   macro avg       0.48      0.47      0.48        86
weighted avg       0.45      0.45      0.45        86


Fold 4
[[ 0  1  0  0  0]
 [ 0 25 11  1  0]
 [ 0 11 14  6  0]
 [ 0  1  5 10  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.66      0.68      0.67        37
          B1       0.47      0.45      0.46        31
          B2       0.56      0.62      0.59        16
          C1       0.00      0.00      0.00         1

    accuracy                           0.57        86
   macro avg       0.34      0.35      0.34        86
weighted avg       0.55      0.57      0.56        86


K-fold scores
[0.5309272936643685, 0.5970712383921034, 0.47818605391911756, 0.45062921574549486, 0.5617202275496551]
SKF f1 score mean 0.523706805854148

SAME LANG EVAL DONE FOR THIS LANG
************for dimension:  Orthography  ***************
Extracted all features: 
Printing class statistics
Counter({'B1': 269, 'B2': 114, 'A2': 46, 'C1': 5})
With Word ngrams: 
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
1161
1161
Fold 0
[[ 3  7  0  0]
 [ 1 52  1  0]
 [ 0 18  5  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.75      0.30      0.43        10
          B1       0.68      0.96      0.79        54
          B2       0.71      0.22      0.33        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.68        88
   macro avg       0.53      0.37      0.39        88
weighted avg       0.69      0.68      0.62        88


Fold 1
[[ 0  9  0  0]
 [ 1 52  1  0]
 [ 0 17  6  0]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A2       0.00      0.00      0.00         9
          B1       0.66      0.96      0.78        54
          B2       0.86      0.26      0.40        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.67        87
   macro avg       0.38      0.31      0.30        87
weighted avg       0.64      0.67      0.59        87


Fold 2
[[ 4  5  0  0]
 [ 0 53  1  0]
 [ 0 17  6  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       1.00      0.44      0.62         9
          B1       0.71      0.98      0.82        54
          B2       0.75      0.26      0.39        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.72        87
   macro avg       0.61      0.42      0.46        87
weighted avg       0.74      0.72      0.68        87


Fold 3
[[ 1  8  0  0]
 [ 0 46  8  0]
 [ 0 15  8  0]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A2       1.00      0.11      0.20         9
          B1       0.66      0.85      0.74        54
          B2       0.50      0.35      0.41        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.63        87
   macro avg       0.54      0.33      0.34        87
weighted avg       0.64      0.63      0.59        87


Fold 4
[[ 2  7  0  0]
 [ 3 47  3  0]
 [ 0 13  9  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.40      0.22      0.29         9
          B1       0.70      0.89      0.78        53
          B2       0.69      0.41      0.51        22
          C1       0.00      0.00      0.00         1

    accuracy                           0.68        85
   macro avg       0.45      0.38      0.40        85
weighted avg       0.66      0.68      0.65        85


K-fold scores
[0.6229842040910744, 0.5910984357445338, 0.6760204640019422, 0.5896599260336746, 0.6517927170868347]
SKF f1 score mean 0.6263111493916119

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
1161
1161
Fold 0
[[ 3  5  1  1]
 [ 3 43  8  0]
 [ 0 14  9  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.50      0.30      0.37        10
          B1       0.69      0.80      0.74        54
          B2       0.47      0.39      0.43        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.62        88
   macro avg       0.42      0.37      0.39        88
weighted avg       0.61      0.62      0.61        88


Fold 1
[[ 4  4  1  0]
 [ 5 43  6  0]
 [ 2 11 10  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.36      0.44      0.40         9
          B1       0.74      0.80      0.77        54
          B2       0.56      0.43      0.49        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.66        87
   macro avg       0.42      0.42      0.41        87
weighted avg       0.64      0.66      0.65        87


Fold 2
[[ 5  3  1  0]
 [ 3 42  9  0]
 [ 1 12 10  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.56      0.56      0.56         9
          B1       0.74      0.78      0.76        54
          B2       0.48      0.43      0.45        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.66        87
   macro avg       0.44      0.44      0.44        87
weighted avg       0.64      0.66      0.65        87


Fold 3
[[ 1  8  0  0]
 [ 1 37 16  0]
 [ 1 14  8  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.33      0.11      0.17         9
          B1       0.63      0.69      0.65        54
          B2       0.32      0.35      0.33        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.53        87
   macro avg       0.32      0.29      0.29        87
weighted avg       0.51      0.53      0.51        87


Fold 4
[[ 2  7  0  0]
 [ 5 34 13  1]
 [ 0 11 11  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.29      0.22      0.25         9
          B1       0.65      0.64      0.65        53
          B2       0.44      0.50      0.47        22
          C1       0.00      0.00      0.00         1

    accuracy                           0.55        85
   macro avg       0.34      0.34      0.34        85
weighted avg       0.55      0.55      0.55        85


K-fold scores
[0.6095639274518585, 0.6469402058552606, 0.647349543901268, 0.511833316380158, 0.5514315513439418]
SKF f1 score mean 0.5934237089864973

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
1161
1161
Fold 0
[[ 3  5  1  1]
 [ 1 45  8  0]
 [ 0 15  8  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.75      0.30      0.43        10
          B1       0.69      0.83      0.76        54
          B2       0.44      0.35      0.39        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.64        88
   macro avg       0.47      0.37      0.39        88
weighted avg       0.63      0.64      0.61        88


Fold 1
[[ 4  4  1  0]
 [ 5 44  5  0]
 [ 0 11 12  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.44      0.44      0.44         9
          B1       0.75      0.81      0.78        54
          B2       0.63      0.52      0.57        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.69        87
   macro avg       0.46      0.45      0.45        87
weighted avg       0.68      0.69      0.68        87


Fold 2
[[ 5  3  1  0]
 [ 2 44  8  0]
 [ 0 14  9  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.71      0.56      0.63         9
          B1       0.72      0.81      0.77        54
          B2       0.47      0.39      0.43        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.67        87
   macro avg       0.48      0.44      0.45        87
weighted avg       0.65      0.67      0.65        87


Fold 3
[[ 2  7  0  0]
 [ 1 37 16  0]
 [ 0 13  9  1]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.67      0.22      0.33         9
          B1       0.65      0.69      0.67        54
          B2       0.35      0.39      0.37        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.55        87
   macro avg       0.42      0.32      0.34        87
weighted avg       0.56      0.55      0.55        87


Fold 4
[[ 2  6  1  0]
 [ 6 38  9  0]
 [ 0  8 14  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.25      0.22      0.24         9
          B1       0.73      0.72      0.72        53
          B2       0.56      0.64      0.60        22
          C1       0.00      0.00      0.00         1

    accuracy                           0.64        85
   macro avg       0.39      0.39      0.39        85
weighted avg       0.63      0.64      0.63        85


K-fold scores
[0.6147915929121095, 0.6804132699768953, 0.6529181837652602, 0.5453905700211119, 0.6304227623464905]
SKF f1 score mean 0.6247872758043734

SAME LANG EVAL DONE FOR THIS LANG
With POS ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3975
3975
Fold 0
[[ 1  9  0  0]
 [ 2 48  4  0]
 [ 0 20  3  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.33      0.10      0.15        10
          B1       0.62      0.89      0.73        54
          B2       0.38      0.13      0.19        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.59        88
   macro avg       0.33      0.28      0.27        88
weighted avg       0.52      0.59      0.52        88


Fold 1
[[ 1  8  0  0]
 [ 0 51  3  0]
 [ 0 18  5  0]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A2       1.00      0.11      0.20         9
          B1       0.65      0.94      0.77        54
          B2       0.62      0.22      0.32        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.66        87
   macro avg       0.57      0.32      0.32        87
weighted avg       0.67      0.66      0.59        87


Fold 2
[[ 1  8  0  0]
 [ 1 53  0  0]
 [ 0 19  4  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.50      0.11      0.18         9
          B1       0.66      0.98      0.79        54
          B2       0.80      0.17      0.29        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.67        87
   macro avg       0.49      0.32      0.31        87
weighted avg       0.67      0.67      0.59        87


Fold 3
[[ 1  8  0  0]
 [ 0 54  0  0]
 [ 0 20  3  0]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A2       1.00      0.11      0.20         9
          B1       0.65      1.00      0.79        54
          B2       1.00      0.13      0.23        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.67        87
   macro avg       0.66      0.31      0.30        87
weighted avg       0.77      0.67      0.57        87


Fold 4
[[ 0  9  0  0]
 [ 2 50  1  0]
 [ 0 15  7  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.00      0.00      0.00         9
          B1       0.68      0.94      0.79        53
          B2       0.78      0.32      0.45        22
          C1       0.00      0.00      0.00         1

    accuracy                           0.67        85
   macro avg       0.36      0.32      0.31        85
weighted avg       0.62      0.67      0.61        85


K-fold scores
[0.5177567446097365, 0.5855934202986484, 0.5853357484855168, 0.5710004065906406, 0.607856086300408]
SKF f1 score mean 0.57350848125699

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3975
3975
Fold 0
[[ 3  6  1  0]
 [ 5 42  7  0]
 [ 1 14  6  2]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A2       0.33      0.30      0.32        10
          B1       0.67      0.78      0.72        54
          B2       0.43      0.26      0.32        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.58        88
   macro avg       0.36      0.33      0.34        88
weighted avg       0.56      0.58      0.56        88


Fold 1
[[ 6  2  1  0]
 [ 5 37 12  0]
 [ 0 14  9  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.55      0.67      0.60         9
          B1       0.70      0.69      0.69        54
          B2       0.39      0.39      0.39        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.60        87
   macro avg       0.41      0.44      0.42        87
weighted avg       0.59      0.60      0.59        87


Fold 2
[[ 1  8  0  0]
 [ 6 40  8  0]
 [ 1 13  9  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.12      0.11      0.12         9
          B1       0.66      0.74      0.70        54
          B2       0.50      0.39      0.44        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.57        87
   macro avg       0.32      0.31      0.31        87
weighted avg       0.55      0.57      0.56        87


Fold 3
[[ 1  6  2  0]
 [ 5 34 15  0]
 [ 1 15  7  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.14      0.11      0.12         9
          B1       0.62      0.63      0.62        54
          B2       0.28      0.30      0.29        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.48        87
   macro avg       0.26      0.26      0.26        87
weighted avg       0.47      0.48      0.48        87


Fold 4
[[ 2  6  1  0]
 [ 4 41  8  0]
 [ 0  8 14  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.33      0.22      0.27         9
          B1       0.75      0.77      0.76        53
          B2       0.58      0.64      0.61        22
          C1       0.00      0.00      0.00         1

    accuracy                           0.67        85
   macro avg       0.42      0.41      0.41        85
weighted avg       0.65      0.67      0.66        85


K-fold scores
[0.5612111927901401, 0.5947792458910731, 0.5600184126014468, 0.4772575485957327, 0.6592005304537274]
SKF f1 score mean 0.570493386066424

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3975
3975
Fold 0
[[ 4  5  1  0]
 [ 3 45  6  0]
 [ 0 15  6  2]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A2       0.57      0.40      0.47        10
          B1       0.68      0.83      0.75        54
          B2       0.46      0.26      0.33        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.62        88
   macro avg       0.43      0.37      0.39        88
weighted avg       0.60      0.62      0.60        88


Fold 1
[[ 5  2  2  0]
 [ 4 43  7  0]
 [ 0 13 10  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.56      0.56      0.56         9
          B1       0.74      0.80      0.77        54
          B2       0.50      0.43      0.47        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.67        87
   macro avg       0.45      0.45      0.45        87
weighted avg       0.65      0.67      0.66        87


Fold 2
[[ 1  8  0  0]
 [ 6 41  7  0]
 [ 0 13 10  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.14      0.11      0.12         9
          B1       0.66      0.76      0.71        54
          B2       0.56      0.43      0.49        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.60        87
   macro avg       0.34      0.33      0.33        87
weighted avg       0.57      0.60      0.58        87


Fold 3
[[ 1  6  2  0]
 [ 1 37 16  0]
 [ 1 14  8  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.33      0.11      0.17         9
          B1       0.65      0.69      0.67        54
          B2       0.30      0.35      0.32        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.53        87
   macro avg       0.32      0.29      0.29        87
weighted avg       0.52      0.53      0.52        87


Fold 4
[[ 1  7  1  0]
 [ 4 41  8  0]
 [ 0  8 14  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.20      0.11      0.14         9
          B1       0.73      0.77      0.75        53
          B2       0.58      0.64      0.61        22
          C1       0.00      0.00      0.00         1

    accuracy                           0.66        85
   macro avg       0.38      0.38      0.38        85
weighted avg       0.63      0.66      0.64        85


K-fold scores
[0.6008244206773617, 0.6570340245159813, 0.5806543217037401, 0.515632183908046, 0.6417479796066811]
SKF f1 score mean 0.5991785860823621

SAME LANG EVAL DONE FOR THIS LANG
Dep ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2943
2943
Fold 0
[[ 2  8  0  0]
 [ 2 51  1  0]
 [ 0 18  5  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.50      0.20      0.29        10
          B1       0.66      0.94      0.78        54
          B2       0.71      0.22      0.33        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.66        88
   macro avg       0.47      0.34      0.35        88
weighted avg       0.65      0.66      0.60        88


Fold 1
[[ 0  9  0  0]
 [ 0 51  3  0]
 [ 0 17  6  0]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A2       0.00      0.00      0.00         9
          B1       0.65      0.94      0.77        54
          B2       0.67      0.26      0.38        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.66        87
   macro avg       0.33      0.30      0.29        87
weighted avg       0.58      0.66      0.58        87


Fold 2
[[ 2  7  0  0]
 [ 1 52  1  0]
 [ 0 21  2  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.67      0.22      0.33         9
          B1       0.65      0.96      0.78        54
          B2       0.50      0.09      0.15        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.64        87
   macro avg       0.45      0.32      0.31        87
weighted avg       0.60      0.64      0.56        87


Fold 3
[[ 2  7  0  0]
 [ 2 50  2  0]
 [ 0 19  4  0]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A2       0.50      0.22      0.31         9
          B1       0.65      0.93      0.76        54
          B2       0.67      0.17      0.28        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.64        87
   macro avg       0.45      0.33      0.34        87
weighted avg       0.63      0.64      0.58        87


Fold 4
[[ 0  9  0  0]
 [ 2 50  1  0]
 [ 0 16  6  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.00      0.00      0.00         9
          B1       0.67      0.94      0.78        53
          B2       0.75      0.27      0.40        22
          C1       0.00      0.00      0.00         1

    accuracy                           0.66        85
   macro avg       0.35      0.30      0.30        85
weighted avg       0.61      0.66      0.59        85


K-fold scores
[0.5973819437559895, 0.5787617554858934, 0.5553776456161085, 0.5785681885199907, 0.5906617647058823]
SKF f1 score mean 0.5801502596167729

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2943
2943
Fold 0
[[ 2  7  1  0]
 [ 5 35 14  0]
 [ 2 10 11  0]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A2       0.22      0.20      0.21        10
          B1       0.66      0.65      0.65        54
          B2       0.42      0.48      0.45        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.55        88
   macro avg       0.33      0.33      0.33        88
weighted avg       0.54      0.55      0.54        88


Fold 1
[[ 2  6  1  0]
 [ 3 37 14  0]
 [ 0 13 10  0]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A2       0.40      0.22      0.29         9
          B1       0.65      0.69      0.67        54
          B2       0.40      0.43      0.42        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.56        87
   macro avg       0.36      0.34      0.34        87
weighted avg       0.55      0.56      0.55        87


Fold 2
[[ 1  8  0  0]
 [ 4 39 11  0]
 [ 0 14  9  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.20      0.11      0.14         9
          B1       0.64      0.72      0.68        54
          B2       0.43      0.39      0.41        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.56        87
   macro avg       0.32      0.31      0.31        87
weighted avg       0.53      0.56      0.54        87


Fold 3
[[ 1  7  1  0]
 [ 2 42 10  0]
 [ 1 17  5  0]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A2       0.25      0.11      0.15         9
          B1       0.63      0.78      0.69        54
          B2       0.31      0.22      0.26        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.55        87
   macro avg       0.30      0.28      0.28        87
weighted avg       0.50      0.55      0.51        87


Fold 4
[[ 2  7  0  0]
 [ 3 38 12  0]
 [ 1  9 12  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.33      0.22      0.27         9
          B1       0.70      0.72      0.71        53
          B2       0.48      0.55      0.51        22
          C1       0.00      0.00      0.00         1

    accuracy                           0.61        85
   macro avg       0.38      0.37      0.37        85
weighted avg       0.60      0.61      0.60        85


K-fold scores
[0.5427147337940678, 0.5535030103995621, 0.5439183005899647, 0.5145937308946714, 0.6032812043091247]
SKF f1 score mean 0.5516021959974782

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2943
2943
Fold 0
[[ 2  7  1  0]
 [ 5 35 14  0]
 [ 1 11 11  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.25      0.20      0.22        10
          B1       0.66      0.65      0.65        54
          B2       0.41      0.48      0.44        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.55        88
   macro avg       0.33      0.33      0.33        88
weighted avg       0.54      0.55      0.54        88


Fold 1
[[ 2  6  1  0]
 [ 2 40 12  0]
 [ 0 11 12  0]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A2       0.50      0.22      0.31         9
          B1       0.69      0.74      0.71        54
          B2       0.48      0.52      0.50        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.62        87
   macro avg       0.42      0.37      0.38        87
weighted avg       0.61      0.62      0.61        87


Fold 2
[[ 1  8  0  0]
 [ 5 40  9  0]
 [ 0 14  9  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.17      0.11      0.13         9
          B1       0.65      0.74      0.69        54
          B2       0.47      0.39      0.43        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.57        87
   macro avg       0.32      0.31      0.31        87
weighted avg       0.54      0.57      0.56        87


Fold 3
[[ 1  7  1  0]
 [ 2 42 10  0]
 [ 1 13  9  0]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A2       0.25      0.11      0.15         9
          B1       0.67      0.78      0.72        54
          B2       0.45      0.39      0.42        23
          C1       0.00      0.00      0.00         1

    accuracy                           0.60        87
   macro avg       0.34      0.32      0.32        87
weighted avg       0.56      0.60      0.57        87


Fold 4
[[ 2  7  0  0]
 [ 3 40 10  0]
 [ 1  9 12  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.33      0.22      0.27         9
          B1       0.71      0.75      0.73        53
          B2       0.52      0.55      0.53        22
          C1       0.00      0.00      0.00         1

    accuracy                           0.64        85
   macro avg       0.39      0.38      0.38        85
weighted avg       0.62      0.64      0.62        85


K-fold scores
[0.5416968752950062, 0.6073639004673487, 0.5551554272125021, 0.5722040589723028, 0.623910775319302]
SKF f1 score mean 0.5800662074532924

SAME LANG EVAL DONE FOR THIS LANG
************for dimension:  Vocabularyrange  ***************
Extracted all features: 
Printing class statistics
Counter({'B2': 143, 'B1': 141, 'A2': 132, 'C1': 10, 'A1': 8})
With Word ngrams: 
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
1161
1161
Fold 0
[[ 0  2  0  0  0]
 [ 0 21  6  0  0]
 [ 0  6 18  5  0]
 [ 0  0  0 29  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.72      0.78      0.75        27
          B1       0.75      0.62      0.68        29
          B2       0.81      1.00      0.89        29
          C1       0.00      0.00      0.00         2

    accuracy                           0.76        89
   macro avg       0.46      0.48      0.46        89
weighted avg       0.73      0.76      0.74        89


Fold 1
[[ 0  2  0  0  0]
 [ 0 17 10  0  0]
 [ 0  6 17  5  0]
 [ 0  0  0 29  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.68      0.63      0.65        27
          B1       0.63      0.61      0.62        28
          B2       0.81      1.00      0.89        29
          C1       0.00      0.00      0.00         2

    accuracy                           0.72        88
   macro avg       0.42      0.45      0.43        88
weighted avg       0.67      0.72      0.69        88


Fold 2
[[ 0  2  0  0  0]
 [ 0 18  8  0  0]
 [ 0  6 15  7  0]
 [ 0  0  0 29  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.69      0.69      0.69        26
          B1       0.65      0.54      0.59        28
          B2       0.76      1.00      0.87        29
          C1       0.00      0.00      0.00         2

    accuracy                           0.71        87
   macro avg       0.42      0.45      0.43        87
weighted avg       0.67      0.71      0.68        87


Fold 3
[[ 0  1  0  0  0]
 [ 0 18  8  0  0]
 [ 0  2 22  4  0]
 [ 0  0  0 28  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.86      0.69      0.77        26
          B1       0.73      0.79      0.76        28
          B2       0.82      1.00      0.90        28
          C1       0.00      0.00      0.00         2

    accuracy                           0.80        85
   macro avg       0.48      0.50      0.49        85
weighted avg       0.78      0.80      0.78        85


Fold 4
[[ 0  1  0  0  0]
 [ 0 18  8  0  0]
 [ 0  1 23  4  0]
 [ 0  0  0 28  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.90      0.69      0.78        26
          B1       0.74      0.82      0.78        28
          B2       0.82      1.00      0.90        28
          C1       0.00      0.00      0.00         2

    accuracy                           0.81        85
   macro avg       0.49      0.50      0.49        85
weighted avg       0.79      0.81      0.79        85


K-fold scores
[0.7396071492637107, 0.6913620470438652, 0.6847708718072921, 0.7817246530354266, 0.7937489075550348]
SKF f1 score mean 0.738242725741066

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
1161
1161
Fold 0
[[ 0  2  0  0  0]
 [ 1 22  4  0  0]
 [ 0 11 16  2  0]
 [ 0  0  1 28  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.63      0.81      0.71        27
          B1       0.76      0.55      0.64        29
          B2       0.88      0.97      0.92        29
          C1       0.00      0.00      0.00         2

    accuracy                           0.74        89
   macro avg       0.45      0.47      0.45        89
weighted avg       0.72      0.74      0.72        89


Fold 1
[[ 0  2  0  0  0]
 [ 0 18  9  0  0]
 [ 0  8 16  4  0]
 [ 0  0  3 25  1]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.64      0.67      0.65        27
          B1       0.57      0.57      0.57        28
          B2       0.81      0.86      0.83        29
          C1       0.00      0.00      0.00         2

    accuracy                           0.67        88
   macro avg       0.40      0.42      0.41        88
weighted avg       0.64      0.67      0.66        88


Fold 2
[[ 0  2  0  0  0]
 [ 0 18  8  0  0]
 [ 0  6 16  5  1]
 [ 0  0  2 27  0]
 [ 0  0  1  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.69      0.69      0.69        26
          B1       0.59      0.57      0.58        28
          B2       0.82      0.93      0.87        29
          C1       0.00      0.00      0.00         2

    accuracy                           0.70        87
   macro avg       0.42      0.44      0.43        87
weighted avg       0.67      0.70      0.68        87


Fold 3
[[ 1  0  0  0  0]
 [ 0 17  9  0  0]
 [ 1 10 14  3  0]
 [ 0  0  3 25  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.50      1.00      0.67         1
          A2       0.63      0.65      0.64        26
          B1       0.54      0.50      0.52        28
          B2       0.83      0.89      0.86        28
          C1       0.00      0.00      0.00         2

    accuracy                           0.67        85
   macro avg       0.50      0.61      0.54        85
weighted avg       0.65      0.67      0.66        85


Fold 4
[[ 0  1  0  0  0]
 [ 1 14 11  0  0]
 [ 0  7 17  4  0]
 [ 0  0  8 19  1]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.64      0.54      0.58        26
          B1       0.47      0.61      0.53        28
          B2       0.76      0.68      0.72        28
          C1       0.00      0.00      0.00         2

    accuracy                           0.59        85
   macro avg       0.37      0.36      0.37        85
weighted avg       0.60      0.59      0.59        85


K-fold scores
[0.7229690015983458, 0.6572658402203857, 0.6844709610004381, 0.6588513117963154, 0.5896133925268221]
SKF f1 score mean 0.6626341014284614

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
1161
1161
Fold 0
[[ 0  2  0  0  0]
 [ 1 22  4  0  0]
 [ 0 11 14  4  0]
 [ 0  0  0 29  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.63      0.81      0.71        27
          B1       0.78      0.48      0.60        29
          B2       0.83      1.00      0.91        29
          C1       0.00      0.00      0.00         2

    accuracy                           0.73        89
   macro avg       0.45      0.46      0.44        89
weighted avg       0.71      0.73      0.70        89


Fold 1
[[ 0  2  0  0  0]
 [ 0 20  7  0  0]
 [ 0  9 15  4  0]
 [ 0  0  4 24  1]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.65      0.74      0.69        27
          B1       0.58      0.54      0.56        28
          B2       0.80      0.83      0.81        29
          C1       0.00      0.00      0.00         2

    accuracy                           0.67        88
   macro avg       0.40      0.42      0.41        88
weighted avg       0.65      0.67      0.66        88


Fold 2
[[ 0  2  0  0  0]
 [ 0 18  8  0  0]
 [ 0  6 16  6  0]
 [ 0  0  1 27  1]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.69      0.69      0.69        26
          B1       0.64      0.57      0.60        28
          B2       0.77      0.93      0.84        29
          C1       0.00      0.00      0.00         2

    accuracy                           0.70        87
   macro avg       0.42      0.44      0.43        87
weighted avg       0.67      0.70      0.68        87


Fold 3
[[ 1  0  0  0  0]
 [ 0 19  7  0  0]
 [ 0  7 18  3  0]
 [ 0  0  2 26  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       1.00      1.00      1.00         1
          A2       0.73      0.73      0.73        26
          B1       0.67      0.64      0.65        28
          B2       0.84      0.93      0.88        28
          C1       0.00      0.00      0.00         2

    accuracy                           0.75        85
   macro avg       0.65      0.66      0.65        85
weighted avg       0.73      0.75      0.74        85


Fold 4
[[ 0  1  0  0  0]
 [ 0 14 12  0  0]
 [ 0  5 19  4  0]
 [ 0  0  6 21  1]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.70      0.54      0.61        26
          B1       0.51      0.68      0.58        28
          B2       0.78      0.75      0.76        28
          C1       0.00      0.00      0.00         2

    accuracy                           0.64        85
   macro avg       0.40      0.39      0.39        85
weighted avg       0.64      0.64      0.63        85


K-fold scores
[0.7047093940141741, 0.6564711994285343, 0.6824644870960748, 0.7412381038702076, 0.6303192459714199]
SKF f1 score mean 0.6830404860760821

SAME LANG EVAL DONE FOR THIS LANG
With POS ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3975
3975
Fold 0
[[ 0  2  0  0  0]
 [ 0 21  6  0  0]
 [ 0  5 19  5  0]
 [ 0  0  1 28  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.75      0.78      0.76        27
          B1       0.73      0.66      0.69        29
          B2       0.80      0.97      0.88        29
          C1       0.00      0.00      0.00         2

    accuracy                           0.76        89
   macro avg       0.46      0.48      0.47        89
weighted avg       0.73      0.76      0.74        89


Fold 1
[[ 0  2  0  0  0]
 [ 0 18  9  0  0]
 [ 0  6 16  6  0]
 [ 0  0  0 29  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.69      0.67      0.68        27
          B1       0.64      0.57      0.60        28
          B2       0.78      1.00      0.88        29
          C1       0.00      0.00      0.00         2

    accuracy                           0.72        88
   macro avg       0.42      0.45      0.43        88
weighted avg       0.67      0.72      0.69        88


Fold 2
[[ 0  2  0  0  0]
 [ 0 18  8  0  0]
 [ 0  6 17  5  0]
 [ 0  0  0 29  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.69      0.69      0.69        26
          B1       0.68      0.61      0.64        28
          B2       0.81      1.00      0.89        29
          C1       0.00      0.00      0.00         2

    accuracy                           0.74        87
   macro avg       0.44      0.46      0.45        87
weighted avg       0.69      0.74      0.71        87


Fold 3
[[ 0  1  0  0  0]
 [ 0 20  6  0  0]
 [ 0  2 22  4  0]
 [ 0  1  3 24  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.83      0.77      0.80        26
          B1       0.71      0.79      0.75        28
          B2       0.80      0.86      0.83        28
          C1       0.00      0.00      0.00         2

    accuracy                           0.78        85
   macro avg       0.47      0.48      0.47        85
weighted avg       0.75      0.78      0.76        85


Fold 4
[[ 0  1  0  0  0]
 [ 0 17  9  0  0]
 [ 0  1 22  5  0]
 [ 0  0  0 28  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.89      0.65      0.76        26
          B1       0.71      0.79      0.75        28
          B2       0.80      1.00      0.89        28
          C1       0.00      0.00      0.00         2

    accuracy                           0.79        85
   macro avg       0.48      0.49      0.48        85
weighted avg       0.77      0.79      0.77        85


K-fold scores
[0.7419050051072523, 0.6901151307240501, 0.710795255492718, 0.7629855261800805, 0.7695845795945496]
SKF f1 score mean 0.7350770994197301

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3975
3975
Fold 0
[[ 0  1  1  0  0]
 [ 0 19  7  1  0]
 [ 1 12 13  3  0]
 [ 0  1  2 26  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.58      0.70      0.63        27
          B1       0.57      0.45      0.50        29
          B2       0.81      0.90      0.85        29
          C1       0.00      0.00      0.00         2

    accuracy                           0.65        89
   macro avg       0.39      0.41      0.40        89
weighted avg       0.62      0.65      0.63        89


Fold 1
[[ 0  1  1  0  0]
 [ 0 16 10  1  0]
 [ 0 10 14  4  0]
 [ 0  0  6 23  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.59      0.59      0.59        27
          B1       0.45      0.50      0.47        28
          B2       0.77      0.79      0.78        29
          C1       0.00      0.00      0.00         2

    accuracy                           0.60        88
   macro avg       0.36      0.38      0.37        88
weighted avg       0.58      0.60      0.59        88


Fold 2
[[ 0  1  1  0  0]
 [ 0 16  9  1  0]
 [ 0  5 17  6  0]
 [ 0  0  6 22  1]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.73      0.62      0.67        26
          B1       0.52      0.61      0.56        28
          B2       0.71      0.76      0.73        29
          C1       0.00      0.00      0.00         2

    accuracy                           0.63        87
   macro avg       0.39      0.40      0.39        87
weighted avg       0.62      0.63      0.62        87


Fold 3
[[ 0  1  0  0  0]
 [ 0 20  6  0  0]
 [ 0  8 15  5  0]
 [ 0  3  4 21  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.62      0.77      0.69        26
          B1       0.60      0.54      0.57        28
          B2       0.75      0.75      0.75        28
          C1       0.00      0.00      0.00         2

    accuracy                           0.66        85
   macro avg       0.40      0.41      0.40        85
weighted avg       0.64      0.66      0.64        85


Fold 4
[[ 0  1  0  0  0]
 [ 0 17  9  0  0]
 [ 0 10 14  4  0]
 [ 0  2  5 21  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.57      0.65      0.61        26
          B1       0.50      0.50      0.50        28
          B2       0.78      0.75      0.76        28
          C1       0.00      0.00      0.00         2

    accuracy                           0.61        85
   macro avg       0.37      0.38      0.37        85
weighted avg       0.59      0.61      0.60        85


K-fold scores
[0.6328237244428071, 0.589753466872111, 0.623063877897117, 0.6444716598415554, 0.6019709702062643]
SKF f1 score mean 0.618416739851971

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3975
3975
Fold 0
[[ 0  1  1  0  0]
 [ 0 18  8  1  0]
 [ 1 11 15  2  0]
 [ 0  1  2 26  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.58      0.67      0.62        27
          B1       0.58      0.52      0.55        29
          B2       0.84      0.90      0.87        29
          C1       0.00      0.00      0.00         2

    accuracy                           0.66        89
   macro avg       0.40      0.42      0.41        89
weighted avg       0.64      0.66      0.65        89


Fold 1
[[ 0  2  0  0  0]
 [ 0 15 11  1  0]
 [ 0  7 17  4  0]
 [ 0  0  5 24  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.62      0.56      0.59        27
          B1       0.52      0.61      0.56        28
          B2       0.77      0.83      0.80        29
          C1       0.00      0.00      0.00         2

    accuracy                           0.64        88
   macro avg       0.38      0.40      0.39        88
weighted avg       0.61      0.64      0.62        88


Fold 2
[[ 0  1  1  0  0]
 [ 0 14 10  1  1]
 [ 0  5 17  6  0]
 [ 0  0  5 23  1]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.70      0.54      0.61        26
          B1       0.52      0.61      0.56        28
          B2       0.72      0.79      0.75        29
          C1       0.00      0.00      0.00         2

    accuracy                           0.62        87
   macro avg       0.39      0.39      0.38        87
weighted avg       0.61      0.62      0.61        87


Fold 3
[[ 0  1  0  0  0]
 [ 0 20  6  0  0]
 [ 0  6 17  5  0]
 [ 0  3  3 22  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.67      0.77      0.71        26
          B1       0.65      0.61      0.63        28
          B2       0.76      0.79      0.77        28
          C1       0.00      0.00      0.00         2

    accuracy                           0.69        85
   macro avg       0.42      0.43      0.42        85
weighted avg       0.67      0.69      0.68        85


Fold 4
[[ 0  1  0  0  0]
 [ 0 16 10  0  0]
 [ 0  9 14  5  0]
 [ 0  2  1 24  1]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.57      0.62      0.59        26
          B1       0.56      0.50      0.53        28
          B2       0.77      0.86      0.81        28
          C1       0.00      0.00      0.00         2

    accuracy                           0.64        85
   macro avg       0.38      0.39      0.39        85
weighted avg       0.61      0.64      0.62        85


K-fold scores
[0.6484284925974193, 0.621464889979837, 0.6126608826734173, 0.6801775681032647, 0.6232884853475912]
SKF f1 score mean 0.6372040637403058

SAME LANG EVAL DONE FOR THIS LANG
Dep ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2943
2943
Fold 0
[[ 0  2  0  0  0]
 [ 0 22  5  0  0]
 [ 0  6 19  4  0]
 [ 0  2  1 26  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.69      0.81      0.75        27
          B1       0.76      0.66      0.70        29
          B2       0.81      0.90      0.85        29
          C1       0.00      0.00      0.00         2

    accuracy                           0.75        89
   macro avg       0.45      0.47      0.46        89
weighted avg       0.72      0.75      0.73        89


Fold 1
[[ 0  2  0  0  0]
 [ 0 19  8  0  0]
 [ 0  7 15  6  0]
 [ 0  0  1 28  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.68      0.70      0.69        27
          B1       0.62      0.54      0.58        28
          B2       0.78      0.97      0.86        29
          C1       0.00      0.00      0.00         2

    accuracy                           0.70        88
   macro avg       0.42      0.44      0.43        88
weighted avg       0.66      0.70      0.68        88


Fold 2
[[ 0  2  0  0  0]
 [ 0 18  8  0  0]
 [ 0  7 17  4  0]
 [ 0  0  0 29  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.67      0.69      0.68        26
          B1       0.68      0.61      0.64        28
          B2       0.83      1.00      0.91        29
          C1       0.00      0.00      0.00         2

    accuracy                           0.74        87
   macro avg       0.44      0.46      0.45        87
weighted avg       0.69      0.74      0.71        87


Fold 3
[[ 0  1  0  0  0]
 [ 0 22  4  0  0]
 [ 0  3 23  2  0]
 [ 0  0  0 28  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.85      0.85      0.85        26
          B1       0.85      0.82      0.84        28
          B2       0.88      1.00      0.93        28
          C1       0.00      0.00      0.00         2

    accuracy                           0.86        85
   macro avg       0.51      0.53      0.52        85
weighted avg       0.83      0.86      0.84        85


Fold 4
[[ 0  1  0  0  0]
 [ 0 18  8  0  0]
 [ 0  2 20  6  0]
 [ 0  0  0 28  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.86      0.69      0.77        26
          B1       0.71      0.71      0.71        28
          B2       0.78      1.00      0.88        28
          C1       0.00      0.00      0.00         2

    accuracy                           0.78        85
   macro avg       0.47      0.48      0.47        85
weighted avg       0.75      0.78      0.76        85


K-fold scores
[0.733306877563553, 0.6794659885568977, 0.7115389828670571, 0.841782531194296, 0.7578222778473092]
SKF f1 score mean 0.7447833316058226

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2943
2943
Fold 0
[[ 0  1  1  0  0]
 [ 0 23  4  0  0]
 [ 0 12 12  5  0]
 [ 0  2  1 26  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.61      0.85      0.71        27
          B1       0.67      0.41      0.51        29
          B2       0.79      0.90      0.84        29
          C1       0.00      0.00      0.00         2

    accuracy                           0.69        89
   macro avg       0.41      0.43      0.41        89
weighted avg       0.66      0.69      0.65        89


Fold 1
[[ 0  2  0  0  0]
 [ 0 15 12  0  0]
 [ 0 10 14  4  0]
 [ 0  1  3 25  0]
 [ 0  0  1  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.54      0.56      0.55        27
          B1       0.47      0.50      0.48        28
          B2       0.83      0.86      0.85        29
          C1       0.00      0.00      0.00         2

    accuracy                           0.61        88
   macro avg       0.37      0.38      0.38        88
weighted avg       0.59      0.61      0.60        88


Fold 2
[[ 0  2  0  0  0]
 [ 0 16  8  2  0]
 [ 0  7 18  3  0]
 [ 0  2  3 24  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.59      0.62      0.60        26
          B1       0.62      0.64      0.63        28
          B2       0.77      0.83      0.80        29
          C1       0.00      0.00      0.00         2

    accuracy                           0.67        87
   macro avg       0.40      0.42      0.41        87
weighted avg       0.63      0.67      0.65        87


Fold 3
[[ 1  0  0  0  0]
 [ 0 18  8  0  0]
 [ 0  8 18  2  0]
 [ 0  3  1 23  1]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       1.00      1.00      1.00         1
          A2       0.62      0.69      0.65        26
          B1       0.67      0.64      0.65        28
          B2       0.85      0.82      0.84        28
          C1       0.00      0.00      0.00         2

    accuracy                           0.71        85
   macro avg       0.63      0.63      0.63        85
weighted avg       0.70      0.71      0.70        85


Fold 4
[[ 0  1  0  0  0]
 [ 1 13 12  0  0]
 [ 0  9 14  5  0]
 [ 0  1  3 24  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.54      0.50      0.52        26
          B1       0.48      0.50      0.49        28
          B2       0.77      0.86      0.81        28
          C1       0.00      0.00      0.00         2

    accuracy                           0.60        85
   macro avg       0.36      0.37      0.36        85
weighted avg       0.58      0.60      0.59        85


K-fold scores
[0.6543683549567582, 0.6002361965116335, 0.6503715371708386, 0.7031016042780749, 0.5888711409630757]
SKF f1 score mean 0.6393897667760762

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2943
2943
Fold 0
[[ 0  1  1  0  0]
 [ 0 21  5  1  0]
 [ 0 10 14  5  0]
 [ 0  2  0 27  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.62      0.78      0.69        27
          B1       0.70      0.48      0.57        29
          B2       0.77      0.93      0.84        29
          C1       0.00      0.00      0.00         2

    accuracy                           0.70        89
   macro avg       0.42      0.44      0.42        89
weighted avg       0.67      0.70      0.67        89


Fold 1
[[ 0  2  0  0  0]
 [ 0 14 12  1  0]
 [ 0 10 14  4  0]
 [ 0  0  3 26  0]
 [ 0  0  1  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.54      0.52      0.53        27
          B1       0.47      0.50      0.48        28
          B2       0.81      0.90      0.85        29
          C1       0.00      0.00      0.00         2

    accuracy                           0.61        88
   macro avg       0.36      0.38      0.37        88
weighted avg       0.58      0.61      0.60        88


Fold 2
[[ 0  2  0  0  0]
 [ 0 17  7  2  0]
 [ 0  7 18  3  0]
 [ 0  0  1 27  1]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         2
          A2       0.65      0.65      0.65        26
          B1       0.69      0.64      0.67        28
          B2       0.79      0.93      0.86        29
          C1       0.00      0.00      0.00         2

    accuracy                           0.71        87
   macro avg       0.43      0.45      0.44        87
weighted avg       0.68      0.71      0.70        87


Fold 3
[[ 0  1  0  0  0]
 [ 0 19  7  0  0]
 [ 0  7 19  2  0]
 [ 0  3  1 23  1]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.63      0.73      0.68        26
          B1       0.70      0.68      0.69        28
          B2       0.85      0.82      0.84        28
          C1       0.00      0.00      0.00         2

    accuracy                           0.72        85
   macro avg       0.44      0.45      0.44        85
weighted avg       0.71      0.72      0.71        85


Fold 4
[[ 0  1  0  0  0]
 [ 1 13 12  0  0]
 [ 0  9 14  5  0]
 [ 0  0  2 26  0]
 [ 0  0  0  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.57      0.50      0.53        26
          B1       0.50      0.50      0.50        28
          B2       0.79      0.93      0.85        28
          C1       0.00      0.00      0.00         2

    accuracy                           0.62        85
   macro avg       0.37      0.39      0.38        85
weighted avg       0.60      0.62      0.61        85


K-fold scores
[0.670003848380391, 0.5966216340695046, 0.6956759715380404, 0.7106646294881589, 0.6078208332513333]
SKF f1 score mean 0.6561573833454857

SAME LANG EVAL DONE FOR THIS LANG
************for dimension:  Vocabularycontrol  ***************
Extracted all features: 
Printing class statistics
Counter({'B1': 182, 'A2': 131, 'B2': 100, 'C1': 16, 'A1': 5})
With Word ngrams: 
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
1161
1161
Fold 0
[[ 0  1  0  0  0]
 [ 0 17 10  0  0]
 [ 0  3 27  7  0]
 [ 0  0  9 11  0]
 [ 0  0  1  3  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.81      0.63      0.71        27
          B1       0.57      0.73      0.64        37
          B2       0.52      0.55      0.54        20
          C1       0.00      0.00      0.00         4

    accuracy                           0.62        89
   macro avg       0.38      0.38      0.38        89
weighted avg       0.60      0.62      0.60        89


Fold 1
[[ 0  1  0  0  0]
 [ 0 19  7  0  0]
 [ 0 11 20  6  0]
 [ 0  1 10  9  0]
 [ 0  0  1  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.59      0.73      0.66        26
          B1       0.53      0.54      0.53        37
          B2       0.53      0.45      0.49        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.55        87
   macro avg       0.33      0.34      0.33        87
weighted avg       0.52      0.55      0.53        87


Fold 2
[[ 0  1  0  0  0]
 [ 0 19  7  0  0]
 [ 0  9 17 10  0]
 [ 0  0  4 16  0]
 [ 0  0  3  0  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.66      0.73      0.69        26
          B1       0.55      0.47      0.51        36
          B2       0.62      0.80      0.70        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.60        86
   macro avg       0.36      0.40      0.38        86
weighted avg       0.57      0.60      0.58        86


Fold 3
[[ 0  1  0  0  0]
 [ 0 13 13  0  0]
 [ 0  2 30  4  0]
 [ 0  2  9  9  0]
 [ 0  0  0  3  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.72      0.50      0.59        26
          B1       0.58      0.83      0.68        36
          B2       0.56      0.45      0.50        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.60        86
   macro avg       0.37      0.36      0.35        86
weighted avg       0.59      0.60      0.58        86


Fold 4
[[ 0  1  0  0  0]
 [ 0 15 11  0  0]
 [ 0 11 20  5  0]
 [ 0  1 13  6  0]
 [ 0  0  3  0  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.54      0.58      0.56        26
          B1       0.43      0.56      0.48        36
          B2       0.55      0.30      0.39        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.48        86
   macro avg       0.30      0.29      0.28        86
weighted avg       0.47      0.48      0.46        86


K-fold scores
[0.602723838233567, 0.5344545496745258, 0.583085308817618, 0.580338266384778, 0.4597183432404286]
SKF f1 score mean 0.5520640612701835

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
1161
1161
Fold 0
[[ 0  1  0  0  0]
 [ 1 18  8  0  0]
 [ 0 11 18  8  0]
 [ 0  1 10  8  1]
 [ 0  0  2  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.58      0.67      0.62        27
          B1       0.47      0.49      0.48        37
          B2       0.44      0.40      0.42        20
          C1       0.00      0.00      0.00         4

    accuracy                           0.49        89
   macro avg       0.30      0.31      0.30        89
weighted avg       0.47      0.49      0.48        89


Fold 1
[[ 0  1  0  0  0]
 [ 0 15 11  0  0]
 [ 0 10 21  6  0]
 [ 0  0 13  7  0]
 [ 0  0  1  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.58      0.58      0.58        26
          B1       0.46      0.57      0.51        37
          B2       0.47      0.35      0.40        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.49        87
   macro avg       0.30      0.30      0.30        87
weighted avg       0.47      0.49      0.48        87


Fold 2
[[ 0  0  1  0  0]
 [ 1 17  7  1  0]
 [ 0 13 14  9  0]
 [ 0  0  7 13  0]
 [ 0  1  2  0  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.55      0.65      0.60        26
          B1       0.45      0.39      0.42        36
          B2       0.57      0.65      0.60        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.51        86
   macro avg       0.31      0.34      0.32        86
weighted avg       0.49      0.51      0.50        86


Fold 3
[[ 0  1  0  0  0]
 [ 0 12 14  0  0]
 [ 0  9 18  7  2]
 [ 0  3  8  9  0]
 [ 0  0  0  3  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.48      0.46      0.47        26
          B1       0.45      0.50      0.47        36
          B2       0.47      0.45      0.46        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.45        86
   macro avg       0.28      0.28      0.28        86
weighted avg       0.44      0.45      0.45        86


Fold 4
[[ 0  1  0  0  0]
 [ 2 14 10  0  0]
 [ 0 10 19  7  0]
 [ 0  2 10  7  1]
 [ 0  0  0  3  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.52      0.54      0.53        26
          B1       0.49      0.53      0.51        36
          B2       0.41      0.35      0.38        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.47        86
   macro avg       0.28      0.28      0.28        86
weighted avg       0.46      0.47      0.46        86


K-fold scores
[0.48246823956442836, 0.4795734662789087, 0.4958903640121105, 0.4478918014809728, 0.45980717004850397]
SKF f1 score mean 0.47312620827698487

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
1161
1161
Fold 0
[[ 0  1  0  0  0]
 [ 1 18  8  0  0]
 [ 0 10 20  7  0]
 [ 0  1  9  9  1]
 [ 0  0  2  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.60      0.67      0.63        27
          B1       0.51      0.54      0.53        37
          B2       0.50      0.45      0.47        20
          C1       0.00      0.00      0.00         4

    accuracy                           0.53        89
   macro avg       0.32      0.33      0.33        89
weighted avg       0.51      0.53      0.52        89


Fold 1
[[ 0  1  0  0  0]
 [ 0 16 10  0  0]
 [ 0 10 21  6  0]
 [ 0  0 10 10  0]
 [ 0  0  1  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.59      0.62      0.60        26
          B1       0.50      0.57      0.53        37
          B2       0.56      0.50      0.53        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.54        87
   macro avg       0.33      0.34      0.33        87
weighted avg       0.52      0.54      0.53        87


Fold 2
[[ 0  0  1  0  0]
 [ 1 18  5  2  0]
 [ 0 11 13 12  0]
 [ 0  0  5 15  0]
 [ 0  1  2  0  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.60      0.69      0.64        26
          B1       0.50      0.36      0.42        36
          B2       0.52      0.75      0.61        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.53        86
   macro avg       0.32      0.36      0.33        86
weighted avg       0.51      0.53      0.51        86


Fold 3
[[ 0  1  0  0  0]
 [ 0 17  9  0  0]
 [ 0  9 18  7  2]
 [ 0  3  6  9  2]
 [ 0  0  0  3  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.57      0.65      0.61        26
          B1       0.55      0.50      0.52        36
          B2       0.47      0.45      0.46        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.51        86
   macro avg       0.32      0.32      0.32        86
weighted avg       0.51      0.51      0.51        86


Fold 4
[[ 0  1  0  0  0]
 [ 1 15 10  0  0]
 [ 0 10 18  6  2]
 [ 0  1  9 10  0]
 [ 0  0  1  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.56      0.58      0.57        26
          B1       0.47      0.50      0.49        36
          B2       0.56      0.50      0.53        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.50        86
   macro avg       0.32      0.32      0.32        86
weighted avg       0.50      0.50      0.50        86


K-fold scores
[0.5168539325842697, 0.5275323571605772, 0.5122785798490439, 0.5092917699085545, 0.4971722143611939]
SKF f1 score mean 0.512625770772728

SAME LANG EVAL DONE FOR THIS LANG
With POS ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3975
3975
Fold 0
[[ 0  1  0  0  0]
 [ 0 18  9  0  0]
 [ 0  4 30  3  0]
 [ 0  0 13  7  0]
 [ 0  0  2  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.78      0.67      0.72        27
          B1       0.56      0.81      0.66        37
          B2       0.58      0.35      0.44        20
          C1       0.00      0.00      0.00         4

    accuracy                           0.62        89
   macro avg       0.38      0.37      0.36        89
weighted avg       0.60      0.62      0.59        89


Fold 1
[[ 0  1  0  0  0]
 [ 0 18  8  0  0]
 [ 0  7 23  7  0]
 [ 0  0  9 11  0]
 [ 0  0  1  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.69      0.69      0.69        26
          B1       0.56      0.62      0.59        37
          B2       0.55      0.55      0.55        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.60        87
   macro avg       0.36      0.37      0.37        87
weighted avg       0.57      0.60      0.58        87


Fold 2
[[ 0  1  0  0  0]
 [ 0 17  9  0  0]
 [ 0 11 20  5  0]
 [ 0  0  8 12  0]
 [ 0  0  1  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.59      0.65      0.62        26
          B1       0.53      0.56      0.54        36
          B2       0.63      0.60      0.62        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.57        86
   macro avg       0.35      0.36      0.35        86
weighted avg       0.54      0.57      0.56        86


Fold 3
[[ 0  1  0  0  0]
 [ 0 14 12  0  0]
 [ 0  5 26  5  0]
 [ 0  2 11  7  0]
 [ 0  0  0  3  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.64      0.54      0.58        26
          B1       0.53      0.72      0.61        36
          B2       0.47      0.35      0.40        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.55        86
   macro avg       0.33      0.32      0.32        86
weighted avg       0.52      0.55      0.53        86


Fold 4
[[ 0  1  0  0  0]
 [ 0 16 10  0  0]
 [ 0  8 21  7  0]
 [ 0  2  9  9  0]
 [ 0  0  1  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.59      0.62      0.60        26
          B1       0.51      0.58      0.55        36
          B2       0.50      0.45      0.47        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.53        86
   macro avg       0.32      0.33      0.32        86
weighted avg       0.51      0.53      0.52        86


K-fold scores
[0.5908494875910606, 0.5841438255231359, 0.5562776632544074, 0.5254673962608298, 0.5210251285399665]
SKF f1 score mean 0.55555270023388

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3975
3975
Fold 0
[[ 0  1  0  0  0]
 [ 0 17  9  1  0]
 [ 0  6 20  9  2]
 [ 0  1  7 12  0]
 [ 0  0  1  3  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.68      0.63      0.65        27
          B1       0.54      0.54      0.54        37
          B2       0.48      0.60      0.53        20
          C1       0.00      0.00      0.00         4

    accuracy                           0.55        89
   macro avg       0.34      0.35      0.35        89
weighted avg       0.54      0.55      0.54        89


Fold 1
[[ 0  1  0  0  0]
 [ 0 16  9  1  0]
 [ 0 13 18  6  0]
 [ 0  2 10  8  0]
 [ 0  0  3  0  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.50      0.62      0.55        26
          B1       0.45      0.49      0.47        37
          B2       0.53      0.40      0.46        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.48        87
   macro avg       0.30      0.30      0.30        87
weighted avg       0.46      0.48      0.47        87


Fold 2
[[ 0  0  1  0  0]
 [ 0 14 11  1  0]
 [ 0  9 19  8  0]
 [ 0  1 10  9  0]
 [ 0  0  0  2  1]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.58      0.54      0.56        26
          B1       0.46      0.53      0.49        36
          B2       0.45      0.45      0.45        20
          C1       1.00      0.33      0.50         3

    accuracy                           0.50        86
   macro avg       0.50      0.37      0.40        86
weighted avg       0.51      0.50      0.50        86


Fold 3
[[ 0  1  0  0  0]
 [ 0 20  5  1  0]
 [ 0 10 18  7  1]
 [ 0  3  9  7  1]
 [ 0  0  1  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.59      0.77      0.67        26
          B1       0.55      0.50      0.52        36
          B2       0.41      0.35      0.38        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.52        86
   macro avg       0.31      0.32      0.31        86
weighted avg       0.50      0.52      0.51        86


Fold 4
[[ 0  0  1  0  0]
 [ 0 15  8  3  0]
 [ 0  9 18  9  0]
 [ 0  2 13  5  0]
 [ 0  0  1  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.58      0.58      0.58        26
          B1       0.44      0.50      0.47        36
          B2       0.26      0.25      0.26        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.44        86
   macro avg       0.26      0.27      0.26        86
weighted avg       0.42      0.44      0.43        86


K-fold scores
[0.5429271103428407, 0.46880903480190045, 0.4979794623980671, 0.5079477860064311, 0.4297601623183019]
SKF f1 score mean 0.4894847111735082

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3975
3975
Fold 0
[[ 0  1  0  0  0]
 [ 0 18  8  1  0]
 [ 0  6 20  9  2]
 [ 0  1  5 13  1]
 [ 0  0  0  4  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.69      0.67      0.68        27
          B1       0.61      0.54      0.57        37
          B2       0.48      0.65      0.55        20
          C1       0.00      0.00      0.00         4

    accuracy                           0.57        89
   macro avg       0.36      0.37      0.36        89
weighted avg       0.57      0.57      0.57        89


Fold 1
[[ 0  1  0  0  0]
 [ 0 16  9  1  0]
 [ 0 12 17  8  0]
 [ 0  1  8 11  0]
 [ 0  0  3  0  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.53      0.62      0.57        26
          B1       0.46      0.46      0.46        37
          B2       0.55      0.55      0.55        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.51        87
   macro avg       0.31      0.32      0.32        87
weighted avg       0.48      0.51      0.49        87


Fold 2
[[ 0  1  0  0  0]
 [ 0 16  9  1  0]
 [ 0  7 21  8  0]
 [ 0  1  9 10  0]
 [ 0  0  0  1  2]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.64      0.62      0.63        26
          B1       0.54      0.58      0.56        36
          B2       0.50      0.50      0.50        20
          C1       1.00      0.67      0.80         3

    accuracy                           0.57        86
   macro avg       0.54      0.47      0.50        86
weighted avg       0.57      0.57      0.57        86


Fold 3
[[ 0  1  0  0  0]
 [ 0 20  5  1  0]
 [ 0 11 17  7  1]
 [ 0  2 10  7  1]
 [ 0  0  1  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.59      0.77      0.67        26
          B1       0.52      0.47      0.49        36
          B2       0.41      0.35      0.38        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.51        86
   macro avg       0.30      0.32      0.31        86
weighted avg       0.49      0.51      0.50        86


Fold 4
[[ 0  0  1  0  0]
 [ 0 15  8  3  0]
 [ 0  9 18  9  0]
 [ 0  2 11  7  0]
 [ 0  0  1  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.58      0.58      0.58        26
          B1       0.46      0.50      0.48        36
          B2       0.33      0.35      0.34        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.47        86
   macro avg       0.27      0.29      0.28        86
weighted avg       0.45      0.47      0.45        86


K-fold scores
[0.5679360626022542, 0.4926108374384237, 0.5682991336069311, 0.495814317856785, 0.45475893363584796]
SKF f1 score mean 0.5158838570280484

SAME LANG EVAL DONE FOR THIS LANG
Dep ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2943
2943
Fold 0
[[ 0  1  0  0  0]
 [ 0 18  9  0  0]
 [ 0  3 32  2  0]
 [ 0  0 11  9  0]
 [ 0  0  2  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.82      0.67      0.73        27
          B1       0.59      0.86      0.70        37
          B2       0.69      0.45      0.55        20
          C1       0.00      0.00      0.00         4

    accuracy                           0.66        89
   macro avg       0.42      0.40      0.40        89
weighted avg       0.65      0.66      0.64        89


Fold 1
[[ 0  1  0  0  0]
 [ 0 17  9  0  0]
 [ 0  7 25  5  0]
 [ 0  0 15  5  0]
 [ 0  0  1  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.68      0.65      0.67        26
          B1       0.50      0.68      0.57        37
          B2       0.42      0.25      0.31        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.54        87
   macro avg       0.32      0.32      0.31        87
weighted avg       0.51      0.54      0.52        87


Fold 2
[[ 0  0  1  0  0]
 [ 0 16 10  0  0]
 [ 0 10 21  5  0]
 [ 0  0 11  9  0]
 [ 0  0  2  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.62      0.62      0.62        26
          B1       0.47      0.58      0.52        36
          B2       0.60      0.45      0.51        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.53        86
   macro avg       0.34      0.33      0.33        86
weighted avg       0.52      0.53      0.52        86


Fold 3
[[ 0  1  0  0  0]
 [ 0 13 13  0  0]
 [ 0  6 26  4  0]
 [ 0  2  9  9  0]
 [ 0  0  0  3  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.59      0.50      0.54        26
          B1       0.54      0.72      0.62        36
          B2       0.56      0.45      0.50        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.56        86
   macro avg       0.34      0.33      0.33        86
weighted avg       0.54      0.56      0.54        86


Fold 4
[[ 0  1  0  0  0]
 [ 0 15 11  0  0]
 [ 0  8 25  3  0]
 [ 0  0 12  8  0]
 [ 0  0  0  3  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.62      0.58      0.60        26
          B1       0.52      0.69      0.60        36
          B2       0.57      0.40      0.47        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.56        86
   macro avg       0.34      0.33      0.33        86
weighted avg       0.54      0.56      0.54        86


K-fold scores
[0.637840490167938, 0.515490817809486, 0.522702104097453, 0.5391749723145072, 0.5400039085401603]
SKF f1 score mean 0.551042458585909

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2943
2943
Fold 0
[[ 0  1  0  0  0]
 [ 0 19  8  0  0]
 [ 0  8 18  9  2]
 [ 0  3  8  9  0]
 [ 0  0  1  2  1]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.61      0.70      0.66        27
          B1       0.51      0.49      0.50        37
          B2       0.45      0.45      0.45        20
          C1       0.33      0.25      0.29         4

    accuracy                           0.53        89
   macro avg       0.38      0.38      0.38        89
weighted avg       0.52      0.53      0.52        89


Fold 1
[[ 0  1  0  0  0]
 [ 0 17  7  2  0]
 [ 0 13 21  3  0]
 [ 0  1 10  8  1]
 [ 0  0  1  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.53      0.65      0.59        26
          B1       0.54      0.57      0.55        37
          B2       0.53      0.40      0.46        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.53        87
   macro avg       0.32      0.32      0.32        87
weighted avg       0.51      0.53      0.52        87


Fold 2
[[ 0  0  1  0  0]
 [ 1 17  8  0  0]
 [ 1  8 23  3  1]
 [ 0  1  5 12  2]
 [ 0  0  0  3  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.65      0.65      0.65        26
          B1       0.62      0.64      0.63        36
          B2       0.67      0.60      0.63        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.60        86
   macro avg       0.39      0.38      0.38        86
weighted avg       0.61      0.60      0.61        86


Fold 3
[[ 0  1  0  0  0]
 [ 0 18  7  1  0]
 [ 0  9 16  9  2]
 [ 0  4  6  8  2]
 [ 0  0  0  2  1]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.56      0.69      0.62        26
          B1       0.55      0.44      0.49        36
          B2       0.40      0.40      0.40        20
          C1       0.20      0.33      0.25         3

    accuracy                           0.50        86
   macro avg       0.34      0.37      0.35        86
weighted avg       0.50      0.50      0.50        86


Fold 4
[[ 0  0  1  0  0]
 [ 0 16  9  1  0]
 [ 0 10 19  7  0]
 [ 0  6  8  6  0]
 [ 0  0  0  3  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.50      0.62      0.55        26
          B1       0.51      0.53      0.52        36
          B2       0.35      0.30      0.32        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.48        86
   macro avg       0.27      0.29      0.28        86
weighted avg       0.45      0.48      0.46        86


K-fold scores
[0.5205900260142802, 0.5153058031523519, 0.6083315169095086, 0.4954768367158102, 0.46012837325686795]
SKF f1 score mean 0.5199665112097638

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2943
2943
Fold 0
[[ 0  1  0  0  0]
 [ 0 18  9  0  0]
 [ 0  7 18 10  2]
 [ 0  2  9  9  0]
 [ 0  0  1  3  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.64      0.67      0.65        27
          B1       0.49      0.49      0.49        37
          B2       0.41      0.45      0.43        20
          C1       0.00      0.00      0.00         4

    accuracy                           0.51        89
   macro avg       0.31      0.32      0.31        89
weighted avg       0.49      0.51      0.50        89


Fold 1
[[ 0  1  0  0  0]
 [ 0 17  7  2  0]
 [ 0 11 22  4  0]
 [ 0  1 10  8  1]
 [ 0  0  1  2  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.57      0.65      0.61        26
          B1       0.55      0.59      0.57        37
          B2       0.50      0.40      0.44        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.54        87
   macro avg       0.32      0.33      0.32        87
weighted avg       0.52      0.54      0.53        87


Fold 2
[[ 0  0  1  0  0]
 [ 1 15  9  1  0]
 [ 1  8 22  4  1]
 [ 0  1  6 11  2]
 [ 0  0  0  3  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.62      0.58      0.60        26
          B1       0.58      0.61      0.59        36
          B2       0.58      0.55      0.56        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.56        86
   macro avg       0.36      0.35      0.35        86
weighted avg       0.57      0.56      0.56        86


Fold 3
[[ 0  1  0  0  0]
 [ 0 18  7  1  0]
 [ 0  9 16  9  2]
 [ 0  3  7  8  2]
 [ 0  0  0  2  1]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.58      0.69      0.63        26
          B1       0.53      0.44      0.48        36
          B2       0.40      0.40      0.40        20
          C1       0.20      0.33      0.25         3

    accuracy                           0.50        86
   macro avg       0.34      0.37      0.35        86
weighted avg       0.50      0.50      0.50        86


Fold 4
[[ 0  0  1  0  0]
 [ 0 16  9  1  0]
 [ 0 10 18  8  0]
 [ 0  5  9  6  0]
 [ 0  0  0  3  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.52      0.62      0.56        26
          B1       0.49      0.50      0.49        36
          B2       0.33      0.30      0.32        20
          C1       0.00      0.00      0.00         3

    accuracy                           0.47        86
   macro avg       0.27      0.28      0.27        86
weighted avg       0.44      0.47      0.45        86


K-fold scores
[0.4971253465635488, 0.526637474913337, 0.5614820545053103, 0.4956464893735395, 0.4496012251080073]
SKF f1 score mean 0.5060985180927485

SAME LANG EVAL DONE FOR THIS LANG
************for dimension:  CoherenceCohesion  ***************
Extracted all features: 
Printing class statistics
Counter({'B1': 171, 'B2': 156, 'A2': 101, 'C1': 5, 'A1': 1})
With Word ngrams: 
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
1161
1161
Fold 0
[[16  5  0  0]
 [10 20  5  0]
 [ 0  0 32  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.62      0.76      0.68        21
          B1       0.80      0.57      0.67        35
          B2       0.84      1.00      0.91        32
          C1       0.00      0.00      0.00         1

    accuracy                           0.76        89
   macro avg       0.56      0.58      0.57        89
weighted avg       0.76      0.76      0.75        89


Fold 1
[[ 0  1  0  0  0]
 [ 0 13  6  1  0]
 [ 0  1 29  4  0]
 [ 0  2  1 28  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.76      0.65      0.70        20
          B1       0.81      0.85      0.83        34
          B2       0.82      0.90      0.86        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.80        87
   macro avg       0.48      0.48      0.48        87
weighted avg       0.78      0.80      0.79        87


Fold 2
[[16  4  0  0]
 [ 4 24  6  0]
 [ 0  1 30  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.80      0.80      0.80        20
          B1       0.83      0.71      0.76        34
          B2       0.81      0.97      0.88        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.81        86
   macro avg       0.61      0.62      0.61        86
weighted avg       0.81      0.81      0.81        86


Fold 3
[[20  0  0  0]
 [ 5 27  2  0]
 [ 0  1 30  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.80      1.00      0.89        20
          B1       0.96      0.79      0.87        34
          B2       0.91      0.97      0.94        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.90        86
   macro avg       0.67      0.69      0.67        86
weighted avg       0.89      0.90      0.89        86


Fold 4
[[13  7  0  0]
 [ 5 22  7  0]
 [ 0  1 30  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.72      0.65      0.68        20
          B1       0.73      0.65      0.69        34
          B2       0.79      0.97      0.87        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.76        86
   macro avg       0.56      0.57      0.56        86
weighted avg       0.74      0.76      0.74        86


K-fold scores
[0.7515544778753003, 0.792335344059482, 0.8053221288515408, 0.8889904767858632, 0.74436897983077]
SKF f1 score mean 0.7965142814805912

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
1161
1161
Fold 0
[[14  7  0  0]
 [10 20  5  0]
 [ 0  4 28  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.58      0.67      0.62        21
          B1       0.65      0.57      0.61        35
          B2       0.82      0.88      0.85        32
          C1       0.00      0.00      0.00         1

    accuracy                           0.70        89
   macro avg       0.51      0.53      0.52        89
weighted avg       0.69      0.70      0.69        89


Fold 1
[[ 0  1  0  0  0]
 [ 0 10  9  1  0]
 [ 0  2 29  3  0]
 [ 0  0  6 25  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.77      0.50      0.61        20
          B1       0.66      0.85      0.74        34
          B2       0.83      0.81      0.82        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.74        87
   macro avg       0.45      0.43      0.43        87
weighted avg       0.73      0.74      0.72        87


Fold 2
[[14  6  0  0]
 [ 5 25  4  0]
 [ 0  6 25  0]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A2       0.74      0.70      0.72        20
          B1       0.66      0.74      0.69        34
          B2       0.86      0.81      0.83        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.74        86
   macro avg       0.56      0.56      0.56        86
weighted avg       0.74      0.74      0.74        86


Fold 3
[[18  2  0  0]
 [12 21  1  0]
 [ 0  4 27  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.60      0.90      0.72        20
          B1       0.78      0.62      0.69        34
          B2       0.93      0.87      0.90        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.77        86
   macro avg       0.58      0.60      0.58        86
weighted avg       0.78      0.77      0.76        86


Fold 4
[[12  8  0  0]
 [ 5 24  5  0]
 [ 0  4 27  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.71      0.60      0.65        20
          B1       0.67      0.71      0.69        34
          B2       0.82      0.87      0.84        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.73        86
   macro avg       0.55      0.54      0.54        86
weighted avg       0.72      0.73      0.73        86


K-fold scores
[0.690228123935989, 0.7219896490670941, 0.7419002186444047, 0.7640678612276021, 0.7260873103169615]
SKF f1 score mean 0.7288546326384102

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
1161
1161
Fold 0
[[16  5  0  0]
 [12 20  3  0]
 [ 0  3 29  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.57      0.76      0.65        21
          B1       0.71      0.57      0.63        35
          B2       0.88      0.91      0.89        32
          C1       0.00      0.00      0.00         1

    accuracy                           0.73        89
   macro avg       0.54      0.56      0.55        89
weighted avg       0.73      0.73      0.72        89


Fold 1
[[ 0  1  0  0  0]
 [ 0 10  9  1  0]
 [ 0  1 31  2  0]
 [ 0  0  5 26  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.83      0.50      0.62        20
          B1       0.69      0.91      0.78        34
          B2       0.87      0.84      0.85        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.77        87
   macro avg       0.48      0.45      0.45        87
weighted avg       0.77      0.77      0.75        87


Fold 2
[[14  6  0  0]
 [ 6 24  4  0]
 [ 0  4 27  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.70      0.70      0.70        20
          B1       0.71      0.71      0.71        34
          B2       0.84      0.87      0.86        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.76        86
   macro avg       0.56      0.57      0.57        86
weighted avg       0.75      0.76      0.75        86


Fold 3
[[18  2  0  0]
 [ 9 24  1  0]
 [ 0  2 29  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.67      0.90      0.77        20
          B1       0.86      0.71      0.77        34
          B2       0.94      0.94      0.94        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.83        86
   macro avg       0.61      0.64      0.62        86
weighted avg       0.83      0.83      0.82        86


Fold 4
[[13  7  0  0]
 [ 5 24  5  0]
 [ 0  3 28  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.72      0.65      0.68        20
          B1       0.71      0.71      0.71        34
          B2       0.82      0.90      0.86        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.76        86
   macro avg       0.56      0.56      0.56        86
weighted avg       0.74      0.76      0.75        86


K-fold scores
[0.7246107201163381, 0.754135331172347, 0.7508305647840531, 0.8214154602480407, 0.7487430562093964]
SKF f1 score mean 0.759947026506035

SAME LANG EVAL DONE FOR THIS LANG
With POS ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3975
3975
Fold 0
[[16  5  0  0]
 [ 8 24  3  0]
 [ 0  1 31  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.67      0.76      0.71        21
          B1       0.80      0.69      0.74        35
          B2       0.89      0.97      0.93        32
          C1       0.00      0.00      0.00         1

    accuracy                           0.80        89
   macro avg       0.59      0.60      0.59        89
weighted avg       0.79      0.80      0.79        89


Fold 1
[[ 0  1  0  0  0]
 [ 0 12  7  1  0]
 [ 0  0 30  4  0]
 [ 0  1  2 28  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.86      0.60      0.71        20
          B1       0.77      0.88      0.82        34
          B2       0.82      0.90      0.86        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.80        87
   macro avg       0.49      0.48      0.48        87
weighted avg       0.79      0.80      0.79        87


Fold 2
[[16  4  0  0]
 [ 4 26  4  0]
 [ 0  3 28  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.80      0.80      0.80        20
          B1       0.79      0.76      0.78        34
          B2       0.85      0.90      0.88        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.81        86
   macro avg       0.61      0.62      0.61        86
weighted avg       0.80      0.81      0.81        86


Fold 3
[[18  2  0  0]
 [ 6 27  1  0]
 [ 0  2 29  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.75      0.90      0.82        20
          B1       0.87      0.79      0.83        34
          B2       0.94      0.94      0.94        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.86        86
   macro avg       0.64      0.66      0.65        86
weighted avg       0.86      0.86      0.86        86


Fold 4
[[16  4  0  0]
 [ 4 23  7  0]
 [ 0  1 30  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.80      0.80      0.80        20
          B1       0.82      0.68      0.74        34
          B2       0.79      0.97      0.87        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.80        86
   macro avg       0.60      0.61      0.60        86
weighted avg       0.80      0.80      0.79        86


K-fold scores
[0.7909149154830858, 0.7904660327122746, 0.8082913918778202, 0.8559277931370955, 0.7928177696598063]
SKF f1 score mean 0.8076835805740163

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3975
3975
Fold 0
[[14  6  1  0]
 [12 20  3  0]
 [ 4  5 23  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.47      0.67      0.55        21
          B1       0.65      0.57      0.61        35
          B2       0.82      0.72      0.77        32
          C1       0.00      0.00      0.00         1

    accuracy                           0.64        89
   macro avg       0.48      0.49      0.48        89
weighted avg       0.66      0.64      0.64        89


Fold 1
[[ 0  1  0  0  0]
 [ 0 10  9  1  0]
 [ 0  3 26  5  0]
 [ 0  1  3 27  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.67      0.50      0.57        20
          B1       0.68      0.76      0.72        34
          B2       0.79      0.87      0.83        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.72        87
   macro avg       0.43      0.43      0.42        87
weighted avg       0.70      0.72      0.71        87


Fold 2
[[12  6  2  0]
 [ 6 22  6  0]
 [ 1  4 26  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.63      0.60      0.62        20
          B1       0.69      0.65      0.67        34
          B2       0.74      0.84      0.79        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.70        86
   macro avg       0.52      0.52      0.52        86
weighted avg       0.69      0.70      0.69        86


Fold 3
[[13  6  1  0]
 [ 8 26  0  0]
 [ 3  5 23  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.54      0.65      0.59        20
          B1       0.70      0.76      0.73        34
          B2       0.92      0.74      0.82        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.72        86
   macro avg       0.54      0.54      0.54        86
weighted avg       0.74      0.72      0.72        86


Fold 4
[[12  8  0  0]
 [ 8 21  5  0]
 [ 0  2 29  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.60      0.60      0.60        20
          B1       0.68      0.62      0.65        34
          B2       0.83      0.94      0.88        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.72        86
   macro avg       0.53      0.54      0.53        86
weighted avg       0.71      0.72      0.71        86


K-fold scores
[0.6435378237096676, 0.7096318751491166, 0.6906814116116442, 0.7230683253857181, 0.7117634303680815]
SKF f1 score mean 0.6957365732448456

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3975
3975
Fold 0
[[12  8  1  0]
 [10 21  4  0]
 [ 1  4 27  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.52      0.57      0.55        21
          B1       0.64      0.60      0.62        35
          B2       0.82      0.84      0.83        32
          C1       0.00      0.00      0.00         1

    accuracy                           0.67        89
   macro avg       0.49      0.50      0.50        89
weighted avg       0.67      0.67      0.67        89


Fold 1
[[ 0  1  0  0  0]
 [ 0 10  9  1  0]
 [ 0  4 26  4  0]
 [ 0  1  3 27  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.62      0.50      0.56        20
          B1       0.68      0.76      0.72        34
          B2       0.82      0.87      0.84        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.72        87
   macro avg       0.43      0.43      0.42        87
weighted avg       0.70      0.72      0.71        87


Fold 2
[[12  6  2  0]
 [ 5 21  8  0]
 [ 0  4 27  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.71      0.60      0.65        20
          B1       0.68      0.62      0.65        34
          B2       0.71      0.87      0.78        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.70        86
   macro avg       0.52      0.52      0.52        86
weighted avg       0.69      0.70      0.69        86


Fold 3
[[13  6  1  0]
 [ 7 26  1  0]
 [ 3  4 24  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.57      0.65      0.60        20
          B1       0.72      0.76      0.74        34
          B2       0.89      0.77      0.83        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.73        86
   macro avg       0.54      0.55      0.54        86
weighted avg       0.74      0.73      0.73        86


Fold 4
[[12  8  0  0]
 [ 8 21  5  0]
 [ 0  1 30  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.60      0.60      0.60        20
          B1       0.70      0.62      0.66        34
          B2       0.83      0.97      0.90        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.73        86
   macro avg       0.53      0.55      0.54        86
weighted avg       0.72      0.73      0.72        86


K-fold scores
[0.670301212336903, 0.7106082375478927, 0.6884078291560597, 0.7326202154273246, 0.7217871398819854]
SKF f1 score mean 0.7047449268700331

SAME LANG EVAL DONE FOR THIS LANG
Dep ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2943
2943
Fold 0
[[16  5  0  0]
 [ 7 25  3  0]
 [ 0  2 30  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.70      0.76      0.73        21
          B1       0.78      0.71      0.75        35
          B2       0.88      0.94      0.91        32
          C1       0.00      0.00      0.00         1

    accuracy                           0.80        89
   macro avg       0.59      0.60      0.60        89
weighted avg       0.79      0.80      0.79        89


Fold 1
[[ 0  1  0  0  0]
 [ 0 14  5  1  0]
 [ 0  1 28  5  0]
 [ 0  1  1 29  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.82      0.70      0.76        20
          B1       0.82      0.82      0.82        34
          B2       0.81      0.94      0.87        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.82        87
   macro avg       0.49      0.49      0.49        87
weighted avg       0.80      0.82      0.80        87


Fold 2
[[15  5  0  0]
 [ 6 24  4  0]
 [ 0  1 30  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.71      0.75      0.73        20
          B1       0.80      0.71      0.75        34
          B2       0.86      0.97      0.91        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.80        86
   macro avg       0.59      0.61      0.60        86
weighted avg       0.79      0.80      0.79        86


Fold 3
[[18  1  1  0]
 [ 7 25  2  0]
 [ 0  1 30  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.72      0.90      0.80        20
          B1       0.93      0.74      0.82        34
          B2       0.88      0.97      0.92        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.85        86
   macro avg       0.63      0.65      0.64        86
weighted avg       0.85      0.85      0.84        86


Fold 4
[[13  6  1  0]
 [ 3 25  6  0]
 [ 0  2 29  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.81      0.65      0.72        20
          B1       0.76      0.74      0.75        34
          B2       0.78      0.94      0.85        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.78        86
   macro avg       0.59      0.58      0.58        86
weighted avg       0.77      0.78      0.77        86


K-fold scores
[0.7919442623450673, 0.8042638624213508, 0.7943716805032743, 0.8428399659814071, 0.7704506423650104]
SKF f1 score mean 0.8007740827232219

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2943
2943
Fold 0
[[12  9  0  0]
 [11 18  6  0]
 [ 0  4 28  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.52      0.57      0.55        21
          B1       0.58      0.51      0.55        35
          B2       0.80      0.88      0.84        32
          C1       0.00      0.00      0.00         1

    accuracy                           0.65        89
   macro avg       0.48      0.49      0.48        89
weighted avg       0.64      0.65      0.64        89


Fold 1
[[ 0  1  0  0  0]
 [ 0 12  8  0  0]
 [ 0  4 25  5  0]
 [ 0  4  5 22  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.57      0.60      0.59        20
          B1       0.66      0.74      0.69        34
          B2       0.79      0.71      0.75        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.68        87
   macro avg       0.40      0.41      0.41        87
weighted avg       0.67      0.68      0.67        87


Fold 2
[[12  7  1  0]
 [ 8 23  3  0]
 [ 1  5 25  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.57      0.60      0.59        20
          B1       0.66      0.68      0.67        34
          B2       0.83      0.81      0.82        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.70        86
   macro avg       0.52      0.52      0.52        86
weighted avg       0.69      0.70      0.70        86


Fold 3
[[14  4  2  0]
 [ 9 22  3  0]
 [ 2  3 26  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.56      0.70      0.62        20
          B1       0.76      0.65      0.70        34
          B2       0.81      0.84      0.83        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.72        86
   macro avg       0.53      0.55      0.54        86
weighted avg       0.72      0.72      0.72        86


Fold 4
[[ 9 11  0  0]
 [ 4 25  5  0]
 [ 2  4 25  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.60      0.45      0.51        20
          B1       0.62      0.74      0.68        34
          B2       0.81      0.81      0.81        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.69        86
   macro avg       0.51      0.50      0.50        86
weighted avg       0.68      0.69      0.68        86


K-fold scores
[0.6437272269906851, 0.6716904856560741, 0.6951606954117578, 0.7183462532299741, 0.6774265960312472]
SKF f1 score mean 0.6812702514639477

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2943
2943
Fold 0
[[11 10  0  0]
 [11 19  5  0]
 [ 0  3 29  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.50      0.52      0.51        21
          B1       0.59      0.54      0.57        35
          B2       0.83      0.91      0.87        32
          C1       0.00      0.00      0.00         1

    accuracy                           0.66        89
   macro avg       0.48      0.49      0.49        89
weighted avg       0.65      0.66      0.66        89


Fold 1
[[ 0  1  0  0  0]
 [ 0  9 10  1  0]
 [ 0  2 25  7  0]
 [ 0  4  4 23  0]
 [ 0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         1
          A2       0.56      0.45      0.50        20
          B1       0.64      0.74      0.68        34
          B2       0.72      0.74      0.73        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.66        87
   macro avg       0.38      0.39      0.38        87
weighted avg       0.64      0.66      0.64        87


Fold 2
[[12  7  1  0]
 [ 8 23  3  0]
 [ 1  5 25  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.57      0.60      0.59        20
          B1       0.66      0.68      0.67        34
          B2       0.83      0.81      0.82        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.70        86
   macro avg       0.52      0.52      0.52        86
weighted avg       0.69      0.70      0.70        86


Fold 3
[[14  4  2  0]
 [ 9 22  3  0]
 [ 2  3 26  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.56      0.70      0.62        20
          B1       0.76      0.65      0.70        34
          B2       0.81      0.84      0.83        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.72        86
   macro avg       0.53      0.55      0.54        86
weighted avg       0.72      0.72      0.72        86


Fold 4
[[ 9 11  0  0]
 [ 3 25  6  0]
 [ 2  4 25  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A2       0.64      0.45      0.53        20
          B1       0.62      0.74      0.68        34
          B2       0.78      0.81      0.79        31
          C1       0.00      0.00      0.00         1

    accuracy                           0.69        86
   macro avg       0.51      0.50      0.50        86
weighted avg       0.68      0.69      0.68        86


K-fold scores
[0.655016009578447, 0.6427884122735328, 0.6951606954117578, 0.7183462532299741, 0.6763300333751769]
SKF f1 score mean 0.6775282807737778

SAME LANG EVAL DONE FOR THIS LANG
************for dimension:  Sociolinguisticappropriateness  ***************
Extracted all features: 
Printing class statistics
Counter({'A1': 261, 'A2': 106, 'B1': 63, 'B2': 4})
With Word ngrams: 
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
1161
1161
Fold 0
[[50  3  0  0]
 [ 2 17  3  0]
 [ 0  4  9  0]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.96      0.94      0.95        53
          A2       0.68      0.77      0.72        22
          B1       0.75      0.69      0.72        13
          B2       0.00      0.00      0.00         1

    accuracy                           0.85        89
   macro avg       0.60      0.60      0.60        89
weighted avg       0.85      0.85      0.85        89


Fold 1
[[50  2  0  0]
 [ 3 12  6  0]
 [ 0  9  4  0]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.94      0.96      0.95        52
          A2       0.50      0.57      0.53        21
          B1       0.40      0.31      0.35        13
          B2       0.00      0.00      0.00         1

    accuracy                           0.76        87
   macro avg       0.46      0.46      0.46        87
weighted avg       0.74      0.76      0.75        87


Fold 2
[[49  3  0]
 [ 0 15  6]
 [ 1  6  6]]
              precision    recall  f1-score   support

          A1       0.98      0.94      0.96        52
          A2       0.62      0.71      0.67        21
          B1       0.50      0.46      0.48        13

    accuracy                           0.81        86
   macro avg       0.70      0.71      0.70        86
weighted avg       0.82      0.81      0.82        86


Fold 3
[[50  2  0  0]
 [ 1 18  2  0]
 [ 0  2 10  0]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.98      0.96      0.97        52
          A2       0.78      0.86      0.82        21
          B1       0.83      0.83      0.83        12
          B2       0.00      0.00      0.00         1

    accuracy                           0.91        86
   macro avg       0.65      0.66      0.66        86
weighted avg       0.90      0.91      0.90        86


Fold 4
[[49  3  0  0]
 [ 5 15  1  0]
 [ 0  7  5  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.91      0.94      0.92        52
          A2       0.60      0.71      0.65        21
          B1       0.71      0.42      0.53        12
          B2       0.00      0.00      0.00         1

    accuracy                           0.80        86
   macro avg       0.56      0.52      0.53        86
weighted avg       0.79      0.80      0.79        86


K-fold scores
[0.851135776328222, 0.7499488351062563, 0.8162881896944825, 0.9031076171514194, 0.7917082947175634]
SKF f1 score mean 0.8224377425995886

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
1161
1161
Fold 0
[[50  3  0  0]
 [ 2 17  3  0]
 [ 0  5  8  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.96      0.94      0.95        53
          A2       0.68      0.77      0.72        22
          B1       0.67      0.62      0.64        13
          B2       0.00      0.00      0.00         1

    accuracy                           0.84        89
   macro avg       0.58      0.58      0.58        89
weighted avg       0.84      0.84      0.84        89


Fold 1
[[49  2  1  0]
 [ 4 12  5  0]
 [ 1  8  4  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.91      0.94      0.92        52
          A2       0.55      0.57      0.56        21
          B1       0.36      0.31      0.33        13
          B2       0.00      0.00      0.00         1

    accuracy                           0.75        87
   macro avg       0.45      0.46      0.45        87
weighted avg       0.73      0.75      0.74        87


Fold 2
[[48  4  0]
 [ 2 13  6]
 [ 1  3  9]]
              precision    recall  f1-score   support

          A1       0.94      0.92      0.93        52
          A2       0.65      0.62      0.63        21
          B1       0.60      0.69      0.64        13

    accuracy                           0.81        86
   macro avg       0.73      0.74      0.74        86
weighted avg       0.82      0.81      0.82        86


Fold 3
[[50  1  1  0]
 [ 1 15  5  0]
 [ 2  5  5  0]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.94      0.96      0.95        52
          A2       0.68      0.71      0.70        21
          B1       0.45      0.42      0.43        12
          B2       0.00      0.00      0.00         1

    accuracy                           0.81        86
   macro avg       0.52      0.52      0.52        86
weighted avg       0.80      0.81      0.81        86


Fold 4
[[49  2  1  0]
 [ 4 14  3  0]
 [ 0  6  6  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.92      0.94      0.93        52
          A2       0.64      0.67      0.65        21
          B1       0.55      0.50      0.52        12
          B2       0.00      0.00      0.00         1

    accuracy                           0.80        86
   macro avg       0.53      0.53      0.53        86
weighted avg       0.79      0.80      0.80        86


K-fold scores
[0.839450383069795, 0.7371233938391343, 0.8155841330854686, 0.8068879490564538, 0.7961467616651382]
SKF f1 score mean 0.799038524143198

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
1161
1161
Fold 0
[[50  3  0  0]
 [ 1 15  6  0]
 [ 0  5  8  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.98      0.94      0.96        53
          A2       0.65      0.68      0.67        22
          B1       0.53      0.62      0.57        13
          B2       0.00      0.00      0.00         1

    accuracy                           0.82        89
   macro avg       0.54      0.56      0.55        89
weighted avg       0.82      0.82      0.82        89


Fold 1
[[49  2  1  0]
 [ 2 13  6  0]
 [ 1  8  4  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.94      0.94      0.94        52
          A2       0.57      0.62      0.59        21
          B1       0.33      0.31      0.32        13
          B2       0.00      0.00      0.00         1

    accuracy                           0.76        87
   macro avg       0.46      0.47      0.46        87
weighted avg       0.75      0.76      0.75        87


Fold 2
[[47  5  0]
 [ 1 14  6]
 [ 1  3  9]]
              precision    recall  f1-score   support

          A1       0.96      0.90      0.93        52
          A2       0.64      0.67      0.65        21
          B1       0.60      0.69      0.64        13

    accuracy                           0.81        86
   macro avg       0.73      0.75      0.74        86
weighted avg       0.83      0.81      0.82        86


Fold 3
[[50  1  1  0]
 [ 1 15  5  0]
 [ 1  5  6  0]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.96      0.96      0.96        52
          A2       0.68      0.71      0.70        21
          B1       0.50      0.50      0.50        12
          B2       0.00      0.00      0.00         1

    accuracy                           0.83        86
   macro avg       0.54      0.54      0.54        86
weighted avg       0.82      0.83      0.82        86


Fold 4
[[49  3  0  0]
 [ 1 17  3  0]
 [ 0  6  6  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.98      0.94      0.96        52
          A2       0.65      0.81      0.72        21
          B1       0.60      0.50      0.55        12
          B2       0.00      0.00      0.00         1

    accuracy                           0.84        86
   macro avg       0.56      0.56      0.56        86
weighted avg       0.84      0.84      0.83        86


K-fold scores
[0.8208626579413096, 0.753667711598746, 0.818925593787842, 0.8215251487290427, 0.8336945141963042]
SKF f1 score mean 0.8097351252506491

SAME LANG EVAL DONE FOR THIS LANG
With POS ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3975
3975
Fold 0
[[48  5  0  0]
 [ 5 15  2  0]
 [ 2  5  6  0]
 [ 1  0  0  0]]
              precision    recall  f1-score   support

          A1       0.86      0.91      0.88        53
          A2       0.60      0.68      0.64        22
          B1       0.75      0.46      0.57        13
          B2       0.00      0.00      0.00         1

    accuracy                           0.78        89
   macro avg       0.55      0.51      0.52        89
weighted avg       0.77      0.78      0.77        89


Fold 1
[[49  3  0  0]
 [ 6 10  5  0]
 [ 6  4  3  0]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.80      0.94      0.87        52
          A2       0.56      0.48      0.51        21
          B1       0.38      0.23      0.29        13
          B2       0.00      0.00      0.00         1

    accuracy                           0.71        87
   macro avg       0.43      0.41      0.42        87
weighted avg       0.67      0.71      0.68        87


Fold 2
[[47  5  0]
 [ 6 13  2]
 [ 1  4  8]]
              precision    recall  f1-score   support

          A1       0.87      0.90      0.89        52
          A2       0.59      0.62      0.60        21
          B1       0.80      0.62      0.70        13

    accuracy                           0.79        86
   macro avg       0.75      0.71      0.73        86
weighted avg       0.79      0.79      0.79        86


Fold 3
[[50  2  0  0]
 [ 6 13  2  0]
 [ 3  6  3  0]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.85      0.96      0.90        52
          A2       0.59      0.62      0.60        21
          B1       0.60      0.25      0.35        12
          B2       0.00      0.00      0.00         1

    accuracy                           0.77        86
   macro avg       0.51      0.46      0.46        86
weighted avg       0.74      0.77      0.74        86


Fold 4
[[48  4  0  0]
 [ 5 12  4  0]
 [ 5  2  5  0]
 [ 1  0  0  0]]
              precision    recall  f1-score   support

          A1       0.81      0.92      0.86        52
          A2       0.67      0.57      0.62        21
          B1       0.56      0.42      0.48        12
          B2       0.00      0.00      0.00         1

    accuracy                           0.76        86
   macro avg       0.51      0.48      0.49        86
weighted avg       0.73      0.76      0.74        86


K-fold scores
[0.7657306034003325, 0.6848374898420672, 0.7890041886819074, 0.7416257602685878, 0.7396550652364606]
SKF f1 score mean 0.744170621485871

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3975
3975
Fold 0
[[52  1  0  0]
 [ 3 14  5  0]
 [ 0  7  6  0]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.95      0.98      0.96        53
          A2       0.61      0.64      0.62        22
          B1       0.55      0.46      0.50        13
          B2       0.00      0.00      0.00         1

    accuracy                           0.81        89
   macro avg       0.52      0.52      0.52        89
weighted avg       0.79      0.81      0.80        89


Fold 1
[[48  4  0  0]
 [ 2 14  5  0]
 [ 2  8  3  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.92      0.92      0.92        52
          A2       0.54      0.67      0.60        21
          B1       0.33      0.23      0.27        13
          B2       0.00      0.00      0.00         1

    accuracy                           0.75        87
   macro avg       0.45      0.46      0.45        87
weighted avg       0.73      0.75      0.74        87


Fold 2
[[47  5  0]
 [ 5 14  2]
 [ 0  7  6]]
              precision    recall  f1-score   support

          A1       0.90      0.90      0.90        52
          A2       0.54      0.67      0.60        21
          B1       0.75      0.46      0.57        13

    accuracy                           0.78        86
   macro avg       0.73      0.68      0.69        86
weighted avg       0.79      0.78      0.78        86


Fold 3
[[51  1  0  0]
 [ 1 16  4  0]
 [ 2  6  4  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.94      0.98      0.96        52
          A2       0.70      0.76      0.73        21
          B1       0.44      0.33      0.38        12
          B2       0.00      0.00      0.00         1

    accuracy                           0.83        86
   macro avg       0.52      0.52      0.52        86
weighted avg       0.80      0.83      0.81        86


Fold 4
[[46  5  1  0]
 [ 5 11  5  0]
 [ 3  2  7  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.85      0.88      0.87        52
          A2       0.61      0.52      0.56        21
          B1       0.50      0.58      0.54        12
          B2       0.00      0.00      0.00         1

    accuracy                           0.74        86
   macro avg       0.49      0.50      0.49        86
weighted avg       0.73      0.74      0.74        86


K-fold scores
[0.8002913025384935, 0.7362769292336423, 0.7783629037958577, 0.8125801359675865, 0.7376717183650049]
SKF f1 score mean 0.773036597980117

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
3975
3975
Fold 0
[[51  2  0  0]
 [ 3 14  5  0]
 [ 0  4  9  0]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.94      0.96      0.95        53
          A2       0.67      0.64      0.65        22
          B1       0.64      0.69      0.67        13
          B2       0.00      0.00      0.00         1

    accuracy                           0.83        89
   macro avg       0.56      0.57      0.57        89
weighted avg       0.82      0.83      0.83        89


Fold 1
[[48  4  0  0]
 [ 2 14  5  0]
 [ 2  7  4  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.92      0.92      0.92        52
          A2       0.56      0.67      0.61        21
          B1       0.40      0.31      0.35        13
          B2       0.00      0.00      0.00         1

    accuracy                           0.76        87
   macro avg       0.47      0.47      0.47        87
weighted avg       0.75      0.76      0.75        87


Fold 2
[[47  5  0]
 [ 3 14  4]
 [ 0  7  6]]
              precision    recall  f1-score   support

          A1       0.94      0.90      0.92        52
          A2       0.54      0.67      0.60        21
          B1       0.60      0.46      0.52        13

    accuracy                           0.78        86
   macro avg       0.69      0.68      0.68        86
weighted avg       0.79      0.78      0.78        86


Fold 3
[[50  1  1  0]
 [ 2 14  5  0]
 [ 2  6  4  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.93      0.96      0.94        52
          A2       0.67      0.67      0.67        21
          B1       0.36      0.33      0.35        12
          B2       0.00      0.00      0.00         1

    accuracy                           0.79        86
   macro avg       0.49      0.49      0.49        86
weighted avg       0.77      0.79      0.78        86


Fold 4
[[47  4  1  0]
 [ 5 11  5  0]
 [ 3  1  8  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.85      0.90      0.88        52
          A2       0.69      0.52      0.59        21
          B1       0.53      0.67      0.59        12
          B2       0.00      0.00      0.00         1

    accuracy                           0.77        86
   macro avg       0.52      0.52      0.52        86
weighted avg       0.76      0.77      0.76        86


K-fold scores
[0.8260181185168183, 0.7506246876561719, 0.7815676234997151, 0.7817501955472461, 0.7590679138168814]
SKF f1 score mean 0.7798057078073665

SAME LANG EVAL DONE FOR THIS LANG
Dep ngrams:  
 ******
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2943
2943
Fold 0
[[52  1  0  0]
 [ 5 15  2  0]
 [ 2  2  9  0]
 [ 1  0  0  0]]
              precision    recall  f1-score   support

          A1       0.87      0.98      0.92        53
          A2       0.83      0.68      0.75        22
          B1       0.82      0.69      0.75        13
          B2       0.00      0.00      0.00         1

    accuracy                           0.85        89
   macro avg       0.63      0.59      0.61        89
weighted avg       0.84      0.85      0.84        89


Fold 1
[[50  2  0  0]
 [ 8 11  2  0]
 [ 8  2  3  0]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.76      0.96      0.85        52
          A2       0.69      0.52      0.59        21
          B1       0.60      0.23      0.33        13
          B2       0.00      0.00      0.00         1

    accuracy                           0.74        87
   macro avg       0.51      0.43      0.44        87
weighted avg       0.71      0.74      0.70        87


Fold 2
[[47  5  0]
 [ 7 11  3]
 [ 1  4  8]]
              precision    recall  f1-score   support

          A1       0.85      0.90      0.88        52
          A2       0.55      0.52      0.54        21
          B1       0.73      0.62      0.67        13

    accuracy                           0.77        86
   macro avg       0.71      0.68      0.69        86
weighted avg       0.76      0.77      0.76        86


Fold 3
[[50  2  0  0]
 [ 6 13  2  0]
 [ 3  3  6  0]
 [ 1  0  0  0]]
              precision    recall  f1-score   support

          A1       0.83      0.96      0.89        52
          A2       0.72      0.62      0.67        21
          B1       0.75      0.50      0.60        12
          B2       0.00      0.00      0.00         1

    accuracy                           0.80        86
   macro avg       0.58      0.52      0.54        86
weighted avg       0.78      0.80      0.79        86


Fold 4
[[50  2  0  0]
 [ 7 11  3  0]
 [ 4  4  4  0]
 [ 1  0  0  0]]
              precision    recall  f1-score   support

          A1       0.81      0.96      0.88        52
          A2       0.65      0.52      0.58        21
          B1       0.57      0.33      0.42        12
          B2       0.00      0.00      0.00         1

    accuracy                           0.76        86
   macro avg       0.51      0.45      0.47        86
weighted avg       0.73      0.76      0.73        86


K-fold scores
[0.8430197872128865, 0.6998576601148198, 0.7629907248866011, 0.7863787375415283, 0.7305181558547531]
SKF f1 score mean 0.7645530131221178

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2943
2943
Fold 0
[[49  4  0  0]
 [ 2 18  2  0]
 [ 2  4  7  0]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.92      0.92      0.92        53
          A2       0.67      0.82      0.73        22
          B1       0.78      0.54      0.64        13
          B2       0.00      0.00      0.00         1

    accuracy                           0.83        89
   macro avg       0.59      0.57      0.57        89
weighted avg       0.83      0.83      0.83        89


Fold 1
[[49  2  1  0]
 [ 5 14  2  0]
 [ 3  5  5  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.86      0.94      0.90        52
          A2       0.67      0.67      0.67        21
          B1       0.56      0.38      0.45        13
          B2       0.00      0.00      0.00         1

    accuracy                           0.78        87
   macro avg       0.52      0.50      0.51        87
weighted avg       0.76      0.78      0.77        87


Fold 2
[[45  7  0]
 [ 2 10  9]
 [ 1  8  4]]
              precision    recall  f1-score   support

          A1       0.94      0.87      0.90        52
          A2       0.40      0.48      0.43        21
          B1       0.31      0.31      0.31        13

    accuracy                           0.69        86
   macro avg       0.55      0.55      0.55        86
weighted avg       0.71      0.69      0.70        86


Fold 3
[[48  4  0  0]
 [ 3 13  5  0]
 [ 1  7  4  0]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.92      0.92      0.92        52
          A2       0.52      0.62      0.57        21
          B1       0.44      0.33      0.38        12
          B2       0.00      0.00      0.00         1

    accuracy                           0.76        86
   macro avg       0.47      0.47      0.47        86
weighted avg       0.75      0.76      0.75        86


Fold 4
[[48  4  0  0]
 [ 4 12  5  0]
 [ 1  2  9  0]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.91      0.92      0.91        52
          A2       0.63      0.57      0.60        21
          B1       0.64      0.75      0.69        12
          B2       0.00      0.00      0.00         1

    accuracy                           0.80        86
   macro avg       0.55      0.56      0.55        86
weighted avg       0.79      0.80      0.80        86


K-fold scores
[0.8251235121219069, 0.7662228101962363, 0.6968655207280081, 0.7493138812653475, 0.7959366215180169]
SKF f1 score mean 0.7666924691659032

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
2943
2943
Fold 0
[[50  3  0  0]
 [ 1 17  4  0]
 [ 2  4  7  0]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.94      0.94      0.94        53
          A2       0.68      0.77      0.72        22
          B1       0.64      0.54      0.58        13
          B2       0.00      0.00      0.00         1

    accuracy                           0.83        89
   macro avg       0.56      0.56      0.56        89
weighted avg       0.82      0.83      0.83        89


Fold 1
[[49  2  1  0]
 [ 5 14  2  0]
 [ 3  5  5  0]
 [ 0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.86      0.94      0.90        52
          A2       0.67      0.67      0.67        21
          B1       0.56      0.38      0.45        13
          B2       0.00      0.00      0.00         1

    accuracy                           0.78        87
   macro avg       0.52      0.50      0.51        87
weighted avg       0.76      0.78      0.77        87


Fold 2
[[45  7  0]
 [ 2 10  9]
 [ 0  8  5]]
              precision    recall  f1-score   support

          A1       0.96      0.87      0.91        52
          A2       0.40      0.48      0.43        21
          B1       0.36      0.38      0.37        13

    accuracy                           0.70        86
   macro avg       0.57      0.58      0.57        86
weighted avg       0.73      0.70      0.71        86


Fold 3
[[48  4  0  0]
 [ 4 12  5  0]
 [ 1  7  4  0]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.91      0.92      0.91        52
          A2       0.50      0.57      0.53        21
          B1       0.44      0.33      0.38        12
          B2       0.00      0.00      0.00         1

    accuracy                           0.74        86
   macro avg       0.46      0.46      0.46        86
weighted avg       0.73      0.74      0.74        86


Fold 4
[[48  4  0  0]
 [ 5 11  5  0]
 [ 1  2  9  0]
 [ 0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.89      0.92      0.91        52
          A2       0.61      0.52      0.56        21
          B1       0.64      0.75      0.69        12
          B2       0.00      0.00      0.00         1

    accuracy                           0.79        86
   macro avg       0.54      0.55      0.54        86
weighted avg       0.78      0.79      0.78        86


K-fold scores
[0.8258227747230855, 0.7662228101962363, 0.7118369403505905, 0.7362126245847176, 0.7819556485638102]
SKF f1 score mean 0.7644101596836881

SAME LANG EVAL DONE FOR THIS LANG
Cross lingual classification, DE Train, IT Test for dimension OverallCEFRrating
Printing cross-corpus classification evaluation results: 
******* 
 Setting - Train with:  DE  Test with:  IT  ****** 

Features: pos
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.72875
[[  3  25   0   0   0   0]
 [  9 331  36   4   0   0]
 [  2  86 249  55   0   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.7479857105377596
Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.26125
[[  5  18   2   3   0   0]
 [ 20 184  53 113  10   0]
 [  7  85  20 279   1   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.3098723202638509
Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.2925
[[  4  18   2   4   0   0]
 [ 19 206  52  98   5   0]
 [  7  96  24 264   1   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.3344415679698145
Features: dep
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.54375
[[  4  24   0   0   0   0]
 [ 11 281  77  11   0   0]
 [  7  46 150 189   0   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.6082644846615056
Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.39625
[[  4  19   3   1   1   0]
 [ 20 182  85  80  13   0]
 [  5  84 131 160  12   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.4750268469865334
Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.3525
[[  3  21   2   1   1   0]
 [ 17 187  66  98  12   0]
 [  6  70  92 215   9   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.4372070246538332
Cross lingual classification, DE Train, CZ Test for dimension OverallCEFRrating
Printing cross-corpus classification evaluation results: 
******* 
 Setting - Train with:  DE  Test with:  CZ  ****** 

Features: pos
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.6935483870967742
[[  0   0   0   0   0   0]
 [  0 141  46   1   0   0]
 [  0  20 112  33   0   0]
 [  0   1  32  48   0   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.6988306065507289
Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.45622119815668205
[[  0   0   0   0   0   0]
 [  0 149   6  29   4   0]
 [  0 106   4  41  14   0]
 [  0  27   1  45   8   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.3776356625599549
Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.47465437788018433
[[  0   0   0   0   0   0]
 [  0 149   5  32   2   0]
 [  0  87   5  52  21   0]
 [  0  20   1  52   8   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.40178676653888484
Features: dep
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.6497695852534562
[[  0   0   0   0   0   0]
 [  0 128  60   0   0   0]
 [  0  25 102  38   0   0]
 [  0   3  26  52   0   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.6555858117226915
Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.34101382488479265
[[  0   0   0   0   0   0]
 [  3 103  46   2  34   0]
 [  0  55  40   5  65   0]
 [  0   9   8   5  59   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.388866097076232
Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.31336405529953915
[[ 0  0  0  0  0  0]
 [ 3 97 44  2 42  0]
 [ 0 43 32 11 79  0]
 [ 0  3  3  7 68  0]
 [ 0  0  0  0  0  0]
 [ 0  0  0  0  0  0]]
CROSS LANG EVAL DONE. F1score: 
0.379478623214551
Cross lingual classification, DE Train, IT Test for dimension Grammaticalaccuracy
Printing cross-corpus classification evaluation results: 
******* 
 Setting - Train with:  DE  Test with:  IT  ****** 

Features: pos
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.51375
[[ 12  58   1   0   0   0]
 [ 17 191  30   4   0   0]
 [  5 149 192  30   0   0]
 [  1  11  83  16   0   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.489809250705014
Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.365
[[ 22  27  10   8   4   0]
 [ 54  93  42  47   6   0]
 [ 41  86 117 126   6   0]
 [  4  13  34  60   0   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.3796377560989306
Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.3425
[[ 24  25   7   7   8   0]
 [ 62  76  35  51  18   0]
 [ 50  56 103 154  13   0]
 [  2   9  28  71   1   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.35944221147911015
Features: dep
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.46625
[[  6  60   5   0   0   0]
 [ 16 162  55   9   0   0]
 [  8 123 141 104   0   0]
 [  2   8  37  64   0   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.45259235374995
Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.33625
[[ 32  13  18   8   0   0]
 [100  34  78  26   4   0]
 [ 94  23 170  86   3   0]
 [ 10   5  61  33   2   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.34539024061822204
Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.3575
[[ 27  10  26   8   0   0]
 [ 93  26  89  30   4   0]
 [ 74  17 186  97   2   0]
 [  8   1  54  47   1   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.35438969539031745
Cross lingual classification, DE Train, CZ Test for dimension Grammaticalaccuracy
Printing cross-corpus classification evaluation results: 
******* 
 Setting - Train with:  DE  Test with:  CZ  ****** 

Features: pos
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.5783410138248848
[[  0   5   1   0   0   0]
 [  0 120  56   9   0   0]
 [  0  42  87  27   0   0]
 [  0   4  34  44   0   0]
 [  0   0   3   2   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.5743435373015608
Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.3709677419354839
[[  1   4   0   0   1   0]
 [  6 122  27  21   9   0]
 [  1 100  12  31  12   0]
 [  0  52   0  26   4   0]
 [  0   4   0   1   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.32995655256856044
Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.33410138248847926
[[ 1  2  0  1  2  0]
 [10 87 20 41 27  0]
 [ 2 57 10 52 35  0]
 [ 0 24  0 45 13  0]
 [ 0  0  0  3  2  0]
 [ 0  0  0  0  0  0]]
CROSS LANG EVAL DONE. F1score: 
0.32549850839471767
Features: dep
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.5414746543778802
[[ 0  5  1  0  0  0]
 [ 2 98 80  5  0  0]
 [ 1 41 90 24  0  0]
 [ 0  9 26 47  0  0]
 [ 0  0  1  4  0  0]
 [ 0  0  0  0  0  0]]
CROSS LANG EVAL DONE. F1score: 
0.5395861459003711
Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.3778801843317972
[[ 2  1  2  1  0  0]
 [37 41 58 43  6  0]
 [11 24 59 54  8  0]
 [ 0  6  8 60  8  0]
 [ 0  0  0  3  2  0]
 [ 0  0  0  0  0  0]]
CROSS LANG EVAL DONE. F1score: 
0.38176319703070133
Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.3709677419354839
[[ 2  1  2  1  0  0]
 [26 31 67 52  9  0]
 [ 8 16 61 62  9  0]
 [ 0  4  4 65  9  0]
 [ 0  0  0  3  2  0]
 [ 0  0  0  0  0  0]]
CROSS LANG EVAL DONE. F1score: 
0.3580882249754752
Cross lingual classification, DE Train, IT Test for dimension Orthography
Printing cross-corpus classification evaluation results: 
******* 
 Setting - Train with:  DE  Test with:  IT  ****** 

Features: pos
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.295
[[  0  14   8   0   0   0]
 [  0  58  84   1   0   0]
 [  0  65 145  10   0   0]
 [  0   1  43  33   0   0]
 [  0   4  99 159   0   0]
 [  0   2  44  30   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.21676731602261298
Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.17625
[[  0   8   8   6   0   0]
 [  2  23  45  70   2   1]
 [  2  31  57 124   4   2]
 [  0   8  10  58   1   0]
 [  1  14  27 216   3   1]
 [  0   6   9  60   1   0]]
CROSS LANG EVAL DONE. F1score: 
0.1441384837653443
Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.15375
[[  0   7   5   9   1   0]
 [  4  21  34  81   2   1]
 [  5  30  37 147   1   0]
 [  1   6   8  61   1   0]
 [  2  12   8 234   4   2]
 [  2   1   3  70   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.12568988571513107
Features: dep
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.31
[[  1   9  12   0   0   0]
 [  1  35 102   5   0   0]
 [  3  25 164  28   0   0]
 [  0   1  28  48   0   0]
 [  0   3  55 204   0   0]
 [  0   2  28  46   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.23019254330787817
Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.1975
[[  0   8   7   5   2   0]
 [  0  61  25  55   2   0]
 [  1  72  35 105   7   0]
 [  0  12   9  49   7   0]
 [  0  31  17 198  13   3]
 [  1  10  11  46   8   0]]
CROSS LANG EVAL DONE. F1score: 
0.170044140405841
Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.19375
[[  0   7   7   6   2   0]
 [  0  53  26  57   6   1]
 [  1  65  35 107   9   3]
 [  1   7   8  50  10   1]
 [  1  17  18 206  17   3]
 [  1   7  11  49   8   0]]
CROSS LANG EVAL DONE. F1score: 
0.1754987120905495
Cross lingual classification, DE Train, CZ Test for dimension Orthography
Printing cross-corpus classification evaluation results: 
******* 
 Setting - Train with:  DE  Test with:  CZ  ****** 

Features: pos
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.5714285714285714
[[  0   0   0   0   0   0]
 [  0   6  33   7   0   0]
 [  0  18 156  95   0   0]
 [  0   6  22  86   0   0]
 [  0   0   0   5   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.5667807943205707
Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.3778801843317972
[[  0   0   0   0   0   0]
 [  0  14  14  18   0   0]
 [  0  46  98 105   3  17]
 [  0  10  49  52   2   1]
 [  0   0   1   4   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.4006845125707286
Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.31105990783410137
[[  0   0   0   0   0   0]
 [  0  11   9  23   1   2]
 [  0  39  56 150   2  22]
 [  0   9  31  68   2   4]
 [  0   0   0   5   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.31162955681609805
Features: dep
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.5506912442396313
[[  0   0   0   0   0   0]
 [  0   7  32   7   0   0]
 [  0  10 151 108   0   0]
 [  0   3  30  81   0   0]
 [  0   0   0   5   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.5459210038038652
Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.3387096774193548
[[  0   0   0   0   0   0]
 [  0   9  18  19   0   0]
 [  0  16  88 123  42   0]
 [  0   7  26  50  28   3]
 [  0   0   2   3   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.3801555223213488
Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.315668202764977
[[  0   0   0   0   0   0]
 [  0   7  16  21   1   1]
 [  0   8  67 151  40   3]
 [  0   3  18  63  27   3]
 [  0   0   0   4   0   1]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.3414178335298001
Cross lingual classification, DE Train, IT Test for dimension Vocabularyrange
Printing cross-corpus classification evaluation results: 
******* 
 Setting - Train with:  DE  Test with:  IT  ****** 

Features: pos
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.62125
[[  2  32   5   0   0   0]
 [  7 144  92   0   0   0]
 [  7  46 230  45   0   0]
 [  0   2  65 121   0   0]
 [  0   0   0   2   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.6125604304722271
Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.3675
[[  4  21   6   3   4   1]
 [ 10 135  47  13  23  15]
 [  8 162  39  84  29   6]
 [  2  49   7 116  13   1]
 [  0   1   0   1   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.34992905130197305
Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.3875
[[  5  22   4   4   4   0]
 [ 12 138  41  16  24  12]
 [  9 142  36  97  40   4]
 [  2  30   6 131  18   1]
 [  0   0   0   2   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.36477942577917477
Features: dep
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.56
[[  3  25  11   0   0   0]
 [  8  82 151   2   0   0]
 [  8  20 219  80   1   0]
 [  2   0  40 144   2   0]
 [  0   0   0   2   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.5419621224002066
Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.3975
[[  6  26   1   6   0   0]
 [ 18 130  34  29  28   4]
 [ 14 116  56  99  41   2]
 [  1  25  14 126  22   0]
 [  0   0   0   2   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.39140080831408774
Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.39
[[  6  27   3   3   0   0]
 [  9 127  43  32  30   2]
 [ 12 101  46 117  48   4]
 [  2  14  11 133  28   0]
 [  0   0   0   2   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.378408819131082
Cross lingual classification, DE Train, CZ Test for dimension Vocabularyrange
Printing cross-corpus classification evaluation results: 
******* 
 Setting - Train with:  DE  Test with:  CZ  ****** 

Features: pos
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.5737327188940092
[[  0   7   1   0   0   0]
 [  0  62  70   0   0   0]
 [  0   9 121  10   1   0]
 [  0   0  67  64  12   0]
 [  0   0   2   6   2   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.5679819623192897
Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.24423963133640553
[[ 0  4  0  0  4  0]
 [ 0 91  2  5 34  0]
 [ 0 99  1  1 40  0]
 [ 0 43  0  6 94  0]
 [ 0  1  0  1  8  0]
 [ 0  0  0  0  0  0]]
CROSS LANG EVAL DONE. F1score: 
0.1814059178023524
Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.22119815668202766
[[  0   4   0   0   4   0]
 [  0  78   1   3  50   0]
 [  0  78   1   3  59   0]
 [  0  27   0   7 109   0]
 [  0   0   0   0  10   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.1847546813923596
Features: dep
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.5046082949308756
[[  0   7   1   0   0   0]
 [  0  37  95   0   0   0]
 [  0   3 131   7   0   0]
 [  0   1  87  48   7   0]
 [  0   0   1   6   3   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.47367267789968287
Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.3387096774193548
[[  0   6   0   0   2   0]
 [  0  68  34   7  22   1]
 [  1  27  56  14  43   0]
 [  0   9  10  15 109   0]
 [  0   0   0   2   8   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.3784220941538705
Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.30184331797235026
[[  0   5   0   0   3   0]
 [  0  63  29   9  31   0]
 [  0  21  48  15  57   0]
 [  0   5   1  11 126   0]
 [  0   0   0   1   9   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.35423795020941173
Cross lingual classification, DE Train, IT Test for dimension Vocabularycontrol
Printing cross-corpus classification evaluation results: 
******* 
 Setting - Train with:  DE  Test with:  IT  ****** 

Features: pos
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.52375
[[ 11  55   4   0   0   0]
 [ 11 174  19   0   0   0]
 [  8 131 135  53   0   0]
 [  0   9  88  99   0   0]
 [  0   0   1   2   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.5049882737107102
Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.34125
[[ 18  15  16  18   3   0]
 [ 37  33  60  51  21   2]
 [ 20  28  68 197  13   1]
 [  3   7  29 154   2   1]
 [  0   0   0   3   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.3130111992510149
Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.33875
[[ 22  14  11  20   3   0]
 [ 50  31  45  56  21   1]
 [ 33  24  55 201  14   0]
 [  5   4  23 163   0   1]
 [  0   0   0   3   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.300989414398077
Features: dep
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.55
[[  5  52  13   0   0   0]
 [  9 146  48   1   0   0]
 [  9  87 143  88   0   0]
 [  3   4  43 146   0   0]
 [  0   0   0   3   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.5286506677554361
Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.3475
[[ 21  16  14  13   5   1]
 [ 56  36  47  53  12   0]
 [ 36  38  85 155  13   0]
 [  4   5  43 136   8   0]
 [  0   0   1   2   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.3355343832005692
Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.335
[[ 20  16  14  15   3   2]
 [ 56  34  49  59   6   0]
 [ 31  37  62 186  11   0]
 [  4   4  31 152   4   1]
 [  0   0   0   3   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.3049530927952025
Cross lingual classification, DE Train, CZ Test for dimension Vocabularycontrol
Printing cross-corpus classification evaluation results: 
******* 
 Setting - Train with:  DE  Test with:  CZ  ****** 

Features: pos
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.5714285714285714
[[ 0  5  0  0  0  0]
 [ 0 89 39  3  0  0]
 [ 0 44 98 40  0  0]
 [ 0  1 38 61  0  0]
 [ 0  0  3 13  0  0]
 [ 0  0  0  0  0  0]]
CROSS LANG EVAL DONE. F1score: 
0.556850345641783
Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.35253456221198154
[[  1   2   1   1   0   0]
 [  1  92   3  34   1   0]
 [  0 123   0  54   5   0]
 [  0  36   0  58   6   0]
 [  0   4   0  10   2   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.25534977620400684
Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.3317972350230415
[[ 1  2  1  1  0  0]
 [ 3 73  3 52  0  0]
 [ 0 94  0 81  6  1]
 [ 0 26  0 68  5  1]
 [ 0  3  0 11  2  0]
 [ 0  0  0  0  0  0]]
CROSS LANG EVAL DONE. F1score: 
0.2417101969997369
Features: dep
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.5483870967741935
[[  0   4   1   0   0   0]
 [  0  70  59   2   0   0]
 [  0  39 106  37   0   0]
 [  0   4  34  62   0   0]
 [  0   0   4  12   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.5348528215837288
Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.29493087557603687
[[ 0  2  3  0  0  0]
 [ 3 60 34  8 25  1]
 [ 3 65 45 13 53  3]
 [ 1 30  8 16 44  1]
 [ 0  4  0  3  7  2]
 [ 0  0  0  0  0  0]]
CROSS LANG EVAL DONE. F1score: 
0.3190280532649771
Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.2903225806451613
[[ 0  2  3  0  0  0]
 [ 3 55 33 14 26  0]
 [ 3 58 40 21 57  3]
 [ 0 18  8 25 48  1]
 [ 0  2  1  6  6  1]
 [ 0  0  0  0  0  0]]
CROSS LANG EVAL DONE. F1score: 
0.3227653258647805
Cross lingual classification, DE Train, IT Test for dimension CoherenceCohesion
Printing cross-corpus classification evaluation results: 
******* 
 Setting - Train with:  DE  Test with:  IT  ****** 

Features: pos
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.5925
[[  6  80  23   0   0   0]
 [ 14 198 118   4   0   0]
 [  3  16 260  21   0   0]
 [  0   1  46  10   0   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.5517654577414075
Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.28625
[[ 21  45  16  12  13   2]
 [ 38 147  40  64  34  11]
 [  9  30  20 199  42   0]
 [  1   3   6  41   6   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.3066590374253668
Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.265
[[ 24  41  20  10  13   1]
 [ 47 129  47  64  43   4]
 [  9  24  17 204  46   0]
 [  2   4   4  42   5   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.28544879851486804
Features: dep
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.39625
[[  6  60  43   0   0   0]
 [ 12 105 206  11   0   0]
 [  5   5 173 117   0   0]
 [  1   0  23  33   0   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.38198374115897465
Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.26375
[[ 23  38  14  21  13   0]
 [ 42  97  37 124  34   0]
 [  5  27  45 203  20   0]
 [  1   3   6  46   1   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.2956236656545119
Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.2425
[[ 23  34  17  21  14   0]
 [ 45  83  43 128  35   0]
 [  4  22  41 213  20   0]
 [  1   2   6  47   1   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.2702672194941
Cross lingual classification, DE Train, CZ Test for dimension CoherenceCohesion
Printing cross-corpus classification evaluation results: 
******* 
 Setting - Train with:  DE  Test with:  CZ  ****** 

Features: pos
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.6682027649769585
[[  0   1   0   0   0   0]
 [  1  57  42   1   0   0]
 [  0  13 147  11   0   0]
 [  0   0  70  86   0   0]
 [  0   0   1   4   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.6626425623641575
Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.1728110599078341
[[ 0  1  0  0  0  0]
 [17 33  9 18 24  0]
 [ 9 61  5 24 71  1]
 [ 3 21  0 33 99  0]
 [ 0  0  0  1  4  0]
 [ 0  0  0  0  0  0]]
CROSS LANG EVAL DONE. F1score: 
0.1947891519176562
Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.1497695852534562
[[  0   1   0   0   0   0]
 [ 20  26   9  22  23   1]
 [ 12  25   4  34  96   0]
 [  1   6   2  31 116   0]
 [  0   0   0   1   4   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.18476858050479844
Features: dep
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.5806451612903226
[[  0   1   0   0   0   0]
 [  2  39  60   0   0   0]
 [  0  12 154   5   0   0]
 [  0   1  96  59   0   0]
 [  0   0   1   4   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.5589956800183964
Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.12903225806451613
[[  1   0   0   0   0   0]
 [ 17  14  23   6  41   0]
 [  3  18  22  20 108   0]
 [  0   6   2  14 134   0]
 [  0   0   0   0   5   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.17835566140480255
Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.1359447004608295
[[  1   0   0   0   0   0]
 [ 16  17  14   5  49   0]
 [  2  15  20  15 119   0]
 [  0   2   0  16 138   0]
 [  0   0   0   0   5   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.1959933606997117
Cross lingual classification, DE Train, IT Test for dimension Sociolinguisticappropriateness
Printing cross-corpus classification evaluation results: 
******* 
 Setting - Train with:  DE  Test with:  IT  ****** 

Features: pos
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.5175
[[  4  48   4   0   0   0]
 [  5 253  58  20   0   0]
 [  3  94 134 141   0   0]
 [  0   3  10  23   0   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.5227867061000623
Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.29625
[[  7  28   5  13   3   0]
 [ 28 157  37  83  31   0]
 [  9 106  48 187  22   0]
 [  1   7   3  25   0   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.32025628626797825
Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.26625
[[  9  30   2  12   3   0]
 [ 33 141  30  91  41   0]
 [ 14  64  32 224  38   0]
 [  1   3   1  31   0   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.2926738106654778
Features: dep
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.39125
[[  4  45   6   1   0   0]
 [ 11 201  79  45   0   0]
 [  6  66  75 225   0   0]
 [  1   1   1  33   0   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.40693190390168965
Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.2325
[[ 10  22   2  21   1   0]
 [ 36 114  32 149   5   0]
 [ 16  49  32 268   7   0]
 [  1   4   1  30   0   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.2673122815413181
Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.21125
[[  8  21   7  19   1   0]
 [ 25  92  46 169   4   0]
 [ 12  31  38 285   6   0]
 [  1   2   2  31   0   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.2524790090309983
Cross lingual classification, DE Train, CZ Test for dimension Sociolinguisticappropriateness
Printing cross-corpus classification evaluation results: 
******* 
 Setting - Train with:  DE  Test with:  CZ  ****** 

Features: pos
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.20276497695852536
[[  0  69  44 148   0   0]
 [  0  44  57   5   0   0]
 [  0  11  43   9   0   0]
 [  0   0   3   1   0   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.1530055897021825
Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.1497695852534562
[[  0 107   6 116  32   0]
 [  1  62   2  33   8   0]
 [  0  33   0  24   6   0]
 [  0   1   0   3   0   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.09831924000417581
Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.10138248847926268
[[  0  69   5 139  48   0]
 [  3  41   2  50  10   0]
 [  0  20   0  33  10   0]
 [  0   1   0   3   0   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.08474633405226234
Features: dep
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.1774193548387097
[[  0  73  31 157   0   0]
 [  0  45  41  20   0   0]
 [  0  11  31  21   0   0]
 [  0   1   2   1   0   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.14680446470173825
Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.09216589861751152
[[ 6 69  8 85 93  0]
 [14 29  3 52  8  0]
 [ 2 24  2 35  0  0]
 [ 0  0  0  3  1  0]
 [ 0  0  0  0  0  0]
 [ 0  0  0  0  0  0]]
CROSS LANG EVAL DONE. F1score: 
0.0955804557047552
Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
0.07603686635944701
[[  2  42   8  96 113   0]
 [  8  26   3  60   9   0]
 [  0  15   2  46   0   0]
 [  0   0   0   3   1   0]
 [  0   0   0   0   0   0]
 [  0   0   0   0   0   0]]
CROSS LANG EVAL DONE. F1score: 
0.08397935875017462
Multi lingual classification for dimension OverallCEFRrating
Doing: take all data as if it belongs to one large dataset, and do classification
Mega classification for:  pos  features
2260 2260 2260 633
Distribution of labels: 
Counter({'B1': 885, 'A2': 874, 'B2': 374, 'A1': 85, 'C1': 42})
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13231
13231
Fold 0
[[  2  15   0   0   0]
 [  0 144  31   0   0]
 [  0  40 123  14   0]
 [  0   0  10  65   0]
 [  0   0   0   9   0]]
              precision    recall  f1-score   support

          A1       1.00      0.12      0.21        17
          A2       0.72      0.82      0.77       175
          B1       0.75      0.69      0.72       177
          B2       0.74      0.87      0.80        75
          C1       0.00      0.00      0.00         9

    accuracy                           0.74       453
   macro avg       0.64      0.50      0.50       453
weighted avg       0.73      0.74      0.72       453


Fold 1
[[  3  14   0   0   0]
 [  3 151  21   0   0]
 [  0  38 128  11   0]
 [  0   1  15  59   0]
 [  0   0   0   9   0]]
              precision    recall  f1-score   support

          A1       0.50      0.18      0.26        17
          A2       0.74      0.86      0.80       175
          B1       0.78      0.72      0.75       177
          B2       0.75      0.79      0.77        75
          C1       0.00      0.00      0.00         9

    accuracy                           0.75       453
   macro avg       0.55      0.51      0.51       453
weighted avg       0.73      0.75      0.74       453


Fold 2
[[  2  15   0   0   0]
 [  3 141  31   0   0]
 [  0  33 124  20   0]
 [  0   1  20  54   0]
 [  0   0   0   8   0]]
              precision    recall  f1-score   support

          A1       0.40      0.12      0.18        17
          A2       0.74      0.81      0.77       175
          B1       0.71      0.70      0.70       177
          B2       0.66      0.72      0.69        75
          C1       0.00      0.00      0.00         8

    accuracy                           0.71       452
   macro avg       0.50      0.47      0.47       452
weighted avg       0.69      0.71      0.70       452


Fold 3
[[  6  11   0   0   0]
 [  1 158  16   0   0]
 [  0  32 135  10   0]
 [  0   1  20  54   0]
 [  0   0   0   8   0]]
              precision    recall  f1-score   support

          A1       0.86      0.35      0.50        17
          A2       0.78      0.90      0.84       175
          B1       0.79      0.76      0.78       177
          B2       0.75      0.72      0.73        75
          C1       0.00      0.00      0.00         8

    accuracy                           0.78       452
   macro avg       0.64      0.55      0.57       452
weighted avg       0.77      0.78      0.77       452


Fold 4
[[  5  12   0   0   0]
 [  0 150  24   0   0]
 [  0  31 128  18   0]
 [  0   1  23  50   0]
 [  0   0   0   8   0]]
              precision    recall  f1-score   support

          A1       1.00      0.29      0.45        17
          A2       0.77      0.86      0.82       174
          B1       0.73      0.72      0.73       177
          B2       0.66      0.68      0.67        74
          C1       0.00      0.00      0.00         8

    accuracy                           0.74       450
   macro avg       0.63      0.51      0.53       450
weighted avg       0.73      0.74      0.73       450


K-fold scores
[0.7193011173014433, 0.7378101341332451, 0.696002855498689, 0.7690574715837368, 0.7280793441663006]
SKF f1 score mean 0.7300501845366829

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13231
13231
Fold 0
[[  3  13   1   0   0]
 [  5 123  45   2   0]
 [  0  47 110  19   1]
 [  0   1  20  53   1]
 [  0   0   1   8   0]]
              precision    recall  f1-score   support

          A1       0.38      0.18      0.24        17
          A2       0.67      0.70      0.69       175
          B1       0.62      0.62      0.62       177
          B2       0.65      0.71      0.68        75
          C1       0.00      0.00      0.00         9

    accuracy                           0.64       453
   macro avg       0.46      0.44      0.44       453
weighted avg       0.62      0.64      0.63       453


Fold 1
[[  5   9   3   0   0]
 [ 12 118  38   7   0]
 [  0  50 109  18   0]
 [  0   3  25  46   1]
 [  0   0   2   6   1]]
              precision    recall  f1-score   support

          A1       0.29      0.29      0.29        17
          A2       0.66      0.67      0.66       175
          B1       0.62      0.62      0.62       177
          B2       0.60      0.61      0.61        75
          C1       0.50      0.11      0.18         9

    accuracy                           0.62       453
   macro avg       0.53      0.46      0.47       453
weighted avg       0.61      0.62      0.61       453


Fold 2
[[  4  12   1   0   0]
 [  8 121  44   2   0]
 [  0  56 106  15   0]
 [  0   6  17  49   3]
 [  0   0   0   7   1]]
              precision    recall  f1-score   support

          A1       0.33      0.24      0.28        17
          A2       0.62      0.69      0.65       175
          B1       0.63      0.60      0.61       177
          B2       0.67      0.65      0.66        75
          C1       0.25      0.12      0.17         8

    accuracy                           0.62       452
   macro avg       0.50      0.46      0.47       452
weighted avg       0.62      0.62      0.62       452


Fold 3
[[  3  13   0   1   0]
 [ 11 136  26   2   0]
 [  0  50 115  12   0]
 [  0   4  21  44   6]
 [  0   1   0   6   1]]
              precision    recall  f1-score   support

          A1       0.21      0.18      0.19        17
          A2       0.67      0.78      0.72       175
          B1       0.71      0.65      0.68       177
          B2       0.68      0.59      0.63        75
          C1       0.14      0.12      0.13         8

    accuracy                           0.66       452
   macro avg       0.48      0.46      0.47       452
weighted avg       0.66      0.66      0.66       452


Fold 4
[[  5  10   2   0   0]
 [  4 124  44   2   0]
 [  0  38 123  15   1]
 [  0   3  22  47   2]
 [  0   0   1   6   1]]
              precision    recall  f1-score   support

          A1       0.56      0.29      0.38        17
          A2       0.71      0.71      0.71       174
          B1       0.64      0.69      0.67       177
          B2       0.67      0.64      0.65        74
          C1       0.25      0.12      0.17         8

    accuracy                           0.67       450
   macro avg       0.57      0.49      0.52       450
weighted avg       0.66      0.67      0.66       450


K-fold scores
[0.6283297510277025, 0.6122938822240243, 0.6170571405280378, 0.6574823219282396, 0.6618267768172257]
SKF f1 score mean 0.635397974505046

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13231
13231
Fold 0
[[  3  13   1   0   0]
 [  5 124  45   1   0]
 [  0  38 119  19   1]
 [  0   1  17  56   1]
 [  0   0   1   8   0]]
              precision    recall  f1-score   support

          A1       0.38      0.18      0.24        17
          A2       0.70      0.71      0.71       175
          B1       0.65      0.67      0.66       177
          B2       0.67      0.75      0.70        75
          C1       0.00      0.00      0.00         9

    accuracy                           0.67       453
   macro avg       0.48      0.46      0.46       453
weighted avg       0.65      0.67      0.66       453


Fold 1
[[  6   8   3   0   0]
 [ 11 124  34   6   0]
 [  0  47 115  15   0]
 [  0   3  19  52   1]
 [  0   0   1   7   1]]
              precision    recall  f1-score   support

          A1       0.35      0.35      0.35        17
          A2       0.68      0.71      0.69       175
          B1       0.67      0.65      0.66       177
          B2       0.65      0.69      0.67        75
          C1       0.50      0.11      0.18         9

    accuracy                           0.66       453
   macro avg       0.57      0.50      0.51       453
weighted avg       0.66      0.66      0.65       453


Fold 2
[[  4  13   0   0   0]
 [  5 124  44   2   0]
 [  0  50 108  19   0]
 [  0   4  16  53   2]
 [  0   0   0   8   0]]
              precision    recall  f1-score   support

          A1       0.44      0.24      0.31        17
          A2       0.65      0.71      0.68       175
          B1       0.64      0.61      0.63       177
          B2       0.65      0.71      0.68        75
          C1       0.00      0.00      0.00         8

    accuracy                           0.64       452
   macro avg       0.48      0.45      0.46       452
weighted avg       0.63      0.64      0.63       452


Fold 3
[[  3  13   0   1   0]
 [  8 142  22   3   0]
 [  0  45 118  14   0]
 [  0   2  23  44   6]
 [  0   0   0   7   1]]
              precision    recall  f1-score   support

          A1       0.27      0.18      0.21        17
          A2       0.70      0.81      0.75       175
          B1       0.72      0.67      0.69       177
          B2       0.64      0.59      0.61        75
          C1       0.14      0.12      0.13         8

    accuracy                           0.68       452
   macro avg       0.50      0.47      0.48       452
weighted avg       0.67      0.68      0.68       452


Fold 4
[[  3  13   1   0   0]
 [  2 133  38   1   0]
 [  0  35 126  15   1]
 [  0   2  19  51   2]
 [  0   0   1   6   1]]
              precision    recall  f1-score   support

          A1       0.60      0.18      0.27        17
          A2       0.73      0.76      0.75       174
          B1       0.68      0.71      0.70       177
          B2       0.70      0.69      0.69        74
          C1       0.25      0.12      0.17         8

    accuracy                           0.70       450
   macro avg       0.59      0.49      0.51       450
weighted avg       0.69      0.70      0.69       450


K-fold scores
[0.6568953178645377, 0.6538082476874599, 0.6311157924401635, 0.6752918570775649, 0.689287031515547]
SKF f1 score mean 0.6612796493170545

SAME LANG EVAL DONE FOR THIS LANG
Doing: take all data as if it belongs to one large dataset, and do classification
Mega classification for:  dep  features
2260 2260 2260 1922
Distribution of labels: 
Counter({'B1': 885, 'A2': 874, 'B2': 374, 'A1': 85, 'C1': 42})
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13151
13151
Fold 0
[[  2  15   0   0   0]
 [  0 146  29   0   0]
 [  0  52 108  17   0]
 [  0   1  17  57   0]
 [  0   0   0   9   0]]
              precision    recall  f1-score   support

          A1       1.00      0.12      0.21        17
          A2       0.68      0.83      0.75       175
          B1       0.70      0.61      0.65       177
          B2       0.69      0.76      0.72        75
          C1       0.00      0.00      0.00         9

    accuracy                           0.69       453
   macro avg       0.61      0.46      0.47       453
weighted avg       0.69      0.69      0.67       453


Fold 1
[[  4  12   1   0   0]
 [  3 151  20   1   0]
 [  0  48 118  11   0]
 [  0   1  15  59   0]
 [  0   0   0   9   0]]
              precision    recall  f1-score   support

          A1       0.57      0.24      0.33        17
          A2       0.71      0.86      0.78       175
          B1       0.77      0.67      0.71       177
          B2       0.74      0.79      0.76        75
          C1       0.00      0.00      0.00         9

    accuracy                           0.73       453
   macro avg       0.56      0.51      0.52       453
weighted avg       0.72      0.73      0.72       453


Fold 2
[[  2  15   0   0   0]
 [  5 146  24   0   0]
 [  0  39 117  21   0]
 [  0   2  22  51   0]
 [  0   0   0   8   0]]
              precision    recall  f1-score   support

          A1       0.29      0.12      0.17        17
          A2       0.72      0.83      0.77       175
          B1       0.72      0.66      0.69       177
          B2       0.64      0.68      0.66        75
          C1       0.00      0.00      0.00         8

    accuracy                           0.70       452
   macro avg       0.47      0.46      0.46       452
weighted avg       0.68      0.70      0.68       452


Fold 3
[[  6  11   0   0   0]
 [  0 159  16   0   0]
 [  0  42 120  15   0]
 [  0   2  19  54   0]
 [  0   0   1   7   0]]
              precision    recall  f1-score   support

          A1       1.00      0.35      0.52        17
          A2       0.74      0.91      0.82       175
          B1       0.77      0.68      0.72       177
          B2       0.71      0.72      0.72        75
          C1       0.00      0.00      0.00         8

    accuracy                           0.75       452
   macro avg       0.64      0.53      0.56       452
weighted avg       0.74      0.75      0.74       452


Fold 4
[[  6  11   0   0   0]
 [  0 152  22   0   0]
 [  0  45 118  14   0]
 [  0   3  15  56   0]
 [  0   0   0   8   0]]
              precision    recall  f1-score   support

          A1       1.00      0.35      0.52        17
          A2       0.72      0.87      0.79       174
          B1       0.76      0.67      0.71       177
          B2       0.72      0.76      0.74        74
          C1       0.00      0.00      0.00         8

    accuracy                           0.74       450
   macro avg       0.64      0.53      0.55       450
weighted avg       0.73      0.74      0.73       450


K-fold scores
[0.6723175960428969, 0.7186007596559556, 0.6848442161063797, 0.7370323956967466, 0.7257941464611256]
SKF f1 score mean 0.7077178227926209

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13151
13151
Fold 0
[[  3  12   2   0   0]
 [ 13 120  40   2   0]
 [  3  38 121  15   0]
 [  0   5  26  42   2]
 [  0   0   2   7   0]]
              precision    recall  f1-score   support

          A1       0.16      0.18      0.17        17
          A2       0.69      0.69      0.69       175
          B1       0.63      0.68      0.66       177
          B2       0.64      0.56      0.60        75
          C1       0.00      0.00      0.00         9

    accuracy                           0.63       453
   macro avg       0.42      0.42      0.42       453
weighted avg       0.62      0.63      0.63       453


Fold 1
[[  3  11   3   0   0]
 [  9 123  38   5   0]
 [  0  56 101  20   0]
 [  0   1  19  54   1]
 [  0   0   0   9   0]]
              precision    recall  f1-score   support

          A1       0.25      0.18      0.21        17
          A2       0.64      0.70      0.67       175
          B1       0.63      0.57      0.60       177
          B2       0.61      0.72      0.66        75
          C1       0.00      0.00      0.00         9

    accuracy                           0.62       453
   macro avg       0.43      0.43      0.43       453
weighted avg       0.60      0.62      0.61       453


Fold 2
[[  7   9   1   0   0]
 [  7 128  39   0   1]
 [  0  58 107  12   0]
 [  0   6  21  47   1]
 [  0   0   1   6   1]]
              precision    recall  f1-score   support

          A1       0.50      0.41      0.45        17
          A2       0.64      0.73      0.68       175
          B1       0.63      0.60      0.62       177
          B2       0.72      0.63      0.67        75
          C1       0.33      0.12      0.18         8

    accuracy                           0.64       452
   macro avg       0.57      0.50      0.52       452
weighted avg       0.64      0.64      0.64       452


Fold 3
[[  2  14   1   0   0]
 [  7 142  26   0   0]
 [  0  54 105  18   0]
 [  0   4  22  48   1]
 [  0   0   2   6   0]]
              precision    recall  f1-score   support

          A1       0.22      0.12      0.15        17
          A2       0.66      0.81      0.73       175
          B1       0.67      0.59      0.63       177
          B2       0.67      0.64      0.65        75
          C1       0.00      0.00      0.00         8

    accuracy                           0.66       452
   macro avg       0.45      0.43      0.43       452
weighted avg       0.64      0.66      0.64       452


Fold 4
[[  3  14   0   0   0]
 [  3 130  40   1   0]
 [  0  36 126  15   0]
 [  0   5  30  37   2]
 [  0   0   1   7   0]]
              precision    recall  f1-score   support

          A1       0.50      0.18      0.26        17
          A2       0.70      0.75      0.72       174
          B1       0.64      0.71      0.67       177
          B2       0.62      0.50      0.55        74
          C1       0.00      0.00      0.00         8

    accuracy                           0.66       450
   macro avg       0.49      0.43      0.44       450
weighted avg       0.64      0.66      0.65       450


K-fold scores
[0.6267349305244987, 0.610628054793559, 0.6374160006107216, 0.6437612703931411, 0.6457315542843782]
SKF f1 score mean 0.6328543621212598

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13151
13151
Fold 0
[[  3  12   2   0   0]
 [  9 124  40   2   0]
 [  4  39 118  16   0]
 [  0   2  22  48   3]
 [  0   1   1   7   0]]
              precision    recall  f1-score   support

          A1       0.19      0.18      0.18        17
          A2       0.70      0.71      0.70       175
          B1       0.64      0.67      0.66       177
          B2       0.66      0.64      0.65        75
          C1       0.00      0.00      0.00         9

    accuracy                           0.65       453
   macro avg       0.44      0.44      0.44       453
weighted avg       0.64      0.65      0.64       453


Fold 1
[[  2  12   3   0   0]
 [  8 123  40   4   0]
 [  1  52 105  19   0]
 [  0   0  15  57   3]
 [  0   0   0   9   0]]
              precision    recall  f1-score   support

          A1       0.18      0.12      0.14        17
          A2       0.66      0.70      0.68       175
          B1       0.64      0.59      0.62       177
          B2       0.64      0.76      0.70        75
          C1       0.00      0.00      0.00         9

    accuracy                           0.63       453
   macro avg       0.42      0.43      0.43       453
weighted avg       0.62      0.63      0.62       453


Fold 2
[[  6   9   2   0   0]
 [  6 128  40   1   0]
 [  0  47 112  18   0]
 [  0   6  19  48   2]
 [  0   0   1   6   1]]
              precision    recall  f1-score   support

          A1       0.50      0.35      0.41        17
          A2       0.67      0.73      0.70       175
          B1       0.64      0.63      0.64       177
          B2       0.66      0.64      0.65        75
          C1       0.33      0.12      0.18         8

    accuracy                           0.65       452
   macro avg       0.56      0.50      0.52       452
weighted avg       0.65      0.65      0.65       452


Fold 3
[[  3  13   1   0   0]
 [  5 145  25   0   0]
 [  0  48 113  16   0]
 [  0   2  18  54   1]
 [  0   0   2   6   0]]
              precision    recall  f1-score   support

          A1       0.38      0.18      0.24        17
          A2       0.70      0.83      0.76       175
          B1       0.71      0.64      0.67       177
          B2       0.71      0.72      0.72        75
          C1       0.00      0.00      0.00         8

    accuracy                           0.70       452
   macro avg       0.50      0.47      0.48       452
weighted avg       0.68      0.70      0.68       452


Fold 4
[[  3  14   0   0   0]
 [  2 130  42   0   0]
 [  0  37 125  15   0]
 [  0   3  25  43   3]
 [  0   0   1   6   1]]
              precision    recall  f1-score   support

          A1       0.60      0.18      0.27        17
          A2       0.71      0.75      0.73       174
          B1       0.65      0.71      0.68       177
          B2       0.67      0.58      0.62        74
          C1       0.25      0.12      0.17         8

    accuracy                           0.67       450
   macro avg       0.58      0.47      0.49       450
weighted avg       0.67      0.67      0.66       450


K-fold scores
[0.6417639441728142, 0.6243022055529828, 0.6478643093948606, 0.6842532808621136, 0.6623309970601686]
SKF f1 score mean 0.652102947408588

SAME LANG EVAL DONE FOR THIS LANG
Multi lingual classification for dimension Grammaticalaccuracy
Doing: take all data as if it belongs to one large dataset, and do classification
Mega classification for:  pos  features
2260 2260 2260 633
Distribution of labels: 
Counter({'B1': 881, 'A2': 711, 'B2': 451, 'A1': 165, 'C1': 52})
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13231
13231
Fold 0
[[  5  25   3   0   0]
 [  5  92  41   5   0]
 [  1  58 101  17   0]
 [  0   3  36  52   0]
 [  0   0   1  10   0]]
              precision    recall  f1-score   support

          A1       0.45      0.15      0.23        33
          A2       0.52      0.64      0.57       143
          B1       0.55      0.57      0.56       177
          B2       0.62      0.57      0.59        91
          C1       0.00      0.00      0.00        11

    accuracy                           0.55       455
   macro avg       0.43      0.39      0.39       455
weighted avg       0.54      0.55      0.53       455


Fold 1
[[  8  24   1   0   0]
 [  4 100  37   1   0]
 [  0  50 106  20   0]
 [  0   2  36  52   0]
 [  0   0   0  11   0]]
              precision    recall  f1-score   support

          A1       0.67      0.24      0.36        33
          A2       0.57      0.70      0.63       142
          B1       0.59      0.60      0.60       176
          B2       0.62      0.58      0.60        90
          C1       0.00      0.00      0.00        11

    accuracy                           0.59       452
   macro avg       0.49      0.43      0.44       452
weighted avg       0.58      0.59      0.57       452


Fold 2
[[  9  21   3   0   0]
 [  2 113  27   0   0]
 [  1  53 101  21   0]
 [  0   5  35  50   0]
 [  0   0   0  10   0]]
              precision    recall  f1-score   support

          A1       0.75      0.27      0.40        33
          A2       0.59      0.80      0.68       142
          B1       0.61      0.57      0.59       176
          B2       0.62      0.56      0.58        90
          C1       0.00      0.00      0.00        10

    accuracy                           0.61       451
   macro avg       0.51      0.44      0.45       451
weighted avg       0.60      0.61      0.59       451


Fold 3
[[  5  27   1   0   0]
 [  6  92  44   0   0]
 [  4  43 113  16   0]
 [  0   2  39  49   0]
 [  0   0   2   8   0]]
              precision    recall  f1-score   support

          A1       0.33      0.15      0.21        33
          A2       0.56      0.65      0.60       142
          B1       0.57      0.64      0.60       176
          B2       0.67      0.54      0.60        90
          C1       0.00      0.00      0.00        10

    accuracy                           0.57       451
   macro avg       0.43      0.40      0.40       451
weighted avg       0.56      0.57      0.56       451


Fold 4
[[ 12  20   1   0   0]
 [  3  93  46   0   0]
 [  1  33 117  25   0]
 [  0   4  40  46   0]
 [  0   1   1   8   0]]
              precision    recall  f1-score   support

          A1       0.75      0.36      0.49        33
          A2       0.62      0.65      0.63       142
          B1       0.57      0.66      0.61       176
          B2       0.58      0.51      0.54        90
          C1       0.00      0.00      0.00        10

    accuracy                           0.59       451
   macro avg       0.50      0.44      0.46       451
weighted avg       0.59      0.59      0.58       451


K-fold scores
[0.5343783782752882, 0.5744327470050511, 0.5895091522327424, 0.5597347753796426, 0.5840247916745746]
SKF f1 score mean 0.5684159689134598

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13231
13231
Fold 0
[[ 12  15   6   0   0]
 [ 17  71  49   6   0]
 [  9  51 102  15   0]
 [  1   8  36  42   4]
 [  0   0   3   8   0]]
              precision    recall  f1-score   support

          A1       0.31      0.36      0.33        33
          A2       0.49      0.50      0.49       143
          B1       0.52      0.58      0.55       177
          B2       0.59      0.46      0.52        91
          C1       0.00      0.00      0.00        11

    accuracy                           0.50       455
   macro avg       0.38      0.38      0.38       455
weighted avg       0.50      0.50      0.50       455


Fold 1
[[12 14  7  0  0]
 [14 70 52  6  0]
 [ 9 50 97 20  0]
 [ 0  7 33 48  2]
 [ 0  2  3  6  0]]
              precision    recall  f1-score   support

          A1       0.34      0.36      0.35        33
          A2       0.49      0.49      0.49       142
          B1       0.51      0.55      0.53       176
          B2       0.60      0.53      0.56        90
          C1       0.00      0.00      0.00        11

    accuracy                           0.50       452
   macro avg       0.39      0.39      0.39       452
weighted avg       0.50      0.50      0.50       452


Fold 2
[[13 17  2  1  0]
 [ 9 87 45  1  0]
 [ 4 56 87 28  1]
 [ 0 10 38 39  3]
 [ 0  0  1  9  0]]
              precision    recall  f1-score   support

          A1       0.50      0.39      0.44        33
          A2       0.51      0.61      0.56       142
          B1       0.50      0.49      0.50       176
          B2       0.50      0.43      0.46        90
          C1       0.00      0.00      0.00        10

    accuracy                           0.50       451
   macro avg       0.40      0.39      0.39       451
weighted avg       0.49      0.50      0.50       451


Fold 3
[[ 8 17  8  0  0]
 [17 71 52  2  0]
 [ 3 57 85 31  0]
 [ 0 15 32 36  7]
 [ 0  1  3  5  1]]
              precision    recall  f1-score   support

          A1       0.29      0.24      0.26        33
          A2       0.44      0.50      0.47       142
          B1       0.47      0.48      0.48       176
          B2       0.49      0.40      0.44        90
          C1       0.12      0.10      0.11        10

    accuracy                           0.45       451
   macro avg       0.36      0.35      0.35       451
weighted avg       0.44      0.45      0.44       451


Fold 4
[[14 12  7  0  0]
 [12 72 53  5  0]
 [ 5 51 91 29  0]
 [ 0  9 38 42  1]
 [ 0  1  0  8  1]]
              precision    recall  f1-score   support

          A1       0.45      0.42      0.44        33
          A2       0.50      0.51      0.50       142
          B1       0.48      0.52      0.50       176
          B2       0.50      0.47      0.48        90
          C1       0.50      0.10      0.17        10

    accuracy                           0.49       451
   macro avg       0.49      0.40      0.42       451
weighted avg       0.49      0.49      0.48       451


K-fold scores
[0.4955965256568474, 0.49780438693774015, 0.4950515429318032, 0.44317481717003027, 0.48460916496209877]
SKF f1 score mean 0.48324728753170393

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13231
13231
Fold 0
[[ 11  15   7   0   0]
 [ 17  73  47   6   0]
 [  9  47 103  18   0]
 [  1   6  31  48   5]
 [  0   0   1  10   0]]
              precision    recall  f1-score   support

          A1       0.29      0.33      0.31        33
          A2       0.52      0.51      0.51       143
          B1       0.54      0.58      0.56       177
          B2       0.59      0.53      0.55        91
          C1       0.00      0.00      0.00        11

    accuracy                           0.52       455
   macro avg       0.39      0.39      0.39       455
weighted avg       0.51      0.52      0.51       455


Fold 1
[[12 16  5  0  0]
 [12 77 45  8  0]
 [ 7 48 93 28  0]
 [ 0  4 32 51  3]
 [ 0  2  3  6  0]]
              precision    recall  f1-score   support

          A1       0.39      0.36      0.38        33
          A2       0.52      0.54      0.53       142
          B1       0.52      0.53      0.53       176
          B2       0.55      0.57      0.56        90
          C1       0.00      0.00      0.00        11

    accuracy                           0.52       452
   macro avg       0.40      0.40      0.40       452
weighted avg       0.51      0.52      0.51       452


Fold 2
[[14 16  2  1  0]
 [11 91 38  2  0]
 [ 6 54 86 29  1]
 [ 0  9 36 42  3]
 [ 0  0  1  9  0]]
              precision    recall  f1-score   support

          A1       0.45      0.42      0.44        33
          A2       0.54      0.64      0.58       142
          B1       0.53      0.49      0.51       176
          B2       0.51      0.47      0.49        90
          C1       0.00      0.00      0.00        10

    accuracy                           0.52       451
   macro avg       0.40      0.40      0.40       451
weighted avg       0.51      0.52      0.51       451


Fold 3
[[ 7 17  8  1  0]
 [14 74 52  2  0]
 [ 3 52 88 33  0]
 [ 0 13 31 39  7]
 [ 0  0  1  7  2]]
              precision    recall  f1-score   support

          A1       0.29      0.21      0.25        33
          A2       0.47      0.52      0.50       142
          B1       0.49      0.50      0.49       176
          B2       0.48      0.43      0.45        90
          C1       0.22      0.20      0.21        10

    accuracy                           0.47       451
   macro avg       0.39      0.37      0.38       451
weighted avg       0.46      0.47      0.46       451


Fold 4
[[15 13  5  0  0]
 [14 72 54  2  0]
 [ 5 46 93 32  0]
 [ 0  5 36 47  2]
 [ 0  1  0  8  1]]
              precision    recall  f1-score   support

          A1       0.44      0.45      0.45        33
          A2       0.53      0.51      0.52       142
          B1       0.49      0.53      0.51       176
          B2       0.53      0.52      0.53        90
          C1       0.33      0.10      0.15        10

    accuracy                           0.51       451
   macro avg       0.46      0.42      0.43       451
weighted avg       0.50      0.51      0.50       451


K-fold scores
[0.513976917068088, 0.5103569264748232, 0.5105724842614103, 0.46243726317539613, 0.5028858964269621]
SKF f1 score mean 0.500045897481336

SAME LANG EVAL DONE FOR THIS LANG
Doing: take all data as if it belongs to one large dataset, and do classification
Mega classification for:  dep  features
2260 2260 2260 1922
Distribution of labels: 
Counter({'B1': 881, 'A2': 711, 'B2': 451, 'A1': 165, 'C1': 52})
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13151
13151
Fold 0
[[ 5 27  1  0  0]
 [ 8 91 42  2  0]
 [ 2 62 98 15  0]
 [ 0  3 38 50  0]
 [ 0  0  0 11  0]]
              precision    recall  f1-score   support

          A1       0.33      0.15      0.21        33
          A2       0.50      0.64      0.56       143
          B1       0.55      0.55      0.55       177
          B2       0.64      0.55      0.59        91
          C1       0.00      0.00      0.00        11

    accuracy                           0.54       455
   macro avg       0.40      0.38      0.38       455
weighted avg       0.52      0.54      0.52       455


Fold 1
[[  9  23   1   0   0]
 [  6  95  38   3   0]
 [  0  51 109  16   0]
 [  0   2  41  47   0]
 [  0   0   1  10   0]]
              precision    recall  f1-score   support

          A1       0.60      0.27      0.37        33
          A2       0.56      0.67      0.61       142
          B1       0.57      0.62      0.60       176
          B2       0.62      0.52      0.57        90
          C1       0.00      0.00      0.00        11

    accuracy                           0.58       452
   macro avg       0.47      0.42      0.43       452
weighted avg       0.56      0.58      0.56       452


Fold 2
[[ 10  21   2   0   0]
 [  5 114  23   0   0]
 [  2  59  99  16   0]
 [  0   4  41  45   0]
 [  0   0   1   9   0]]
              precision    recall  f1-score   support

          A1       0.59      0.30      0.40        33
          A2       0.58      0.80      0.67       142
          B1       0.60      0.56      0.58       176
          B2       0.64      0.50      0.56        90
          C1       0.00      0.00      0.00        10

    accuracy                           0.59       451
   macro avg       0.48      0.43      0.44       451
weighted avg       0.59      0.59      0.58       451


Fold 3
[[  6  25   2   0   0]
 [  6  93  43   0   0]
 [  2  49 110  15   0]
 [  0   2  39  49   0]
 [  0   0   2   8   0]]
              precision    recall  f1-score   support

          A1       0.43      0.18      0.26        33
          A2       0.55      0.65      0.60       142
          B1       0.56      0.62      0.59       176
          B2       0.68      0.54      0.60        90
          C1       0.00      0.00      0.00        10

    accuracy                           0.57       451
   macro avg       0.44      0.40      0.41       451
weighted avg       0.56      0.57      0.56       451


Fold 4
[[  7  25   1   0   0]
 [  3  94  44   1   0]
 [  1  37 114  24   0]
 [  0   5  46  39   0]
 [  0   1   0   9   0]]
              precision    recall  f1-score   support

          A1       0.64      0.21      0.32        33
          A2       0.58      0.66      0.62       142
          B1       0.56      0.65      0.60       176
          B2       0.53      0.43      0.48        90
          C1       0.00      0.00      0.00        10

    accuracy                           0.56       451
   macro avg       0.46      0.39      0.40       451
weighted avg       0.55      0.56      0.55       451


K-fold scores
[0.5230877975310372, 0.562760043322291, 0.5785881735119068, 0.5584967691370101, 0.5470201968557212]
SKF f1 score mean 0.5539905960715933

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13151
13151
Fold 0
[[  8  21   3   1   0]
 [ 14  72  50   7   0]
 [  6  47 101  23   0]
 [  0   9  46  35   1]
 [  0   2   2   6   1]]
              precision    recall  f1-score   support

          A1       0.29      0.24      0.26        33
          A2       0.48      0.50      0.49       143
          B1       0.50      0.57      0.53       177
          B2       0.49      0.38      0.43        91
          C1       0.50      0.09      0.15        11

    accuracy                           0.48       455
   macro avg       0.45      0.36      0.37       455
weighted avg       0.47      0.48      0.47       455


Fold 1
[[13 19  1  0  0]
 [17 78 41  4  2]
 [ 1 57 93 25  0]
 [ 0 14 41 34  1]
 [ 0  1  4  6  0]]
              precision    recall  f1-score   support

          A1       0.42      0.39      0.41        33
          A2       0.46      0.55      0.50       142
          B1       0.52      0.53      0.52       176
          B2       0.49      0.38      0.43        90
          C1       0.00      0.00      0.00        11

    accuracy                           0.48       452
   macro avg       0.38      0.37      0.37       452
weighted avg       0.47      0.48      0.48       452


Fold 2
[[ 9 18  6  0  0]
 [12 78 49  3  0]
 [ 7 56 86 27  0]
 [ 0 13 40 35  2]
 [ 0  0  1  8  1]]
              precision    recall  f1-score   support

          A1       0.32      0.27      0.30        33
          A2       0.47      0.55      0.51       142
          B1       0.47      0.49      0.48       176
          B2       0.48      0.39      0.43        90
          C1       0.33      0.10      0.15        10

    accuracy                           0.46       451
   macro avg       0.42      0.36      0.37       451
weighted avg       0.46      0.46      0.46       451


Fold 3
[[ 6 19  8  0  0]
 [ 8 80 50  4  0]
 [ 4 62 80 29  1]
 [ 0  8 38 42  2]
 [ 0  0  1  9  0]]
              precision    recall  f1-score   support

          A1       0.33      0.18      0.24        33
          A2       0.47      0.56      0.51       142
          B1       0.45      0.45      0.45       176
          B2       0.50      0.47      0.48        90
          C1       0.00      0.00      0.00        10

    accuracy                           0.46       451
   macro avg       0.35      0.33      0.34       451
weighted avg       0.45      0.46      0.45       451


Fold 4
[[ 6 22  5  0  0]
 [15 72 50  5  0]
 [ 7 43 93 33  0]
 [ 1  7 34 44  4]
 [ 0  0  2  8  0]]
              precision    recall  f1-score   support

          A1       0.21      0.18      0.19        33
          A2       0.50      0.51      0.50       142
          B1       0.51      0.53      0.52       176
          B2       0.49      0.49      0.49        90
          C1       0.00      0.00      0.00        10

    accuracy                           0.48       451
   macro avg       0.34      0.34      0.34       451
weighted avg       0.47      0.48      0.47       451


K-fold scores
[0.4699040597707599, 0.4758410797288647, 0.4581851091227686, 0.4524191025025698, 0.47187790156104303]
SKF f1 score mean 0.4656454505372013

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13151
13151
Fold 0
[[ 10  20   3   0   0]
 [ 16  70  50   7   0]
 [  7  45 103  22   0]
 [  0   8  47  36   0]
 [  0   2   1   6   2]]
              precision    recall  f1-score   support

          A1       0.30      0.30      0.30        33
          A2       0.48      0.49      0.49       143
          B1       0.50      0.58      0.54       177
          B2       0.51      0.40      0.44        91
          C1       1.00      0.18      0.31        11

    accuracy                           0.49       455
   macro avg       0.56      0.39      0.42       455
weighted avg       0.50      0.49      0.48       455


Fold 1
[[12 21  0  0  0]
 [16 83 39  3  1]
 [ 1 60 91 24  0]
 [ 0  8 42 38  2]
 [ 0  1  3  7  0]]
              precision    recall  f1-score   support

          A1       0.41      0.36      0.39        33
          A2       0.48      0.58      0.53       142
          B1       0.52      0.52      0.52       176
          B2       0.53      0.42      0.47        90
          C1       0.00      0.00      0.00        11

    accuracy                           0.50       452
   macro avg       0.39      0.38      0.38       452
weighted avg       0.49      0.50      0.49       452


Fold 2
[[10 18  5  0  0]
 [16 81 41  4  0]
 [ 4 56 86 30  0]
 [ 0 13 35 40  2]
 [ 0  0  0  9  1]]
              precision    recall  f1-score   support

          A1       0.33      0.30      0.32        33
          A2       0.48      0.57      0.52       142
          B1       0.51      0.49      0.50       176
          B2       0.48      0.44      0.46        90
          C1       0.33      0.10      0.15        10

    accuracy                           0.48       451
   macro avg       0.43      0.38      0.39       451
weighted avg       0.48      0.48      0.48       451


Fold 3
[[ 6 20  7  0  0]
 [ 8 84 46  4  0]
 [ 5 53 85 32  1]
 [ 0  9 33 45  3]
 [ 0  0  1  9  0]]
              precision    recall  f1-score   support

          A1       0.32      0.18      0.23        33
          A2       0.51      0.59      0.55       142
          B1       0.49      0.48      0.49       176
          B2       0.50      0.50      0.50        90
          C1       0.00      0.00      0.00        10

    accuracy                           0.49       451
   macro avg       0.36      0.35      0.35       451
weighted avg       0.48      0.49      0.48       451


Fold 4
[[ 7 21  5  0  0]
 [16 73 48  5  0]
 [ 6 39 97 34  0]
 [ 1  7 33 45  4]
 [ 0  0  2  8  0]]
              precision    recall  f1-score   support

          A1       0.23      0.21      0.22        33
          A2       0.52      0.51      0.52       142
          B1       0.52      0.55      0.54       176
          B2       0.49      0.50      0.49        90
          C1       0.00      0.00      0.00        10

    accuracy                           0.49       451
   macro avg       0.35      0.36      0.35       451
weighted avg       0.48      0.49      0.49       451


K-fold scores
[0.48141480465829345, 0.489131462525696, 0.4791489146433386, 0.4790397817387648, 0.4876680172656436]
SKF f1 score mean 0.4832805961663473

SAME LANG EVAL DONE FOR THIS LANG
Multi lingual classification for dimension Orthography
Doing: take all data as if it belongs to one large dataset, and do classification
Mega classification for:  pos  features
2260 2260 2260 633
Distribution of labels: 
Counter({'B1': 810, 'B2': 529, 'C1': 428, 'A2': 339, 'C2': 114, 'A1': 40})
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13231
13231
Fold 0
[[  0   6   1   0   1   0]
 [  1  26  41   0   0   0]
 [  0  11 139   8   4   0]
 [  0   2  41  44  19   0]
 [  0   1   9  29  47   0]
 [  0   0   7   7   9   0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.57      0.38      0.46        68
          B1       0.58      0.86      0.70       162
          B2       0.50      0.42      0.45       106
          C1       0.59      0.55      0.57        86
          C2       0.00      0.00      0.00        23

    accuracy                           0.57       453
   macro avg       0.37      0.37      0.36       453
weighted avg       0.52      0.57      0.53       453


Fold 1
[[  0   7   1   0   0   0]
 [  0  26  41   1   0   0]
 [  0  24 119  13   6   0]
 [  0   5  29  53  19   0]
 [  0   0  12  21  53   0]
 [  0   0   4   5  14   0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.42      0.38      0.40        68
          B1       0.58      0.73      0.65       162
          B2       0.57      0.50      0.53       106
          C1       0.58      0.62      0.60        86
          C2       0.00      0.00      0.00        23

    accuracy                           0.55       453
   macro avg       0.36      0.37      0.36       453
weighted avg       0.51      0.55      0.53       453


Fold 2
[[  0   6   2   0   0   0]
 [  0  21  46   0   1   0]
 [  0  16 130  12   4   0]
 [  0   6  44  42  14   0]
 [  0   1  12  31  42   0]
 [  0   0   4   6  13   0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.42      0.31      0.36        68
          B1       0.55      0.80      0.65       162
          B2       0.46      0.40      0.43       106
          C1       0.57      0.49      0.53        86
          C2       0.00      0.00      0.00        23

    accuracy                           0.52       453
   macro avg       0.33      0.33      0.33       453
weighted avg       0.47      0.52      0.49       453


Fold 3
[[  0   5   3   0   0   0]
 [  1  27  36   2   2   0]
 [  0  20 123  16   3   0]
 [  0   4  37  52  13   0]
 [  0   0   5  25  55   0]
 [  0   0   6   6  11   0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.48      0.40      0.44        68
          B1       0.59      0.76      0.66       162
          B2       0.51      0.49      0.50       106
          C1       0.65      0.65      0.65        85
          C2       0.00      0.00      0.00        23

    accuracy                           0.57       452
   macro avg       0.37      0.38      0.38       452
weighted avg       0.53      0.57      0.54       452


Fold 4
[[  0   5   3   0   0   0]
 [  0  24  43   0   0   0]
 [  0  12 126  18   6   0]
 [  0   3  38  54  10   0]
 [  0   0  14  32  39   0]
 [  0   0   2   6  14   0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.55      0.36      0.43        67
          B1       0.56      0.78      0.65       162
          B2       0.49      0.51      0.50       105
          C1       0.57      0.46      0.51        85
          C2       0.00      0.00      0.00        22

    accuracy                           0.54       449
   macro avg       0.36      0.35      0.35       449
weighted avg       0.50      0.54      0.51       449


K-fold scores
[0.5306596319363712, 0.5290232534973315, 0.4853230891250678, 0.5427509245977792, 0.512217376206667]
SKF f1 score mean 0.5199948550726433

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13231
13231
Fold 0
[[  2   5   1   0   0   0]
 [  9  18  31   7   1   2]
 [  3  29 107  21   2   0]
 [  0   7  42  39  17   1]
 [  1   5  12  23  38   7]
 [  0   4   3   4  11   1]]
              precision    recall  f1-score   support

          A1       0.13      0.25      0.17         8
          A2       0.26      0.26      0.26        68
          B1       0.55      0.66      0.60       162
          B2       0.41      0.37      0.39       106
          C1       0.55      0.44      0.49        86
          C2       0.09      0.04      0.06        23

    accuracy                           0.45       453
   macro avg       0.33      0.34      0.33       453
weighted avg       0.44      0.45      0.44       453


Fold 1
[[ 1  5  2  0  0  0]
 [ 2 30 30  5  1  0]
 [ 0 36 87 30  7  2]
 [ 0  5 29 41 27  4]
 [ 0  0  9 20 51  6]
 [ 0  0  5  4 10  4]]
              precision    recall  f1-score   support

          A1       0.33      0.12      0.18         8
          A2       0.39      0.44      0.42        68
          B1       0.54      0.54      0.54       162
          B2       0.41      0.39      0.40       106
          C1       0.53      0.59      0.56        86
          C2       0.25      0.17      0.21        23

    accuracy                           0.47       453
   macro avg       0.41      0.38      0.38       453
weighted avg       0.47      0.47      0.47       453


Fold 2
[[ 0  5  2  1  0  0]
 [ 0 25 33  7  2  1]
 [ 0 36 87 30  6  3]
 [ 0  9 38 29 26  4]
 [ 0  3  9 20 48  6]
 [ 0  0  1  3 16  3]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.32      0.37      0.34        68
          B1       0.51      0.54      0.52       162
          B2       0.32      0.27      0.30       106
          C1       0.49      0.56      0.52        86
          C2       0.18      0.13      0.15        23

    accuracy                           0.42       453
   macro avg       0.30      0.31      0.31       453
weighted avg       0.41      0.42      0.41       453


Fold 3
[[ 1  2  1  3  1  0]
 [ 5 23 32  8  0  0]
 [ 1 30 91 36  2  2]
 [ 0  7 33 46 19  1]
 [ 0  0 10 26 44  5]
 [ 0  1  2  2 16  2]]
              precision    recall  f1-score   support

          A1       0.14      0.12      0.13         8
          A2       0.37      0.34      0.35        68
          B1       0.54      0.56      0.55       162
          B2       0.38      0.43      0.41       106
          C1       0.54      0.52      0.53        85
          C2       0.20      0.09      0.12        23

    accuracy                           0.46       452
   macro avg       0.36      0.34      0.35       452
weighted avg       0.45      0.46      0.45       452


Fold 4
[[ 1  2  3  0  1  1]
 [ 2 16 48  0  0  1]
 [ 1 37 95 20  6  3]
 [ 0  4 41 36 22  2]
 [ 0  2 10 26 39  8]
 [ 0  1  1  5 15  0]]
              precision    recall  f1-score   support

          A1       0.25      0.12      0.17         8
          A2       0.26      0.24      0.25        67
          B1       0.48      0.59      0.53       162
          B2       0.41      0.34      0.38       105
          C1       0.47      0.46      0.46        85
          C2       0.00      0.00      0.00        22

    accuracy                           0.42       449
   macro avg       0.31      0.29      0.30       449
weighted avg       0.40      0.42      0.41       449


K-fold scores
[0.4439072324207421, 0.4677655727613935, 0.41474215860237273, 0.45256351401649514, 0.40599726968013083]
SKF f1 score mean 0.43699514949622686

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13231
13231
Fold 0
[[  2   5   1   0   0   0]
 [  7  19  33   6   1   2]
 [  3  26 112  19   1   1]
 [  0   5  40  41  18   2]
 [  0   3  12  22  45   4]
 [  0   3   3   4  12   1]]
              precision    recall  f1-score   support

          A1       0.17      0.25      0.20         8
          A2       0.31      0.28      0.29        68
          B1       0.56      0.69      0.62       162
          B2       0.45      0.39      0.41       106
          C1       0.58      0.52      0.55        86
          C2       0.10      0.04      0.06        23

    accuracy                           0.49       453
   macro avg       0.36      0.36      0.36       453
weighted avg       0.47      0.49      0.47       453


Fold 1
[[ 1  5  2  0  0  0]
 [ 2 29 32  4  1  0]
 [ 0 33 94 26  6  3]
 [ 0  5 26 42 26  7]
 [ 0  0  5 21 54  6]
 [ 0  0  4  4 11  4]]
              precision    recall  f1-score   support

          A1       0.33      0.12      0.18         8
          A2       0.40      0.43      0.41        68
          B1       0.58      0.58      0.58       162
          B2       0.43      0.40      0.41       106
          C1       0.55      0.63      0.59        86
          C2       0.20      0.17      0.19        23

    accuracy                           0.49       453
   macro avg       0.42      0.39      0.39       453
weighted avg       0.49      0.49      0.49       453


Fold 2
[[ 0  5  2  1  0  0]
 [ 1 24 36  4  2  1]
 [ 0 36 99 21  4  2]
 [ 0  7 36 30 28  5]
 [ 0  4  7 19 51  5]
 [ 0  0  1  3 16  3]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.32      0.35      0.33        68
          B1       0.55      0.61      0.58       162
          B2       0.38      0.28      0.33       106
          C1       0.50      0.59      0.55        86
          C2       0.19      0.13      0.15        23

    accuracy                           0.46       453
   macro avg       0.32      0.33      0.32       453
weighted avg       0.44      0.46      0.44       453


Fold 3
[[ 1  2  2  2  1  0]
 [ 3 24 34  6  1  0]
 [ 0 28 95 36  3  0]
 [ 0  5 36 47 17  1]
 [ 0  0  8 24 48  5]
 [ 0  1  2  2 17  1]]
              precision    recall  f1-score   support

          A1       0.25      0.12      0.17         8
          A2       0.40      0.35      0.38        68
          B1       0.54      0.59      0.56       162
          B2       0.40      0.44      0.42       106
          C1       0.55      0.56      0.56        85
          C2       0.14      0.04      0.07        23

    accuracy                           0.48       452
   macro avg       0.38      0.35      0.36       452
weighted avg       0.46      0.48      0.47       452


Fold 4
[[ 0  6  1  0  1  0]
 [ 1 16 49  0  0  1]
 [ 0 30 96 28  6  2]
 [ 0  2 43 39 19  2]
 [ 0  2  8 25 43  7]
 [ 0  1  1  5 15  0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.28      0.24      0.26        67
          B1       0.48      0.59      0.53       162
          B2       0.40      0.37      0.39       105
          C1       0.51      0.51      0.51        85
          C2       0.00      0.00      0.00        22

    accuracy                           0.43       449
   macro avg       0.28      0.28      0.28       449
weighted avg       0.41      0.43      0.42       449


K-fold scores
[0.4732351915459486, 0.48996941028535884, 0.444140113666548, 0.4674482337578538, 0.417570857251477]
SKF f1 score mean 0.45847276130143727

SAME LANG EVAL DONE FOR THIS LANG
Doing: take all data as if it belongs to one large dataset, and do classification
Mega classification for:  dep  features
2260 2260 2260 1922
Distribution of labels: 
Counter({'B1': 810, 'B2': 529, 'C1': 428, 'A2': 339, 'C2': 114, 'A1': 40})
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13151
13151
Fold 0
[[  0   6   2   0   0   0]
 [  1  26  40   0   1   0]
 [  0  17 132   8   5   0]
 [  0   2  47  39  18   0]
 [  0   1   9  31  45   0]
 [  0   0   6   9   8   0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.50      0.38      0.43        68
          B1       0.56      0.81      0.66       162
          B2       0.45      0.37      0.40       106
          C1       0.58      0.52      0.55        86
          C2       0.00      0.00      0.00        23

    accuracy                           0.53       453
   macro avg       0.35      0.35      0.34       453
weighted avg       0.49      0.53      0.50       453


Fold 1
[[  0   7   1   0   0   0]
 [  0  27  40   1   0   0]
 [  0  21 125   9   7   0]
 [  0   4  37  43  22   0]
 [  0   0   9  20  57   0]
 [  0   0   5   5  13   0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.46      0.40      0.43        68
          B1       0.58      0.77      0.66       162
          B2       0.55      0.41      0.47       106
          C1       0.58      0.66      0.62        86
          C2       0.00      0.00      0.00        23

    accuracy                           0.56       453
   macro avg       0.36      0.37      0.36       453
weighted avg       0.51      0.56      0.53       453


Fold 2
[[  0   5   3   0   0   0]
 [  0  23  43   0   2   0]
 [  0  15 131  12   4   0]
 [  0   5  50  40  11   0]
 [  0   1  13  31  41   0]
 [  0   0   4   6  13   0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.47      0.34      0.39        68
          B1       0.54      0.81      0.65       162
          B2       0.45      0.38      0.41       106
          C1       0.58      0.48      0.52        86
          C2       0.00      0.00      0.00        23

    accuracy                           0.52       453
   macro avg       0.34      0.33      0.33       453
weighted avg       0.48      0.52      0.48       453


Fold 3
[[  0   4   4   0   0   0]
 [  1  29  34   0   4   0]
 [  0  20 122  14   6   0]
 [  0   5  37  54  10   0]
 [  0   0   6  26  53   0]
 [  0   0   4   7  12   0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.50      0.43      0.46        68
          B1       0.59      0.75      0.66       162
          B2       0.53      0.51      0.52       106
          C1       0.62      0.62      0.62        85
          C2       0.00      0.00      0.00        23

    accuracy                           0.57       452
   macro avg       0.37      0.39      0.38       452
weighted avg       0.53      0.57      0.55       452


Fold 4
[[  0   4   4   0   0   0]
 [  0  26  41   0   0   0]
 [  0  11 133  13   5   0]
 [  0   3  43  49  10   0]
 [  0   0  12  28  45   0]
 [  0   0   2   5  15   0]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.59      0.39      0.47        67
          B1       0.57      0.82      0.67       162
          B2       0.52      0.47      0.49       105
          C1       0.60      0.53      0.56        85
          C2       0.00      0.00      0.00        22

    accuracy                           0.56       449
   macro avg       0.38      0.37      0.37       449
weighted avg       0.53      0.56      0.53       449


K-fold scores
[0.5016512006928401, 0.5260742096988553, 0.4849476635665205, 0.5458581556353682, 0.5327259866189786]
SKF f1 score mean 0.5182514432425125

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13151
13151
Fold 0
[[  2   0   4   1   1   0]
 [  2  30  30   3   1   2]
 [  1  33 102  23   3   0]
 [  0   6  39  42  17   2]
 [  1   1   8  29  40   7]
 [  0   3   2   5  12   1]]
              precision    recall  f1-score   support

          A1       0.33      0.25      0.29         8
          A2       0.41      0.44      0.43        68
          B1       0.55      0.63      0.59       162
          B2       0.41      0.40      0.40       106
          C1       0.54      0.47      0.50        86
          C2       0.08      0.04      0.06        23

    accuracy                           0.48       453
   macro avg       0.39      0.37      0.38       453
weighted avg       0.47      0.48      0.47       453


Fold 1
[[ 1  4  3  0  0  0]
 [ 3 29 29  6  1  0]
 [ 1 40 87 24  7  3]
 [ 0  7 30 38 26  5]
 [ 0  3  9 20 50  4]
 [ 1  0  4  6  9  3]]
              precision    recall  f1-score   support

          A1       0.17      0.12      0.14         8
          A2       0.35      0.43      0.38        68
          B1       0.54      0.54      0.54       162
          B2       0.40      0.36      0.38       106
          C1       0.54      0.58      0.56        86
          C2       0.20      0.13      0.16        23

    accuracy                           0.46       453
   macro avg       0.37      0.36      0.36       453
weighted avg       0.45      0.46      0.46       453


Fold 2
[[ 0  4  2  1  1  0]
 [ 1 28 34  4  1  0]
 [ 0 30 97 26  9  0]
 [ 0 14 23 48 20  1]
 [ 0  5  9 23 47  2]
 [ 0  0  2  5 14  2]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.35      0.41      0.38        68
          B1       0.58      0.60      0.59       162
          B2       0.45      0.45      0.45       106
          C1       0.51      0.55      0.53        86
          C2       0.40      0.09      0.14        23

    accuracy                           0.49       453
   macro avg       0.38      0.35      0.35       453
weighted avg       0.48      0.49      0.48       453


Fold 3
[[  0   4   2   1   1   0]
 [  4  19  41   2   1   1]
 [  4  24 110  20   4   0]
 [  0   7  48  28  21   2]
 [  0   0  16  18  44   7]
 [  0   1   2   7  11   2]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.35      0.28      0.31        68
          B1       0.50      0.68      0.58       162
          B2       0.37      0.26      0.31       106
          C1       0.54      0.52      0.53        85
          C2       0.17      0.09      0.11        23

    accuracy                           0.45       452
   macro avg       0.32      0.30      0.31       452
weighted avg       0.43      0.45      0.43       452


Fold 4
[[ 1  4  2  1  0  0]
 [ 2 28 33  3  1  0]
 [ 0 29 99 26  7  1]
 [ 0  4 38 43 18  2]
 [ 0  5 15 17 42  6]
 [ 0  3  0  7 10  2]]
              precision    recall  f1-score   support

          A1       0.33      0.12      0.18         8
          A2       0.38      0.42      0.40        67
          B1       0.53      0.61      0.57       162
          B2       0.44      0.41      0.43       105
          C1       0.54      0.49      0.52        85
          C2       0.18      0.09      0.12        22

    accuracy                           0.48       449
   macro avg       0.40      0.36      0.37       449
weighted avg       0.47      0.48      0.47       449


K-fold scores
[0.4710336293813169, 0.45522811065269086, 0.4802626387555056, 0.4304995848972294, 0.47068189945528793]
SKF f1 score mean 0.4615411726284061

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13151
13151
Fold 0
[[  2   0   4   1   1   0]
 [  2  31  30   3   2   0]
 [  0  32 107  22   1   0]
 [  0   4  39  44  17   2]
 [  1   2   8  25  43   7]
 [  0   3   2   6  11   1]]
              precision    recall  f1-score   support

          A1       0.40      0.25      0.31         8
          A2       0.43      0.46      0.44        68
          B1       0.56      0.66      0.61       162
          B2       0.44      0.42      0.43       106
          C1       0.57      0.50      0.53        86
          C2       0.10      0.04      0.06        23

    accuracy                           0.50       453
   macro avg       0.42      0.39      0.40       453
weighted avg       0.49      0.50      0.49       453


Fold 1
[[ 1  6  1  0  0  0]
 [ 3 29 29  5  2  0]
 [ 1 37 89 27  8  0]
 [ 0  6 28 40 27  5]
 [ 0  1  7 20 53  5]
 [ 1  0  3  4 11  4]]
              precision    recall  f1-score   support

          A1       0.17      0.12      0.14         8
          A2       0.37      0.43      0.39        68
          B1       0.57      0.55      0.56       162
          B2       0.42      0.38      0.40       106
          C1       0.52      0.62      0.57        86
          C2       0.29      0.17      0.22        23

    accuracy                           0.48       453
   macro avg       0.39      0.38      0.38       453
weighted avg       0.47      0.48      0.47       453


Fold 2
[[  0   4   2   1   1   0]
 [  0  28  37   1   2   0]
 [  0  23 105  26   8   0]
 [  0  12  23  48  22   1]
 [  0   2   9  18  52   5]
 [  0   0   1   5  15   2]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.41      0.41      0.41        68
          B1       0.59      0.65      0.62       162
          B2       0.48      0.45      0.47       106
          C1       0.52      0.60      0.56        86
          C2       0.25      0.09      0.13        23

    accuracy                           0.52       453
   macro avg       0.38      0.37      0.36       453
weighted avg       0.50      0.52      0.51       453


Fold 3
[[  0   4   2   1   1   0]
 [  3  20  40   1   2   2]
 [  3  28 104  22   5   0]
 [  0   6  44  32  21   3]
 [  0   0  10  19  50   6]
 [  0   1   2   7  11   2]]
              precision    recall  f1-score   support

          A1       0.00      0.00      0.00         8
          A2       0.34      0.29      0.31        68
          B1       0.51      0.64      0.57       162
          B2       0.39      0.30      0.34       106
          C1       0.56      0.59      0.57        85
          C2       0.15      0.09      0.11        23

    accuracy                           0.46       452
   macro avg       0.33      0.32      0.32       452
weighted avg       0.44      0.46      0.45       452


Fold 4
[[  1   4   1   1   1   0]
 [  1  28  32   5   1   0]
 [  0  28 103  23   7   1]
 [  0   4  36  45  17   3]
 [  0   3  11  17  48   6]
 [  0   2   0   6  12   2]]
              precision    recall  f1-score   support

          A1       0.50      0.12      0.20         8
          A2       0.41      0.42      0.41        67
          B1       0.56      0.64      0.60       162
          B2       0.46      0.43      0.45       105
          C1       0.56      0.56      0.56        85
          C2       0.17      0.09      0.12        22

    accuracy                           0.51       449
   macro avg       0.44      0.38      0.39       449
weighted avg       0.49      0.51      0.50       449


K-fold scores
[0.4932871745353749, 0.4725598400354179, 0.5051708401102544, 0.4451346060089316, 0.49667791054405297]
SKF f1 score mean 0.4825660742468063

SAME LANG EVAL DONE FOR THIS LANG
Multi lingual classification for dimension Vocabularyrange
Doing: take all data as if it belongs to one large dataset, and do classification
Mega classification for:  pos  features
2260 2260 2260 633
Distribution of labels: 
Counter({'B1': 818, 'B2': 602, 'A2': 588, 'C1': 150, 'A1': 97, 'C2': 5})
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13231
13231
Fold 0
[[  1  16   3   0   0   0]
 [  2  93  23   0   0   0]
 [  2  31 115  16   0   0]
 [  0   2  21  89   9   0]
 [  0   0   0  15  15   0]
 [  0   0   0   0   1   0]]
              precision    recall  f1-score   support

          A1       0.20      0.05      0.08        20
          A2       0.65      0.79      0.72       118
          B1       0.71      0.70      0.71       164
          B2       0.74      0.74      0.74       121
          C1       0.60      0.50      0.55        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.69       454
   macro avg       0.48      0.46      0.46       454
weighted avg       0.67      0.69      0.68       454


Fold 1
[[  4  14   2   0   0   0]
 [  3  90  25   0   0   0]
 [  0  35 106  23   0   0]
 [  0   1  14 101   5   0]
 [  0   0   3  14  13   0]
 [  0   0   0   0   1   0]]
              precision    recall  f1-score   support

          A1       0.57      0.20      0.30        20
          A2       0.64      0.76      0.70       118
          B1       0.71      0.65      0.68       164
          B2       0.73      0.83      0.78       121
          C1       0.68      0.43      0.53        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.69       454
   macro avg       0.56      0.48      0.50       454
weighted avg       0.69      0.69      0.68       454


Fold 2
[[  5  13   1   0   0   0]
 [  0  81  37   0   0   0]
 [  1  21 114  28   0   0]
 [  0   0  19  92   9   0]
 [  0   0   0  13  17   0]
 [  0   0   0   0   1   0]]
              precision    recall  f1-score   support

          A1       0.83      0.26      0.40        19
          A2       0.70      0.69      0.70       118
          B1       0.67      0.70      0.68       164
          B2       0.69      0.77      0.73       120
          C1       0.63      0.57      0.60        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.68       452
   macro avg       0.59      0.50      0.52       452
weighted avg       0.69      0.68      0.68       452


Fold 3
[[  4  13   2   0   0   0]
 [  1  84  32   0   0   0]
 [  2  26 113  22   0   0]
 [  0   0  18  98   4   0]
 [  0   0   1  12  17   0]
 [  0   0   0   1   0   0]]
              precision    recall  f1-score   support

          A1       0.57      0.21      0.31        19
          A2       0.68      0.72      0.70       117
          B1       0.68      0.69      0.69       163
          B2       0.74      0.82      0.77       120
          C1       0.81      0.57      0.67        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.70       450
   macro avg       0.58      0.50      0.52       450
weighted avg       0.70      0.70      0.69       450


Fold 4
[[  4  15   0   0   0   0]
 [  0  85  32   0   0   0]
 [  1  32 109  20   1   0]
 [  0   1  19  95   5   0]
 [  0   0   0  16  14   0]
 [  0   0   0   0   1   0]]
              precision    recall  f1-score   support

          A1       0.80      0.21      0.33        19
          A2       0.64      0.73      0.68       117
          B1       0.68      0.67      0.67       163
          B2       0.73      0.79      0.76       120
          C1       0.67      0.47      0.55        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.68       450
   macro avg       0.59      0.48      0.50       450
weighted avg       0.68      0.68      0.67       450


K-fold scores
[0.6772110948396977, 0.681204065393285, 0.6779387013662153, 0.694844856859658, 0.6738065752355789]
SKF f1 score mean 0.681001058738887

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13231
13231
Fold 0
[[ 3 14  2  1  0  0]
 [ 7 75 32  3  1  0]
 [ 5 51 91 17  0  0]
 [ 0  5 27 79 10  0]
 [ 0  1  1 13 15  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.20      0.15      0.17        20
          A2       0.51      0.64      0.57       118
          B1       0.59      0.55      0.57       164
          B2       0.70      0.65      0.68       121
          C1       0.56      0.50      0.53        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.58       454
   macro avg       0.43      0.42      0.42       454
weighted avg       0.58      0.58      0.58       454


Fold 1
[[ 3  8  8  1  0  0]
 [11 67 36  4  0  0]
 [ 1 57 78 28  0  0]
 [ 0  4 33 78  6  0]
 [ 0  1  1 17 11  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.20      0.15      0.17        20
          A2       0.49      0.57      0.53       118
          B1       0.50      0.48      0.49       164
          B2       0.61      0.64      0.63       121
          C1       0.61      0.37      0.46        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.52       454
   macro avg       0.40      0.37      0.38       454
weighted avg       0.52      0.52      0.52       454


Fold 2
[[ 8  8  3  0  0  0]
 [ 6 67 42  3  0  0]
 [ 4 34 98 28  0  0]
 [ 0  5 43 63  9  0]
 [ 0  0  3  8 19  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.44      0.42      0.43        19
          A2       0.59      0.57      0.58       118
          B1       0.52      0.60      0.56       164
          B2       0.62      0.53      0.57       120
          C1       0.66      0.63      0.64        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.56       452
   macro avg       0.47      0.46      0.46       452
weighted avg       0.57      0.56      0.56       452


Fold 3
[[ 5  9  5  0  0  0]
 [ 8 64 43  2  0  0]
 [ 1 50 88 23  1  0]
 [ 0  2 28 80 10  0]
 [ 0  1  0  9 20  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.36      0.26      0.30        19
          A2       0.51      0.55      0.53       117
          B1       0.54      0.54      0.54       163
          B2       0.70      0.67      0.68       120
          C1       0.65      0.67      0.66        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.57       450
   macro avg       0.46      0.45      0.45       450
weighted avg       0.57      0.57      0.57       450


Fold 4
[[ 4 12  3  0  0  0]
 [ 6 65 44  2  0  0]
 [ 3 36 92 32  0  0]
 [ 0  2 32 78  8  0]
 [ 0  0  1 15 14  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.31      0.21      0.25        19
          A2       0.57      0.56      0.56       117
          B1       0.53      0.56      0.55       163
          B2       0.61      0.65      0.63       120
          C1       0.64      0.47      0.54        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.56       450
   macro avg       0.44      0.41      0.42       450
weighted avg       0.56      0.56      0.56       450


K-fold scores
[0.577361416444308, 0.5174970120737642, 0.5638518157553282, 0.5699830023210528, 0.5588364892402878]
SKF f1 score mean 0.5575059471669481

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13231
13231
Fold 0
[[ 3 14  3  0  0  0]
 [ 8 76 30  4  0  0]
 [ 3 45 98 18  0  0]
 [ 0  2 27 83  9  0]
 [ 0  1  1 13 15  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.21      0.15      0.18        20
          A2       0.55      0.64      0.59       118
          B1       0.62      0.60      0.61       164
          B2       0.70      0.69      0.69       121
          C1       0.60      0.50      0.55        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.61       454
   macro avg       0.45      0.43      0.44       454
weighted avg       0.60      0.61      0.60       454


Fold 1
[[ 3  9  6  2  0  0]
 [10 67 37  4  0  0]
 [ 1 55 82 26  0  0]
 [ 0  2 30 83  6  0]
 [ 0  1  1 14 14  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.21      0.15      0.18        20
          A2       0.50      0.57      0.53       118
          B1       0.53      0.50      0.51       164
          B2       0.64      0.69      0.66       121
          C1       0.67      0.47      0.55        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.55       454
   macro avg       0.43      0.40      0.41       454
weighted avg       0.54      0.55      0.54       454


Fold 2
[[ 8  9  2  0  0  0]
 [ 5 71 40  2  0  0]
 [ 3 32 98 31  0  0]
 [ 0  3 38 69 10  0]
 [ 0  0  1  8 21  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.50      0.42      0.46        19
          A2       0.62      0.60      0.61       118
          B1       0.55      0.60      0.57       164
          B2       0.63      0.57      0.60       120
          C1       0.66      0.70      0.68        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.59       452
   macro avg       0.49      0.48      0.49       452
weighted avg       0.59      0.59      0.59       452


Fold 3
[[ 5  9  5  0  0  0]
 [ 6 67 43  1  0  0]
 [ 1 47 93 22  0  0]
 [ 0  1 22 89  8  0]
 [ 0  0  0  9 21  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.42      0.26      0.32        19
          A2       0.54      0.57      0.56       117
          B1       0.57      0.57      0.57       163
          B2       0.73      0.74      0.74       120
          C1       0.72      0.70      0.71        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.61       450
   macro avg       0.50      0.47      0.48       450
weighted avg       0.61      0.61      0.61       450


Fold 4
[[ 4 12  3  0  0  0]
 [ 6 69 40  2  0  0]
 [ 2 37 94 30  0  0]
 [ 0  2 24 84 10  0]
 [ 0  0  1 15 14  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.33      0.21      0.26        19
          A2       0.57      0.59      0.58       117
          B1       0.58      0.58      0.58       163
          B2       0.64      0.70      0.67       120
          C1       0.58      0.47      0.52        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.59       450
   macro avg       0.45      0.42      0.43       450
weighted avg       0.58      0.59      0.58       450


K-fold scores
[0.6024546629874525, 0.5443613034053564, 0.5899043002993143, 0.6084519315113008, 0.584165765354931]
SKF f1 score mean 0.585867592711671

SAME LANG EVAL DONE FOR THIS LANG
Doing: take all data as if it belongs to one large dataset, and do classification
Mega classification for:  dep  features
2260 2260 2260 1922
Distribution of labels: 
Counter({'B1': 818, 'B2': 602, 'A2': 588, 'C1': 150, 'A1': 97, 'C2': 5})
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13151
13151
Fold 0
[[  1  17   2   0   0   0]
 [  4  91  23   0   0   0]
 [  2  34 113  15   0   0]
 [  0   3  23  90   5   0]
 [  0   0   0  18  12   0]
 [  0   0   0   0   1   0]]
              precision    recall  f1-score   support

          A1       0.14      0.05      0.07        20
          A2       0.63      0.77      0.69       118
          B1       0.70      0.69      0.70       164
          B2       0.73      0.74      0.74       121
          C1       0.67      0.40      0.50        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.68       454
   macro avg       0.48      0.44      0.45       454
weighted avg       0.66      0.68      0.66       454


Fold 1
[[  4  13   3   0   0   0]
 [  2  93  22   1   0   0]
 [  0  41 104  19   0   0]
 [  0   1  16 102   2   0]
 [  0   0   2  18  10   0]
 [  0   0   0   0   1   0]]
              precision    recall  f1-score   support

          A1       0.67      0.20      0.31        20
          A2       0.63      0.79      0.70       118
          B1       0.71      0.63      0.67       164
          B2       0.73      0.84      0.78       121
          C1       0.77      0.33      0.47        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.69       454
   macro avg       0.58      0.47      0.49       454
weighted avg       0.69      0.69      0.68       454


Fold 2
[[  7  11   1   0   0   0]
 [  1  84  33   0   0   0]
 [  1  20 122  21   0   0]
 [  0   0  28  87   5   0]
 [  0   0   0  15  15   0]
 [  0   0   0   1   0   0]]
              precision    recall  f1-score   support

          A1       0.78      0.37      0.50        19
          A2       0.73      0.71      0.72       118
          B1       0.66      0.74      0.70       164
          B2       0.70      0.72      0.71       120
          C1       0.75      0.50      0.60        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.70       452
   macro avg       0.60      0.51      0.54       452
weighted avg       0.70      0.70      0.69       452


Fold 3
[[  6  10   3   0   0   0]
 [  1  84  32   0   0   0]
 [  2  31 106  24   0   0]
 [  0   0  22  95   3   0]
 [  0   1   0  19  10   0]
 [  0   0   0   1   0   0]]
              precision    recall  f1-score   support

          A1       0.67      0.32      0.43        19
          A2       0.67      0.72      0.69       117
          B1       0.65      0.65      0.65       163
          B2       0.68      0.79      0.73       120
          C1       0.77      0.33      0.47        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.67       450
   macro avg       0.57      0.47      0.49       450
weighted avg       0.67      0.67      0.66       450


Fold 4
[[  3  15   1   0   0   0]
 [  0  85  32   0   0   0]
 [  1  33 103  26   0   0]
 [  0   1  13  98   8   0]
 [  0   0   2  20   8   0]
 [  0   0   0   0   1   0]]
              precision    recall  f1-score   support

          A1       0.75      0.16      0.26        19
          A2       0.63      0.73      0.68       117
          B1       0.68      0.63      0.66       163
          B2       0.68      0.82      0.74       120
          C1       0.47      0.27      0.34        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.66       450
   macro avg       0.54      0.43      0.45       450
weighted avg       0.66      0.66      0.65       450


K-fold scores
[0.6639749959544752, 0.6759432000548747, 0.6927960648303261, 0.6600358276327268, 0.6454211786853703]
SKF f1 score mean 0.6676342534315547

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13151
13151
Fold 0
[[  1  14   4   1   0   0]
 [  6  69  41   2   0   0]
 [  2  43 101  18   0   0]
 [  1   0  36  73  11   0]
 [  0   0   1  15  14   0]
 [  0   0   0   1   0   0]]
              precision    recall  f1-score   support

          A1       0.10      0.05      0.07        20
          A2       0.55      0.58      0.57       118
          B1       0.55      0.62      0.58       164
          B2       0.66      0.60      0.63       121
          C1       0.56      0.47      0.51        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.57       454
   macro avg       0.40      0.39      0.39       454
weighted avg       0.56      0.57      0.56       454


Fold 1
[[ 5  9  5  1  0  0]
 [13 70 32  3  0  0]
 [ 2 47 80 33  2  0]
 [ 0  4 27 83  7  0]
 [ 0  0  3 14 13  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.25      0.25      0.25        20
          A2       0.54      0.59      0.56       118
          B1       0.54      0.49      0.51       164
          B2       0.62      0.69      0.65       121
          C1       0.57      0.43      0.49        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.55       454
   macro avg       0.42      0.41      0.41       454
weighted avg       0.55      0.55      0.55       454


Fold 2
[[  3  13   3   0   0   0]
 [  6  68  43   1   0   0]
 [  3  34 103  24   0   0]
 [  0   6  34  69  11   0]
 [  0   0   2   9  19   0]
 [  0   0   0   0   1   0]]
              precision    recall  f1-score   support

          A1       0.25      0.16      0.19        19
          A2       0.56      0.58      0.57       118
          B1       0.56      0.63      0.59       164
          B2       0.67      0.57      0.62       120
          C1       0.61      0.63      0.62        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.58       452
   macro avg       0.44      0.43      0.43       452
weighted avg       0.58      0.58      0.58       452


Fold 3
[[ 3 12  4  0  0  0]
 [10 64 39  4  0  0]
 [ 2 49 86 25  1  0]
 [ 0  5 37 72  6  0]
 [ 0  0  1 11 18  0]
 [ 0  0  1  0  0  0]]
              precision    recall  f1-score   support

          A1       0.20      0.16      0.18        19
          A2       0.49      0.55      0.52       117
          B1       0.51      0.53      0.52       163
          B2       0.64      0.60      0.62       120
          C1       0.72      0.60      0.65        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.54       450
   macro avg       0.43      0.41      0.41       450
weighted avg       0.54      0.54      0.54       450


Fold 4
[[ 2 16  1  0  0  0]
 [ 3 55 56  2  1  0]
 [ 3 42 96 22  0  0]
 [ 0  5 33 74  8  0]
 [ 0  0  2 18 10  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.25      0.11      0.15        19
          A2       0.47      0.47      0.47       117
          B1       0.51      0.59      0.55       163
          B2       0.63      0.62      0.62       120
          C1       0.53      0.33      0.41        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.53       450
   macro avg       0.40      0.35      0.37       450
weighted avg       0.52      0.53      0.52       450


K-fold scores
[0.5623121071239859, 0.5494967007985904, 0.5764928526755663, 0.5395656638340071, 0.5198328272087993]
SKF f1 score mean 0.5495400303281898

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13151
13151
Fold 0
[[  2  14   3   1   0   0]
 [  6  75  35   2   0   0]
 [  1  43 102  18   0   0]
 [  1   0  32  77  11   0]
 [  0   0   0  16  14   0]
 [  0   0   0   1   0   0]]
              precision    recall  f1-score   support

          A1       0.20      0.10      0.13        20
          A2       0.57      0.64      0.60       118
          B1       0.59      0.62      0.61       164
          B2       0.67      0.64      0.65       121
          C1       0.56      0.47      0.51        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.59       454
   macro avg       0.43      0.41      0.42       454
weighted avg       0.59      0.59      0.59       454


Fold 1
[[ 5  9  5  1  0  0]
 [11 69 33  5  0  0]
 [ 1 44 80 38  1  0]
 [ 0  3 22 88  8  0]
 [ 0  0  2 15 13  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.29      0.25      0.27        20
          A2       0.55      0.58      0.57       118
          B1       0.56      0.49      0.52       164
          B2       0.60      0.73      0.66       121
          C1       0.57      0.43      0.49        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.56       454
   macro avg       0.43      0.41      0.42       454
weighted avg       0.56      0.56      0.56       454


Fold 2
[[  4  13   2   0   0   0]
 [  5  73  38   2   0   0]
 [  3  34 103  24   0   0]
 [  0   3  31  75  11   0]
 [  0   0   1   8  21   0]
 [  0   0   0   0   1   0]]
              precision    recall  f1-score   support

          A1       0.33      0.21      0.26        19
          A2       0.59      0.62      0.61       118
          B1       0.59      0.63      0.61       164
          B2       0.69      0.62      0.66       120
          C1       0.64      0.70      0.67        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.61       452
   macro avg       0.47      0.46      0.47       452
weighted avg       0.61      0.61      0.61       452


Fold 3
[[ 4 12  3  0  0  0]
 [10 65 38  4  0  0]
 [ 3 43 92 24  1  0]
 [ 0  3 28 81  8  0]
 [ 0  0  1 10 19  0]
 [ 0  0  1  0  0  0]]
              precision    recall  f1-score   support

          A1       0.24      0.21      0.22        19
          A2       0.53      0.56      0.54       117
          B1       0.56      0.56      0.56       163
          B2       0.68      0.68      0.68       120
          C1       0.68      0.63      0.66        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.58       450
   macro avg       0.45      0.44      0.44       450
weighted avg       0.58      0.58      0.58       450


Fold 4
[[  2  16   1   0   0   0]
 [  1  65  51   0   0   0]
 [  3  37 102  21   0   0]
 [  0   3  30  75  12   0]
 [  0   0   2  17  11   0]
 [  0   0   0   0   1   0]]
              precision    recall  f1-score   support

          A1       0.33      0.11      0.16        19
          A2       0.54      0.56      0.55       117
          B1       0.55      0.63      0.58       163
          B2       0.66      0.62      0.64       120
          C1       0.46      0.37      0.41        30
          C2       0.00      0.00      0.00         1

    accuracy                           0.57       450
   macro avg       0.42      0.38      0.39       450
weighted avg       0.56      0.57      0.56       450


K-fold scores
[0.588697025615565, 0.5558349179968843, 0.6076308411091939, 0.5790917928220145, 0.5593354246540071]
SKF f1 score mean 0.578118000439533

SAME LANG EVAL DONE FOR THIS LANG
Multi lingual classification for dimension Vocabularycontrol
Doing: take all data as if it belongs to one large dataset, and do classification
Mega classification for:  pos  features
2260 2260 2260 633
Distribution of labels: 
Counter({'B1': 828, 'A2': 594, 'B2': 574, 'A1': 152, 'C1': 101, 'C2': 11})
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13231
13231
Fold 0
[[ 8 21  2  0  0  0]
 [ 2 78 38  1  0  0]
 [ 3 34 94 35  0  0]
 [ 0  5 33 77  0  0]
 [ 0  0  1 20  0  0]
 [ 0  0  0  3  0  0]]
              precision    recall  f1-score   support

          A1       0.62      0.26      0.36        31
          A2       0.57      0.66      0.61       119
          B1       0.56      0.57      0.56       166
          B2       0.57      0.67      0.61       115
          C1       0.00      0.00      0.00        21
          C2       0.00      0.00      0.00         3

    accuracy                           0.56       455
   macro avg       0.38      0.36      0.36       455
weighted avg       0.54      0.56      0.54       455


Fold 1
[[ 6 22  3  0  0  0]
 [ 4 87 28  0  0  0]
 [ 0 46 94 26  0  0]
 [ 0  2 39 74  0  0]
 [ 0  0  1 19  0  0]
 [ 0  0  1  1  0  0]]
              precision    recall  f1-score   support

          A1       0.60      0.19      0.29        31
          A2       0.55      0.73      0.63       119
          B1       0.57      0.57      0.57       166
          B2       0.62      0.64      0.63       115
          C1       0.00      0.00      0.00        20
          C2       0.00      0.00      0.00         2

    accuracy                           0.58       453
   macro avg       0.39      0.36      0.35       453
weighted avg       0.55      0.58      0.55       453


Fold 2
[[ 9 17  4  0  0  0]
 [ 0 84 35  0  0  0]
 [ 2 41 94 29  0  0]
 [ 0  3 32 80  0  0]
 [ 0  0  4 16  0  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.82      0.30      0.44        30
          A2       0.58      0.71      0.64       119
          B1       0.56      0.57      0.56       166
          B2       0.63      0.70      0.66       115
          C1       0.00      0.00      0.00        20
          C2       0.00      0.00      0.00         2

    accuracy                           0.59       452
   macro avg       0.43      0.38      0.38       452
weighted avg       0.57      0.59      0.57       452


Fold 3
[[ 6 19  5  0  0  0]
 [ 2 77 40  0  0  0]
 [ 2 34 93 36  0  0]
 [ 0  2 30 83  0  0]
 [ 0  0  3 17  0  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.60      0.20      0.30        30
          A2       0.58      0.65      0.61       119
          B1       0.54      0.56      0.55       165
          B2       0.60      0.72      0.66       115
          C1       0.00      0.00      0.00        20
          C2       0.00      0.00      0.00         2

    accuracy                           0.57       451
   macro avg       0.39      0.36      0.35       451
weighted avg       0.55      0.57      0.55       451


Fold 4
[[10 16  4  0  0  0]
 [ 2 85 31  0  0  0]
 [ 4 46 85 30  0  0]
 [ 0  2 27 85  0  0]
 [ 0  0  1 19  0  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.62      0.33      0.43        30
          A2       0.57      0.72      0.64       118
          B1       0.57      0.52      0.54       165
          B2       0.62      0.75      0.68       114
          C1       0.00      0.00      0.00        20
          C2       0.00      0.00      0.00         2

    accuracy                           0.59       449
   macro avg       0.40      0.39      0.38       449
weighted avg       0.56      0.59      0.57       449


K-fold scores
[0.5439584285286868, 0.5530252577859508, 0.5709939629226843, 0.5516757945899473, 0.5686217768857951]
SKF f1 score mean 0.5576550441426129

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13231
13231
Fold 0
[[10 14  6  1  0  0]
 [12 66 37  4  0  0]
 [ 5 40 92 29  0  0]
 [ 2  7 43 56  7  0]
 [ 0  1  8 10  2  0]
 [ 0  0  0  3  0  0]]
              precision    recall  f1-score   support

          A1       0.34      0.32      0.33        31
          A2       0.52      0.55      0.53       119
          B1       0.49      0.55      0.52       166
          B2       0.54      0.49      0.51       115
          C1       0.22      0.10      0.13        21
          C2       0.00      0.00      0.00         3

    accuracy                           0.50       455
   macro avg       0.35      0.34      0.34       455
weighted avg       0.49      0.50      0.49       455


Fold 1
[[ 6 13 10  1  1  0]
 [16 62 37  4  0  0]
 [ 2 41 87 33  3  0]
 [ 0  9 51 50  5  0]
 [ 0  0  5 11  4  0]
 [ 0  0  0  1  1  0]]
              precision    recall  f1-score   support

          A1       0.25      0.19      0.22        31
          A2       0.50      0.52      0.51       119
          B1       0.46      0.52      0.49       166
          B2       0.50      0.43      0.47       115
          C1       0.29      0.20      0.24        20
          C2       0.00      0.00      0.00         2

    accuracy                           0.46       453
   macro avg       0.33      0.31      0.32       453
weighted avg       0.45      0.46      0.46       453


Fold 2
[[12  9  9  0  0  0]
 [11 66 36  6  0  0]
 [ 1 52 74 39  0  0]
 [ 1  9 38 59  8  0]
 [ 0  2  4  9  5  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.48      0.40      0.44        30
          A2       0.48      0.55      0.51       119
          B1       0.46      0.45      0.45       166
          B2       0.51      0.51      0.51       115
          C1       0.38      0.25      0.30        20
          C2       0.00      0.00      0.00         2

    accuracy                           0.48       452
   macro avg       0.39      0.36      0.37       452
weighted avg       0.47      0.48      0.47       452


Fold 3
[[ 9 13  8  0  0  0]
 [ 7 70 39  3  0  0]
 [ 7 44 71 42  1  0]
 [ 1  8 49 53  4  0]
 [ 0  1  4 13  1  1]
 [ 0  0  0  1  1  0]]
              precision    recall  f1-score   support

          A1       0.38      0.30      0.33        30
          A2       0.51      0.59      0.55       119
          B1       0.42      0.43      0.42       165
          B2       0.47      0.46      0.47       115
          C1       0.14      0.05      0.07        20
          C2       0.00      0.00      0.00         2

    accuracy                           0.45       451
   macro avg       0.32      0.30      0.31       451
weighted avg       0.44      0.45      0.44       451


Fold 4
[[16  8  6  0  0  0]
 [11 62 39  5  1  0]
 [ 8 47 79 30  1  0]
 [ 0 15 27 61 11  0]
 [ 0  0  1 12  7  0]
 [ 0  0  1  1  0  0]]
              precision    recall  f1-score   support

          A1       0.46      0.53      0.49        30
          A2       0.47      0.53      0.50       118
          B1       0.52      0.48      0.50       165
          B2       0.56      0.54      0.55       114
          C1       0.35      0.35      0.35        20
          C2       0.00      0.00      0.00         2

    accuracy                           0.50       449
   macro avg       0.39      0.40      0.40       449
weighted avg       0.50      0.50      0.50       449


K-fold scores
[0.4891951013490219, 0.4560002916254758, 0.4743443278552136, 0.4440075348128427, 0.5003253359670085]
SKF f1 score mean 0.4727745183219125

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13231
13231
Fold 0
[[10 16  4  1  0  0]
 [ 8 74 34  3  0  0]
 [ 5 36 91 34  0  0]
 [ 1  4 41 62  7  0]
 [ 0  0  5 14  2  0]
 [ 0  0  0  3  0  0]]
              precision    recall  f1-score   support

          A1       0.42      0.32      0.36        31
          A2       0.57      0.62      0.59       119
          B1       0.52      0.55      0.53       166
          B2       0.53      0.54      0.53       115
          C1       0.22      0.10      0.13        21
          C2       0.00      0.00      0.00         3

    accuracy                           0.53       455
   macro avg       0.38      0.35      0.36       455
weighted avg       0.51      0.53      0.52       455


Fold 1
[[ 6 14  9  1  1  0]
 [17 63 37  2  0  0]
 [ 4 43 81 34  4  0]
 [ 1  7 48 52  7  0]
 [ 0  0  3 12  5  0]
 [ 0  0  1  1  0  0]]
              precision    recall  f1-score   support

          A1       0.21      0.19      0.20        31
          A2       0.50      0.53      0.51       119
          B1       0.45      0.49      0.47       166
          B2       0.51      0.45      0.48       115
          C1       0.29      0.25      0.27        20
          C2       0.00      0.00      0.00         2

    accuracy                           0.46       453
   macro avg       0.33      0.32      0.32       453
weighted avg       0.45      0.46      0.45       453


Fold 2
[[ 9 13  8  0  0  0]
 [ 9 68 36  6  0  0]
 [ 2 48 77 39  0  0]
 [ 1  5 37 66  6  0]
 [ 0  1  3 13  3  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.43      0.30      0.35        30
          A2       0.50      0.57      0.54       119
          B1       0.48      0.46      0.47       166
          B2       0.52      0.57      0.55       115
          C1       0.33      0.15      0.21        20
          C2       0.00      0.00      0.00         2

    accuracy                           0.49       452
   macro avg       0.38      0.34      0.35       452
weighted avg       0.48      0.49      0.49       452


Fold 3
[[ 9 14  7  0  0  0]
 [ 7 73 37  2  0  0]
 [ 4 40 81 39  1  0]
 [ 0  6 49 55  5  0]
 [ 0  1  3 14  1  1]
 [ 0  0  0  1  1  0]]
              precision    recall  f1-score   support

          A1       0.45      0.30      0.36        30
          A2       0.54      0.61      0.58       119
          B1       0.46      0.49      0.47       165
          B2       0.50      0.48      0.49       115
          C1       0.12      0.05      0.07        20
          C2       0.00      0.00      0.00         2

    accuracy                           0.49       451
   macro avg       0.35      0.32      0.33       451
weighted avg       0.47      0.49      0.48       451


Fold 4
[[14 10  6  0  0  0]
 [ 9 70 35  4  0  0]
 [ 7 45 78 34  1  0]
 [ 0  9 33 58 14  0]
 [ 0  0  1 12  7  0]
 [ 0  0  1  1  0  0]]
              precision    recall  f1-score   support

          A1       0.47      0.47      0.47        30
          A2       0.52      0.59      0.56       118
          B1       0.51      0.47      0.49       165
          B2       0.53      0.51      0.52       114
          C1       0.32      0.35      0.33        20
          C2       0.00      0.00      0.00         2

    accuracy                           0.51       449
   macro avg       0.39      0.40      0.39       449
weighted avg       0.50      0.51      0.50       449


K-fold scores
[0.5161921069848218, 0.45413850579159304, 0.4858575744678589, 0.4767890338176063, 0.5038136432630941]
SKF f1 score mean 0.48735817286499483

SAME LANG EVAL DONE FOR THIS LANG
Doing: take all data as if it belongs to one large dataset, and do classification
Mega classification for:  dep  features
2260 2260 2260 1922
Distribution of labels: 
Counter({'B1': 828, 'A2': 594, 'B2': 574, 'A1': 152, 'C1': 101, 'C2': 11})
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13151
13151
Fold 0
[[ 6 22  3  0  0  0]
 [ 2 85 32  0  0  0]
 [ 3 38 96 29  0  0]
 [ 0  5 42 68  0  0]
 [ 0  0  4 17  0  0]
 [ 0  0  0  3  0  0]]
              precision    recall  f1-score   support

          A1       0.55      0.19      0.29        31
          A2       0.57      0.71      0.63       119
          B1       0.54      0.58      0.56       166
          B2       0.58      0.59      0.59       115
          C1       0.00      0.00      0.00        21
          C2       0.00      0.00      0.00         3

    accuracy                           0.56       455
   macro avg       0.37      0.35      0.34       455
weighted avg       0.53      0.56      0.54       455


Fold 1
[[  8  17   6   0   0   0]
 [  8  84  27   0   0   0]
 [  0  36 104  26   0   0]
 [  0   2  37  76   0   0]
 [  0   0   1  19   0   0]
 [  0   0   0   2   0   0]]
              precision    recall  f1-score   support

          A1       0.50      0.26      0.34        31
          A2       0.60      0.71      0.65       119
          B1       0.59      0.63      0.61       166
          B2       0.62      0.66      0.64       115
          C1       0.00      0.00      0.00        20
          C2       0.00      0.00      0.00         2

    accuracy                           0.60       453
   macro avg       0.39      0.38      0.37       453
weighted avg       0.57      0.60      0.58       453


Fold 2
[[ 4 23  3  0  0  0]
 [ 1 82 36  0  0  0]
 [ 0 40 96 30  0  0]
 [ 0  3 38 74  0  0]
 [ 0  0  2 18  0  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.80      0.13      0.23        30
          A2       0.55      0.69      0.61       119
          B1       0.55      0.58      0.56       166
          B2       0.60      0.64      0.62       115
          C1       0.00      0.00      0.00        20
          C2       0.00      0.00      0.00         2

    accuracy                           0.57       452
   macro avg       0.42      0.34      0.34       452
weighted avg       0.55      0.57      0.54       452


Fold 3
[[ 6 17  7  0  0  0]
 [ 3 77 38  1  0  0]
 [ 2 36 93 34  0  0]
 [ 0  3 32 80  0  0]
 [ 0  2  3 15  0  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.55      0.20      0.29        30
          A2       0.57      0.65      0.61       119
          B1       0.54      0.56      0.55       165
          B2       0.61      0.70      0.65       115
          C1       0.00      0.00      0.00        20
          C2       0.00      0.00      0.00         2

    accuracy                           0.57       451
   macro avg       0.38      0.35      0.35       451
weighted avg       0.54      0.57      0.55       451


Fold 4
[[ 9 17  4  0  0  0]
 [ 3 78 37  0  0  0]
 [ 2 46 88 29  0  0]
 [ 0  2 28 84  0  0]
 [ 0  0  3 17  0  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.64      0.30      0.41        30
          A2       0.55      0.66      0.60       118
          B1       0.55      0.53      0.54       165
          B2       0.64      0.74      0.68       114
          C1       0.00      0.00      0.00        20
          C2       0.00      0.00      0.00         2

    accuracy                           0.58       449
   macro avg       0.40      0.37      0.37       449
weighted avg       0.55      0.58      0.56       449


K-fold scores
[0.5371355598372918, 0.5800045777007834, 0.5412177889316533, 0.5459486425668383, 0.5568128454239258]
SKF f1 score mean 0.5522238828920986

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13151
13151
Fold 0
[[10 14  7  0  0  0]
 [17 68 31  3  0  0]
 [ 8 48 78 32  0  0]
 [ 1 10 44 54  6  0]
 [ 0  2  4 13  2  0]
 [ 0  0  1  2  0  0]]
              precision    recall  f1-score   support

          A1       0.28      0.32      0.30        31
          A2       0.48      0.57      0.52       119
          B1       0.47      0.47      0.47       166
          B2       0.52      0.47      0.49       115
          C1       0.25      0.10      0.14        21
          C2       0.00      0.00      0.00         3

    accuracy                           0.47       455
   macro avg       0.33      0.32      0.32       455
weighted avg       0.46      0.47      0.46       455


Fold 1
[[ 8 14  9  0  0  0]
 [19 58 37  4  1  0]
 [ 4 34 87 36  5  0]
 [ 0  8 47 56  4  0]
 [ 0  0  2 16  2  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.26      0.26      0.26        31
          A2       0.51      0.49      0.50       119
          B1       0.48      0.52      0.50       166
          B2       0.49      0.49      0.49       115
          C1       0.17      0.10      0.12        20
          C2       0.00      0.00      0.00         2

    accuracy                           0.47       453
   macro avg       0.32      0.31      0.31       453
weighted avg       0.46      0.47      0.46       453


Fold 2
[[ 7 13  9  1  0  0]
 [ 9 61 43  5  1  0]
 [ 3 46 77 39  1  0]
 [ 0  5 51 51  8  0]
 [ 0  0  6 11  3  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.37      0.23      0.29        30
          A2       0.49      0.51      0.50       119
          B1       0.41      0.46      0.44       166
          B2       0.47      0.44      0.46       115
          C1       0.23      0.15      0.18        20
          C2       0.00      0.00      0.00         2

    accuracy                           0.44       452
   macro avg       0.33      0.30      0.31       452
weighted avg       0.43      0.44      0.44       452


Fold 3
[[ 5 17  8  0  0  0]
 [ 6 64 42  7  0  0]
 [ 6 48 81 30  0  0]
 [ 1  8 43 57  5  1]
 [ 0  0  6 12  2  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.28      0.17      0.21        30
          A2       0.47      0.54      0.50       119
          B1       0.45      0.49      0.47       165
          B2       0.53      0.50      0.51       115
          C1       0.29      0.10      0.15        20
          C2       0.00      0.00      0.00         2

    accuracy                           0.46       451
   macro avg       0.33      0.30      0.31       451
weighted avg       0.45      0.46      0.45       451


Fold 4
[[10 14  5  1  0  0]
 [10 62 37  8  1  0]
 [ 9 41 78 36  1  0]
 [ 0  5 39 60  9  1]
 [ 0  0  2 13  5  0]
 [ 0  0  1  1  0  0]]
              precision    recall  f1-score   support

          A1       0.34      0.33      0.34        30
          A2       0.51      0.53      0.52       118
          B1       0.48      0.47      0.48       165
          B2       0.50      0.53      0.52       114
          C1       0.31      0.25      0.28        20
          C2       0.00      0.00      0.00         2

    accuracy                           0.48       449
   macro avg       0.36      0.35      0.35       449
weighted avg       0.48      0.48      0.48       449


K-fold scores
[0.4595734598167508, 0.46134476087825427, 0.43517447707160095, 0.45450213193057565, 0.4768814174384243]
SKF f1 score mean 0.4574952494271212

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13151
13151
Fold 0
[[ 9 16  6  0  0  0]
 [15 77 24  3  0  0]
 [ 7 47 83 29  0  0]
 [ 0  8 43 56  8  0]
 [ 0  1  3 15  2  0]
 [ 0  0  1  2  0  0]]
              precision    recall  f1-score   support

          A1       0.29      0.29      0.29        31
          A2       0.52      0.65      0.57       119
          B1       0.52      0.50      0.51       166
          B2       0.53      0.49      0.51       115
          C1       0.20      0.10      0.13        21
          C2       0.00      0.00      0.00         3

    accuracy                           0.50       455
   macro avg       0.34      0.34      0.34       455
weighted avg       0.49      0.50      0.49       455


Fold 1
[[ 7 15  9  0  0  0]
 [18 62 34  3  1  1]
 [ 5 33 84 39  5  0]
 [ 0  5 44 61  4  1]
 [ 0  0  1 15  4  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.23      0.23      0.23        31
          A2       0.54      0.52      0.53       119
          B1       0.49      0.51      0.50       166
          B2       0.51      0.53      0.52       115
          C1       0.29      0.20      0.24        20
          C2       0.00      0.00      0.00         2

    accuracy                           0.48       453
   macro avg       0.34      0.33      0.34       453
weighted avg       0.48      0.48      0.48       453


Fold 2
[[ 6 14  9  1  0  0]
 [ 8 67 40  3  1  0]
 [ 2 40 82 42  0  0]
 [ 0  5 50 51  9  0]
 [ 0  0  6 11  3  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.38      0.20      0.26        30
          A2       0.53      0.56      0.55       119
          B1       0.44      0.49      0.46       166
          B2       0.46      0.44      0.45       115
          C1       0.23      0.15      0.18        20
          C2       0.00      0.00      0.00         2

    accuracy                           0.46       452
   macro avg       0.34      0.31      0.32       452
weighted avg       0.45      0.46      0.46       452


Fold 3
[[ 6 17  7  0  0  0]
 [ 7 67 40  5  0  0]
 [ 5 45 79 34  1  1]
 [ 1  6 41 62  5  0]
 [ 0  0  6 12  2  0]
 [ 0  0  0  2  0  0]]
              precision    recall  f1-score   support

          A1       0.32      0.20      0.24        30
          A2       0.50      0.56      0.53       119
          B1       0.46      0.48      0.47       165
          B2       0.54      0.54      0.54       115
          C1       0.25      0.10      0.14        20
          C2       0.00      0.00      0.00         2

    accuracy                           0.48       451
   macro avg       0.34      0.31      0.32       451
weighted avg       0.47      0.48      0.47       451


Fold 4
[[ 9 17  3  1  0  0]
 [12 65 35  6  0  0]
 [ 6 38 81 39  1  0]
 [ 0  5 39 62  8  0]
 [ 0  0  2 13  5  0]
 [ 0  0  1  1  0  0]]
              precision    recall  f1-score   support

          A1       0.33      0.30      0.32        30
          A2       0.52      0.55      0.53       118
          B1       0.50      0.49      0.50       165
          B2       0.51      0.54      0.53       114
          C1       0.36      0.25      0.29        20
          C2       0.00      0.00      0.00         2

    accuracy                           0.49       449
   macro avg       0.37      0.36      0.36       449
weighted avg       0.49      0.49      0.49       449


K-fold scores
[0.49046887108065496, 0.4792306156961377, 0.4553170602723329, 0.47031881753104454, 0.4908146531928381]
SKF f1 score mean 0.47723000355460155

SAME LANG EVAL DONE FOR THIS LANG
Multi lingual classification for dimension CoherenceCohesion
Doing: take all data as if it belongs to one large dataset, and do classification
Mega classification for:  pos  features
2260 2260 2260 633
Distribution of labels: 
Counter({'B1': 827, 'A2': 662, 'B2': 482, 'A1': 194, 'C1': 90, 'C2': 5})
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13231
13231
Fold 0
[[  4  31   4   0   0   0]
 [  3 104  26   0   0   0]
 [  1  31 123  11   0   0]
 [  0   4  20  73   0   0]
 [  0   0   0  18   0   0]
 [  0   0   0   1   0   0]]
              precision    recall  f1-score   support

          A1       0.50      0.10      0.17        39
          A2       0.61      0.78      0.69       133
          B1       0.71      0.74      0.73       166
          B2       0.71      0.75      0.73        97
          C1       0.00      0.00      0.00        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.67       454
   macro avg       0.42      0.40      0.39       454
weighted avg       0.63      0.67      0.64       454


Fold 1
[[  8  28   3   0   0   0]
 [  5 101  25   2   0   0]
 [  1  25 115  25   0   0]
 [  0   0  28  69   0   0]
 [  0   0   0  18   0   0]
 [  0   0   0   1   0   0]]
              precision    recall  f1-score   support

          A1       0.57      0.21      0.30        39
          A2       0.66      0.76      0.70       133
          B1       0.67      0.69      0.68       166
          B2       0.60      0.71      0.65        97
          C1       0.00      0.00      0.00        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.65       454
   macro avg       0.42      0.39      0.39       454
weighted avg       0.62      0.65      0.62       454


Fold 2
[[  9  30   0   0   0   0]
 [  5 106  21   0   0   0]
 [  1  23 128  13   0   0]
 [  0   0  22  74   0   0]
 [  0   0   1  17   0   0]
 [  0   0   0   1   0   0]]
              precision    recall  f1-score   support

          A1       0.60      0.23      0.33        39
          A2       0.67      0.80      0.73       132
          B1       0.74      0.78      0.76       165
          B2       0.70      0.77      0.74        96
          C1       0.00      0.00      0.00        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.70       451
   macro avg       0.45      0.43      0.43       451
weighted avg       0.67      0.70      0.68       451


Fold 3
[[  8  31   0   0   0   0]
 [  4  93  35   0   0   0]
 [  0  26 125  14   0   0]
 [  0   1  21  74   0   0]
 [  0   0   0  18   0   0]
 [  0   0   0   1   0   0]]
              precision    recall  f1-score   support

          A1       0.67      0.21      0.31        39
          A2       0.62      0.70      0.66       132
          B1       0.69      0.76      0.72       165
          B2       0.69      0.77      0.73        96
          C1       0.00      0.00      0.00        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.67       451
   macro avg       0.44      0.41      0.40       451
weighted avg       0.64      0.67      0.64       451


Fold 4
[[ 10  28   0   0   0   0]
 [  6  94  32   0   0   0]
 [  3  27 126   9   0   0]
 [  0   1  24  71   0   0]
 [  0   0   0  18   0   0]
 [  0   0   0   1   0   0]]
              precision    recall  f1-score   support

          A1       0.53      0.26      0.35        38
          A2       0.63      0.71      0.67       132
          B1       0.69      0.76      0.73       165
          B2       0.72      0.74      0.73        96
          C1       0.00      0.00      0.00        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.67       450
   macro avg       0.43      0.41      0.41       450
weighted avg       0.64      0.67      0.65       450


K-fold scores
[0.6370237993314849, 0.6207458567911402, 0.6767023545644978, 0.6390272257775435, 0.6468180332848921]
SKF f1 score mean 0.6440634539499117

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13231
13231
Fold 0
[[  9  25   4   0   1   0]
 [ 25  72  33   3   0   0]
 [  6  35 102  20   3   0]
 [  0   9  20  61   7   0]
 [  0   0   2  12   4   0]
 [  0   0   0   1   0   0]]
              precision    recall  f1-score   support

          A1       0.23      0.23      0.23        39
          A2       0.51      0.54      0.53       133
          B1       0.63      0.61      0.62       166
          B2       0.63      0.63      0.63        97
          C1       0.27      0.22      0.24        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.55       454
   macro avg       0.38      0.37      0.37       454
weighted avg       0.55      0.55      0.55       454


Fold 1
[[15 22  2  0  0  0]
 [25 69 36  3  0  0]
 [ 2 45 99 19  1  0]
 [ 0  6 32 54  5  0]
 [ 0  0  4 11  3  0]
 [ 0  0  0  0  1  0]]
              precision    recall  f1-score   support

          A1       0.36      0.38      0.37        39
          A2       0.49      0.52      0.50       133
          B1       0.57      0.60      0.58       166
          B2       0.62      0.56      0.59        97
          C1       0.30      0.17      0.21        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.53       454
   macro avg       0.39      0.37      0.38       454
weighted avg       0.53      0.53      0.53       454


Fold 2
[[ 11  20   8   0   0   0]
 [ 19  78  30   3   2   0]
 [  1  38 104  20   2   0]
 [  0   5  27  58   6   0]
 [  0   0   3   9   6   0]
 [  0   0   0   0   1   0]]
              precision    recall  f1-score   support

          A1       0.35      0.28      0.31        39
          A2       0.55      0.59      0.57       132
          B1       0.60      0.63      0.62       165
          B2       0.64      0.60      0.62        96
          C1       0.35      0.33      0.34        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.57       451
   macro avg       0.42      0.41      0.41       451
weighted avg       0.57      0.57      0.57       451


Fold 3
[[  8  25   5   1   0   0]
 [ 16  65  44   6   1   0]
 [  4  42 100  19   0   0]
 [  0   3  33  58   2   0]
 [  0   1   1  15   1   0]
 [  0   0   0   1   0   0]]
              precision    recall  f1-score   support

          A1       0.29      0.21      0.24        39
          A2       0.48      0.49      0.49       132
          B1       0.55      0.61      0.57       165
          B2       0.58      0.60      0.59        96
          C1       0.25      0.06      0.09        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.51       451
   macro avg       0.36      0.33      0.33       451
weighted avg       0.50      0.51      0.50       451


Fold 4
[[ 14  22   1   1   0   0]
 [ 25  58  44   5   0   0]
 [  3  35 103  23   1   0]
 [  2   5  25  59   5   0]
 [  0   0   3  13   2   0]
 [  0   0   0   1   0   0]]
              precision    recall  f1-score   support

          A1       0.32      0.37      0.34        38
          A2       0.48      0.44      0.46       132
          B1       0.59      0.62      0.60       165
          B2       0.58      0.61      0.60        96
          C1       0.25      0.11      0.15        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.52       450
   macro avg       0.37      0.36      0.36       450
weighted avg       0.52      0.52      0.52       450


K-fold scores
[0.5456104748006707, 0.5262862121244285, 0.5666693419298322, 0.5024912637604478, 0.5186584130093179]
SKF f1 score mean 0.5319431411249393

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13231
13231
Fold 0
[[  9  26   4   0   0   0]
 [ 24  70  35   4   0   0]
 [  7  32 109  17   1   0]
 [  0   6  23  61   7   0]
 [  0   0   1  11   6   0]
 [  0   0   0   1   0   0]]
              precision    recall  f1-score   support

          A1       0.23      0.23      0.23        39
          A2       0.52      0.53      0.52       133
          B1       0.63      0.66      0.64       166
          B2       0.65      0.63      0.64        97
          C1       0.43      0.33      0.38        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.56       454
   macro avg       0.41      0.40      0.40       454
weighted avg       0.56      0.56      0.56       454


Fold 1
[[ 11  25   3   0   0   0]
 [ 21  69  39   4   0   0]
 [  2  36 109  18   1   0]
 [  0   4  30  58   5   0]
 [  0   0   3  12   3   0]
 [  0   0   0   0   1   0]]
              precision    recall  f1-score   support

          A1       0.32      0.28      0.30        39
          A2       0.51      0.52      0.52       133
          B1       0.59      0.66      0.62       166
          B2       0.63      0.60      0.61        97
          C1       0.30      0.17      0.21        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.55       454
   macro avg       0.39      0.37      0.38       454
weighted avg       0.54      0.55      0.54       454


Fold 2
[[ 15  18   6   0   0   0]
 [ 18  82  28   2   2   0]
 [  1  41 103  18   2   0]
 [  0   3  26  60   7   0]
 [  0   0   1   9   8   0]
 [  0   0   0   0   1   0]]
              precision    recall  f1-score   support

          A1       0.44      0.38      0.41        39
          A2       0.57      0.62      0.59       132
          B1       0.63      0.62      0.63       165
          B2       0.67      0.62      0.65        96
          C1       0.40      0.44      0.42        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.59       451
   macro avg       0.45      0.45      0.45       451
weighted avg       0.59      0.59      0.59       451


Fold 3
[[10 23  5  1  0  0]
 [16 67 43  6  0  0]
 [ 5 43 96 21  0  0]
 [ 0  2 30 61  3  0]
 [ 0  1  2 14  1  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.32      0.26      0.29        39
          A2       0.49      0.51      0.50       132
          B1       0.55      0.58      0.56       165
          B2       0.59      0.64      0.61        96
          C1       0.25      0.06      0.09        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.52       451
   macro avg       0.37      0.34      0.34       451
weighted avg       0.51      0.52      0.51       451


Fold 4
[[ 10  26   2   0   0   0]
 [ 22  59  46   5   0   0]
 [  3  32 109  21   0   0]
 [  1   5  24  62   4   0]
 [  0   0   2  11   5   0]
 [  0   0   0   1   0   0]]
              precision    recall  f1-score   support

          A1       0.28      0.26      0.27        38
          A2       0.48      0.45      0.46       132
          B1       0.60      0.66      0.63       165
          B2       0.62      0.65      0.63        96
          C1       0.56      0.28      0.37        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.54       450
   macro avg       0.42      0.38      0.39       450
weighted avg       0.54      0.54      0.54       450


K-fold scores
[0.5603460519941968, 0.5446714048393441, 0.5934023667771076, 0.5105154026289853, 0.5385700765014988]
SKF f1 score mean 0.5495010605482264

SAME LANG EVAL DONE FOR THIS LANG
Doing: take all data as if it belongs to one large dataset, and do classification
Mega classification for:  dep  features
2260 2260 2260 1922
Distribution of labels: 
Counter({'B1': 827, 'A2': 662, 'B2': 482, 'A1': 194, 'C1': 90, 'C2': 5})
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13151
13151
Fold 0
[[  5  30   4   0   0   0]
 [  7 104  22   0   0   0]
 [  1  35 121   9   0   0]
 [  0   5  23  68   1   0]
 [  0   0   2  16   0   0]
 [  0   0   0   1   0   0]]
              precision    recall  f1-score   support

          A1       0.38      0.13      0.19        39
          A2       0.60      0.78      0.68       133
          B1       0.70      0.73      0.72       166
          B2       0.72      0.70      0.71        97
          C1       0.00      0.00      0.00        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.66       454
   macro avg       0.40      0.39      0.38       454
weighted avg       0.62      0.66      0.63       454


Fold 1
[[ 11  25   3   0   0   0]
 [  6  98  28   1   0   0]
 [  1  24 125  16   0   0]
 [  0   2  32  63   0   0]
 [  0   0   1  17   0   0]
 [  0   0   0   1   0   0]]
              precision    recall  f1-score   support

          A1       0.61      0.28      0.39        39
          A2       0.66      0.74      0.70       133
          B1       0.66      0.75      0.70       166
          B2       0.64      0.65      0.65        97
          C1       0.00      0.00      0.00        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.65       454
   macro avg       0.43      0.40      0.41       454
weighted avg       0.62      0.65      0.63       454


Fold 2
[[  7  32   0   0   0   0]
 [ 10  99  23   0   0   0]
 [  0  32 121  12   0   0]
 [  0   1  21  74   0   0]
 [  0   0   1  17   0   0]
 [  0   0   0   1   0   0]]
              precision    recall  f1-score   support

          A1       0.41      0.18      0.25        39
          A2       0.60      0.75      0.67       132
          B1       0.73      0.73      0.73       165
          B2       0.71      0.77      0.74        96
          C1       0.00      0.00      0.00        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.67       451
   macro avg       0.41      0.41      0.40       451
weighted avg       0.63      0.67      0.64       451


Fold 3
[[  6  33   0   0   0   0]
 [  6  94  32   0   0   0]
 [  1  32 120  12   0   0]
 [  0   3  24  69   0   0]
 [  0   0   3  15   0   0]
 [  0   0   0   1   0   0]]
              precision    recall  f1-score   support

          A1       0.46      0.15      0.23        39
          A2       0.58      0.71      0.64       132
          B1       0.67      0.73      0.70       165
          B2       0.71      0.72      0.72        96
          C1       0.00      0.00      0.00        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.64       451
   macro avg       0.40      0.39      0.38       451
weighted avg       0.61      0.64      0.61       451


Fold 4
[[ 10  28   0   0   0   0]
 [  3 106  23   0   0   0]
 [  3  31 125   6   0   0]
 [  0   2  27  67   0   0]
 [  0   0   0  18   0   0]
 [  0   0   0   1   0   0]]
              precision    recall  f1-score   support

          A1       0.62      0.26      0.37        38
          A2       0.63      0.80      0.71       132
          B1       0.71      0.76      0.74       165
          B2       0.73      0.70      0.71        96
          C1       0.00      0.00      0.00        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.68       450
   macro avg       0.45      0.42      0.42       450
weighted avg       0.66      0.68      0.66       450


K-fold scores
[0.6289226498211674, 0.6323142718472058, 0.6423985330042364, 0.6145608190281723, 0.6609224636552878]
SKF f1 score mean 0.635823747471214

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13151
13151
Fold 0
[[ 6 29  4  0  0  0]
 [20 78 30  5  0  0]
 [ 5 40 99 20  2  0]
 [ 1  6 31 53  6  0]
 [ 0  0  3 11  4  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.19      0.15      0.17        39
          A2       0.51      0.59      0.55       133
          B1       0.59      0.60      0.59       166
          B2       0.59      0.55      0.57        97
          C1       0.33      0.22      0.27        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.53       454
   macro avg       0.37      0.35      0.36       454
weighted avg       0.52      0.53      0.52       454


Fold 1
[[11 23  4  1  0  0]
 [22 76 33  2  0  0]
 [ 0 42 94 28  2  0]
 [ 0  4 40 45  8  0]
 [ 0  0  3 14  1  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.33      0.28      0.31        39
          A2       0.52      0.57      0.55       133
          B1       0.54      0.57      0.55       166
          B2       0.49      0.46      0.48        97
          C1       0.09      0.06      0.07        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.50       454
   macro avg       0.33      0.32      0.33       454
weighted avg       0.49      0.50      0.49       454


Fold 2
[[14 21  4  0  0  0]
 [16 85 27  4  0  0]
 [ 6 43 97 18  1  0]
 [ 0  6 24 60  6  0]
 [ 0  1  1  9  7  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.39      0.36      0.37        39
          A2       0.54      0.64      0.59       132
          B1       0.63      0.59      0.61       165
          B2       0.65      0.62      0.64        96
          C1       0.50      0.39      0.44        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.58       451
   macro avg       0.45      0.43      0.44       451
weighted avg       0.58      0.58      0.58       451


Fold 3
[[  9  25   5   0   0   0]
 [ 17  73  39   2   0   1]
 [  7  42 103  13   0   0]
 [  1   8  33  49   5   0]
 [  0   0   6  11   1   0]
 [  0   0   1   0   0   0]]
              precision    recall  f1-score   support

          A1       0.26      0.23      0.25        39
          A2       0.49      0.55      0.52       132
          B1       0.55      0.62      0.59       165
          B2       0.65      0.51      0.57        96
          C1       0.17      0.06      0.08        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.52       451
   macro avg       0.35      0.33      0.33       451
weighted avg       0.51      0.52      0.51       451


Fold 4
[[12 26  0  0  0  0]
 [16 72 42  2  0  0]
 [ 7 37 97 22  2  0]
 [ 1  8 36 46  5  0]
 [ 0  0  3 11  4  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.33      0.32      0.32        38
          A2       0.50      0.55      0.52       132
          B1       0.54      0.59      0.57       165
          B2       0.56      0.48      0.52        96
          C1       0.36      0.22      0.28        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.51       450
   macro avg       0.38      0.36      0.37       450
weighted avg       0.51      0.51      0.51       450


K-fold scores
[0.5234001404475402, 0.49361646683772414, 0.5815712633477563, 0.513359320719257, 0.5096698538980964]
SKF f1 score mean 0.5243234090500748

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13151
13151
Fold 0
[[  8  29   2   0   0   0]
 [ 17  81  30   5   0   0]
 [  6  34 104  21   1   0]
 [  1   6  30  55   5   0]
 [  0   0   3  11   4   0]
 [  0   0   0   1   0   0]]
              precision    recall  f1-score   support

          A1       0.25      0.21      0.23        39
          A2       0.54      0.61      0.57       133
          B1       0.62      0.63      0.62       166
          B2       0.59      0.57      0.58        97
          C1       0.40      0.22      0.29        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.56       454
   macro avg       0.40      0.37      0.38       454
weighted avg       0.55      0.56      0.55       454


Fold 1
[[12 22  5  0  0  0]
 [19 79 32  3  0  0]
 [ 2 35 99 28  2  0]
 [ 0  3 36 51  7  0]
 [ 0  0  2 14  2  0]
 [ 0  0  0  1  0  0]]
              precision    recall  f1-score   support

          A1       0.36      0.31      0.33        39
          A2       0.57      0.59      0.58       133
          B1       0.57      0.60      0.58       166
          B2       0.53      0.53      0.53        97
          C1       0.18      0.11      0.14        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.54       454
   macro avg       0.37      0.36      0.36       454
weighted avg       0.53      0.54      0.53       454


Fold 2
[[ 12  23   4   0   0   0]
 [ 15  89  26   2   0   0]
 [  3  42 102  17   1   0]
 [  0   4  25  61   6   0]
 [  0   0   0  12   6   0]
 [  0   0   0   0   1   0]]
              precision    recall  f1-score   support

          A1       0.40      0.31      0.35        39
          A2       0.56      0.67      0.61       132
          B1       0.65      0.62      0.63       165
          B2       0.66      0.64      0.65        96
          C1       0.43      0.33      0.38        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.60       451
   macro avg       0.45      0.43      0.44       451
weighted avg       0.60      0.60      0.59       451


Fold 3
[[ 10  26   2   1   0   0]
 [ 16  72  41   3   0   0]
 [  7  34 107  17   0   0]
 [  1   3  29  58   5   0]
 [  0   0   4  12   2   0]
 [  0   0   0   0   1   0]]
              precision    recall  f1-score   support

          A1       0.29      0.26      0.27        39
          A2       0.53      0.55      0.54       132
          B1       0.58      0.65      0.61       165
          B2       0.64      0.60      0.62        96
          C1       0.25      0.11      0.15        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.55       451
   macro avg       0.38      0.36      0.37       451
weighted avg       0.54      0.55      0.54       451


Fold 4
[[ 12  25   1   0   0   0]
 [ 13  68  47   4   0   0]
 [  8  30 101  24   2   0]
 [  1   6  28  55   6   0]
 [  0   0   2  11   5   0]
 [  0   0   0   1   0   0]]
              precision    recall  f1-score   support

          A1       0.35      0.32      0.33        38
          A2       0.53      0.52      0.52       132
          B1       0.56      0.61      0.59       165
          B2       0.58      0.57      0.58        96
          C1       0.38      0.28      0.32        18
          C2       0.00      0.00      0.00         1

    accuracy                           0.54       450
   macro avg       0.40      0.38      0.39       450
weighted avg       0.53      0.54      0.53       450


K-fold scores
[0.5491022392265593, 0.5295389863373574, 0.5946074077108326, 0.5447039753712902, 0.5320716010530586]
SKF f1 score mean 0.5500048419398196

SAME LANG EVAL DONE FOR THIS LANG
Multi lingual classification for dimension Sociolinguisticappropriateness
Doing: take all data as if it belongs to one large dataset, and do classification
Mega classification for:  pos  features
2260 2260 2260 633
Distribution of labels: 
Counter({'B1': 734, 'A2': 700, 'B2': 386, 'A1': 371, 'C1': 69})
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13231
13231
Fold 0
[[ 48  26   1   0   0]
 [  3 106  29   2   0]
 [  2  44  83  18   0]
 [  0   5  13  60   0]
 [  0   0   0  14   0]]
              precision    recall  f1-score   support

          A1       0.91      0.64      0.75        75
          A2       0.59      0.76      0.66       140
          B1       0.66      0.56      0.61       147
          B2       0.64      0.77      0.70        78
          C1       0.00      0.00      0.00        14

    accuracy                           0.65       454
   macro avg       0.56      0.55      0.54       454
weighted avg       0.65      0.65      0.64       454


Fold 1
[[ 43  31   0   0   0]
 [  5 116  18   1   0]
 [  9  33  83  22   0]
 [  0   2  21  54   0]
 [  0   0   0  14   0]]
              precision    recall  f1-score   support

          A1       0.75      0.58      0.66        74
          A2       0.64      0.83      0.72       140
          B1       0.68      0.56      0.62       147
          B2       0.59      0.70      0.64        77
          C1       0.00      0.00      0.00        14

    accuracy                           0.65       452
   macro avg       0.53      0.54      0.53       452
weighted avg       0.64      0.65      0.64       452


Fold 2
[[ 57  16   1   0   0]
 [  5 104  31   0   0]
 [  3  35  93  16   0]
 [  0   2  19  56   0]
 [  0   0   0  14   0]]
              precision    recall  f1-score   support

          A1       0.88      0.77      0.82        74
          A2       0.66      0.74      0.70       140
          B1       0.65      0.63      0.64       147
          B2       0.65      0.73      0.69        77
          C1       0.00      0.00      0.00        14

    accuracy                           0.69       452
   macro avg       0.57      0.57      0.57       452
weighted avg       0.67      0.69      0.68       452


Fold 3
[[ 49  24   1   0   0]
 [  7 105  28   0   0]
 [  1  32 104  10   0]
 [  0   6  19  52   0]
 [  0   0   0  14   0]]
              precision    recall  f1-score   support

          A1       0.86      0.66      0.75        74
          A2       0.63      0.75      0.68       140
          B1       0.68      0.71      0.70       147
          B2       0.68      0.68      0.68        77
          C1       0.00      0.00      0.00        14

    accuracy                           0.69       452
   macro avg       0.57      0.56      0.56       452
weighted avg       0.67      0.69      0.68       452


Fold 4
[[ 51  23   0   0   0]
 [  2 110  27   1   0]
 [  3  35  95  13   0]
 [  2   2  13  60   0]
 [  0   0   1  12   0]]
              precision    recall  f1-score   support

          A1       0.88      0.69      0.77        74
          A2       0.65      0.79      0.71       140
          B1       0.70      0.65      0.67       146
          B2       0.70      0.78      0.74        77
          C1       0.00      0.00      0.00        13

    accuracy                           0.70       450
   macro avg       0.58      0.58      0.58       450
weighted avg       0.69      0.70      0.69       450


K-fold scores
[0.6443045797925315, 0.6408483905663434, 0.6761162032652058, 0.6763827207630994, 0.6924279284047865]
SKF f1 score mean 0.6660159645583933

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13231
13231
Fold 0
[[45 25  4  1  0]
 [17 88 28  6  1]
 [ 7 49 60 31  0]
 [ 4  4 15 53  2]
 [ 0  0  1 11  2]]
              precision    recall  f1-score   support

          A1       0.62      0.60      0.61        75
          A2       0.53      0.63      0.58       140
          B1       0.56      0.41      0.47       147
          B2       0.52      0.68      0.59        78
          C1       0.40      0.14      0.21        14

    accuracy                           0.55       454
   macro avg       0.52      0.49      0.49       454
weighted avg       0.55      0.55      0.54       454


Fold 1
[[48 23  1  2  0]
 [20 83 34  3  0]
 [ 4 53 70 20  0]
 [ 0  8 25 39  5]
 [ 0  0  2  9  3]]
              precision    recall  f1-score   support

          A1       0.67      0.65      0.66        74
          A2       0.50      0.59      0.54       140
          B1       0.53      0.48      0.50       147
          B2       0.53      0.51      0.52        77
          C1       0.38      0.21      0.27        14

    accuracy                           0.54       452
   macro avg       0.52      0.49      0.50       452
weighted avg       0.54      0.54      0.54       452


Fold 2
[[52 17  5  0  0]
 [20 83 35  2  0]
 [ 6 39 83 18  1]
 [ 0  7 26 41  3]
 [ 0  0  2 11  1]]
              precision    recall  f1-score   support

          A1       0.67      0.70      0.68        74
          A2       0.57      0.59      0.58       140
          B1       0.55      0.56      0.56       147
          B2       0.57      0.53      0.55        77
          C1       0.20      0.07      0.11        14

    accuracy                           0.58       452
   macro avg       0.51      0.49      0.50       452
weighted avg       0.57      0.58      0.57       452


Fold 3
[[51 14  7  2  0]
 [10 85 42  3  0]
 [ 4 42 84 17  0]
 [ 2  5 28 35  7]
 [ 0  0  2 11  1]]
              precision    recall  f1-score   support

          A1       0.76      0.69      0.72        74
          A2       0.58      0.61      0.59       140
          B1       0.52      0.57      0.54       147
          B2       0.51      0.45      0.48        77
          C1       0.12      0.07      0.09        14

    accuracy                           0.57       452
   macro avg       0.50      0.48      0.49       452
weighted avg       0.56      0.57      0.56       452


Fold 4
[[50 19  4  1  0]
 [18 79 38  5  0]
 [ 5 50 74 16  1]
 [ 2  3 31 34  7]
 [ 1  0  0  9  3]]
              precision    recall  f1-score   support

          A1       0.66      0.68      0.67        74
          A2       0.52      0.56      0.54       140
          B1       0.50      0.51      0.51       146
          B2       0.52      0.44      0.48        77
          C1       0.27      0.23      0.25        13

    accuracy                           0.53       450
   macro avg       0.50      0.48      0.49       450
weighted avg       0.53      0.53      0.53       450


K-fold scores
[0.5378593750390371, 0.5353528377386532, 0.5699684720864578, 0.563845918816954, 0.5315950194387954]
SKF f1 score mean 0.5477243246239796

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13231
13231
Fold 0
[[48 23  4  0  0]
 [14 86 35  3  2]
 [ 3 51 64 29  0]
 [ 3  4 14 55  2]
 [ 0  0  0 12  2]]
              precision    recall  f1-score   support

          A1       0.71      0.64      0.67        75
          A2       0.52      0.61      0.57       140
          B1       0.55      0.44      0.48       147
          B2       0.56      0.71      0.62        78
          C1       0.33      0.14      0.20        14

    accuracy                           0.56       454
   macro avg       0.53      0.51      0.51       454
weighted avg       0.56      0.56      0.56       454


Fold 1
[[49 23  1  1  0]
 [17 87 34  2  0]
 [ 5 53 70 18  1]
 [ 0  6 23 43  5]
 [ 0  0  2  9  3]]
              precision    recall  f1-score   support

          A1       0.69      0.66      0.68        74
          A2       0.51      0.62      0.56       140
          B1       0.54      0.48      0.51       147
          B2       0.59      0.56      0.57        77
          C1       0.33      0.21      0.26        14

    accuracy                           0.56       452
   macro avg       0.53      0.51      0.52       452
weighted avg       0.56      0.56      0.56       452


Fold 2
[[56 15  3  0  0]
 [11 93 34  2  0]
 [ 3 36 88 19  1]
 [ 0  5 23 45  4]
 [ 0  0  1 11  2]]
              precision    recall  f1-score   support

          A1       0.80      0.76      0.78        74
          A2       0.62      0.66      0.64       140
          B1       0.59      0.60      0.59       147
          B2       0.58      0.58      0.58        77
          C1       0.29      0.14      0.19        14

    accuracy                           0.63       452
   macro avg       0.58      0.55      0.56       452
weighted avg       0.62      0.63      0.63       452


Fold 3
[[52 15  5  2  0]
 [ 8 89 40  3  0]
 [ 2 38 87 20  0]
 [ 0  6 28 39  4]
 [ 0  0  1 10  3]]
              precision    recall  f1-score   support

          A1       0.84      0.70      0.76        74
          A2       0.60      0.64      0.62       140
          B1       0.54      0.59      0.56       147
          B2       0.53      0.51      0.52        77
          C1       0.43      0.21      0.29        14

    accuracy                           0.60       452
   macro avg       0.59      0.53      0.55       452
weighted avg       0.60      0.60      0.60       452


Fold 4
[[52 19  2  1  0]
 [12 84 39  5  0]
 [ 2 46 80 17  1]
 [ 1  3 27 38  8]
 [ 1  1  0  8  3]]
              precision    recall  f1-score   support

          A1       0.76      0.70      0.73        74
          A2       0.55      0.60      0.57       140
          B1       0.54      0.55      0.54       146
          B2       0.55      0.49      0.52        77
          C1       0.25      0.23      0.24        13

    accuracy                           0.57       450
   macro avg       0.53      0.51      0.52       450
weighted avg       0.57      0.57      0.57       450


K-fold scores
[0.5553028198443911, 0.5551849867706538, 0.6255120218053127, 0.5972042080850355, 0.5713959883000017]
SKF f1 score mean 0.580920004961079

SAME LANG EVAL DONE FOR THIS LANG
Doing: take all data as if it belongs to one large dataset, and do classification
Mega classification for:  dep  features
2260 2260 2260 1922
Distribution of labels: 
Counter({'B1': 734, 'A2': 700, 'B2': 386, 'A1': 371, 'C1': 69})
Printing results for: RandomForestClassifier(bootstrap=True, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=None, oob_score=False,
                       random_state=1234, verbose=0, warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13151
13151
Fold 0
[[50 24  1  0  0]
 [ 9 99 30  2  0]
 [ 2 51 70 24  0]
 [ 0  5 12 61  0]
 [ 0  0  0 14  0]]
              precision    recall  f1-score   support

          A1       0.82      0.67      0.74        75
          A2       0.55      0.71      0.62       140
          B1       0.62      0.48      0.54       147
          B2       0.60      0.78      0.68        78
          C1       0.00      0.00      0.00        14

    accuracy                           0.62       454
   macro avg       0.52      0.53      0.52       454
weighted avg       0.61      0.62      0.60       454


Fold 1
[[ 49  24   1   0   0]
 [ 10 108  20   2   0]
 [  8  44  72  23   0]
 [  0   5  18  54   0]
 [  0   0   0  14   0]]
              precision    recall  f1-score   support

          A1       0.73      0.66      0.70        74
          A2       0.60      0.77      0.67       140
          B1       0.65      0.49      0.56       147
          B2       0.58      0.70      0.64        77
          C1       0.00      0.00      0.00        14

    accuracy                           0.63       452
   macro avg       0.51      0.52      0.51       452
weighted avg       0.61      0.63      0.61       452


Fold 2
[[ 57  16   1   0   0]
 [  5 100  35   0   0]
 [  3  41  83  20   0]
 [  0   3  18  56   0]
 [  0   0   0  14   0]]
              precision    recall  f1-score   support

          A1       0.88      0.77      0.82        74
          A2       0.62      0.71      0.67       140
          B1       0.61      0.56      0.58       147
          B2       0.62      0.73      0.67        77
          C1       0.00      0.00      0.00        14

    accuracy                           0.65       452
   macro avg       0.55      0.56      0.55       452
weighted avg       0.64      0.65      0.65       452


Fold 3
[[ 50  24   0   0   0]
 [ 10 109  20   1   0]
 [  4  36  96  11   0]
 [  0   5  17  55   0]
 [  0   0   0  14   0]]
              precision    recall  f1-score   support

          A1       0.78      0.68      0.72        74
          A2       0.63      0.78      0.69       140
          B1       0.72      0.65      0.69       147
          B2       0.68      0.71      0.70        77
          C1       0.00      0.00      0.00        14

    accuracy                           0.69       452
   macro avg       0.56      0.56      0.56       452
weighted avg       0.67      0.69      0.68       452


Fold 4
[[ 51  23   0   0   0]
 [  7 108  22   3   0]
 [  4  47  80  15   0]
 [  3   2  12  60   0]
 [  0   0   0  13   0]]
              precision    recall  f1-score   support

          A1       0.78      0.69      0.73        74
          A2       0.60      0.77      0.68       140
          B1       0.70      0.55      0.62       146
          B2       0.66      0.78      0.71        77
          C1       0.00      0.00      0.00        13

    accuracy                           0.66       450
   macro avg       0.55      0.56      0.55       450
weighted avg       0.66      0.66      0.65       450


K-fold scores
[0.6043160085345767, 0.6119521908779672, 0.6451045013041805, 0.6752837070172683, 0.6525518047100781]
SKF f1 score mean 0.6378416424888141

Printing results for: LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
          verbose=0)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13151
13151
Fold 0
[[46 24  4  1  0]
 [18 77 41  4  0]
 [ 3 47 78 19  0]
 [ 0  8 21 41  8]
 [ 0  0  3 11  0]]
              precision    recall  f1-score   support

          A1       0.69      0.61      0.65        75
          A2       0.49      0.55      0.52       140
          B1       0.53      0.53      0.53       147
          B2       0.54      0.53      0.53        78
          C1       0.00      0.00      0.00        14

    accuracy                           0.53       454
   macro avg       0.45      0.44      0.45       454
weighted avg       0.53      0.53      0.53       454


Fold 1
[[50 19  3  2  0]
 [24 83 29  4  0]
 [ 4 40 79 23  1]
 [ 2  7 25 38  5]
 [ 0  0  3  8  3]]
              precision    recall  f1-score   support

          A1       0.62      0.68      0.65        74
          A2       0.56      0.59      0.57       140
          B1       0.57      0.54      0.55       147
          B2       0.51      0.49      0.50        77
          C1       0.33      0.21      0.26        14

    accuracy                           0.56       452
   macro avg       0.52      0.50      0.51       452
weighted avg       0.56      0.56      0.56       452


Fold 2
[[57 10  6  1  0]
 [17 84 35  4  0]
 [ 6 39 84 18  0]
 [ 0  4 33 37  3]
 [ 0  0  4 10  0]]
              precision    recall  f1-score   support

          A1       0.71      0.77      0.74        74
          A2       0.61      0.60      0.61       140
          B1       0.52      0.57      0.54       147
          B2       0.53      0.48      0.50        77
          C1       0.00      0.00      0.00        14

    accuracy                           0.58       452
   macro avg       0.47      0.48      0.48       452
weighted avg       0.57      0.58      0.57       452


Fold 3
[[49 20  5  0  0]
 [12 92 33  3  0]
 [ 1 42 90 14  0]
 [ 3 10 22 40  2]
 [ 0  1  2 10  1]]
              precision    recall  f1-score   support

          A1       0.75      0.66      0.71        74
          A2       0.56      0.66      0.60       140
          B1       0.59      0.61      0.60       147
          B2       0.60      0.52      0.56        77
          C1       0.33      0.07      0.12        14

    accuracy                           0.60       452
   macro avg       0.57      0.50      0.52       452
weighted avg       0.60      0.60      0.60       452


Fold 4
[[49 21  4  0  0]
 [14 86 37  3  0]
 [ 7 45 78 15  1]
 [ 0  5 26 43  3]
 [ 0  1  0 11  1]]
              precision    recall  f1-score   support

          A1       0.70      0.66      0.68        74
          A2       0.54      0.61      0.58       140
          B1       0.54      0.53      0.54       146
          B2       0.60      0.56      0.58        77
          C1       0.20      0.08      0.11        13

    accuracy                           0.57       450
   macro avg       0.52      0.49      0.50       450
weighted avg       0.57      0.57      0.57       450


K-fold scores
[0.5307529838459912, 0.5571441086023817, 0.5716221303775039, 0.5963528719974455, 0.5673820420895999]
SKF f1 score mean 0.5646508273825844

Printing results for: LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',
                   random_state=1234, solver='warn', tol=0.0001, verbose=0,
                   warm_start=False)CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                lowercase=True, max_df=1.0, max_features=None, min_df=10,
                ngram_range=(1, 5), preprocessor=None, stop_words=None,
                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                tokenizer=None, vocabulary=None)
13151
13151
Fold 0
[[47 25  2  1  0]
 [15 81 38  6  0]
 [ 2 44 82 19  0]
 [ 0  7 19 45  7]
 [ 0  0  1 12  1]]
              precision    recall  f1-score   support

          A1       0.73      0.63      0.68        75
          A2       0.52      0.58      0.55       140
          B1       0.58      0.56      0.57       147
          B2       0.54      0.58      0.56        78
          C1       0.12      0.07      0.09        14

    accuracy                           0.56       454
   macro avg       0.50      0.48      0.49       454
weighted avg       0.56      0.56      0.56       454


Fold 1
[[49 20  4  1  0]
 [19 87 33  1  0]
 [ 3 38 81 24  1]
 [ 1  8 21 38  9]
 [ 0  0  2  9  3]]
              precision    recall  f1-score   support

          A1       0.68      0.66      0.67        74
          A2       0.57      0.62      0.59       140
          B1       0.57      0.55      0.56       147
          B2       0.52      0.49      0.51        77
          C1       0.23      0.21      0.22        14

    accuracy                           0.57       452
   macro avg       0.51      0.51      0.51       452
weighted avg       0.57      0.57      0.57       452


Fold 2
[[59  9  5  1  0]
 [11 89 38  2  0]
 [ 3 37 89 18  0]
 [ 0  2 34 38  3]
 [ 0  0  2 12  0]]
              precision    recall  f1-score   support

          A1       0.81      0.80      0.80        74
          A2       0.65      0.64      0.64       140
          B1       0.53      0.61      0.57       147
          B2       0.54      0.49      0.51        77
          C1       0.00      0.00      0.00        14

    accuracy                           0.61       452
   macro avg       0.50      0.51      0.50       452
weighted avg       0.60      0.61      0.60       452


Fold 3
[[51 19  4  0  0]
 [ 9 98 31  2  0]
 [ 0 39 90 17  1]
 [ 0  8 22 44  3]
 [ 0  0  1 12  1]]
              precision    recall  f1-score   support

          A1       0.85      0.69      0.76        74
          A2       0.60      0.70      0.64       140
          B1       0.61      0.61      0.61       147
          B2       0.59      0.57      0.58        77
          C1       0.20      0.07      0.11        14

    accuracy                           0.63       452
   macro avg       0.57      0.53      0.54       452
weighted avg       0.63      0.63      0.62       452


Fold 4
[[50 22  2  0  0]
 [13 84 40  3  0]
 [ 5 46 76 18  1]
 [ 0  3 26 44  4]
 [ 0  0  0 12  1]]
              precision    recall  f1-score   support

          A1       0.74      0.68      0.70        74
          A2       0.54      0.60      0.57       140
          B1       0.53      0.52      0.52       146
          B2       0.57      0.57      0.57        77
          C1       0.17      0.08      0.11        13

    accuracy                           0.57       450
   macro avg       0.51      0.49      0.49       450
weighted avg       0.56      0.57      0.56       450


K-fold scores
[0.5625043120924058, 0.5699626306676352, 0.6017089970539654, 0.6246439445506121, 0.5638534413318359]
SKF f1 score mean 0.584534665139291

SAME LANG EVAL DONE FOR THIS LANG
